Abstract
In this work, we study the cross-view information fusion problem in the task of self-supervised 3D hand pose estima-tion from the depth image. Previous methods usually adopt a hand-crafted rule to generate pseudo labels from multi-view estimations in order to supervise the network training in each view. However, these methods ignore the rich se-mantic information in each view and ignore the complex de-pendencies between different regions of different views. To solve these problems, we propose a cross-view fusion net-work to fully exploit and adaptively aggregate multi-view information. We encode diverse semantic information in each view into multiple compact nodes. Then, we introduce the graph convolution to model the complex dependencies between nodes and perform cross-view information inter-action. Based on the cross-view fusion network, we propose a strong self-supervised framework for 3D hand pose and hand mesh estimation. Furthermore, we propose a pseudo multi-view training strategy to extend our framework to a more general scenario in which only single-view training data is used. Results on NYU dataset demonstrate that our method outperforms the previous self-supervised methods by 17.5% and 30.3% in multi-view and single-view scenar-ios. Meanwhile, our framework achieves comparable re-sults to several strongly supervised methods. 1.

Introduction 3D hand pose estimation plays an essential role in human-computer interaction, virtual reality, and augmented reality. With the development of deep learning and the in-crease of the amount of labeled data, depth-based 3D hand pose estimation has made significant progress [3, 12, 13, 20, 31, 32, 49, 54, 59]. However, acquiring large-scale hand datasets with 3D hand pose and mesh annotations is time-consuming and labor-consuming. Meanwhile, even after
*Corresponding author careful design and manual correction, the annotation qual-ity of the existing automatic or semi-automatic annotation algorithms is difficult to guarantee [33, 57, 64].
Recently, some methods [8, 53, 55] achieve accurate 3D hand pose estimation through self-supervised learning.
These methods introduce a 3D hand model into the neural network and optimize the network by penalizing the dif-ferences between the hand model and the input depth im-age. As mentioned in [53, 55], adopting multi-view in-formation during training is the key to the success of the self-supervised methods. Complementary multi-view infor-mation can alleviate the uncertainty of estimation caused by self-occlusion or holes. The previous methods adopt a hand-crafted rule, e.g., taking median value, to aggregate the estimated poses from multiple views as pseudo labels in order to supervise the network training in each view.
However, the hand-crafted rule only considers the co-ordinate information of the joint itself in multiple views.
This method ignores the rich semantic information in vi-sual features in each view and ignores the complex depen-dencies between different hand regions in different views.
Thus, this method is susceptible to interference from the low-quality estimations that frequently occurs in a self-supervised framework. To better exploit multi-view in-formation, some multi-view human pose estimation meth-ods propose to perform pixel-wise cross-view interaction in 2D feature space by establishing point-to-point correspon-dence [19, 29, 38, 58, 67]. However, performing pixel-by-pixel matching based on feature similarity is computation-ally expensive and redundant. Meanwhile, these methods are sensitive to self-occlusion and holes [19]. For exam-ple, when some hand regions in one view are occluded or missing, the local features of these regions are difficult to be detected and matched robustly in other views.
To solve these problems, we propose a cross-view fusion network to fully exploit multi-view information to generate more accurate and robust pseudo labels. Specifically, we first encode high-dimensional visual features and the esti-mated hand pose in each view into multiple semantic nodes.
Then, we adopt a hierarchical graph convolutional network to perform intra-view and cross-view information interac-tion according to the hand bone structure and the cross-view joint correspondence. Furthermore, when construct-ing the graph nodes, we adopt a group-wise confidence en-coding strategy to prevent high-quality features from being corrupted by low-quality features in information passing.
Our method can fully mine the rich semantic information in each view and efficiently model the dependencies between different views. Meanwhile, performing cross-view infor-mation interaction according to the intrinsic hand structure avoids complex pixel-by-pixel matching and reduces the in-terference of self-occlusion and depth holes.
Based on the cross-view fusion network, we propose a strong self-supervised framework for 3D hand pose and mesh estimation. Considering that the multi-view setup may increase the difficulty of data collection and limits the application scenarios of our method, we further extend our framework to a more general scenario in which only single-view training data is used. By treating different augmented samples of the same input data as different view images, we propose a pseudo multi-view training strategy for the single-view scenario. Unlike previous methods [62,66] that perform self-supervised learning by maintaining the predic-tive consistency between different augmented samples, our method aggregates the multiple pseudo views information to generate more accurate estimations, which can more ef-fectively guide network training in each view.
We conduct experiments on three 3D hand pose esti-mation datasets (NYU [52], ICVL [49], and MSRA [47]).
On NYU dataset, our method improves the state-of-the-art (SOTA) self-supervised methods by 17.5% in the multi-view scenario and by 30.3% in the single-view scenario.
Meanwhile, our method achieves comparable results to strongly supervised methods. Qualitative experiments on
ICVL and MSRA datasets show that our method generates more accurate hand poses than the annotations. We evalu-ate our method in real-world scenarios and the results also verify the effectiveness of our model. Code is available at https://github.com/RenFeiTemp/MMI.
Our contributions can be summarized as follows:
• We propose a cross-view fusion network to fully mine the rich semantic information in each view and efficiently model the dependencies between different views.
• We propose a strong self-supervised framework for depth-based 3D hand pose and mesh estimation. Further-more, we extend our framework to a more general scenario in which only single-view training data is used.
• Our method outperforms existing self-supervised methods by a large margin and achieves comparable results to several strongly supervised approaches. In addition, our method can yield more accurate hand poses than the anno-tations of some existing datasets. 2.