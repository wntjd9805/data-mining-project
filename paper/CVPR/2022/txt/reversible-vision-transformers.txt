Abstract
We present Reversible Vision Transformers, a memory efficient architecture design for visual recognition. By de-coupling the GPU memory footprint from the depth of the model, Reversible Vision Transformers enable memory ef-ficient scaling of transformer architectures. We adapt two popular models, namely Vision Transformer and Multiscale
Vision Transformers, to reversible variants and benchmark extensively across both model sizes and tasks of image clas-sification, object detection and video classification. Re-versible Vision Transformers achieve a reduced memory footprint of up to 15.5× at identical model complexity, pa-rameters and accuracy, demonstrating the promise of re-versible vision transformers as an efficient backbone for re-source limited training regimes. Finally, we find that the ad-ditional computational burden of recomputing activations is more than overcome for deeper models, where through-put can increase up to 3.9× over their non-reversible coun-terparts. Code and models are available at https:// github.com/facebookresearch/mvit. 1.

Introduction
The deep learning revolution in computer vision has rested on the bedrock of high performance hardware ac-celerators. Fueled by special purpose AI accelerators, the compute requirements for state-of-the-art models are grow-ing exponentially. However, compute is only half the story.
The other, and often overlooked half, is memory bandwidth bottleneck, which has been difficult to proportionally scale as compared to peak accelerator FLOPs [51]. In particular, the peak accelerator FLOPs have been increasing at a rate of ∼3.1× every 2 years. However, peak bandwidth only scales at a rate of ∼1.4× every 2 years. This disparity is exacerbated in transformers, which have been doubling in required compute roughly every three months for the past three years, resulting in a so-called memory wall [21] where both the overall model performance as well as the training speed have become tightly memory-bound [33].
As such, for bandwidth bound models, trading compute for memory through re-computation could actually be more efficient than using work-optimal algorithms [67,68]. In the
Figure 1. Reversible Vision Transformers are more memory-efficient, yet powerful reversible counterparts of state-of-the-art Vision Transformer (ViT) [15] and Multiscale Vision Trans-former (MViT) [18] architectures with varying model complex-ity. Numbers in parentheses denote top-1 ImageNet performance.
ResNet [27] and RegNet [55] are only shown for reference. For detailed discussion please refer to §4.1. case of training neural network models, this can be achieved by re-computing activations instead of storing and then loading them from DRAM [30]. Besides training speed, scaling vision transformers in depth naturally hits the GPU memory capacity, especially in memory starved regimes such as video recognition where state-of-the-art models are often limited to batch size 1 due to high memory footprint of intermediate activations.
In this work, we propose Reversible Vision Transform-ers, a family of expressive visual recognition architectures with very favorable activation memory footprints (Figure 1) compared to their non-reversible variants. By trading-off
GPU activation caching with efficient on-the-fly activation re-computation, reversible vision transformers effectively decouple the activation memory growth from the depth of the model. While the natural language processing com-munity has performed some early exploration of reversible transformers for machine translation [37], these techniques focus on longer sequence lengths rather than depth.
Our experiments show that a straightforward adaptation of vision transformers to reversible architectures fails to scale for deeper models because of training convergence in-stabilities due to internal sub-block residual connections.
In this work, we reconfigure the residual paths in Vision
Transformers (ViT) [15] and Multiscale Vision Transform-ers (MViT) [18] to overcome this issue. We further find that reversible structures have stronger inherent regulariza-tion and therefore, we use a lighter augmentation recipe (re-peated augmentation, augmentation magnitude and stochas-tic depth) and lateral connections between residual blocks.
We benchmark extensively across image recognition tasks such as image classification and object detection as well as video classification, across all of which, re-versible vision transformers have competitive performance to their non-reversible counterparts suffering negligible to no performance decay. Moreover, reversible models have extremely favorable per-image memory footprint, saving 15.5× on the ViT-Large model and 2.3× on the MViT-Base model with reversible training.
In summary, our contributions are three-fold. (i) We propose Reversible Vision Transformer (Rev-ViT) and Reversible Multiscale Vision Transformers (Rev-MViT), memory efficient reversible adaptations of state-of-the-art visual recognition backbones. (ii) We observe reversible transformers to have a stronger inherent regularization than vanilla networks. Hence, we develop new training recipes by adapting the original recipes with different repeated augmentations, augmenta-tion magnitudes and drop path rate to match the perfor-mance of their non-reversible counterparts. (iii) We benchmark our models across several tasks: im-age classification, object detection and action recognition, across accuracy, memory, maximum training batch size and model complexity. In particular, at matched complex-ity (FLOPs/parameters) and final accuracy specifications,
Rev-ViT-B and Rev-ViT-L train with per image memory footprints that are 7.6× and 15.5× lighter than ViT-B and
ViT-L respectively. Further, we show how deep reversible networks can achieve up to 3.9× higher throughput than their non-reversible counterparts. 2.