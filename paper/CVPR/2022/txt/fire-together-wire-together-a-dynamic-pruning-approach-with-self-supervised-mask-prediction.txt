Abstract
Dynamic model pruning is a recent direction that al-lows for the inference of a different sub-network for each input sample during deployment. However, current dy-namic methods rely on learning a continuous channel gat-ing through regularization by inducing sparsity loss. This formulation introduces complexity in balancing different losses (e.g task loss, regularization loss). In addition, reg-ularization based methods lack transparent tradeoff hyper-parameter selection to realize a computational budget. Our contribution is two-fold: 1) decoupled task and pruning losses. 2) Simple hyperparameter selection that enables
FLOPs reduction estimation before training.
Inspired by the Hebbian theory in Neuroscience: “neurons that fire to-gether wire together”, we propose to predict a mask to pro-cess k filters in a layer based on the activation of its pre-vious layer. We pose the problem as a self-supervised bi-nary classification problem. Each mask predictor module is trained to predict if the log-likelihood for each filter in the current layer belongs to the top-k activated filters. The value k is dynamically estimated for each input based on a novel criterion using the mass of heatmaps. We show ex-periments on several neural architectures, such as VGG,
ResNet and MobileNet on CIFAR and ImageNet datasets.
On CIFAR, we reach similar accuracy to SOTA methods with 15% and 24% higher FLOPs reduction. Similarly in
ImageNet, we achieve lower drop in accuracy with up to 13% improvement in FLOPs reduction. 1.

Introduction
Convolutional Neural Networks (CNNs) showed un-precedented growth over the past decade which represented the state-of-the-art in many fields. However, CNNs require substantially large computation and memory consumption which limits deployment on edge and embedded platforms.
There are many advances in model compression research
Figure 1. FLOPs reduction vs accuracy drop from baselines for various dynamic and static models on ResNet34 ImageNet. including manually designed lightweight models [16, 17], low-bit precision [23, 47], architecture search [4, 39], and model pruning [9,21,29,34]. Most of the compression tech-niques are agnostic to the input data and optimize for a static efficient model. Recent efforts in pruning literature propose to keep the backbone for the baseline model as a whole and do inference using different sub-networks conditioned on the input. This is known as dynamic pruning, where dif-ferent routes are activated based on the input which allows higher degree of freedom and more flexibility in compari-son to static pruning.
Current dynamic pruning approaches typically introduce a regularization term to induce sparsity over a continuous parameter for channel gating/masking [6, 10, 40]. Others adopt policy gradient introduced in reinforcement learn-ing [42] to learn different routes. These methods require careful tuning in training to tackle issues such as training stability with schedule annealing [40], biased training han-dling [18], or predefined pruning ratio per layer [10, 25].
Also, as noted in [6], additional sparsity loss degrades task loss as it is difficult to balance the task loss and the pruning
(a) Last convolutional layer (b) 8th depthwise convolutional layer
Figure 2. Maximum activations in all features at the last convolutional layer and a middle layer in mobilenetv1 CIFAR-10. Each row in a subplot represents an input sample. Samples that belong to the same class activate same group of filters. Better visualized in color. loss especially under high pruning ratio as shown in Fig-ure 1. Moreover, the FLOPs reduction of these dynamic methods is dependent on the target sparsity preset hyper-parameter. This hyperparameter selection lacks transparent relation between sparsity hyperparameter and the reached
FLOPs; thus, hinders practical efficient training with many iterations of trial and error to achieve a target FLOPs reduc-tion.
In this paper, we tackle these issues by formulating the problem as a self-supervised binary classification task. We generate the binary mask of the current layer (wiring) based on the activation (firing) of the previous layer. We draw inspiration from the Hebbian theory [31] in Neuroscience with a twist that we enforce this wiring-firing relation in-stead of a study of causation as in the theory. Figure 2 plots the maximum response for each filter (x-axis) of the last convolutional layer and a middle layer of MobileNet-V1 for random input samples (y-axis) grouped by their class.
The plot shows that samples that belong to the same class tend to activate the same combination of filters and thus we only need to process a handful of the filters. It is worth not-ing that the number of clusters vary per layer. Similar to other dynamic pruning methods, we learn a decision head for channel gating. However, we learn the gating using bi-nary cross entropy loss per channel. Each layer predicts the filters which are most likely to be highly activated given the layer’s input activations. We generate ground truth bi-nary masks per layer based on the mass of the heatmap per sample. This formulation provides advantages in two as-pects. First, the channel gating loss implicitly complies and adapts to the backbone’s status which stabilizes training in comparison to the case with sparsity regularization or RL-based training. Second, reduction in FLOPs can be esti-mated before training, as the target mask is controlled by the generated ground truth mask which gives an estimate on the reduction. This simplifies the hyperparameter selec-tion that controls pruning ratio. The main contributions are summarized as follows:
• A novel loss formulation with self-supervised ground truth mask generation that is stochastic gradient de-scent (SGD) friendly with no gradient weighting tricks.
• We propose a novel dynamic signature based on the heatmap mass without a pre-defined pruning ratio per layer.
• Simple hyperparameter selection that enables FLOPs reduction estimation before training. This simplifies realizing a prior budget target with bounded hyperpa-rameter search space. 2.