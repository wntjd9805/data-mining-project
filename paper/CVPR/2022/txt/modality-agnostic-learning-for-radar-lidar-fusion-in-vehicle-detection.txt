Abstract
Fusion of multiple sensor modalities such as camera, Li-dar, and Radar, which are commonly found on autonomous vehicles, not only allows for accurate detection but also ro-bustifies perception against adverse weather conditions and individual sensor failures. Due to inherent sensor charac-teristics, Radar performs well under extreme weather con-ditions (snow, rain, fog) that significantly degrade camera and Lidar. Recently, a few works have developed vehicle de-tection methods fusing Lidar and Radar signals, i.e., MVD-Net. However, these models are typically developed under the assumption that the models always have access to two error-free sensor streams. If one of the sensors is unavail-able or missing, the model may fail catastrophically. To mitigate this problem, we propose the Self-Training Mul-timodal Vehicle Detection Network (ST-MVDNet) which leverages a Teacher-Student mutual learning framework and a simulated sensor noise model used in strong data aug-mentation for Lidar and Radar. We show that by (1) enforc-ing output consistency between a Teacher network and a
Student network and by (2) introducing missing modalities (strong augmentations) during training, our learned model breaks away from the error-free sensor assumption. This consistency enforcement enables the Student model to han-dle missing data properly and improve the Teacher model by updating it with the Student model’s exponential mov-ing average. Our experiments demonstrate that our pro-posed learning framework for multi-modal detection is able to better handle missing sensor data during inference. Fur-thermore, our method achieves new state-of-the-art perfor-mance (5% gain) on the Oxford Radar Robotcar dataset under various evaluation settings. 1.

Introduction
In autonomous driving, many vehicles are equipped with multiple sensors, such as camera, Lidar, and Radar, as demonstrated in many datasets [2, 4, 9, 31]. Leveraging dif-ferent types of sensors can be used to tackle any occasional failures for each sensor and can potentially improve the per-Figure 1. Illustration of the problem caused by noisy or missing sensor stream. Models trained on two modalities may suffer from errors when inferenced on missing sensors (only one modality is available). formance of object detections than using each individual sensor. Existing works [6,13,23,37] mainly focus on fusing
Lidar and camera, taking advantage of the camera’s higher resolution and rich texture information. However, these vi-sual sensors are sensitive to adverse weather conditions and suffer from degraded performance in harsh weather such as fog [3], snow, and rain.
In addition to Lidar and camera, Radar has also been widely adopted in autonomous system of vehicles [2, 4] and is more robust in certain weather conditions (e.g., fog, snow, rain). To be particular, Radar uses wavelength in the scale of millimeter as ADC chirp signals which is much larger than the size of rain, fog, or even snow [10], mak-ing them essentially invisible to Radar. Since the collected
Radar data in existing autonomous driving datasets [4] fea-ture sparse and low resolution data (compared to camera and Lidar), the recent Oxford Radar Robotcar [1] (ORR) dataset, whose Radar sensor has high directionality and much finer spatial resolution, has emerged as a new bench-mark for Radar and Lidar fusion. Recently, MVDNet [26] was proposed to fuse Lidar and Radar sensors and achieves state-of-the-art results on the ORR dataset. MVDNet is shown to be successful in adverse conditions such as foggy weather, largely due to the advantageous features of Radar.
However, existing Lidar-Radar fusion models [2, 26] are all developed under the assumption that the models will always have access to two reliable sensor streams.
If one of the sensors is unavailable or corrupted, performance may suffer (Figure 1). In other words, current fusion models may not be applicable to real-world applications where such failures can occur.
To address this issue, one solution can be to train sep-arate models for processing a number of different sensor streams as input. However, this may be prohibitively expen-sive. To avoid this, another potential solution is to directly train the fusion model with both clear and missing streams and optimize the model with ground-truth labels. However, such strong data augmentations causes the model to rely on one clear steam and ignore the stream that is missing, which
In other words, a model is reflected in our experiments. naively trained with randomly missing sensor streams fails to effectively fuse the two features of the two sensors.
In order to properly leverage data augmentation and mitigate the effect of sensor noise, we propose a frame-work named Self-Training Multi-modal Vehicle Detection
Network (ST-MVDNet) which leverages the backbone of
MVDNet [26] and builds upon the self-training pipeline of
Mean Teacher (MT) [32] framework. MT was originally proposed for semi-supervised learning, learning two mod-els in parallel, where the Teacher model is used to stabilize the performance of the Student model. To leverage MT to regularize training in our fusion model with strong augmen-tations (missing streams), our proposed ST-MVDNet also employs two models (Teacher and Student pair), each being architecturally equivalent to MVDNet. The Teacher gener-ates the predictions to train the Student using a consistency constraint while the Student passes the parameters it has learned back to the Teacher via exponential moving aver-age (EMA). The Teacher model only takes clear modalities as input while the Student model additionally takes either missing Lidar or Radar streams as input. We show that by enforcing consistency between the Teacher and the Student, our model is able to prevent a bias towards (over-reliance on) the clear sensor during the training with missing modal-ities. This pipeline not only allows the model to be more robust to missing sensors but also improves multi-modality feature extraction by forcing the model to better interpret the similarities and relationship between the two modalities.
The contributions can be summarized as follows:
• We demonstrate the limitations of a multi-modal detec-tion network when one of its sensors is missing during inference.
• We propose a framework building on Mean Teacher and leverage strong augmentations to address the issue of missing sensor.
• Our developed pipeline is not only able to deal with noisy/missing sensors, which is supported by our de-signed experiments, but is also able to outperform ex-isting state-of-the-art by a large margin (5%) on ORR dataset in several experimental settings. 2.