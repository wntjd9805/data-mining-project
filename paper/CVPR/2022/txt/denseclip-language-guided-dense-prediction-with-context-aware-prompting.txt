Abstract
Recent progress has shown that large-scale pre-training using contrastive image-text pairs can be a promising alter-native for high-quality visual representation learning from natural language supervision. Benefiting from a broader source of supervision, this new paradigm exhibits impres-sive transferability to downstream classification tasks and datasets. However, the problem of transferring the knowl-edge learned from image-text pairs to more complex dense prediction tasks has barely been visited. In this work, we present a new framework for dense prediction by implicitly and explicitly leveraging the pre-trained knowledge from
CLIP. Specifically, we convert the original image-text match-ing problem in CLIP to a pixel-text matching problem and use the pixel-text score maps to guide the learning of dense prediction models. By further using the contextual infor-mation from the image to prompt the language model, we are able to facilitate our model to better exploit the pre-trained knowledge. Our method is model-agnostic, which can be applied to arbitrary dense prediction systems and various pre-trained visual backbones including both CLIP models and ImageNet pre-trained models. Extensive ex-periments demonstrate the superior performance of our methods on semantic segmentation, object detection, and instance segmentation tasks. Code is available at https:
//github.com/raoyongming/DenseCLIP. 1.

Introduction
The “pre-training + fine-tuning” paradigm is recognized as one of the key discoveries that has largely pushed the state-of-the-art for various downstream computer vision tasks, including image classification [12, 22, 23], object de-tection [14, 35], semantic segmentation [6, 30], and action recognition [4]. Due to the high annotation and computa-tion cost of the per-pixel prediction, pre-training is even more critical for dense prediction tasks. As illustrated in Fig-ure 1 (a), the pre-training step is usually accomplished via
*Equal contribution.
†Corresponding author.
Figure 1. Comparisons of the conventional “pre-training + fine-tuning” paradigm and our proposed DenseCLIP. The pre-training + fine-tuning paradigm directly applies the image pre-trained model as the initialization of encoder. Differently, DenseCLIP transfers the knowledge learned with image-text contrastive learning to dense prediction models by introducing a new pixel-text matching task and further using the contextual information from images to prompt pre-trained language model. supervised classification or self-supervised learning of the backbone model on large-scale datasets like ImageNet [11].
Then, a task-specific module like a detector or a segmenta-tion decoder is added to the backbone and the whole model is fine-tuned on the target dataset with less training data [6, 35]. supervised and self-supervised pre-training methods only based on images, Con-trastive Language-Image Pre-training (CLIP) [33] is a new framework to learn high-quality visual representation by exploring contrastive learning with large-scale noisy image-from conventional
Different
Our results show that the CLIP models can outperform the conventional ImageNet pre-trained models with some modifi-cations on hyper-parameters (see the CLIP result in Figure 2).
But the straightforward way cannot fully exploit the poten-tial of the CLIP models. Inspired by the original contrastive learning framework in CLIP, we propose to convert the orig-inal image-text matching problem in CLIP to a pixel-text matching problem and use the pixel-text score maps to guide the learning of dense prediction models explicitly. By further using the contextual information from the image to prompt the language model with a Transformer [40] module, we are able to facilitate our model to better exploit the pre-trained knowledge by optimizing the text embeddings.
Our method can be a plug-and-play module to improve the fine-tuning of CLIP pre-trained models on off-the-shelf dense prediction methods and tasks. By applying our method to the popular semantic segmentation framework semantic
FPN [21] on the challenging ADE20K [59] dataset, we ex-hibit +4.9%, +4.7% and +2.3% mIoU improvement com-pared over ImageNet pre-trained models and +3.9%, +2.4% and +1.2% mIoU improvement compared to vanilla fine-tuning of a CLIP models based on ResNet-50, ResNet-101 [18] and ViT-B [12] respectively. We also observe significant improvements in object detection and instance segmentation tasks. Notably, we show a ResNet-101 model equipped with our method and a lightweight semantic FPN decoder can achieve 46.5% mIoU on ADE20K, which out-performs state-of-the-art solutions like DeepLabV3+ [7] and
UperNet [45] with only 1/3 computation.
Moreover, our framework can also be applied to any backbone models by using the pre-trained language model to guide the training of dense prediction tasks. We observe sig-nificant improvements by applying DenseCLIP to ImageNet pre-trained ResNets [18] and recent Swin Transformers [29] with slight computation overhead. We expect our method to be a new and generic paradigm to improve dense prediction models with guidance from pre-trained language models. 2.