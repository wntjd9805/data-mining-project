Abstract
Video events grounding aims at retrieving the most rele-vant moments from an untrimmed video in terms of a given natural language query. Most previous works focus on
Video Sentence Grounding (VSG), which localizes the mo-ment with a sentence query. Recently, researchers extended this task to Video Paragraph Grounding (VPG) by retrieving multiple events with a paragraph. However, we ﬁnd the ex-isting VPG methods may not perform well on context mod-eling and highly rely on video-paragraph annotations. To tackle this problem, we propose a novel VPG method termed
Semi-supervised Video-Paragraph TRansformer (SVPTR), which can more effectively exploit contextual information in paragraphs and signiﬁcantly reduce the dependency on annotated data. Our SVPTR method consists of two key components: (1) a base model VPTR that learns the video-paragraph alignment with contrastive encoders and tackles the lack of sentence-level contextual interactions and (2) a semi-supervised learning framework with multimodal fea-ture perturbations that reduces the requirements of anno-tated training data. We evaluate our model on three widely-used video grounding datasets, i.e., ActivityNet-Caption,
Charades-CD-OOD, and TACoS. The experimental results show that our SVPTR method establishes the new state-of-the-art performance on all datasets. Even under the condi-tions of fewer annotations, it can also achieve competitive results compared with recent VPG methods. 1.

Introduction
Localizing events in a given untrimmed video is one of the challenging video understanding tasks, which is
ﬁrst proposed by [1, 7]. Following their works, a list of promising methods
[16, 20, 45–47, 50] has been pro-posed. However, most existing methods focus on Video
Sentence Grounding (VSG), addressing this problem in
“single-multi” approaches (as shown in Fig. 1(a)), they ground a moment from a video that consists of several dif-*Corresponding author.
Figure 1. An illustrative example of VSG and VPG: (a) VSG aims at retrieving a particular moment with a single sentence. (b) VPG receives a paragraph consisting of multiple sentences as a query and localizes multiple events in the untrimmed video. ferent events according to an individual sentence query.
Contrastively, as illustrated in Fig. 1(b), Video Paragraph
Grounding (VPG), which is recently proposed by [2], ad-dresses the video events grounding task in the “multi-multi” manner. Speciﬁcally, in the VPG task, given a paragraph describing multiple events instead of a single sentence, it is expected to localize all of the related moments in an untrimmed video. Since a paragraph consisting of multiple sentences in time order contains more temporal information compared with the single sentence input, it is more infor-mative for retrieving moments in videos.
The previous VPG methods [2, 6, 50] ﬁrst generate pro-posals for each sentence, then learn temporal order and se-mantic relations among these proposals to select desired candidates. Nevertheless, these methods exist three prob-lems. Firstly, they rely on the temporal information of para-graphs but hardly exploit the contextual information well from the perspective of text modality. For example, shown in Fig. 1(b), all the sentences in the paragraph are describ-ing two girls cooking in the kitchen and each of the sen-tences is related to others contextually around the cook-ing topic. Recently, the informal work [30] tried to tackle
VPG with Transformer [34], which proved the global con-texts worked in this task. However, these methods includ-ing [30] still fall into the second defect: with the paragraph input, they only focus on the proposal-sentence matching but ignore the video-paragraph matching, which may lead to misalignment on cross-modal fusion. Lastly, compared with moment-sentence annotation, the video-paragraph an-notated data are more expensive and hard to generate. All these VPG methods are required to be trained with tem-poral labeled data, which brings heavy costs to this task.
Although there are also some weakly-supervised video grounding methods [5, 19, 43], most of them are “single-multi” methods and the performance is much worse than fully-supervised methods.
To tackle these problems, we ﬁrst propose a novel base model termed Video-Paragraph TRansformer (VPTR), which introduces contrastive learning and semi-supervised learning into VPG. We further extend it to the semi-supervised version, the Semi-supervised Video-Paragraph
TRansformer (SVPTR), to reduce the dependency on tem-poral annotations. Speciﬁcally, as the general framework of our proposed SVPTR method shown in Fig. 2, to explore the contextual information hidden in paragraphs, we ex-tract hierarchical text features and design a sentence-based query mechanism in the decoder. The individual sentence queries interact with particular words and other sentences with such designs thus we can extract more contextual infor-mation. Moreover, to avoid misalignment between proposal moments and sentences, contrastive learning is introduced into the multimodal encoder to guide the cross-modal fu-sion at the video-paragraph level. As is shown in Fig. 2, the contrastive encoder separately encodes the two modalities and projects them into a common space via self-supervised learning. Finally, we develop an advanced semi-supervised learning VPG method SVPTR that is based on the teacher-student framework, which effectively reduces the consump-tion of video-paragraph temporal annotations.
The primary contributions in this work are as follows:
• We explore contextual information in the paragraph query with hierarchical text features and the sentence-based query mechanism.
It effectively improves the precision of localizing events in untrimmed videos.
• We combine self-supervised learning to optimize the cross-modal fusion in video paragraph grounding. Par-ticularly, we design a contrastive loss at the video-paragraph level without proposing moment candidates.
• We design a semi-supervised learning framework for
VPG and achieve promising results with less annotated data. To the best of our knowledge, we are the ﬁrst to explore the semi-supervised learning on video para-graph grounding.
To evaluate the proposed SVPTR method, we con-duct extensive experiments on three widely-used datasets:
ActivityNet-Caption [12], Charades-CD-OOD [44], and
TACoS [27]. The comprehensive results demonstrate the superiority of our SVPTR method compared with a hand-ful of state-of-the-art VPG approaches under both fully-supervised and semi-supervised settings. 2.