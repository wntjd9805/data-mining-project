Abstract
We introduce FocalPose, a neural render-and-compare method for jointly estimating the camera-object 6D pose and camera focal length given a single RGB input image depicting a known object. The contributions of this work are twofold. First, we derive a focal length update rule that extends an existing state-of-the-art render-and-compare 6D pose estimator to address the joint estimation task. Second, we investigate several different loss functions for jointly es-timating the object pose and focal length. We ﬁnd that a combination of direct focal length regression with a re-projection loss disentangling the contribution of transla-tion, rotation, and focal length leads to improved results.
We show results on three challenging benchmark datasets that depict known 3D models in uncontrolled settings. We demonstrate that our focal length and 6D pose estimates have lower error than the existing state-of-the-art methods. 1.

Introduction
The projection of a 3D object into an image depends not only on the object’s relative pose to the camera, but also on the camera’s intrinsic parameters. While it is possible to capture objects in a controlled environment where the cam-era’s intrinsic parameters are known (e.g., a calibrated cam-era on a robot), for many “in-the-wild” images we do not have control over the capture process and these parameters are unknown, e.g., Internet pictures or archival photographs.
Given an input image, we seek to retrieve a 3D model of a depicted object from a model library and estimate the rel-ative camera-object 6D pose jointly with the camera’s focal length (depicted in Figure 1). This problem has its origins in the early days of computer vision [26,27,36] and has im-portant modern-day applications in augmented reality and computer graphics, such as applying in situ object overlays or editing the position of an object via 3D compositing in uncontrolled consumer-captured images.
The problem of 6D object pose estimation in an uncal-Figure 1. Given a single input photograph (left) and a known 3D model, our approach accurately estimates the 6D camera-object pose together with the focal length of the camera (right), here shown by overlaying the aligned 3D model over the input image.
Our approach handles a large range of focal lengths and the result-ing perspective effects. ibrated setting is, by its nature, challenging. First, it is difﬁcult to distinguish subtle changes of the camera’s fo-cal length from changes in an object’s depth. Second, in-cluding the camera’s focal length increases the number of parameters that must be estimated and hence increases the optimization complexity. Finally, “in-the-wild” consumer-captured images may depict large appearance variation for a particular object instance in the model library. Variation may be due to differences in illumination and the depicted object having slightly different, non-identical shapes or sur-face appearance in different real-world instance captures.
For example, consider different instances of the same car model that have a similar overall shape but may have dif-ferent color, wear and tear, or customizable features (e.g., additional headlights, alloy wheels, or a spoiler).
Previous approaches for this task primarily rely on es-tablishing local 2D-3D correspondences between an image
and a 3D model using either hand-crafted [2, 3, 7, 8, 17, 27] or CNN features [12, 19, 20, 31, 32, 34, 35, 38, 41, 42, 47, 48], followed by robust camera pose estimation using PnP [23].
These approaches often fail on scenes with large texture-less areas where local correspondences cannot be reliably established.
In contrast, the recent best-performing 6D object pose estimation methods are based on the render-and-compare strategy [22, 24, 28, 30, 48], which performs a dense alignment over all pixels of rendered views of the 3D model to its depiction in the input image. However, all prior render-and-compare methods fall short of handling the aforementioned desired uncontrolled, uncalibrated setting as they assume a controlled environment where the cam-era intrinsic parameters are ﬁxed and known a priori. Also, these prior methods typically operate over only a handful of known objects.
To address these challenges, we build on the strengths of render and compare and extend it to handle our desired un-controlled, uncalibrated setting. We introduce FocalPose, a novel render-and-compare approach for jointly estimat-ing an object’s 6D pose and camera focal length based on a monocular image input. Our contributions are twofold.
First, we extend a recent state-of-the-art [18] method for 6D pose estimation (CosyPose [22]) by deriving and inte-grating focal length update rules in a differentiable manner, which allows our method to overcome the added complex-ity of including the focal length. Second, we investigate several different loss functions for jointly estimating object pose and focal length. We ﬁnd that a combination of di-rect focal length regression with a reprojection loss disen-tangling the contribution of translation, rotation, and focal length leads to the best performance and allows our method to distinguish subtle differences due to the focal length and the object’s depth. We apply our method to three real-world consumer-captured image datasets with varying camera fo-cal lengths and show that our focal length and 6D pose esti-mates have lower error compared to the state of the art. As an added beneﬁt, our work is the ﬁrst render-and-compare method applied to a large collection of 3D meshes (20-200 meshes for Pix3D [39], 150 for the car datasets [45]).
⇠ 2.