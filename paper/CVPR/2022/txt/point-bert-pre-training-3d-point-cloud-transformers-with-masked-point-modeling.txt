Abstract
We present Point-BERT, a new paradigm for learning
Transformers to generalize the concept of BERT [8] to 3D point cloud. Inspired by BERT, we devise a Masked Point
Modeling (MPM) task to pre-train point cloud Transform-ers. Specifically, we first divide a point cloud into several local point patches, and a point cloud Tokenizer with a dis-crete Variational AutoEncoder (dVAE) is designed to gen-erate discrete point tokens containing meaningful local in-formation. Then, we randomly mask out some patches of input point clouds and feed them into the backbone Trans-formers. The pre-training objective is to recover the orig-inal point tokens at the masked locations under the super-vision of point tokens obtained by the Tokenizer. Extensive experiments demonstrate that the proposed BERT-style pre-training strategy significantly improves the performance of standard point cloud Transformers. Equipped with our pre-training strategy, we show that a pure Transformer archi-tecture attains 93.8% accuracy on ModelNet40 and 83.1% accuracy on the hardest setting of ScanObjectNN, surpass-ing carefully designed point cloud models with much fewer hand-made designs. We also demonstrate that the rep-resentations learned by Point-BERT transfer well to new tasks and domains, where our models largely advance the state-of-the-art of few-shot point cloud classification task.
The code and pre-trained models are available at https:
//github.com/lulutang0608/Point-BERT. 1.

Introduction
Compared to conventional hand-crafted feature extrac-tion methods, Convolutional Neural Networks (CNN) [20] is dependent on much less prior knowledge. Transform-ers [49] have pushed this trend further as a step towards no inductive bias with minimal man-made assumptions, such as translation equivalence or locality in CNNs. Recently, the structural superiority and versatility of standard Trans-formers are proved in both language [3, 8, 18, 25, 34] and
*Equal contribution. †Corresponding author.
Figure 1. Illustration of our main idea. Point-BERT is designed for pre-training of standard point cloud Transformers. By training a dVAE via point cloud reconstruction, we can convert a point cloud into a sequence of discrete point tokens. Then we are able to pre-train the Transformers with a Mask Point Modeling (MPM) task by predicting the masked tokens. image tasks [2, 6, 9, 45, 55, 66], and the capability of dimin-ishing the inductive biases is also justified by enabling more parameters, more data [9], and longer training schedules.
While Transformers produce astounding results in Natural
Language Processing (NLP) and image processing, it is not well studied in the 3D community. Existing Transformer-based point cloud models [11,63] bring in certain inevitable inductive biases from local feature aggregation [63] and neighbor embedding [11], making them deviate from the mainstream of standard Transformers. To this end, we aim to apply standard Transformers on point cloud directly with minimal inductive bias, as a stepping stone to a neat and unified model for 3D representation learning.
Apparently, the straightforward adoption of Transform-ers does not achieve satisfactory performance on point cloud tasks (see Figure 5). This discouraging result is par-tially attributed to the limited annotated 3D data since pure
Transformers with no inductive bias need massive training data. For example, ViT [9] uses ImageNet [20] (14M im-ages) and JFT [41] (303M images) to train vision Trans-In contrast, accurate annotated point clouds are formers. relatively insufficient. Despite the 3D data acquisition is getting easy with the recent proliferation of modern scan-ning devices, labeling point clouds is still time-consuming, error-prone, and even infeasible in some extreme real-world scenarios. The difficulty motivates a flux of research into learning from unlabelled 3D data. Self-supervised pre-Figure 2. Masked point clouds reconstruction using our Point-BERT model trained on ShapeNet. We show the reconstruction results of synthetic objects from ShapeNet test set with block masking and random masking in the first two groups respectively. Our model also generalize well to unseen real scans from ScanObjectNN (the last two groups). training thereby becomes a viable technique to unleash the scalability and generalization of Transformers for 3D point cloud representation learning.
Among all the Transformer-based pre-training models,
BERT [8] achieved state-of-the-art performance at its re-leased time, setting a milestone in the NLP community. In-spired by BERT [8], we seek to exploit the BERT-style pre-training for 3D point cloud understanding. However, it is challenging to directly employ BERT on point clouds due to a lack of pre-existing vocabulary. In contrast, the language vocabulary has been well-defined (e.g., WordPiece in [8]) and off-the-shelf for model pre-training. In terms of point cloud Transformers, there is no pre-defined vocabulary for point clouds. A naive idea is to treat every point as a ‘word’ and mimic BERT [8] to predict the coordinates of masked points. Such a point-wise regression task surges computa-tional cost quadratically as the number of tokens increases.
Moreover, a word in a sentence contains basic contextual semantic information, while a single point in a point cloud barely entails semantic meaning.
Nevertheless, a local patch partitioned from a holistic point cloud contains plentiful geometric information and can be treated as a component unit. What if we build a vocabulary where different tokens represent different geo-metric patterns of the input units? At this point, we can represent a point cloud as a sequence of such tokens. Now, we can favorably adopt BERT and its efficient implementa-tions almost out of the box. We hypothesize that bridging this gap is a key to extending the successful Transformers and BERT to the 3D vision domain.
Driven by the above analysis, we present Point-BERT, a new scheme for learning point cloud Transformers. Two essential components are conceived: 1) Point Tokenization:
A point cloud Tokenizer is devised via a dVAE-based [37] point cloud reconstruction, where a point cloud can be con-verted into discrete point tokens according to the learned vocabulary. We expect that point tokens should imply lo-cal geometric patterns, and the learned vocabulary should cover diverse geometric patterns, such that a sequence of such tokens can represent any point cloud (even never seen before). 2) Masked Point Modeling: A ‘masked point mod-eling’ (MPM) task is performed to pre-train Transformers, which masks a portion of input point cloud and learns to reconstruct the missing point tokens at the masked regions.
We hope that our model enables reasoning the geometric re-lations among different patches of the point cloud, capturing meaningful geometric features for point cloud understand-ing.
Both two designs are implemented and justified in our experiments. We visualize the reconstruction results both on the synthetic (ShapeNet [5]) and real-world (ScanOb-jectNN [47]) datasets in Figure 2. We observe that Point-BERT correctly predicts the masked tokens and infers di-verse, holistic reconstructions through our dVAE decoder.
The results suggest that the proposed model has learned in-herent and generic knowledge of 3D point clouds, i.e, geo-metric patterns or semantics. More significantly, our model is trained on ShapeNet, the masked point predictions on
ScanObjectNN reflect its superior performance on challeng-ing scenarios with both unseen objects and domain gaps.
Our Point-BERT with a pure Transformer architecture and BERT-style pre-training technique achieves 93.8% ac-curacy on ModelNet40 and 83.1% accuracy on the com-plicated setting of ScanObjectNN, surpassing carefully de-signed point cloud models with much fewer human pri-ors. We also show that the representations learned by
Point-BERT transfer well to new tasks and domains, where our models largely advance the state-of-the-art of few-shot point cloud classification task. We hope a neat and unified
Transformer architecture across images and point clouds could facilitate both domains since it enables joint modeling of 2D and 3D visual signals. 2.