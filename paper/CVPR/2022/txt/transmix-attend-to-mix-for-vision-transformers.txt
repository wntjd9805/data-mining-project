Abstract
Mixup-based augmentation has been found to be effec-tive for generalizing models during training, especially for
Vision Transformers (ViTs) since they can easily overfit.
However, previous mixup-based methods have an underly-ing prior knowledge that the linearly interpolated ratio of targets should be kept the same as the ratio proposed in in-put interpolation. This may lead to a strange phenomenon that sometimes there is no valid object in the mixed image due to the random process in augmentation but there is still response in the label space. To bridge such gap between the input and label spaces, we propose TransMix, which mixes labels based on the attention maps of Vision Transformers.
The confidence of the label will be larger if the correspond-ing input image is weighted higher by the attention map.
TransMix is embarrassingly simple and can be implemented in just a few lines of code without introducing any extra pa-rameters and FLOPs to ViT-based models. Experimental results show that our method can consistently improve var-ious ViT-based models at scales on ImageNet classification.
After pre-trained with TransMix on ImageNet, the ViT-based models also demonstrate better transferability to semantic segmentation, object detection and instance segmentation.
TransMix also exhibits to be more robust when evaluating on 4 different benchmarks. Code is publicly available at https://github.com/Beckschen/TransMix. 1.

Introduction
Transformers [42] have been dominant in nearly all tasks transformer-in natural language processing. Recently, based architectures like Vision Transformer (ViT) [12] have been introduced into the field of computer vision and show great promise on tasks like image classification [12, 13, 30, 40], object detection [48, 30, 15] and image segmenta-tion [48, 30, 37]. However, recent works have found that
ViT-based networks are hard to optimize and can easily overfit if the training data is not sufficient. A quick solu-*These authors contributed equally to this work.
Correspondence to Jie-Neng Chen (jienengchen01@gmail.com) and
Shuyang Sun (kevinsun@robots.ox.ac.uk)
Figure 1. Mixup [54] and CutMix [53] samples λ (proportion of label yA) randomly from a Beta distribution, while our TransMix calculates λ with the sum of the values within the attention map that intersects with A (denoted as AttnA, rendered in blue). tion to this problem is to apply data augmentation and reg-ularization techniques during training. Among them, the mixup-based methods like Mixup [54] and CutMix [53] are proven to be particularly helpful for generalizing the ViT-based network [39].
Mixup takes a pair of inputs xA, xB and their corre-sponding labels yA, yB, then creates an artificial training example λxA + (1 − λ)xB with λyA + (1 − λ)yB as its ground truth. Here λ ∈ [0, 1] is the random mixing pro-portion sampled from a Beta distribution. This pre-assumes that linear interpolations of feature vectors should lead to linear interpolations of the associated targets.
However, we argue that the above pre-assumption does not always stay true since not all pixels are created equal.
As shown in Figure 1, pixels in the background will not con-tribute to the label space as equally as those in the salient area. Some existing works [45, 41, 28] also find this prob-lem and solve it by means of only mixing the most descrip-tive parts on the input level. Nevertheless, manipulating on inputs with the above methods may narrow the space of augmentation since they tend to less consider to put the background image into the mixture. Meanwhile, the above methods cost more number of parameters and/or training throughput to extract the salient region of input. For exam-ple, Puzzle-Mix [28] requires model to forward and back-ward twice in an iteration and Attentive-Cutmix [45] intro-duce a 24M external CNN to extract salient features.
Instead of investigating how to better mix images on the input level, in this paper, we focus more on how to mild the 1
2.