Abstract
We present a colorization network that generates flat-color icons according to given sketches and semantic col-orization styles. Our network contains a style-structure dis-entangled colorization module and a normalizing flow. The colorization module transforms a paired sketch image and style image into a flat-color icon. To enhance network gen-eralization and the quality of icons, we present a pixel-wise decoder, a global style code, and a contour loss to reduce color gradients at flat regions and increase color disconti-nuity at boundaries. The normalizing flow maps Gaussian vectors to diverse style codes conditioned on the given se-mantic colorization label. This conditional sampling en-ables users to control attributes and obtain diverse col-orization results. Compared to previous methods built upon conditional generative adversarial networks, our approach enjoys the advantages of both high image quality and di-versity. To evaluate its effectiveness, we compared the flat-color icons generated by our approach and recent coloriza-tion and image-to-image translation methods on various conditions. Experiment results verify that our method out-performs state-of-the-arts qualitatively and quantitatively. 1.

Introduction
Image colorization aims to generate color images based on grayscale references, such as monochrome photos and line arts. Most methods developed for colorizing monochrome photos are fully automatic because pixels with various intensities contain fruitful semantics. The networks can recognize objects and assign proper colors when col-orizing images. Line arts, however, contain little semantics due to sparse structure lines. Moreover, colors in a line art may not have ground truths, and in some cases, they are not necessarily meaningful. Accordingly, previous line arts col-orization methods require users to provide reference images or color hits to guide the generated results.
Icons and comics are two types of graphic designs that are widely used in communication. A step of creating them is colorization. To save designers’ workload, methods take sketch images as inputs, which contain only black and white pixels for representing objects and backgrounds, and deter-mine the color of each pixel. The methods strive to en-hance color harmony and vividness and prevent colors from spreading the boundaries of adjacent objects. Although the colorization of icons and comics share several similarities, they are two different designs – structure lines are present in comics, but they are absent in icons. The structure of an icon appears because of color discontinuity. Hence, coloriz-ing icons is challenging because methods have to consider where and what colors to assign and whether the change of colors exhibits clear and correct structure lines.
Training a conditional generative adversarial network (c-GAN) is a way to generate flat-color icons. Sun et al. [35] trained a c-GAN with two discriminators, which evaluate the structure and style of icons created by the generator.
Although their generated results are visually appealing, the images frequently contain gradient colors and fail to present small features. In addition, it is known that c-GANs suffer from the diversity problem because the generator often de-generates into a deterministic function.
Inspired by StyleFlow [1], where specifying attributes to control a generator would degrade image qualities, we dis-card the framework of c-GAN. Instead, we train an encoder-decoder network to map paired sketch images and style im-ages to flat-color icons using supervised learning. A pixel-wise decoder, a global style code, and a contour loss were introduced to help the network disentangle style and struc-ture features, reduce color gradients at flat regions, and in-crease color discontinuities at boundaries. We also train a continuous normalizing flow [10] to sample diverse style codes conditioned on the given semantic style label [20].
Figure 2 shows the styles that users can choose when using our system. We concatenate the sampled style codes to the structure embedding and generate colorization results.
We apply our network to generate flat-color icons con-ditioned on a variety of black-and-white sketch images and semantic colorization styles. Figures 5, 6, and 7, and our accompanying video show the results. To evaluate its ef-fectiveness, we compare the flat-color icons generated by our approach and recent colorization and image-to-image translation methods. Experiment results demonstrate that our network outperforms current state-of-the-arts both qual-itatively and quantitatively. 2.