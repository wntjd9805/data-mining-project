Abstract
Self-supervised methods have significantly closed the gap with end-to-end supervised learning for image classifica-tion [13, 24]. In the case of human action videos, however, where both appearance and motion are significant factors of variation, this gap remains significant [28, 58]. One of the key reasons for this is that sampling pairs of similar video clips, a required step for many self-supervised contrastive learning methods, is currently done conservatively to avoid false positives. A typical assumption is that similar clips only occur temporally close within a single video, leading to insufficient examples of motion similarity. To mitigate this, we propose SLIC, a clustering-based self-supervised contrastive learning method for human action videos. Our key contribution is that we improve upon the traditional intra-video positive sampling by using iterative clustering to group similar video instances. This enables our method to leverage pseudo-labels from the cluster assignments to sample harder positives and negatives. SLIC outperforms state-of-the-art video retrieval baselines by +15.4% on top-1 recall on UCF101 and by +5.7% when directly transferred to HMDB51. With end-to-end finetuning for action classi-fication, SLIC achieves 83.2% top-1 accuracy (+0.8%) on
UCF101 and 54.5% on HMDB51 (+1.6%). SLIC is also competitive with the state-of-the-art in action classification after self-supervised pretraining on Kinetics400. 1.

Introduction
Self-supervision tasks have emerged as effective pretrain-ing methods for image classification, retrieval, and other downstream tasks. They have also been shown to outper-form end-to-end supervised learning in a number of set-tings [13, 34]. A key assumption in many self-supervised methods is the ability to do instance discrimination, by sam-pling or generating similar and dissimilar data, given a query.
While this is a reasonable assumption for training image rep-*The two authors contributed equally to this paper.
Figure 1. Nearest neighbor retrieval results of SLIC after pre-training on UCF101. The leftmost column is the query video from the UCF101 test set. On the right are the top 5 nearest neighbours from the training set in UCF101. Incorrect retrieval results are highlighted in red. resentations, for example by generating similar data through augmentations and dissimilar data by random sampling, it becomes much more challenging in the case of video repre-sentations as we need to account for motion-based as well as appearance-based similarities.
Different clips might have dissimilar appearances and the same motion (e.g. running at different locations), or simi-lar appearances but different motions (e.g. playing cricket or golf with a green field as the background). In instance discrimination we sample positive video instances conserva-tively, typically by assuming that they only occur temporally close within a single video, and sample negatives randomly from different videos. Even though some methods attempted to improve upon instance discrimination by sampling harder positives using additional views, self-supervised pretrain-ing of video representations is still not as effective as fully supervised pretraining (when evaluating on downstream clas-sification using only visual inputs) [28, 52, 58].
To mitigate this, we propose SLIC, a self-supervised learn-ing method for videos. SLIC alternates between periodically clustering video representations to produce pseudo-labels, and using those pseudo-labels to inform the sampling of pos-itive and negative pairs to update the video representations, by minimizing a triplet margin loss. SLIC also combines iterative clustering with multi-view encoding and a tempo-ral discrimination loss to learn view-invariant embeddings and fine-grained motion features, in order to distinguish the additional aspect of similarity that arises from the temporal dimension. Figure 2 shows an overview of our method.
Our main contributions are twofold. First, we show that iterative clustering significantly improves upon traditional instance discrimination in self-supervised learning for video representation. While this has already been established for image representations [8, 9], it has not been examined care-fully for videos. Our method is the first to leverage efficient iterative clustering for video representation, specifically for sampling harder positives and negatives for contrastive learn-ing. Second, we integrate iterative clustering with multi-view encoding and a temporal discrimination loss to sample harder positives and negatives during pretraining. We demonstrate that the interaction of these components, which has not been carefully examined by previous methods, is beneficial.
Our experiments show that SLIC achieves state-of-the art results on video retrieval (+15.4% improvement on top-1 recall on UCF101 and +5.7% on HMDB51 respectively, as shown in Table 1), and action classification when pre-trained on UCF101. When finetuning end-to-end for action classification, we observe significant gains from pretraining instead of using a random initialization of weights (about
+24% on top-1 accuracy on both UCF101 and HMDB51, as shown in Table 2). We demonstrate through additional experiments that all three components (i.e. iterative cluster-ing, multi-view encoding, and temporal discrimination loss) complement each other, and result in larger performance improvements when combined. We evaluate the individual contribution from each component in the ablation studies, and identify that iterative clustering and multi-view encoding are the main contributing factors in SLIC (shown in Table 3). 2.