Abstract
We introduce DoubleField, a novel framework combin-ing the merits of both surface ﬁeld and radiance ﬁeld for high-ﬁdelity human reconstruction and rendering. Within
DoubleField, the surface ﬁeld and radiance ﬁeld are as-sociated together by a shared feature embedding and a surface-guided sampling strategy. Moreover, a view-to-view transformer is introduced to fuse multi-view features and learn view-dependent features directly from high-resolution inputs. With the modeling power of DoubleField and the view-to-view transformer, our method signiﬁcantly im-proves the reconstruction quality of both geometry and ap-pearance, while supporting direct inference, scene-speciﬁc high-resolution ﬁnetuning, and fast rendering. The efﬁcacy of DoubleField is validated by the quantitative evaluations on several datasets and the qualitative results in a real-world sparse multi-view system, showing its superior ca-pability for high-quality human model reconstruction and photo-realistic free-viewpoint human rendering. Data and source code will be made public for the research purpose. 1.

Introduction
The surface ﬁelds [2, 31, 35] and the radiance ﬁelds [32, 63] have recently emerged as promising solutions for ge-ometry modeling [12, 39, 40, 66] and texture rendering [37, 59, 65] of 3D human in an implicit and continuous man-ner, respectively. However, their limitations become appar-ent when considering simultaneous geometry and appear-ance reconstruction, not to say under sparse multi-view set-tings. Speciﬁcally, the surface ﬁelds [12, 39, 64, 67] sep-arate the geometry learning from appearance learning and thus block the joint ﬁnetuning ability for more detailed geometry and rendering results. Moreover, the radiance
ﬁelds [21, 32, 36, 37, 44] entangle the learning of geome-try and appearance in an implicit manner without effective mutual constraints, leading to inconsistent geometry recon-struction and relatively low training efﬁciency. Despite the representations, the feature fusion strategy also dominates the ﬁnal reconstruction quality when deploying the algo-Figure 1. Given sparse multi-view RGB images, our method achieves high-ﬁdelity human reconstruction and rendering. rithms under multi-view setups, especially in the real-world systems. Even with high-resolution images as input, the limited representation power of features (feature map or fea-ture volume) [37, 40] as well as the calibration and the ge-ometry inference errors (especially for real captured data) will signiﬁcantly deteriorate the detail reconstruction per-formance due to multi-view inconsistency for current im-plicit ﬁeld based methods [37, 39, 64].
To overcome the limitations above for achieving high-quality 3D human reconstruction from sparse-view setups, we propose a novel DoubleField framework (to effectively bridge the surface and radiance ﬁelds and enable a shared learning space for both geometry and radiance reconstruc-tion) and a view-to-view transformer (to build self attention between multi-view inputs and cross attention between the input views and the query viewpoints for multi-view fea-ture fusion). Speciﬁcally, for DoubleField, we build asso-ciations between the surface and radiance ﬁelds by using a feature embedding shared by these ﬁelds in the network ar-chitecture and a surface-guided sampling strategy. Such a shared learning space allows the surface and radiance ﬁelds be beneﬁted from each other. On the one hand, the surface
ﬁeld imposes a geometry constraint to the radiance ﬁeld and encourages a more consistent density distribution for neural rendering. On the other hand, the radiance ﬁeld enables more geometry details in the surface ﬁeld via differentiable rendering. Moreover, the surface-guided sampling strategy disentangles the geometry component from the appearance modeling, so that DoubleField has a faster learning process while improving the reconstruction and rendering perfor-mances.
When deploying DoubleField with multi-view inputs, we propose a view-to-view transformer to build a self at-tention between multi-view inputs, and more importantly a cross attention between the input views and the query view-points. We achieve this by adopting an encoder-decoder architecture in our view-to-view transformer. Speciﬁcally, the encoder aims to fuse multi-view features while the de-coder aims to produce view-dependent features based on the learned attention between the query view and all input views. Thanks to the attention learning ability of the trans-former, the multi-view inconsistency issue is alleviated in our method, as the attention in the transformer handles the relationships between the input and the query views and is more robust to the geometry inference and calibration errors in real-world multi-view setups. Besides, the view-to-view transformer also enables our method to utilize the original high-resolution images. By taking the raw RGB values into accounts, the view-to-view transformer can directly learn the view-dependent features from high-resolution images and contribute to high-ﬁdelity rendering performances.
In comparison with existing approaches [37, 39, 64] built upon surface and radiance ﬁelds, DoubleField not only im-proves the reconstruction quality of both geometry and ap-pearance but also has the capability to eliminate the pre-requisite SMPL ﬁtting in previous methods [37] and even handle loose clothing (e.g., long dress) . More importantly, beneﬁting from the ability to leverage large dataset, Dou-bleField can fully utilize the priors in the large scale human scan dataset and achieve direct inference and fast ﬁnetun-ing for high-resolution free viewpoint rendering. In sum-mary, Our contributions in this work are: 1) a Double-Field framework (a shared double embedding and a surface-guided sampling strategy) to combine the merits of both surface and radiance ﬁelds for sparse multiview human re-construction and rendering; 2) a view-to-view transformer to fully utilize ultra-high-resolution image inputs in an efﬁ-cient manner; 3) our method achieve state-of-the-art perfor-mance on both geometry reconstruction and texture render-ing of human performances using sparse-view inputs. 2.