Abstract
Temporal modeling is an essential element in video un-derstanding. While deep convolution-based architectures have been successful at solving large-scale video recog-nition datasets, recent work has pointed out that they are biased towards modeling short-range relations, often fail-ing to capture long-term temporal structures in the videos, leading to poor transfer and generalization to new datasets.
In this work, the problem of dynamic representation learn-ing (DRL) is studied. We propose dynamic score, a mea-sure of video dynamic modeling that describes the addi-tional amount of information learned by a video network that cannot be captured by pure spatial student through knowledge distillation. DRL is then formulated as an ad-versarial learning problem between the video and spatial models, with the objective of maximizing the dynamic score of learned spatiotemporal classifier. The quality of learned video representations are evaluated on a diverse set of transfer learning problems concerning many-shot and few-shot action classification. Experimental results show that models learned with DRL outperform baselines in dynamic modeling, demonstrating higher transferability and gener-alization capacity to novel domains and tasks. 1.

Introduction
Following the success of deep learning for image recog-nition [38, 48, 71, 75], convolutional neural networks have also gained popularity for video classification problems, such as action recognition [13, 25, 55, 70, 79, 84, 88], where they outperform other classification architectures, such as recurrent networks [20, 41, 89]. However, current action recognition performance is significantly below the levels of image recognition. This is, in part, due to the difficulty of learning video representations that generalize well. Part of this difficulty stems from current data collection practices, namely the use of action datasets collected from the web (e.g. YouTube) [10, 35, 45, 46, 59]. While these datasets have much larger size and diversity of action classes, per-formers and scenes than those collected in controlled set-tings [6, 66, 86], they are also known to exhibit various types of biases that hinder the generalization of trained video models to unseen domains [17, 53, 54, 67]. One of the most prevalent among these biases is the spatial bias due to the spurious correlation between action labels and the spa-tial appearance of video frames, in the form of background objects, scenes or human pose [53]. For example, the pres-ence of a horse in the video is enough to infer the “horse-back riding” action if that is the only action class in the dataset that involves horses. Spatial bias creates shortcuts that allow classifiers to infer action labels without model-ing the temporal video component, known as video dynam-ics, leading to overoptimistic performance in popular action recognition benchmarks [54].
One of the nefarious consequences of dataset bias is that it favours certain representations over others [53]. In this context, the spatial bias of most video datasets is likely re-sponsible for the dominant performance of convolutional ar-chitectures, known to favor local over long-range dependen-cies, in the action recognition field. This type of “evolution-ary adaptation” of networks to dataset bias has, in fact, been documented in the object recognition literature, leading to the prevalence of networks with a strong bias towards local textures over global object shape [3, 8, 31]. We hypothesize that, in video modeling, the same phenomenon justifies the supremacy of 3D convolutional networks that implement lo-calized spatio-temporal video representations, based on a very small number of frames, basically ignoring long-range video dynamics. This, we claim, hampers the generalization of these networks to unseen domains. For example, a horse representation is insufficient for transfer to a new domain that requires discrimination between “riding a horse” and
“chasing a horse” or the classification of video into Olympic equestrian event classes. However, this problem is hard to diagnose in the predominant action recognition setting, where training and test data come from the same domain.
If “horseback riding” is the only horse related class, de-tecting horses is enough for high performance on test data.
Diagnosing the problem requires deploying the action rec-ognizer outside of the native test set, as is usually done for tasks like few-shot learning [27, 64, 72, 74], domain adapta-Figure 1. We propose dynamic representation learning (DRL), an adversarial learning strategy to enhance the modeling of video dynamics.
DRL alternates between two steps: Dynamic scoring step quantitatively evaluates the temporal dynamics captured by a video model, defined as the expected difference been its predictions and those of a spatial student; representation learning step updates parameters of the video network to optimize dynamic score. tion [29, 30, 42, 81], domain generalization [51, 52, 60, 61], etc. Since, in these settings, target domain videos might not contain the same types of bias as the training set, the ac-tion recognition system should transfer or generalize poorly.
Despite the acknowledgement of dataset and model bias in action recognition, as well as efforts to overcome the lo-cality of convolutional operators [31, 85], little effort has been devoted to the quantitative study of the spatial bias of current models, or how the reduction of this bias improves generalization performance.
In this work, we address this problem by introducing a new approach to dynamic representation learning (DRL) based on the explicit measurement and minimization of spa-tial bias. As shown in Figure 1, DRL is based on an adver-sarial optimization between the video network and a spa-tial student, i.e. a 2D network that processes video frames independently. Video network and spatial student are op-In a dynamic scoring step, shown in timized alternately. the left of the figure, the student is optimized to mimic the predictions of the video network. The expected difference between the predictions of the two networks reflects how dynamic the video representation is. This expected differ-ence is denoted as the dynamic score of the video network, measuring how much it relies on dynamic, over spatial, cues for classification. The lower this score, the more similar the video model is to a spatial classifier, and the greater its spa-tial bias. While the dynamic score is naturally measured by training the student by knowledge distillation [40], we also propose an optimization-free approach based on the pre-processing of video clips to remove temporal information, which is more computationally efficient. In the representa-tion learning step, shown on the right of Figure 1, the video model trained to maximize its dynamic score with respect to a learned spatial student. Two approaches are proposed for this purpose. The first is based on adversarial augmentation, using the spatial student to derive perturbations that, when added to the video, obscure spatial cues. The second poses
DRL as a min-max game between the video network and the spatial student to directly optimize the dynamic score of the former. The two methods share the same key idea—to penalize the video model for leveraging spatial shortcuts to action recognition.
We hypothesize that, when pre-trained on the same dataset with a given architecture, models of larger dynamic score should transfer and generalize better to unseen video domains. To evaluate this hypothesis, we conduct a set of experimental evaluations on the robustness and transfer-ability of the learned video representation. This consists of a) adapting the video network to a set of datasets with different action vocabulary, using linear classification over learned features or fine-tuning; b) few-shot action classifi-cation using learned representations directly; and c) apply-ing the classifier on a set of video actions in absence of their spatial context. Experimental results show that DRL signifi-cantly improves all three tasks. For example, 5-shot gesture recognition on the Jester dataset [58] is improved by 5% us-ing 3D ResNet architecture [37], and up to 8% with TSM network [55].
The contributions of this paper is three-fold: First, we propose dynamic score, a quantitative measure of tempo-ral modeling capacity of video neural networks. Second, we propose dynamic representation learning (DRL), a pre-training strategy that aims to optimize dynamic scores for video models. Finally, we present a comprehensive set of experiments to measure the transferability and general-ization of learned video representations, which empirically validates the importance of dynamic modeling on video transfer learning and demonstrates the advantage of DRL over baseline pre-training methods. 2.