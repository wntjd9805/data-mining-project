Abstract
Recent studies have shown that adversarial examples hand-crafted on one white-box model can be used to at-tack other black-box models. Such cross-model transfer-ability makes it feasible to perform black-box attacks, which has raised security concerns for real-world DNNs applica-tions. Nevertheless, existing works mostly focus on investi-gating the adversarial transferability across different deep models that share the same modality of input data. The cross-modal transferability of adversarial perturbation has never been explored. This paper investigates the transfer-ability of adversarial perturbation across different modali-ties, i.e., leveraging adversarial perturbation generated on white-box image models to attack black-box video models.
Specifically, motivated by the observation that the low-level feature space between images and video frames are sim-ilar, we propose a simple yet effective cross-modal attack method, named as Image To Video (I2V) attack. I2V gen-erates adversarial frames by minimizing the cosine simi-larity between features of pre-trained image models from adversarial and benign examples, then combines the gen-erated adversarial frames to perform black-box attacks on video recognition models. Extensive experiments demon-strate that I2V can achieve high attack success rates on dif-ferent black-box video recognition models. On Kinetics-400 and UCF-101, I2V achieves an average attack success rate of 77.88% and 65.68%, respectively, which sheds light on the feasibility of cross-modal adversarial attacks. 1.

Introduction
Deep learning has achieved significant progress in a series of computer vision tasks, such as image recogni-tion [3, 4, 12], video classification [41], object detection
[24], and action recognition [2, 11], etc. However, re-cent studies have shown that deep neural networks (DNNs)
†Correspondence to: Jingjing Chen. are highly vulnerable to adversarial examples [21, 28, 29], which are generated by adding small human-imperceptible perturbations that can lead to wrong predictions. The ex-istence of adversarial examples has posed serious security threats for the application of DNNs, such as autonomous driving [25], face recognition [8], video analysis [5, 31, 34], etc. As a result, adversarial examples have attracted numer-ous research attentions in recent years.
It has been demonstrated in recent works [22,33] that ad-versarial examples have the property of transferability, i.e., the adversarial example generated from one model can be used to attack other models. Such cross-model transfer-ability makes it feasible to perform black-box attacks by leveraging adversarial examples handcrafted on white-box models. As a result, how to enhance the transferability of adversarial examples for efficient black-box attacks has at-tracted several research interests recently. These works ei-ther perform data augmentation [7, 20, 38], optimize gradi-ent calculations [6, 20, 36], or disrupt common properties among different models [37], to avoid the generated adver-sarial samples being overfitted to white-box models. Never-theless, all of these works require the white-box models and target black-box models to be homomodal, which share the same modal of input data. Transferability between hetero-modal models has never been explored.
To bridge this gap, this paper investigates the cross-modal transferability of adversarial examples. Specifically, we explore the adversarial transferability between image models and video models by performing transfer-based black-box attacks on video models with image models pre-trained on ImageNet only. This is an extremely challeng-ing setting since there are no white-box video models for generating video adversarial samples. There are two ma-jor obstacles in transferring adversarial perturbations gen-erated on images models to attack video models. First, in addition to the domain gap between image and video data, video data contain additional temporal information, which leads to differences in the learned features between image models and video models. The difference makes it diffi-Figure 1. Overview of the proposed I2V attack. Given a video clip with a true label of “Catching or throwing baseball”, where each frame is input into the ImageNet-pretrained image model separately. Then the image model generates adversarial frames by minimizing the cosine similarity between features from adversarial and benign examples. As the image model and the video model share similar feature space, the generated video adversarial example can fool the video recognition models, and be misclassified as “Abseiling”. cult to transfer the adversarial perturbations from images to videos. Second, existing transfer-based attacks on ho-momodal models (e.g., image models) are not applicable to the cross-modal attack scenarios. Unlike existing transfer-based attacks on images, where image labels are available for optimizing task-specific loss function (e.g., Cross En-tropy loss) in the process of adversarial perturbations gen-eration, in the cross-modal image to video attack, no labels are available for video frames.
To address the aforementioned challenges and perform black-box attacks on video models, we propose a simple yet effective cross-modal attack method, named Image To
Video (I2V) attack. Despite there being a domain gap be-tween image data and video data, we observed that the inter-mediate features between images models and video models are similar to a certain extent. This motivates us to per-turb the intermediate features of the ImageNet-pretrained image models to craft adversarial video frames for attack-ing video recognition models. To this end, the proposed I2V optimizes the adversarial perturbations by minimizing the cosine similarity of intermediate features between benign frames and the generated adversarial frames. The minimiza-tion of the cosine similarity makes the features extracted from adversarial video frames orthogonal to that from be-nign frames. Consequently, it will cause the adversarial video features to move away from the benign video features due to the feature similarity between image and video mod-els. Figure 1 gives an overview of the proposed I2V attack method. I2V takes individual frames from a video clip as in-put to the image model and generates adversarial frames one by one. Then the generated adversarial frames are grouped into video adversarial examples according to the temporal information of the benign video clip. We briefly summarize our primary contributions as follows:
• We investigate the transferability of adversarial pertur-bations between image models and video models. Spe-cially, we propose an I2V attack to boost the transfer-ability of video adversarial examples generated from image models across different video recognition mod-els. To the best of our knowledge, this is the first work on cross-modal transfer-based black-box attacks for video recognition models.
• We provide insightful analysis on the correlations of feature maps between image and video models. Based on this observation, I2V optimizes adversarial frames on perturbing feature maps of image models to boost the transferability across different video recognition models.
• We conduct empirical evaluations using six video recognition models trained with the Kinetic-400 dataset and UCF-101 dataset. Extensive experiments demonstrate that our proposed I2V helps to boost the transferability of video adversarial examples generated from image models. 2.