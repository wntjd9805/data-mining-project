Abstract
Transformers are successfully applied to computer vi-sion due to their powerful modeling capacity with self-attention. However, the excellent performance of transform-ers heavily depends on enormous training images. Thus, a data-efficient transformer solution is urgently needed.
In this work, we propose an early knowledge distillation framework, which is termed as DearKD, to improve the data efficiency required by transformers. Our DearKD is a two-stage framework that first distills the inductive bi-ases from the early intermediate layers of a CNN and then gives the transformer full play by training without distil-lation. Further, our DearKD can be readily applied to the extreme data-free case where no real images are avail-able. In this case, we propose a boundary-preserving intra-divergence loss based on DeepInversion to further close the performance gap against the full-data counterpart. Exten-sive experiments on ImageNet, partial ImageNet, data-free setting and other downstream tasks prove the superiority of
DearKD over its baselines and state-of-the-art methods. 1.

Introduction
Transformers [4, 14, 47] have shown a domination trend in NLP studies owing to their strong ability in modeling long-range dependencies by the self-attention mechanism.
Recently, transformers are applied to various computer vi-sion tasks and achieve strong performance [7,15,32]. How-ever, transformers require an enormous amount of training data since they lack certain inductive biases (IB) [12,15,46, 52]. Inductive biases can highly influence the generaliza-tion of learning algorithms, independent of data, by pushing learning algorithms towards particular solutions [16,17,35].
*This work was done when Xianing Chen was intern at JD Explore
Academy.
†Corresponding authors.
Figure 1. Illustration of data-efficient of our DearKD. We com-pare the data-efficient properties of DearKD in three situations with different numbers of real training images: the full ImageNet, the partial ImageNet and the data-free case (i.e. without any real images) with DeiT and DeiT .
Unlike transformers, CNNs are naturally equipped with strong inductive biases by two constraints: locality and weight sharing mechanisms in the convolution operation.
Thus, CNNs are sample-efficient and parameter-efficient due to the translation equivariance properties [12, 41, 42].
Recently, some researchers have proposed to explicitly insert convolution operations into vision transformers to in-troduce inductive biases [11, 18, 30, 50–52]. However, the forcefully modified structure may destroy the intrinsic prop-erties in transformers and reduce their capacity.
Another line of work [46] utilizes Knowledge Distilla-tion (KD) [23] to realize data-efficient transformers. By distillation, the inductive biases reflected in the dark knowl-edge from the teacher network can be transferred to the stu-dent [1]. DeiT [46], as a typical method in this line, has successfully explored the idea of distilling knowledge from
CNNs to transformers and greatly increased the data effi-ciency of transformer training. Nevertheless, DeiT still suf-fers two drawbacks:
Firstly, some works [11, 51] reveal that inserting convo-lutions to the early stage of the network brings the best per-formance, while DeiT only distills from the classification logits of the CNN and thus makes it difficult for the early (i.e. shallow) transformer layers to capture the inductive bi-ases. Furthermore, the distillation throughout the training implicitly hinders transformers from learning their own in-ductive biases [12] and stronger representations [11].
To solve these problems, we propose a two-stage learn-ing framework, named as Data-efficient EARly Knowledge
Distillation (DearKD), to further push the limit of data effi-ciency of training vision transformers. Here the term ‘early’ refers to two novel designs in our proposed framework: knowledge distillation in the early layers in transformers and in the early stage of transformer training. First, we propose to distill from both the classification logits and the intermediate layers of the CNN, which can provide more explicit learning signals for the intermediate transformer layers (especially the early layers) to capture the inductive biases. Specifically, we draw the inspiration from [10] and design a Multi-Head Convolutional-Attention (MHCA) layer to better mimic a convolutional layer without con-straining the expressive capacity of self-attention. Further, we propose an aligner module to solve the problem of fea-ture misalignment between CNN features and transformers tokens. Second, the distillation only happens in the first stage of DearKD training. We let transformers learn their own inductive biases in the second stage, in order to fully leverage the flexibility and strong expressive power of self-attention.
To fully explore the power of DearKD with respect to data efficiency, we investigate DearKD in three situations with different number of real training images (Figure 1): the full ImageNet [13], the partial ImageNet and the data-free case (i.e. without any real images). In the extreme case where no real images are available, networks can be trained using data-free knowledge distillation methods [8, 34, 55].
In this work, we further enhance the performance of trans-former networks under the data-free setting by introducing a boundary-preserving intra-divergence loss based on Deep-Inversion [55]. The proposed loss significantly increases the diversity of the generated images by keeping the positive samples away from others in the latent space while main-taining the class boundaries.
Our main contributions are summarized as follows:
• We introduce DearKD, a two-stage learning frame-work for training vision transformers in a data-efficient manner. In particular, we propose to distill the knowl-edge of intermediate layers from CNNs to transform-ers in the early phase, which has never been explored in previous works.
• We investigate DearKD in three different settings and propose an intra-divergence loss based on DeepInver-sion to greatly diversify the generated images and fur-ther improve the transformer network in the data-free situation.
• With the full ImageNet, our DearKD achieves state-of-the-art performance on image classification with similar or less computation.
Impressively, training
DearKD with only 50% ImageNet data can outperform the baseline transformer trained with all data. Last but not least, the data-free DearKD based on DeiT-Ti achieves 71.2% on ImageNet, which is only 1.0% lower than its full-ImageNet counterpart. 2.