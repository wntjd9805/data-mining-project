Abstract
Predictive learning ideally builds the world model of physical processes in one or more given environments. Typ-ical setups assume that we can collect data from all environ-ments at all times. In practice, however, different prediction tasks may arrive sequentially so that the environments may change persistently throughout the training procedure. Can we develop predictive learning algorithms that can deal with more realistic, non-stationary physical environments?
In this paper, we study a new continual learning problem in the context of video prediction, and observe that most ex-isting methods suffer from severe catastrophic forgetting in this setup. To tackle this problem, we propose the continual predictive learning (CPL) approach, which learns a mix-ture world model via predictive experience replay and per-forms test-time adaptation with non-parametric task infer-ence. We construct two new benchmarks based on RoboNet and KTH, in which different tasks correspond to different physical robotic environments or human actions. Our ap-proach is shown to effectively mitigate forgetting and re-markably outperform the na¨ıve combinations of previous art in video prediction and continual learning. 1.

Introduction
Predictive learning is an unsupervised learning technique to build a world model of the environment by learning the consequences from historical observations, sequences of ac-tions, and corresponding future observation frames. The standard predictive learning setup is assumed to operate the model in a stationary environment with relatively fixed physical dynamics [9, 15, 38, 41]. However, the assump-tion of stationarity does not always hold in more realis-tic scenarios, such as in the settings of continual learning (CL), where the model is learned through tasks that arrive
∗ Equal contribution.
† Corresponding author: Yunbo Wang.
Figure 1. The new problem of continual predictive learning and the general framework of our approach at test time. sequentially. For example, in robotics (see Fig. 1), world models often serve as the representation learners of model-based control systems [11, 17–19], while the agent may be subjected to non-stationary environments in different train-ing periods. Under these circumstances, it is not practical to maintain a single model for each environment or each task, nor is it practical to collect data from all environments at all times. A primary finding of this paper is that most existing predictive networks [9, 15, 38, 41] cannot perform well when trained in non-stationary environments, suffering from a phenomenon known as catastrophic forgetting [13].
We formalize this problem setup as continual predic-tive learning, in which the world model is trained in time-varying environments (i.e., “tasks” in the context of contin-ual learning) with non-stationary physical dynamics. The model is expected to handle both newer tasks and older ones after the entire training phase (see Section 2 for detailed se-tups). There are two major challenges. 1.1. Covariate–Dynamics–Target Shift
Unlike in the settings of domain-incremental or class-incremental CL for deterministic models, the world model,
which can be viewed as a conditioned generative model, cannot assume a stationary distribution of training targets or fixed target space. Therefore, different from all previous
CL problems, the unique challenge of continual predictive learning is due to the co-existence of three types of distribu-tion shift, including the covariate shift in PX , the target shift 1. Notably, the covari-in PY , and the dynamics shift PY ate shift [14, 28–31, 39, 43] and target shift [2, 16, 21, 27, 50] have been widely considered by existing methods, whereas the conditional distribution is typically assumed to be in-variant. In our setup, however, the conditional distribution
X corresponding to the spatiotemporal dynamics also
PY
| changes over training periods. It significantly increases the probability of catastrophic forgetting in the world model.
X
|
To combat the dynamics shift, we first present a new world model that learns multi-modal visual dynamics of different tasks on top of task-specific latent variables. Fu-ture frames are generated by drawing samples from learned mixture-of-Gaussian priors conditioned on a set of categori-cal task variables, and combining them with a deterministic component of future prediction (see Section 3.1).
Second, we specifically design a novel training scheme named predictive experience replay. Like deep generative replay (DGR) [40], the proposed training method leverages a learned generative model to produce samples of previ-ous tasks. Yet, in our approach, these samples are fed into the world model as the first frames to generate entire se-quences, which can be reused as model inputs for rehearsal.
The world model alternates between (i) generating rehearsal data without backpropagating the gradients, (ii) regressing the facilitate future frames of previous tasks produced by the world model itself, and (iii) generating future frames from real data of the current task. Another advantage of this training scheme is about the memory efficiency, as it only retains parts of low-dimensional action vectors in the buffer for action-conditioned predictive replay (see Section 3.2). 1.2. Task Inference: Coupled Forgetting Issues
The second challenge in continual predictive learning is the task ambiguity at test time, which can greatly affect the prediction results. Unlike existing CL methods for fully generative models [33, 40], in our setup, the models are re-quired not only to solve each task seen so far, but also to in-fer which task they are presented with. A na¨ıve solution is to infer the task using another neural network. However, due to the inevitable forgetting issue of the task inference model itself, coupled with that of the world model, this method is unlikely to perform well. In Section 3.3, we propose the non-parametric task inference strategy, which overcomes the intrinsic nature of forgetting of a deterministic model. 1In predictive learning settings, the input X is in forms of sequential observation frames X1:T and the training target Y corresponds to future frames XT +1:T +H . We here skip the input action signals for simplicity.
We also present a self-supervised, test-time training process that recalls the pre-learned knowledge of the inferred task through one or several online adaptation steps.
We construct two new benchmarks for continual predic-tive learning based on real-world datasets, RoboNet [6] and
KTH [37], in which different tasks correspond to different physical robotic environments or human actions. Our CPL approach is shown to effectively avoid forgetting and re-markably outperform the straightforward combinations of previous art in video prediction and continual learning. 2. Problem Setup
Unlike existing predictive learning approaches, we con-) in non-stationary environ-sider to learn a world model ( ments (i.e., the evolution of tasks), such that
M
X1:T , aT :T +H (
XT +1:T +H ∼ M
− 1, ˆk), (1)
X1:T and (cid:98) where
XT +1:T +H are respectively the observed frames and future frames to be predicted. The task index k is known at training, but not observed at test. It requires our approach not only to solve each task seen so far, but also to infer which task it is presented with, denoted as
Tˆk. Here, aT :T +H
M is learned for vision-based robot control, as in the action-conditioned video prediction experiments in this paper. For-mally, continual predictive learning assumes that: 1 is the optional inputs of action signals when
−
Covariate shift: P (X k
Dynamics shift: P (X k
Target shift: P (X k 1:T ) ̸= P (X k+1 1:T ) 1:T ) ̸= P (X k+1
T +1:T +H |X k
T +1:T +H ) ̸= P (X k+1
T +1:T +H ),
T +1:T +H |X k+1 1:T ) (2) where we leave out aT :T +H 1 for simplicity in the con-− ditional distribution of visual dynamics. The setup is in part similar to class-incremental CL for supervised tasks that assumes P (
,
X
}
} denotes a constant label set
P (
} for discriminative models. In contrast, continual predictive learning does not assume a fixed target space, and therefore may have more severe catastrophic forgetting issues. k)
X k+1).
= P ( k
= P ( k+1),
{Y
{Y
{Y k+1 k)
=
Y
Y k 3. Approach
In this section, we present the new continual predictive learning (CPL) approach, which first mitigates catastrophic forgetting within the world model from two aspects:
• Mixture world model: A recurrent network that cap-tures multi-modal visual dynamics. Unlike existing mod-els [9, 18], the learned task-specific priors are in forms of mixture-of-Gaussians to overcome dynamics shift.
• Predictive experience replay: A new rehearsal-based training scheme that combats the forgetting within the world model and is efficient in memory usage.
̸
̸
Figure 2. The overall network architecture of the mixture world model and the predictive experience replay training scheme in the proposed
CPL method. (a) The world model learns representations in the forms of mixture-of-Gaussians based on categorical task variables. (b) As for the predictive experience replay, the world model (M) interacts with the initial-frame generative model (G). In this replay stage, we first use G to generate the first frames of previous tasks without backpropagating the gradients, then use M to predict the corresponding future frames, and finally combine the rehearsal data and real data to jointly train M and G.
To cope with the challenge of task ambiguity when test-ing the world model in an unknown task, we propose:
• Non-parametric task inference: Instead of using any parametric task inference model that may introduce ex-tra forgetting issues, we use a trial-and-error strategy over the task label set to determine the present task. 3.1. Mixture World Model
M
The world model considers a new remedy to catas-trophic forgetting from the perspective of spatiotemporal representation. As mentioned above, the forgetting prob-lem within existing world models [9, 18] is mainly caused by the covariate-dynamics-target shift in time-varying en-vironments. Therefore, the key idea of the proposed world model in CPL is to use mixture-of-Gaussian variables to capture the multi-modal distribution of visual dynamics in the latent space, as well as that of spatial appearance in the input/output observation space. Accordingly, as shown in
Fig. 2(a), the world model consists of three components: module learns the deterministic transition component from inputs to prediction targets. It responds to multi-modal spa-tiotemporal dynamics by taking as input the task-specific latent variable zt. All components are implemented as neu-ral networks, in which the dynamics module is particularly composed of stacked ST-LSTM layers [47].
The task-specific latent representation zt is drawn from a mixture-of-Gaussian distribution, inspired by existing un-supervised learning methods that use Gaussian Mixture pri-ors for variational autoencoders [10, 22, 33]. Our mixture world model is an early work that uses this representa-tion form to model the multi-modal priors in spatiotem-poral dynamics. Specifically, for each task, the represen-tation module and the encoding module are both condi-tioned on the present task label. They are jointly trained to learn the posterior and prior distribution of zt by opti-Tk, the mizing the Kullback-Leibler divergence. At task 1:T +H , ak k objective function 1) combines the
KL loss with the reconstruction loss: (
X 1:T +H
L
M
− k
Representation module: zt ∼
Encoding module: ˆzt ∼
Dynamics module: k 1:t, k) k 1:t
− k 1:t
− 1, k) 1, a1:t qϕ(
X pψ(
X
Xt = pθ(
X (cid:98)
− 1, z1:t, k). (3) 1, . . . , K
The representation module infers the latent state zt from the target frames. It takes as input the categorical task variable to cope with the target shift in continual k predictive learning scenarios. The encoding module corre-sponds to the covariate shift and dynamically maps the input frames to ˆzt in the same latent subspace as zt. The dynamics
∈ {
}
T +H t=2 (cid:88) (cid:2)
−
E q(z1:t
| X k 1:t,k) log p( k t | X k 1:t 1, ak 1:t
−
− 1, z1:t, k) (4)
X
αDKL(q(zt | X k 1:t, k) p(ˆzt | X
|| k 1:t
− 1, k))
, 4 in our experiments. In the test phase, where α is set to 10− we discard the representation module qϕ and only use the encoding module pψ to sample task-specific latent variables for frames generation. (cid:3) 3.2. Predictive Experience Replay
The two main challenges in typical CL setups are catas-trophic forgetting and memory limitation. Due to the co-Algorithm 1 Predictive experience replay
Algorithm 2 Testing procedure 1}
{X
K k=1
Input: Training data
Output: World model
K k=1,
, generative model k ak 1:T +H } 1:T +H
{
M
T1 according to Eq. (4) at
T1 according to Eq. (6) with k = 1
# Replay video sequences (skip the batch size) for ˜k = 1, . . . , k 1 do (a˜k 1: Train
M at 2: Train
G 3: for k = 2, . . . , K do 4: 5:
−
G 9: 6: 7: 8: 1 , a˜k
˜k
−
˜k 1, ˜k) 1 ← G
X
˜k ( 2:T +H ← M
X
X (cid:98) end for
# Mix replayed data at (cid:98) (cid:98) 1:k 1:T +H , a1:k ( 1) 10:
X
− according to Eq. (5)
Train 11:
M
Train (cid:98) 12:
G 13: end for according to Eq. (6)
T1:k (
X
∪
− 1:T +H
− 1 1 2:T +H 1, ˜k)
− 1 and real data at 1)
− 1:T +H , ak k 1:T +H
Tk
− existence of covariate shift, target shift, and dynamics shift, these challenges become even more urgent in the context of continual predictive learning based on video data. One common way to tackle these challenges is generative re-play [33, 40], which considers using a generative model to produce samples of previous tasks. However, the generative replay method cannot be used directly in our setup, as it is extremely difficult to generate a valid video sequence using a generative model alone.
G
M
Therefore, we propose the predictive experience replay, which firmly combines an initial-frame generative model
), which learns to generate the first frame of videos at pre-(
G vious tasks given the task labels, with the world model (
).
To counter the coviariate shift of image appearance in non-stationary environments, also uses learnable mixture-of-Gaussian latent priors, denoted by e. As shown in Fig. 2(b), for each previous task to generate the first frames of the rehearsal video sequences, and then use to predict the corresponding future frames. Finally, we
M mix the rehearsal sequences at previous tasks and real se-quences at the present task in turn. We summarize the training procedure in Alg. 1. The predictive experience replay is different from all existing generative replay methods because the world model plays a key role in the rehearsal process.
˜k, we first use
Tk to train and
M
G
G
T
∼
In particular, for action-conditioned predictive learning scenarios, we maintain a buffer to keep parts ( 7%) of the low-dimensional action sequences from previous tasks.
During predictive experience replay, we first sample an ac-tion sequence from the buffer a˜k 1 at a previous task
˜k. We feed the initial action a˜k
T to ensure that the generated first frame control, and perform
G
˜k 1 is valid for robot to produce predictive replay results 1. In predictive experience 1 and the task label ˜k into
M 1 and a˜k
˜k 2:T +H 1:T +H
˜k 2:T +H given
X
X
X (cid:98)
−
− (cid:98) (cid:98)
Input: Observation frames
Output: Predicted future frames
X1:T , optional actions a1:T +H
XT +1:T +H 1: # Non-parametric task inference 2: for k = 1, . . . , K do (cid:98) k
X1:T /2, a1:T ( 3:
T /2+1:T ← M
X 4: end for
T 5: ˆk = arg mink t=T /2+1( (cid:98)
∈{ 6: # Test-time adaptation (optional) (cid:80)
ˆk 7: Optimize with
X1:T , a1:T (
L
M 8: # Model deployment
X1:T , a1:T +H ( 9:
XT +1:T +H ← M 1,...,K
M
−
−
} 1, k)
Xt − 1)
− 1, ˆk) t )2 k
X (cid:98) (cid:98) replay, we train the world model at
Tk by minimizing
M k 1
− (cid:88)˜k=1
+
=
LM
˜k
L
M (
X
˜k
˜k 1:T +H , a 1:T +H 1)
− (5) k
L
M (cid:98) 1:T +H , ak k 1:T +H (
X 1).
−
The objective function of the initial-frame generative model can be written as
G
= E
LG k k 1 ,k) log p( 1 |
X k 1 , k) q(e
| X
βDKL(q(e 1
| X
|| q(e
˜k 1 ,˜k) log p(
X
|
X
+
− k
−
E (cid:88)˜k=1 (cid:2) e, ak 1, k) p(ˆe k))
|
˜k 1, ˜k) e, a
˜k 1 | (6) (cid:98)
βDKL(q(e
˜k 1 , ˜k)
X (cid:98) p(ˆe
||
˜k))
,
−
| where the reconstruction loss is in an ℓ2 form and β is set to 10− 4 through empirical grid search. (cid:98) (cid:3)
| 3.3. Non-Parametric Task Inference
In the mixture world model, the task label has a signif-icant impact on the learned priors and corresponding pre-diction results. Since it is unknown at test time, it can only be inferred from the input observation sequences, i.e., video classification. However, existing video classification mod-els tend to underperform in the domain-incremental CL set-ting, which will magnify the catastrophic forgetting prob-lem jointly trained with the world model. To avoid the in-herent forgetting issue of model-based task inference, we propose a new non-parametric method that only exploits the learned mixture world model to make task inference.
More precisely, as shown in Alg. 2, we feed the first half of each input sequence into the world model
X1:T /2, along with a hypothetic task label k. We then enumerate each task label k and evaluate the outputs of the world model on the remaining frames of the input sequence
XT /2+1:T . Finally, we choose the task label ˆk that leads to the best prediction quality. 1, . . . , K
∈ {
}
Method
SVG [9]
PredRNN [47]
PhyDNet [15]
PredRNN + LwF [26]
PredRNN + EWC [24]
CPL-base + EWC [24]
CPL-base
CPL-full
CPL-base (Joint training)
Action-conditioned
Action-free
PSNR↑ 18.72 ± 0.61 19.45 19.60 19.10 21.15 21.29 ± 0.30 19.36 ± 0.00 23.26 ± 0.10 24.64 ± 0.01
SSIM↑ (×10−2) 68.59 ± 2.22 66.38 68.68 64.73 74.72 75.16 ± 0.98 63.57 ± 0.00 80.72 ± 0.23 83.73 ± 0.00
PSNR↑ 18.92 ±0.51 19.56 21.00 19.79 21.15 21.38 ± 0.18 20.15 ± 0.02 22.48 ± 0.03 22.56 ± 0.01
SSIM↑ (×10−2) 68.08 ± 2.20 69.92 75.47 71.43 78.02 76.68 ± 0.69 71.15 ± 0.08 78.84 ± 0.07 79.57 ± 0.02
Table 1. Quantitative results of continual predictive learning on the RoboNet benchmark in both action-conditioned and action-free setups. (Lines 1-3) Existing video prediction models with i.i.d. assumption. (Lines 4-6) Combinations of predictive models and continual learning approaches. (Lines 7-8) Our predictive model based on learned mixture-of-Gaussian priors, and the the entire CPL with predictive expe-rience replay and non-parametric task inference. (Line 9) A baseline model jointly trained on all tasks throughout the training procedure, whose results can be roughly viewed as the upper bound of our approach.
In addition to using P (
XT /2+1:T |X1:T /2) to perform task inference, we also use this self-supervision for test-time adaptation, which allows the model to continue train-ing after deployment. Test-time adaptation effectively re-calls the pre-learned knowledge in the inferred task
Tˆk through one-step (or few-steps) online optimization, thus further alleviating the forgetting problem. 4. Experiment 4.1. Experimental Setup
Benchmarks. We quantitatively and qualitatively evalu-ate CPL on the following two real-world datasets:
• RoboNet [6]. The RoboNet dataset contains action-conditioned videos of robotic arms interacting with a va-riety of objects in various environments. We divide the whole dataset into four continual learning tasks according to the environments (i.e., Berkeley
→
Stanford). For each task, we collect about 3,840 training sequences and 960 testing sequences.
Google
Penn
→
→
• KTH action [37]. This dataset contains gray-scale videos which include 6 types of human actions. We directly use the action labels to divide the dataset into 6 tasks (i.e.,
Boxing
→
Running). For each task, we collect about 1,500 training sequences and 800 testing sequences in average.
Clapping
Walking
Jogging
Waving
→
→
→
→
We define the task orders by random sampling, and with-out loss of generality, our approach is effective to any task orders (see Section 4.4). More experimental configurations and the implementation details can be found in the Supple-mentary Material.
Evaluation criteria. We adopt SSIM and PSNR from previous literature [9, 47] to evaluate the prediction results.
We run the continual learning procedure 10 times and report the mean results and standard deviations in the two metrics.
Compared methods. We compare CPL with the follow-ing baselines and existing approaches:
• CPL-base: A baseline model that excludes the new com-ponents of Gaussian mixtures, predictive replay, and task inference.
• PredRNN [47], SVG [9], PhyDNet [15]: Video predic-tion models focused on stochastic, deterministic, and dis-entangled dynamics modeling respectively.
• LwF [26]: It is a distillation-based CL method built on the memory state of PredRNN [47].
• EWC [24]: It constrains the parameters of PredRNN and
CPL-base on new tasks with additional loss terms. 4.2. RoboNet Benchmark
We first evaluate CPL on the real-world RoboNet bench-mark, in which different continual learning tasks are di-vided by laboratory environments. We conduct both action-conditioned and action-free video prediction on RoboNet.
The former follows the common practice [3, 48] to train the world model to predict 10 frames into the future from 2 ob-servations and corresponding action sequence at the 11 time steps. For the action-free setup, we use the first 5 frames as input to predict the next 10 frames.
Quantitative comparison. Table 1 gives the quantitative results on RoboNet, in which the models are evaluated on the test sets of all 4 tasks after the training period on the last task. We have the following findings here. First, CPL out-performs existing video prediction models by a large mar-gin. For instance, in the action-conditioned setup, it im-proves SVG in PSNR by 24.3%, PredRNN by 19.6%, and
PhyDNet by 18.7%. Second, CPL generally performs bet-ter than previous continual learning methods (i.e., LwF and
EWC) combined with video prediction backbones. Note that a na¨ıve implementation of LwF on top of PredRNN even leads to a negative effect on the final results. Third,
Figure 3. Results on the action-conditioned RoboNet benchmark.
The horizontal axis represents the sequential training process, and the vertical axes represent test results on particular tasks after each training period. The purple dashed line indicates the results of the baseline model jointly trained on all tasks. by comparing CPL-full (our final approach) with CPL-base (w/o Gaussian mixture latents, predictive experience replay, or non-parametric task inference), we can see that the new technical contributions have a great impact on the perfor-mance gain. We provide more detailed ablation studies in Section 4.4. Finally, CPL is shown to effectively ease catastrophic forgetting by approaching the results of jointly training the world model on all tasks in the i.i.d. setting (23.26 vs. 24.64 in PSNR). Apart from the average scores for all tasks, in Fig. 3, we provide the test results on par-ticular tasks after individual training periods. As shown in the bar charts right to the main diagonal, CPL performs par-ticularly well on previous tasks, effectively alleviating the forgetting issue. Please refer to the Supplementary Material for detailed comparison results.
Qualitative comparison. Fig. 4 provides the qualitative comparisons on the action-conditioned RoboNet bench-mark. Specifically, we use the final models after the training period of the last task to make predictions on the first task.
We can see from these demonstrations that our approach is more accurate in predicting both future dynamics of the
In objects as well as the static information of the scene. contrast, the predicted frames by PredRNN+LwF and CPL-base+EWC suffer from severe blur effect in the moving ob-ject or the static (but complex) background, indicating that directly combining existing CL algorithms with the world models cannot effectively cope with the dynamics shift in highly non-stationary environments. 4.3. KTH Benchmark
Quantitative comparison. Table 2 shows the quantitative results on the test sets of all 6 tasks after the last training
Figure 4. Showcases of action-conditioned video prediction in the first environment of RoboNet (i.e., Berkeley) after training the models in the last environment (i.e., Stanford).
Method
SVG [9]
PredRNN [47]
PhyDNet [15]
PredRNN + LwF [26]
CPL-base + EWC [24]
CPL-base
CPL-full
CPL-base (Joint train)
PSNR 22.20±0.02 23.27 23.68 24.25 24.32 ± 0.15 22.96 ± 0.05 29.12 ± 0.03 28.12 ± 0.01
SSIM (×10−2) 69.23±0.01 70.47 72.97 70.93 69.02 ± 0.48 68.98 ± 0.02 84.50 ± 0.04 82.16 ± 0.00
Table 2. Quantitative results on the KTH benchmark. period of the models on the last task. We can observe that
CPL significantly outperforms the compared video predic-tion methods and continual learning methods in both PSNR and SSIM. Furthermore, an interesting result is that our ap-proach even outperforms the joint training model, as shown in the bottom line in Table 2. While we do not know the exact reasons, we state two hypotheses that can be inves-tigated in future work. First, the Gaussian mixture priors enable the world model to better disentangle the representa-tions of visual dynamics learned in different continual learn-ing tasks. Second, the predictive experience replay allows the pre-learned knowledge on previous tasks to facilitate the
Figure 5. Results on the KTH benchmark. The horizontal axis rep-resents the sequential training process, and the vertical axes repre-sent test results on particular tasks after each training period.
Replay
Infer k
Random k Adapt
PSNR SSIM (cid:37) (cid:33) (cid:33) (cid:33) (cid:33) (cid:37) (cid:37) (cid:33) (cid:37) (cid:33) (cid:37) (cid:37) (cid:37) (cid:33) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:33) 22.96 27.21 27.82 26.56 29.12 68.98 79.99 81.51 78.64 84.50
Table 3. Ablation study for each component of CPL on the KTH benchmark. “Replay” denotes the use of predictive experience re-play. “Infer k” indicates the use of non-parametric task inference.
“Random k” means that the world model takes as input a random task label at test time. “Adapt” means test-time adaptation. learning process on new tasks. Fig. 5 provides the inter-mediate test results on particular tasks after each training period, which confirm the above conclusions.
Qualitative comparison. We visualize a sequence of pre-dicted frames on the first task of KTH in Fig. 6. As shown, all existing video prediction models and even the one with
LwF generate future frames with the dynamics learned in the last task (i.e., Running), which clearly demonstrates the influence of the dynamics shift. Images generated by
CPL-base+EWC suffer from a severe blur effect, indicat-ing that the model cannot learn disentangled representations for different dynamics in the non-stationary training envi-ronments. In comparison, CPL produces more reasonable results. To testify the necessity of task inference, we also
Figure 6. Showcases of predicted frames of the first task (i.e.,
Boxing) after the training period of the last task (i.e., Running). provide incorrect task labels for CPL. As shown in the third line from the bottom, the model takes as input the Boxing frames along with an erroneous task label of Running. In-terestingly, CPL combines the inherent dynamics of input frames (reflected in motion of arms) with the dynamics pri-ors from the input task label (reflected in motion of legs). 4.4. Ablation Study
Effectiveness of each component in CPL. We conduct ablation studies on the KTH benchmark step by step. In Ta-ble 3, the first line shows the results of the CPL-base model and the bottom line corresponds to our final approach. In the second line, we train CPL-base with predictive expe-rience replay and observe a significant improvement from 22.96 to 27.21 in PSNR. In the third line, we improve the world model with mixture-of-Gaussian priors and accord-ingly perform non-parametric task inference at test time.
We observe consistent improvements in both PSNR and
SSIM upon the previous version of the model. In the fourth line, we skip the non-parametric task inference during test-ing and use a random task label instead. We observe that
Dataset
RoboNet
KTH
PSNR 23.58 ±0.28 28.93±0.14
SSIM (×10−2) 79.67 ±3.75 83.99 ±0.40
Table 4. Robustness of CPL on random task orders. the performance drops from 27.82 to 26.56 in PSNR, indi-cating the importance of task inference to predictive expe-rience replay. Finally, in the bottom line, we introduce the self-supervised test-time adaptation. It shows a remarkable performance boost compared with all the above variants.
Is CPL robust to the task order? As shown in Table 4, we further conduct experiments to analyze that whether
CPL can effectively alleviate catastrophic forgetting regard-less of the task order. We additionally train the CPL model in 3-4 random task orders. From the results, we find that the proposed techniques including mixture world model, predictive experience replay, and non-parametric task infer-ence are still effective despite the change of training order. 5.