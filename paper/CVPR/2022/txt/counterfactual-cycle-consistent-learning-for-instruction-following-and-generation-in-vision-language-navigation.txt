Abstract
Since the rise of vision-language navigation (VLN), great progress has been made in instruction following – building a follower to navigate environments under the guidance of instructions. However, far less attention has been paid to the inverse task: instruction generation – learning a speaker to generate grounded descriptions for navigation routes. Exi-sting VLN methods train a speaker independently and often treat it as a data augmentation tool to strengthen the fol-lower, while ignoring rich cross-task relations. Here we des-cribe an approach that learns the two tasks simultaneously and exploits their intrinsic correlations to boost the training of each: the follower judges whether the speaker-created instruction explains the original navigation route correctly, and vice versa. Without the need of aligned instruction-path pairs, such cycle-consistent learning scheme is complemen-tary to task-speciﬁc training targets deﬁned on labeled data, and can also be applied over unlabeled paths (sampled without paired instructions). Another agent, called creator is added to generate counterfactual environments. It greatly changes current scenes yet leaves novel items – which are vital for the execution of original instructions – unchanged.
Thus more informative training scenes are synthesized and the three agents compose a powerful VLN learning system.
Extensive experiments on a standard benchmark show that our approach improves the performance of various follower models and produces accurate navigation instructions. 1.

Introduction
Vision-language navigation (VLN) [7], i.e., enabling an agent to navigate across realistic environments given human instructions, has received great attention (Fig. 1(a)). Many powerful wayﬁnding agents (i.e., follower) were developed to perform such embodied instruction following task. Un-fortunately, the inverse task – learning a speaker that vividly explains navigation paths – has remained under-explored. In fact, instruction generation is also a crucial ability of AI
∗Corresponding author: Wenguan Wang.
X
A
E
ˆA
¯E
ˆX
Figure 1: (a) VLN [7]. (CCC) learning system consists of three agents, i.e., speaker s ( for instruction generation, follower f ( ing, and creator c ( (b) Our counterfactual cycle-consistent
)
) for instruction follow-) for counterfactual environment creation. agents. In many scenarios, AI agents should be able to com-municate with humans for efﬁcient collaboration, instead of executing instructions only [76, 29]. For example, when a human-robot team is doing search and rescue [68, 18, 75], the human may ﬁrst issue commands (e.g., “explore along this direction until the end of the hallway”) that direct the robot to navigate a building and search for survivors. In this process, the robot is expected to report its progress (e.g., “I have inspected three rooms”) and explain its plan (e.g., “I will continue to navigate this direction and stop at the end of the hallway”) [69]. Robot’s ability to generate linguistic explanations can help human to resolve potential ambiguity (e.g., identifying “this direction” and “hallway”) [18] and establish trust [9, 20]. Hence, the robot can even in turn help the human to navigate its explored area (which is unfamil-iar to the human) [27], e.g., “go straight and you will pass through four rooms before reaching the end of the hallway”.
In addition to emphasizing the importance of instruc-tion generation, we explore the intrinsic correlation between 1
instruction following and generation to derive a powerful
VLN learning framework. Speciﬁcally, given the visual en-vironment space E, linguistic instruction space X , and navi-gation path space A, instruction following learns a follower f : E ×X (cid:55)→ A that maps visual observations and navigable directions into action sequences, while instruction genera-tion learns a speaker s : E ×A (cid:55)→ X that maps observations and action sequences into trustable instructions. Clearly, there exists strong dependencies among the input and output spaces of s and f . Surprisingly, such task correlation was long ignored; current VLN methods only learn an isolated speaker as an one-off plugin for data augmentation [24, 66].
We instead propose to jointly train the speaker and fol-lower in a compact, cycle-consistent learning framework (Fig.1(b)). During training, (E, X) ∈ E ×X is ﬁrst mapped to ˆA ∈ A through the follower f (
) and then translated to an instruction ˜X ∈ X through the speaker ( ), i.e., s(E, ˆA).
In environment E, the dissimilarity between X and ˜X, de-noted as (cid:52)E(X, ˜X), is used as the feedback signal to regu-larize training. Similarly, given (E, A) ∈ E ×A, (cid:52)E(A, ˜A) can be estimated and used for training. As (cid:52) errors are only about the cycle-consistency over (E, X) and (E, A), any other training objectives deﬁned on labeled triplets, i.e., (E, X, A), are compatible. Hence, we can apply such learn-ing system on “unlabeled” data (E, A(cid:48)), i.e., sampling a path A(cid:48) in an environment E without corresponding instruc-tion. Thus both labeled instruction-path samples and un-labeled paths can be simultaneously used during training.
This is more elegant than current de facto VLN training protocol [24, 66] that has three phases: i) train the follower and speaker separately on aligned instruction-path samples; ii) use the speaker to create synthetic instructions for ran-domly sampled paths; and iii) ﬁne-tune the follower on the pseudo instruction-path samples. Further, as learning from pseudo-parallel data inevitably accompanies with the data quality problem (i.e., the quality of the pseudo instruction-path samples is difﬁcult to guarantee) [47], our speaker-follower collaborative learning game is more favored, i.e., (cid:52) errors can be viewed as quality scores and play as super-visory signals to boost the training of both f and s.
Besides the speaker and follower, another agent, called creator ( ), is put into our cycle-consistent learning game.
The creator serves as a plug-and-play component for coun-terfactual environment synthesis, enabling more robust training. Thinking about alternative possibilities for past or future events is central to human thinking [62]. We fre-quently construct counterfactuals (“counter to the facts”): what might happen if · · · ? Counterfactual thinking gives us the ﬂexibility in learning from past limited experience through mental simulation. Nevertheless, this issue is rarely addressed in VLN. Recently, [25] interpreted current pre-vailing data augmentation technique [24] – back-translation – as a kind of counterfactual thoughts – given an instruction like“walk away from the door”, the agent builds a counterfa-ctual like “if I walk in the door, what should the instruction be?” by augmenting sampled paths with artiﬁcial instruc-tions. This allows the agent to be more efﬁciently trained by performing alternative actions that it did not actually make [25]. Although our speaker-follower game naturally supports such path sampling based counterfactual thinking (i.e., involving unlabeled data (E, A(cid:48)) during training, as mentioned before), our creator can build another kind of counterfactuals, e.g., “if I change current environment by, for example, removing or putting in a lot of furniture that are irrelevant to the instruction, I will still execute the orig-inal navigation plan”. Concretely, given an environment-instruction-path tuple (E, X, A) ∈ E × X × A, the creator c : E ×X ×A (cid:55)→ E “images” a new environment ¯E ∈ E, such that i) ¯E is greatly different from E, and ii) ¯E should be still aligned with the original instruction-path pair (X, A). With i) and ii), we address both diversity and realism, which are proved critical in counterfactual thinking [26, 84]. Hence, as ¯E and (X, A) compose a new valid training sample, the cycle-consistent errors, i.e., (cid:52) ¯E(A, ˜A) and (cid:52) ¯E(X, ˜X), can be estimated over ( ¯E, X, A). In this way, our speaker, fol-lower and creator form a powerful learning system, which makes a clever use of cross-task and cross-modal connec-tions as well as resembles a counterfactual thinking process.
Our counterfactual cycle-consistent (CCC) framework is optimized by policy gradient methods and compatible with current imitation learning (IL) and reinforcement learning (RL) based VLN training protocols. We apply our CCC over several VLN baseline models and test it on gold-standard
R2R dataset [7]. Experimental results verify the efﬁcacy of
CCC on both instruction following and generation tasks. 2.