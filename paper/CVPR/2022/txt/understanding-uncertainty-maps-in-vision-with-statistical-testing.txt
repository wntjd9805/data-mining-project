Abstract
Quantitative descriptions of confidence intervals and un-certainties of the predictions of a model are needed in many applications in vision and machine learning. Mechanisms that enable this for deep neural network (DNN) models are slowly becoming available, and occasionally, being in-tegrated within production systems. But the literature is sparse in terms of how to perform statistical tests with the uncertainties produced by these overparameterized models.
For two models with a similar accuracy profile, is the former model’s uncertainty behavior better in a statistically signifi-cant sense compared to the second model? For high resolu-tion images, performing hypothesis tests to generate mean-ingful actionable information (say, at a user specified sig-nificance level α = 0.05) is difficult but needed in both mis-sion critical settings and elsewhere. In this paper, specif-ically for uncertainties defined on images, we show how revisiting results from Random Field theory (RFT) when paired with DNN tools (to get around computational hur-dles) leads to efficient frameworks that can provide a hy-pothesis test capabilities, not otherwise available, for un-certainty maps from models used in many vision tasks. We show via many different experiments the viability of this framework. 1.

Introduction
With the adoption of deep neural network models in pro-duction systems for vision tasks, there is a growing con-sensus that we must be aware of what our model does not know. This is relevant not only for systems used for au-tonomous driving or medical imaging but also in less criti-cal situations where such a model informs decision making in general and/or is responsible for generating triggers for user intervention. For example, inaccurate but overconfi-Figure 1. Top shows a raw uncertainty of the depth estimation process.
Bottom shows significant regions selected by our method, with guarantees to restrict family-wise error rate. This region can be used for calibration, model comparisons, or other use cases. dent predictions can lead to undesirable outcomes in assem-bly line manufacturing and logistics. This need has led to interest in the design of mechanisms for model calibration as well as for estimating uncertainties from deep neural net-work (DNN) models used in vision for tasks including but not limited to prediction [38, 45], segmentation [3, 41, 47], depth estimation [14, 20] and visual odometry [4, 32].
Uncertainties can be roughly categorized into aleatoric (statistical) and epistemic (systematic). Aleatoric uncer-tainty can help capture inherent and irreducible data noise, which cannot be reduced even if more data were collected.
It can be represented by heteroscedastic models [26, 40], since they assume that the observation noise (uncertainty) can vary with the input. Epistemic uncertainty accounts for uncertainty in model parameters, and can be improved by observing more data. Capturing epistemic uncertainty in a
DNN can involve putting a prior on the latent space (e.g.,
Variational Auto Encoder (VAE) [46]) or model parame-ters (e.g., Bayesian Neural Networks (BNN) [7, 33, 42]), and adopting any available scheme to estimate the poste-rior probability. Several strategies exist which use hybrid approaches to capture either aleatoric or epistemic (or both)
by combining heteroscedastic NNs and BNNs, e.g., [30].
Example scenarios. While capturing different types of uncertainties is useful, in practical scientific/industrial set-tings, uncertainty estimates are merely a “means to an end”.
We must understand what actions the estimates enable, re-gardless of whether it is aleatoric or epistemic.
Scenario 1. Uncertainty estimates enable calibration, e.g., by a practitioner evaluating medical images. If a spe-cialist can see that model is uncertain in some specific re-gions, he/she can evaluate whether to acquire more data if the regions where the model is uncertain are anatomi-cally important. In other cases, such information can guide whether to request a biopsy. However, to decide, we need a statistically sound scheme to generate “significant” uncer-tain regions. Otherwise, interpreting the raw uncertainty is entirely subjective. Similar applications appear in depth es-timation for autonomous vehicles [27], Fig. 1.
Scenario 2. Uncertainty can be used to compare confi-dences of models. Say a user is satisfied by the accuracy profiles of two models ModelA and ModelB but the second one has a higher latency. An upgrade to ModelB is only jus-tified if one is 99% confident that it reduces uncertainty in a statistically significant sense on a held-out test dataset. This needs a “go/no-go” answer. Similarly, consider two systems for tumor volume dynamics using segmentation, which will drive treatment options (e.g., RECIST criteria [13] ). Both systems offer similar accuracy and are FDA approved, but one is more expensive. The investment may be justified if the reduction in uncertainty is significant at a 99.9% level.
Alternatively, consider a model on a small form factor de-vice. The choice is between low-precision and high preci-sion operations, the latter will need a larger battery. If both models satisfy client accuracy needs, is the reduction in un-certainty of predictions statistically significant?
Despite the growing body of work on uncertainty, frame-works that enable actionable information are limited. The goal of this work is to close this gap.
Classical techniques from statistics. The problems above can be tackled with classical statistical testing. Here, we can set this up as pixel-wise statistical tests (although not strictly necessary; we will discuss alternative forms shortly). Scenario 1 will be a one sample test, while Sce-nario 2 will be a two sample test: we ask whether the uncer-tainty at a pixel is different across the two models.
Bottleneck. Deriving a scientifically valid conclusion for the image based on pixel-wise statistical tests will re-quire conducting many tests, equal to the number of pixels.
For example, an image of size 28 × 28 leads to 784 tests.
For a common 0.05 critical value (probability of Type-1 er-ror), we expect to select 40 (≈ 784 × 0.05) pixels as sig-nificant, purely by chance (number of false positives). This issue escalates for higher resolution images, say 3D medi-cal images. To control a family-wise error rate and avoid inflating the number of false positives, a multiple testing correction (e.g., Bonferroni, Benjamini-Hochberg) [52] is used. However, for high-resolution images common in vi-sion, this tends to over-correct making none of the tests sig-nificant [2, 56], making the analysis less meaningful.
Many testing setups conservatively assume that the pix-els are independent. The classical strategy to avoid this re-strictive assumption leverages Random Field Theory (RFT), as studied in seminal papers by Adler and Worsley [1,2,57].
However, many theoretical results based on RFT remain re-stricted to the Gaussian Random Fields (GRF) and some specific generalizations.
It is not obvious to what extent these assumptions are viable for uncertainty maps obtained from deep neural networks popular in vision.
Contributions. We show how existing DNN tools when instantiated with suitable results from Random Field the-ory provide a mechanism to perform hypothesis tests on uncertainty maps, generated by different probabilistic DNN models common in vision. Specifically, we develop a prob-abilistic framework, based on Neural ODE and Wasserstein distance, which enables learning a diffeomorphism between uncertainty maps and GRFs. We refer to it as Warping Neu-ral ODE. Roughly, this allows performing hypothesis tests on the resultant GRFs and mapping results back to the do-main of uncertainty maps. 2.