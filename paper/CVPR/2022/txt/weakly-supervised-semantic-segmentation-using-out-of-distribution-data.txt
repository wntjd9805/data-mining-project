Abstract
Weakly supervised semantic segmentation (WSSS) meth-ods are often built on pixel-level localization maps obtained from a classiﬁer. However, training on class labels only, classiﬁers suffer from the spurious correlation between fore-ground and background cues (e.g. train and rail), fundamen-tally bounding the performance of WSSS. There have been previous endeavors to address this issue with additional su-pervision. We propose a novel source of information to distin-guish foreground from the background: Out-of-Distribution (OoD) data, or images devoid of foreground object classes.
In particular, we utilize the hard OoDs that the classiﬁer is likely to make false-positive predictions. These samples typi-cally carry key visual features on the background (e.g. rail) that the classiﬁers often confuse as foreground (e.g. train), so these cues let classiﬁers correctly suppress spurious back-ground cues. Acquiring such hard OoDs does not require an extensive amount of annotation efforts; it only incurs a few additional image-level labeling costs on top of the original efforts to collect class labels. We propose a method, W-OoD, for utilizing the hard OoDs. W-OoD achieves state-of-the-art performance on Pascal VOC 2012. The code is available at: https://github.com/naver-ai/w-ood. 1.

Introduction
Pixel-wise labeling is labor-intensive [8]. Lots of re-search have been dedicated to supervising a semantic seg-mentation model with weaker forms of supervision than pixel-wise labelings, such as scribbles [52], points [3, 21], boxes [20, 31, 49], and class labels [27, 30, 33, 56]. We tackle the ﬁnal category in this paper: weakly supervised semantic segmentation (WSSS) with class labels.
WSSS methods utilizing class labels often follow a two-stage process. First, they generate pixel-level pseudo-target from a classiﬁer using CAM variants [46, 63]. Then, they train the main segmentation network using the pseudo-target
*Correspondence to: Sungroh Yoon (sryoon@snu.ac.kr).
Figure 1. (a) Classiﬁers often confuse background cues to be a foreground concept due to spurious correlations (e.g. “rail” for
“train”). (b) Our W-OoD employs hard OoD images as negative samples (e.g. “rail” is not “train”) to resolve the confusion. generated in the ﬁrst stage. Built on image-level labels only, the pseudo-target is known to suffer from the confusion between foreground and background cues. For example, given a database of duck images where ducks are typically waterborne, a classiﬁer erroneously assigns higher scores on patches containing water than those containing ducks’ feet [7, 22, 27, 34, 36, 62]. The same goes for foreground-background pairs like woodpecker-tree, snowmobile-snow, and train-rail. This is a fundamental problem that cannot be solved solely with the class labels; additional information is needed to learn to fully distinguish the foreground and background cues [7, 34, 36].
Researchers have thus sought various sources of addi-tional guidance to separate the foreground and background cues, each with different pros and cons and different labeling-cost footprints. Image saliency [38, 41] is one of the most widely used ones [18, 28, 34, 43, 51, 60], for it naturally pro-vides the prominent foreground object in the image in a class-agnostic fashion. However, saliency is not very effec-tive for non-salient foreground objects (e.g. low-contrast objects or small objects). Low-level visual features like su-perpixels [25, 55], edges [19], object proposals [31, 43, 49],
and optical ﬂows [15,29] have also been considered. Though cost-effective, they tend to generate inaccurate object bound-aries because such low-level information does not consider semantics associated with the class.
In this paper, we propose another source of guidance that provides a distinction between the foreground and back-ground cues. We propose to use the out-of-distribution (OoD) data that do not contain any of the foreground classes of inter-est. Examples include the rail-only images for the foreground class “train”, since classiﬁers often confuse the rail for the train. By subduing the recognition of “train” on such rail cues in hard OoDs, models successfully distinguish such confusing cues.
Obtaining such OoDs does not incur a signiﬁcant amount of additional annotation efforts compared to collecting only the image-level labels. The OoD images are natural by-products of the typical dataset collection procedure. Vision datasets with image-level category labels (e.g. Pascal [10],
COCO [40], LVIS [11], and OpenImages [24]) all start with a pool of candidate images, from which images correspond-ing to one of the foreground classes are selected and included in the ﬁnal dataset. The remaining pool, or the candidate
OoD set, can be utilized as the source of OoD images.
The candidate OoD set cannot be directly used for guid-ing the WSSS method for two reasons. First, general OoD images do not provide informative signals to distinguish dif-ﬁcult background cues from the foreground (e.g. rail from train). Second, it may still contain foreground objects. We address the ﬁrst problem by selecting hard OoDs whereby classiﬁers falsely assign high prediction scores to one of the foreground classes. The second problem is addressed by a human-in-the-loop process where images containing foreground objects are manually pruned. While this requires additional human efforts, we emphasize that the extra cost is negligible. As we will show later (Sec. 4.3.1), we only need a tiny amount of hard OoD samples to improve the localization maps: even 1 hard OoD image per class boosts the localization performance by 2.0%p. Furthermore, the cost for collecting OoD samples is at the same order of mag-nitude as collecting the category labels for the foreground samples, as opposed to collecting e.g. segmentation maps.
One can also re-direct the budget for collecting a few labeled foreground data to collecting a similar number of hard OoD samples to dramatically improve the WSSS performance.
Given the additional guidance provided by OoD samples, we propose W-OoD, a method of training a classiﬁer by utilizing the hard-OoDs. Note that our data collection pro-cedure provides hard OoD samples which have different patterns and semantics in various contexts. One could ignore this diversity and treat every hard OoD as a combined back-ground class; this approach has proved to be sub-optimal by our experiments. Instead, W-OoD considers every hard
OoD sample with a metric-learning objective: increase the distance between the in-distribution and OoD samples in the feature space. This forces the background cues shared by the in-distribution and OoD samples (e.g. rail for train cate-gory) to be excluded from the feature-space representation.
W-OoD results in high-quality localization maps and lead to the new state-of-the-art performance on the Pascal VOC 2012 benchmark for WSSS.
We contribute (1) a new paradigm of utilizing the OoD samples to address the spurious correlations in weakly super-vised semantic segmentation (WSSS); (2) a dataset of hard
OoDs for 20 Pascal categories that will be published upon acceptance; and (3) a WSSS method, W-OoD, that exploits the hard OoDs and achieves the best-known performance on the Pascal VOC 2012 benchmark for WSSS. 2.