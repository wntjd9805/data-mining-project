Abstract
Flow-based generative models have shown an excellent ability to explicitly learn the probability density function of data via a sequence of invertible transformations. Yet, learning attentions in generative ﬂows remains understud-ied, while it has made breakthroughs in other domains. To
ﬁll the gap, this paper introduces two types of invertible at-tention mechanisms, i.e., map-based and transformer-based attentions, for both unconditional and conditional genera-tive ﬂows. The key idea is to exploit a masked scheme of these two attentions to learn long-range data dependencies in the context of generative ﬂows. The masked scheme al-lows for invertible attention modules with tractable Jaco-bian determinants, enabling its seamless integration at any positions of the ﬂow-based models. The proposed attention mechanisms lead to more efﬁcient generative ﬂows, due to their capability of modeling the long-term data dependen-cies. Evaluation on multiple image synthesis tasks shows that the proposed attention ﬂows result in efﬁcient models and compare favorably against the state-of-the-art uncon-ditional and conditional generative ﬂows. 1.

Introduction
Deep generative models have shown their capability to model complex real-world datasets for various applications, such as image synthesis [10, 15, 26, 42, 45], image super-resolution [29, 53], facial manipulation [7, 9, 19, 38], au-tonomous driving [50, 60], and others. The widely studied modern generative models include generative adversarial nets (GANs) [3,15,23,56], variational autoencoders (VAEs)
[26, 36, 46, 55], autoregressive models [47, 48] and ﬂow-based models [10,11,24]. The GAN models implicitly learn the data distribution to produce samples by transforming a noise distribution into the desired space, where the gener-ated data can approximate the real data distribution. On the other hand, VAEs optimize a lower bound on the data’s log-likelihood, leading to a suitable approximation of the actual data distribution. Although these two models have achieved great success, neither provides exact data likelihood.
Figure 1. Conceptual comparison of the proposed AttnFlow against two representative generative ﬂows, i.e., (a) Glow [24] and (b) Flow++ [17]. Based on [24], Flow++ introduces the con-ventional attention mechanism to model short-term dependencies within one split of each feature map in the context of coupling layers. In contrast, the proposed AttnFlow (shown in (c)) further introduces invertible attention mechanisms that can be introduced at any ﬂow positions to learn long-term correlations.
Autoregressive models [12, 47, 48] and ﬂow-based gen-erative models [10,11,24] optimize the exact log-likelihood of real data. Despite autoregressive models’ better perfor-mance on density estimation benchmarks, its sequential property results in non-trivial parallelization. In contrast, the ﬂow-based generative models are conceptually attrac-tive due to tractable log-likelihood, exact latent-variable in-ference, and parallelizability of both training and synthe-sis. Notably, they allow exact inference of the actual data log-likelihood via normalizing ﬂow. As shown in Fig.1(a), the normalizing ﬂow model transforms a simple distribu-tion into a complex one by applying a sequence of in-vertible transformation functions, which leads to an excel-lent mechanism of simultaneous exact log-likelihood opti-mization and latent-variable inference. However, due to ef-ﬁciency constraints in their network designs, most models require several ﬂow layers to approximate non-linear long-range data dependencies to get globally coherent samples.
To overcome this drawback i.e., modeling dependencies ef-ﬁciently over normalizing ﬂows is the key, and presently one of the most sought-after problems [17, 35].
To efﬁciently model data dependencies in the ﬂow-based generative models, one may opt to combine multi-scale au-toregressive priors [35]. By comparison, exploiting atten-tion mechanisms has emerged as a remarkable way to model such dependencies in deep neural networks. It imitates hu-man brain actions of selectively concentrating on a few rel-evant information while ignoring less correlated ones. Tra-ditional self-attention mechanisms like [49, 52, 59] exhibit a good balance between the ability to model range depen-dencies and the computational and statistical efﬁciency. In general, the self-attention modules measure the response at a point as a weighted sum of the features at all points, where the attention weights are computed at a small computational cost. Although [17] recently applied the conventional atten-tion directly as a dependent component in the coupling layer (Fig.1(b)), it models dependency within a short-range (i.e., one split of each ﬂow feature map) of the coupling layer.
To our knowledge, efﬁcient modeling of data dependencies over normalizing ﬂows is understudied. A natural solution is to exploit new attention mechanisms to learn correlations of the feature maps at any positions of the ﬂow-based mod-els. However, it is generally non-trivial to achieve that goal of exploiting new attention modules as independent ﬂow layers. Concretely, such attentions should maintain the in-vertibility with tractable Jacobian determinants in the ﬂows.
In this paper, we propose invertible attentions for ﬂow (AttnFlow) models to reliably and efﬁciently model net-work data dependencies that can be introduced at any po-sitions of the ﬂow-based models (along the entire ﬂow fea-ture maps, see Fig.1(c)). The key idea is to exploit a masked attention learning scheme to allow for intertible attention learning for normalizing ﬂow based generative models. In addition, the proposed masked attention scheme facilitates tractable Jacobian determinants and hence can be integrated seamlessly into any generative ﬂow models. Particularly, we exploit two different invertible attention mechanisms to encode the various types of correlations respectively on the ﬂow feature maps. The two proposed attention mech-anisms are (i) invertible map-based (iMap) attention that directly models the importance of each position in the at-tention dimension of the ﬂow feature maps, (ii) invertible transformer-based (iTrans) attention that explicitly models the second-order interactions among distant positions in the attention dimension. Since the proposed two invertible at-tention modules explicitly model the dependencies of ﬂow feature maps, it further enhances a ﬂow-based model’s efﬁ-ciency to represent the deep network dependencies. To show the superiority of our approach, we evaluated the introduced attention models in the context of both unconditional and conditional normalizing ﬂow-based generative models for multiple image synthesis tasks. 2.