Abstract
Today’s state of the art visual navigation agents typi-cally consist of large deep learning models trained end to end. Such models offer little to no interpretability about the learned skills or the actions of the agent taken in response to its environment. While past works have explored interpret-ing deep learning models, little attention has been devoted to interpreting embodied AI systems, which often involve reasoning about the structure of the environment, target characteristics and the outcome of one’s actions. In this pa-per, we introduce the Interpretability System for Embodied agEnts (iSEE) for Point Goal and Object Goal navigation agents. We use iSEE to probe the dynamic representations produced by these agents for the presence of information about the agent as well as the environment. We demonstrate interesting insights about navigation agents using iSEE, in-cluding the ability to encode reachable locations (to avoid obstacles), visibility of the target, progress from the initial spawn location as well as the dramatic effect on the behav-iors of agents when we mask out critical individual neurons. 1.

Introduction
The research area of Embodied AI – teaching embodied agents to perceive, communicate, reason and act in their en-vironment – continues to receive a lot of interest from the computer vision, natural language processing and robotics communities. A growing body of work has resulted in the emergence of several powerful and visually rich simulators including AI2-THOR [20], Habitat [25] and iGibson [37]; works that require agents to navigate [2], reason [5], collab-orate [18], manipulate [12] and follow instructions [3].
While fast progress is being made across a variety of tasks and benchmarks, most solutions being employed are black box neural networks trained to either imitate a se-Figure 1. The iSEE framework. (a) An agent learns to perform the OBJECTNAV or POINTNAV tasks. (b) We wish to explore what information is encoded in the hidden representations of the agent. (c) To achieve this, we evaluate how well the agent’s hidden repre-sentation can predict human interpretable concepts e.g. target vis-ibility in ObjectNav. (d) Then we apply an explainablity method
SHAP [23] to identify the top-k relevant units. quence of human/oracle actions or trained via reinforce-ment learning with a careful selection of positive and nega-tive rewards. These models offer little to no interpretability out-of-the-box about the concepts and skills learned by the model or about the actions taken by the model in response to a task or observation. Developing interpretable systems is particularly important in embodied AI since we expect these systems to eventually be deployed onto robots that will nav-igate the real physical world and interact with people in it.
In the image classification literature, a number of inter-pretability methods have been developed over the past few years [7, 14, 28, 49]. These methods rely on probing model activations via various inputs or generating synthetic inputs that lead to a spike in an activation. While such methods are useful in probing Embodied AI models, they do not take into account the rich metadata (such as perfect segmenta-tion, depth maps, precise object localization, etc.) available in synthetic environments commonly used to train these 1
models. Simulated worlds provide us a unique opportunity to expand interpretability research to embodied agents and develop new methods that take advantages of rich metadata.
We propose a framework to interpret the hidden repre-sentations of embodied agents trained in simulated worlds.
We apply our framework to two navigation tasks (Fig-ure 1a): Object Navigation (OBJECTNAV) [6], the task of navigating to a target object and Point Goal Navigation (POINTNAV) [2], the task of navigating to a specified rela-tive co-ordinate, within the AI2THOR environment; but our methods are general and can be easily applied to more tasks and other environments. We train agents to perform these tasks and then probe their hidden representations to evaluate if they encode aspects of their task, progress and surround-ings (Figure 1b and 1c). We then apply the model interpre-tation method SHAP [23] to identify which hidden units are most relevant for predicting these concepts (Figure 1d). Our framework allows us to gather evidence towards answering two fundamental questions about a trained model: (1) Has the model learned a particular concept ? (2) Which units within a recurrent layer encode this concept ? Using this framework, we were able to find several interesting insights about OBJECTNAV and POINTNAV agents.
The key contributions of this work are:
• A new interpretability framework specialized for nav-igation agents with no linearity assumptions between concepts and hidden units.
• New insights about what navigation agents encode and in which units: – sparse target representation in OBJECTNAV (50/512 units) and POINTNAV (5/512 units); – learning of concepts such as reachable locations and visit history by OBJECTNAV agents; encod-ing of progress towards target and less reliance on visual information by POINTNAV agents.
• Ablation experiments showing no impact on model performance after removal of 10% units suggesting re-dundancy in the representation. 2.