Abstract
Optical sensors and learning algorithms for autonomous vehicles have dramatically advanced in the past few years.
Nonetheless, the reliability of today’s autonomous vehicles is hindered by the limited line-of-sight sensing capability and the brittleness of data-driven methods in handling ex-treme situations. With recent developments of telecommuni-cation technologies, cooperative perception with vehicle-to-vehicle communications has become a promising paradigm to enhance autonomous driving in dangerous or emer-gency situations. We introduce COOPERNAUT, an end-to-end learning model that uses cross-vehicle perception for vision-based cooperative driving. Our model encodes Li-DAR information into compact point-based representations that can be transmitted as messages between vehicles via realistic wireless channels. To evaluate our model, we de-velop AUTOCASTSIM, a network-augmented driving sim-ulation framework with example accident-prone scenarios.
Our experiments on AUTOCASTSIM suggest that our coop-erative perception driving models lead to a 40% improve-ment in average success rate over egocentric driving mod-els in these challenging driving situations and a 5× smaller bandwidth requirement than prior work V2VNet. COOPER-NAUT and AUTOCASTSIM are available at https:// ut-austin-rpl.github.io/Coopernaut/. 1.

Introduction
The widespread deployment of autonomous driving and advanced driver assistance systems is challenged by safety concerns. While deep learning has improved autonomy stacks with data-driven techniques [9, 10, 42], learning-based driving policies to date are still brittle, especially in the face of extreme situations and corner cases that one might encounter only a few times every million miles of
∗
Equal contribution. Correspondence: cuijiaxun@utexas.edu, hangqiu@stanford.edu, yukez@cs.texas.edu
Figure 1. COOPERNAUT enables vehicles to communicate critical information beyond occlusion and sensing range for vision-based driving. The blue dashed arrows are information sharing flows.
Through cooperative perception, COOPERNAUT makes more in-formed driving decisions when line-of-sight sensing is limited. driving [8]. The lack of robustness of learning algorithms is exacerbated by the limited sensing capabilities of op-tical sensors on individual vehicles, such as stereo cam-eras and LiDAR, that are confined to line-of-sight sensing and unreliable in bad weather conditions. With the advent of new telecommunication technologies, such as 5G net-works and vehicle-to-vehicle (V2V) communications, co-operative perception [6, 13, 21, 32] is becoming a promis-ing paradigm that enables sensor information to be shared between vehicles (and roadside devices [45]) in real-time.
The shared information can augment the field of view of in-dividual vehicles and convey the intents and path plans of nearby vehicles, offering the potential to improve driving safety, particularly in accident-prone scenarios.
Ideally, learning autonomous driving policies with coop-erative perception should take advantage of existing deep learning methods customized for ego perception [11, 16, 29] by considering the combined sensory data from all ve-hicles as an augmented version of on-board sensing.
In practice, the efficacy of cooperative perception hinges on what data to transmit within the limited network bandwidth and how to use the aggregated information to build a coher-ent and accurate understanding of traffic situations. Recent work on cooperative driving has demonstrated the benefit of cross-vehicle perception for augmenting sensing capa-bilities and driving decisions [6, 21]. Nonetheless, these methods have abstracted away raw sensory data with low-dimensional meta-data. Prior work introduced 3D sensor fusion (AVR [32], Cooper [13]) and representation fusion (V2VNet [41]) algorithms that aggregate perception results from nearby vehicles via V2V channels. They focused on 3D detection and motion forecasting on static datasets, rather than interactive driving policies.
We introduce COOPERNAUT, an end-to-end coopera-tive driving model for networked vehicles. COOPERNAUT learns to fuse encoded LiDAR information shared by nearby vehicles under realistic V2V channel capacity. To commu-nicate meaningful scene information from nearby vehicles while conforming to bandwidth limits, we design our driv-ing policy architecture based on the Point Transformer [46], a self-attention network for point cloud processing. This architecture pre-processes the raw point cloud, on each networked vehicle locally, into spatial-aware neural repre-sentations. These representations are compact, which can be efficiently transmitted over realistic wireless channels.
Meanwhile, they are physically grounded, thus can be spa-tially transformed and aggregated with ego representations.
The entire architecture is end-to-end differentiable, permit-ting control supervision (imitating an oracle planner with access to privileged information) to flow back to the per-ception stack, thus ensuring the learned representations and messages contain task-relevant information.
To examine the effectiveness of COOPERNAUT, we develop a CARLA-based simulation framework, AUTO-CASTSIM, where we designed three accident-prone sce-narios. All the scenarios are designed to be challenging for ego perception to fully comprehend the traffic situa-tion. AUTOCASTSIM has a built-in networking simulation for customizable multi-vehicle communications and an ex-pert driving model with privileged information. We evaluate
COOPERNAUT with voxel-based baselines [41] and differ-ent sensor fusion schemes.
In summary, our main contributions are as follows:
• We introduce COOPERNAUT, an end-to-end driving model with cooperative perception via V2V channels.
Our model learns compact representations for commu-nication that can be easily harnessed by the ego vehicle to improve its driving decisions.
• We develop a network-augmented autonomous driv-ing simulation framework AUTOCASTSIM to evaluate
COOPERNAUT and baselines in accident-prone scenar-ios and to promote future research on vision-based co-operative perception.
• Our results show that COOPERNAUT substantially re-duces safety hazards for line-of-sight sensing. Its de-sign improves both driving performance and commu-nication efficiency over baselines. 2.