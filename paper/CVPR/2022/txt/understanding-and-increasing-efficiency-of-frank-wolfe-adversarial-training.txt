Abstract 1.

Introduction
Deep neural networks are easily fooled by small pertur-bations known as adversarial attacks. Adversarial Training (AT) is a technique that approximately solves a robust op-timization problem to minimize the worst-case loss and is widely regarded as the most effective defense against such attacks. Due to the high computation time for generating strong adversarial examples in the AT process, single-step approaches have been proposed to reduce training time.
However, these methods suffer from catastrophic overfitting where adversarial accuracy drops during training, and al-though improvements have been proposed, they increase training time and robustness is far from that of multi-step AT.
We develop a theoretical framework for adversarial training with FW optimization (FW-AT) that reveals a geometric con-nection between the loss landscape and the distortion of l-inf
FW attacks (the attack’s l-2 norm). Specifically, we analyti-cally show that high distortion of FW attacks is equivalent to small gradient variation along the attack path. It is then ex-perimentally demonstrated on various deep neural network architectures that l-inf attacks against robust models achieve near maximal l-2 distortion, while standard networks have lower distortion. Furthermore, it is experimentally shown that catastrophic overfitting is strongly correlated with low distortion of FW attacks. This mathematical transparency differentiates FW from the more popular Projected Gradi-ent Descent (PGD) optimization. To demonstrate the utility of our theoretical framework we develop FW-AT-Adapt, a novel adversarial training algorithm which uses a simple distortion measure to adapt the number of attack steps dur-ing training to increase efficiency without compromising robustness. FW-AT-Adapt provides training time on par with single-step fast AT methods and improves closing the gap between fast AT methods and multi-step PGD-AT with mini-mal loss in adversarial accuracy in white-box and black-box settings.
*Equal contributions.
Deep neural networks (DNN) achieve excellent perfor-mance across various domains [18]. As these models are deployed across industries (e.g., healthcare or autonomous driving), concerns of robustness and reliability become in-creasingly important. Several organizations have identified important principles of artificial intelligence (AI) that in-clude the notions of reliability and transparency [19, 21, 24].
One issue of large capacity models such as DNNs is that small, carefully chosen input perturbations, known as adver-sarial perturbations, can lead to incorrect predictions [12].
Various enhancement methods have been proposed to de-fend against adversarial perturbations [16, 20, 22, 28]. One of the best performing algorithms is adversarial training (AT) [20], which is formulated as a robust optimization problem [31]. Computation of optimal adversarial perturba-tions is NP-hard [36] and approximate methods are used to solve the inner maximization. The most popular approximate method that has been proven to be successful is projected gra-dient descent (PGD) [10]. Frank-Wolfe (FW) optimization has been recently proposed in [8] and was shown to effec-tively fool standard networks with less distortion, and can be efficiently used to generate sparse counterfactual pertur-bations to explain model predictions and visualize principal class features [27].
Since PGD has proven to be the main algorithm for adver-sarially robust deep learning, reducing its high computational cost without sacrificing performance, i.e. fast adversarial training, is a primary issue. Various methods have been proposed based on using a single PGD step, known as Fast
Gradient Sign Method (FGSM) [37] but fail for large pertur-bations. [37] identified that FGSM-based training achieves some robustness initially during training but robustness dras-tically drops within an epoch, a phenomenon known as catas-trophic overfitting (CO). While some methods have been proposed to ameliorate this problem [2, 14, 30], the training time suffers as a result and/or robustness is not on par with multi-step PGD-AT.
In this paper, we use the Frank-Wolfe optimization to derive a relationship between the ℓ2 norm of ℓ∞ adversarial
√
Figure 1. Low (high) distortion of attacks δ (black) is equivalent to high (low) angular spread of signed gradients gl = ϵ·sgn(∇δL(x+
δl, y)) (all with ∥gl∥2 = ϵ d) computed over K steps along attack path (K = 5 here). Proposition 1 expresses FW adversarial perturbations δ as convex combinations of signed gradients along the attack path. This core concept is quantified in Theorem 1, and forms the basis for the development of adaptive adversarial training algorithm presented in Section 4. perturbations (distortion) and the geometry of the loss land-scape (see Fig. 1). Using this theory and empirical studies we show that this distortion can be used as a signal for CO and propose a fast adversarial training algorithm based on an adaptive Frank-Wolfe adversarial training (FW-AT-ADAPT
) method (see Fig. 2). This method yields training times on par with single step methods without suffering from CO, outperforms numerous single step methods, and begins to close the gap between fast adversarial training methods and multi-step PGD adversarial training.
Our main contributions are summarized below:
• We demonstrate empirically that FW attacks against robust models achieve near-maximal distortion across a variety of network architectures.
• We empirically show that distortion of FW attacks, even with only 2 steps, are strongly correlated with catastrophic overfitting.
• Theoretical guarantees are derived that relate distortion of FW attacks to the gradient variation along the attack path and which imply that high distortion attacks com-puted with several steps result in diminishing increases to the loss.
• Inspired by the connection between distortion and attack path gradient variation, we propose an adap-tive step Frank-Wolfe adversarial training algo-rithm, FW-AT-ADAPT , which achieves superior robustness/training-time tradeoffs compared to single-step AT and closes the gap between such methods and multi-step AT variants when evaluated against strong white- and black-box attacks.
Figure 2. Illustration of concept behind FW-AT-ADAPT training algorithm. At each epoch, distortion d is monitored across the first Bm batches (here Bm = 3). If the average distortion is less than a threshold r, the current number of steps, K, is increased by 2, to K + 2, (and if it is higher than a threshold r, the current number of steps, K, is dropped by a factor of 2, to K/2) for the remaining batches in the epoch. This process is repeated until convergence and reduces the training time of adversarial training without sacrificing robustness. Theorems 2 and 3 provide stability guarantees for robust model weight updates in the high-distortion regime. 2.