Abstract
We propose a method for learning the posture and struc-ture of agents from unlabelled behavioral videos. Start-ing from the observation that behaving agents are gener-ally the main sources of movement in behavioral videos, our method, Behavioral Keypoint Discovery (B-KinD), uses an encoder-decoder architecture with a geometric bottle-neck to reconstruct the spatiotemporal difference between video frames. By focusing only on regions of movement, our approach works directly on input videos without requir-ing manual annotations. Experiments on a variety of agent types (mouse, fly, human, jellyfish, and trees) demonstrate the generality of our approach and reveal that our dis-covered keypoints represent semantically meaningful body parts, which achieve state-of-the-art performance on key-point regression among self-supervised methods. Addition-ally, B-KinD achieve comparable performance to super-*Equal contribution. Correspondence to jjsun@caltech.edu.
â€ Current affiliation: Samsung Advanced Institute of Technology vised keypoints on downstream tasks, such as behavior clas-sification, suggesting that our method can dramatically re-duce model training costs vis-a-vis supervised methods. 1.

Introduction
Automatic recognition of object structure, for example in the form of keypoints and skeletons, enables models to capture the essence of the geometry and movements of ob-jects. Such structural representations are more invariant to background, lighting, and other nuisance variables and are much lower-dimensional than raw pixel values, mak-ing them good intermediates for downstream tasks, such as behavior classification [4, 11, 15, 39, 43], video align-ment [26, 44], and physics-based modeling [7, 12].
However, obtaining annotations to train supervised pose detectors can be expensive, especially for applications in in behavioral neuro-For example, behavior analysis. science [34], datasets are typically small and lab-specific, and the training of a custom supervised keypoint detector
presents a significant bottleneck in terms of cost and ef-fort. Additionally, once trained, supervised detectors often do not generalize well to new agents with different struc-tures without new supervision. The goal of our work is to enable keypoint discovery on new videos without manual supervision, in order to facilitate behavior analysis on novel settings and different agents.
Recent unsupervised/self-supervised methods have made great progress in keypoint discovery [20, 21, 51] these methods are generally (see also Section 2), but not designed for behavioral videos.
In particular, exist-ing methods do not address the case of multiple and/or non-centered agents, and often require inputs as cropped bounding boxes around the object of interest, which would require an additional detector module to run on real-world videos. Furthermore, these methods do not exploit relevant structural properties in behavioral videos (e.g., the camera and the background are typically stationary, as observed in many real-world behavioral datasets [5, 15, 22, 29, 34, 39]).
To address these challenges, the key to our approach is to discover keypoints based on reconstructing the spatiotem-poral difference between video frames. Inspired by previ-ous works based on image reconstruction [20, 37], we use an encoder-decoder setup to encode input frames into a ge-ometric bottleneck, and train the model for reconstruction.
We then use spatiotemporal difference as a novel recon-struction target for keypoint discovery, instead of single im-age reconstruction. Our method enables the model to focus on discovering keypoints on the behaving agents, which are generally the only source of motion in behavioral videos.
Our self-supervised approach, Behavioral Keypoint
Discovery (B-KinD), works without manual supervision across diverse organisms (Figure 1). Results show that our discovered keypoints achieve state-of-the-art performance on downstream tasks among other self-supervised keypoint discovery methods. We demonstrate the performance of our keypoints on behavior classification [42], keypoint re-gression [20], and physics-based modeling [7]. Thus, our method has the potential for transformative impact in be-havior analysis: first, one may discover keypoints from be-havioral videos for new settings and organisms; second, un-like methods that predict behavior directly from video, our low-dimensional keypoints are semantically meaningful so that users can directly compute behavioral features; finally, our method can be applied to videos without the need for manual annotations.
To summarize, our main contributions are: 1. Self-supervised method for discovering keypoints from real-world behavioral videos, based on spatiotempo-ral difference reconstruction. 2. Experiments across a range of organisms (mice, flies, human, jellyfish, and tree) demonstrating the generality of the method and showing that the discovered keypoints are semantically meaningful. 3. Quantitative benchmarking on downstream behavior analysis tasks showing performance that is comparable to supervised keypoints. 2.