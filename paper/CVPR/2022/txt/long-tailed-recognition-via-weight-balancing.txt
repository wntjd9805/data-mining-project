Abstract
In the real open world, data tends to follow long-tailed class distributions, motivating the well-studied long-tailed recognition (LTR) problem. Naive training produces models that are biased toward common classes in terms of higher accuracy. The key to addressing LTR is to balance vari-ous aspects including data distribution, training losses, and gradients in learning. We explore an orthogonal direction, weight balancing, motivated by the empirical observation that the naively trained classiﬁer has “artiﬁcially” larger weights in norm for common classes (because there ex-ists abundant data to train them, unlike the rare classes).
We investigate three techniques to balance weights, L2-normalization, weight decay, and MaxNorm. We ﬁrst point out that L2-normalization “perfectly” balances per-class weights to be unit norm, but such a hard constraint might prevent classes from learning better classiﬁers. In contrast, weight decay penalizes larger weights more heavily and so learns small balanced weights; the MaxNorm constraint en-courages growing small weights within a norm ball but caps all the weights by the radius. Our extensive study shows that both help learn balanced weights and greatly improve the LTR accuracy. Surprisingly, weight decay, although un-derexplored in LTR, signiﬁcantly improves over prior work.
Therefore, we adopt a two-stage training paradigm and pro-pose a simple approach to LTR: (1) learning features using the cross-entropy loss by tuning weight decay, and (2) learn-ing classiﬁers using class-balanced loss by tuning weight decay and MaxNorm. Our approach achieves the state-of-the-art accuracy on ﬁve standard benchmarks, serving as a future baseline for long-tailed recognition. 1
1.

Introduction
In the real open world, data tends to follow long-tailed distributions [8, 60, 84, 85]. Through the lens of classi-ﬁcation, this means that the number of per-class data, or class cardinality, is heavily imbalanced [27, 72]. Numer-ous applications emphasize the rare classes. For exam-ple, autonomous vehicles should recognize not only com-mon objects such as cars and pedestrians, but also rare ones like strollers and animals for driving safety [41]. A bio-image analysis system should recognize both commonly-and rarely-seen species for ecological research [63, 72].
This motivates the well-studied problem of long-tailed recognition (LTR), which trains on class-imbalanced data and aims to achieve high accuracy averaged across all the classes [84]. LTR has attracted increasing attention espe-cially using deep neural networks [12, 38, 78].
Status quo. Because common classes have signiﬁcantly more training data than rare classes, they dominate the train-ing loss, contribute the most of gradients, and obtain high accuracy [84]. Consequently, a naively trained model per-forms well on them but signiﬁcantly worse on the rare classes (Fig. 1a). The key to addressing LTR is to bal-ance various aspects. Many methods propose to balance per-class data distributions during training by upsampling rare classes or downsampling common classes [14, 22, 23].
Some others balance the losses or gradients during train-ing [12, 19, 40, 71]. Some approaches adopt transfer learn-ing that learn features on common classes and use the fea-tures to learn rare-class classiﬁers [37, 49, 74, 87]. It shows that decoupling feature learning and classiﬁer learning leads to signiﬁcant improvement over models that train them jointly [38]. From benchmarking results, the state-of-the-art accuracy is achieved by either ensembling expert mod-els [10,23,26,73,76] or the adoption of self-supervised pre-training with aggressive data augmentation techniques [17].
Motivation. We observe that a naively trained model on long-tailed class distributed data has “artiﬁcially” large weights for common classes (Fig 1b). Prior work also notes this observation [38]. Intuitively, this is because com-mon classes have more training data that signiﬁcantly grows classiﬁer weights (Fig. 2a). This motivates our work to bal-ance network weights across classes for long-tailed recog-nition. In contrast to existing methods (as exhaustively re-viewed in a recent survey paper [84]), our work explores an orthogonal direction of weight balancing.
Contribution. To balance network weights in norm, we study three simple techniques. We ﬁrst point out that L2-normalization perfectly balances classiﬁer weights to have unit norm (Fig. 2b). However, L2-normalization might be too strict to learn ﬂexible parameters for better classiﬁers.
We then study weight decay [29,44] and the MaxNorm con-straint [35,66]. Weight decay penalizes larger weights more heavily and so learns small balanced weights (Fig. 2c);
MaxNorm encourages growing small weights within a norm ball and caps all the weights by the radius (Fig. 2d). We ﬁnd that both effectively learn balanced weights and boost LTR performance, although these well-known regularizers are underexplored in the LTR literature. Please refer to Fig. 1 for a nutshell of our work.
Key Findings. We show how simple regularizers boost
LTR performance. Without inventing new losses or adopt-ing aggressive augmentation techniques or designing new network modules, we follow the simple two-stage training paradigm [38] and derive a simple approach that rivals or outperforms the state-of-the-art methods: (1) train a back-bone using the standard cross-entropy loss by properly tun-ing weight decay, and (2) train the classiﬁer using a class-balanced loss by tuning weight decay and MaxNorm. It is important to note how our simple approach challenges the increasingly complicated LTR models, and hence serves as a strong future baseline for LTR. 2.