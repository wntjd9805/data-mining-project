Abstract
Hands are often severely occluded by objects, which makes 3D hand mesh estimation challenging. Previous works often have disregarded information at occluded re-gions. However, we argue that occluded regions have strong correlations with hands so that they can provide highly ben-eficial information for complete 3D hand mesh estimation.
Thus, in this work, we propose a novel 3D hand mesh es-timation network HandOccNet, that can fully exploits the information at occluded regions as a secondary means to enhance image features and make it much richer. To this end, we design two successive Transformer-based mod-ules, called feature injecting transformer (FIT) and self-enhancing transformer (SET). FIT injects hand information into occluded region by considering their correlation. SET refines the output of FIT by using a self-attention mecha-nism. By injecting the hand information to the occluded region, our HandOccNet reaches the state-of-the-art per-formance on 3D hand mesh benchmarks that contain chal-lenging hand-object occlusions. The codes are available in: https://github.com/namepllet/HandOccNet. 1.

Introduction
Despite promising results of 3D hand mesh estimation from a single RGB image [6, 12, 20, 26–30], making 3D hand mesh estimation method robust to occlusion is still an open challenge. One promising approach for the occlusion-robust system is using a spatial attention mechanism. Al-though the spatial attention mechanism has not been used for the occlusion-robust 3D hand mesh estimation, several 2D human body pose estimation methods [8, 39, 40] have utilized such attention mechanism for the occlusion-robust results. They estimate a spatial attention map and multiply it with a feature map to tell the networks where to focus.
The attention map tends to have high scores on human re-gions and low scores on occluded regions. Therefore, it at-tenuates the magnitude of features at occluded regions and
∗ Authors contributed equally.
Figure 1. Example of the operation of the proposed HandOcc-Net. (a) The output feature map of spatial attention mechanism for the case of severe occlusion, which consists of sparse primary and secondary features . (b) Our feature injection module finds the primary features related to the secondary features, and then injects the information of primary features into the locations of secondary features. makes networks focus on human regions.
Although the spatial attention-based methods have shown noticeable results under the occlusions, there are sev-eral limitations. First, they are mostly for the 2D human body pose estimation, which aims to localize 2D body joint coordinates. Hence, the validity of their spatial attention mechanism is not proved for the occlusion-robust 3D hand mesh estimation. In particular, as hands have quite compli-cated articulations and are often severely occluded by ob-jects, the widely used spatial attention mechanism might fail to produce robust results. Unlike the methods [24, 25] using a depth map, additional depth ambiguity, which arises from 2D image-to-3D hand estimation, is another bottle-neck. Second, when the occlusions are severe, activations of the spatial attention mechanism become sparse because most of the hand regions are occluded. The sparse regions contain limited information of hand; hence, relying only on
such limited information can lead to erroneous results.
To overcome the above limitations, we propose Han-dOccNet, a novel framework for occlusion-robust 3D hand mesh estimation. The main component of the proposed
HandOccNet is a feature injection mechanism, shown in
Figure 1. The conventional spatial attention mechanism disregards the information of features at the occluded re-gions. On the other hand, our feature injection mechanism utilizes those features as a secondary role to obtain richer representation for occlusion-robust 3D hand mesh estima-tion. The primary features and secondary features repre-sent features corresponding to high attention scores and low attention scores, respectively. We leverage information of secondary features to find relevant primary features and in-ject the information of primary features into the locations of secondary features. In this process, we use the term inject to emphasize that the information of secondary features dis-appears and the information of primary features is injected into empty locations.
To inject not only nearby features but also distant fea-tures, we employ Transformer [35], which has an excel-lent ability to model correlations between features regard-less of the distance between features. Here, the distance between features represents the 2D distance in the pixel space. We build two Transformer-based modules, feature injecting transformer (FIT) and self-enhancing transformer (SET). The FIT injects the information of primary features into the regions of the secondary features and outputs a sin-gle feature map by utilizing secondary features as queries and primary features as key-value pairs. The SET utilizes a standard self-attention mechanism to refine the output of the FIT.
Our FIT has two distinctive points compared to the stan-dard Transformer [35] for the feature injection. First, our
FIT computes a correlation map between queries and keys through two types of attention modules, sigmoid-based as well as softmax-based ones, while the standard Transformer uses only softmax-based one. The softmax-based attention module normalizes the multiplications of each query and all elements of the keys using softmax function. As soft-max considers all elements for the normalization, an un-desirable high correlation score can be made when abso-lute values of all the multiplications are very low but some multiplications are relatively large compared to others. To prevent such undesirable high correlation scores, we build an additional sigmoid-based attention module. As the sig-moid activation function does not consider other elements for the normalization, it can avoid the undesired high cor-relations. We obtain the final correlation map by multiply-ing correlation maps from the softmax-based module and sigmoid-based module. Second, we remove a residual con-nection between input queries and output of the attention module, while the standard Transformer uses such residual connection. In other words, the FIT uses queries only when computing correlations between queries and keys and the output feature of FIT does not contain the information of the queries. This is because we intend secondary features (queries) to be replaced with primary features (values).
We demonstrate the effectiveness of our HandOcc-Net, through extensive experiments on recently published hand-object interaction datasets, such as HO-3D [13] and
FPHA [11]. These datasets contain various and challeng-ing occlusions in hand regions which reflects realistic occlu-sions that occur when hands manipulate objects in our daily life. The experimental results show that our HandOccNet achieves significantly better 3D hand mesh estimation ac-curacy compared to previous state-of-the-art 3D hand mesh estimation methods.
To summarize, we make the following contributions:
• We propose a HandOccNet, a novel framework for occlusion-robust 3D hand mesh estimation from a sin-gle RGB image. The proposed HandOccNet utilizes feature injection mechanism that makes feature map robust to occlusion by properly injecting the hand in-formation into the occluded regions.
• For the feature injection and refinement, we propose two Transformer-based modules, FIT and SET. The
FIT performs the injection mechanism under the guid-ance of correlations between primary features and sec-ondary features, which represent features of hand re-gions and occluded regions, respectively. The SET re-fines the output feature map of the FIT using a self-attention mechanism.
• We show our framework significantly outperforms state-of-the-art 3D hand mesh estimation methods on hand-object interaction datasets that contain severe hand occlusions. 2.