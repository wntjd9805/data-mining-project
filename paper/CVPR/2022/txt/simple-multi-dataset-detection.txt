Abstract
How do we build a general and broad object detec-tion system? We use all labels of all concepts ever an-notated. These labels span diverse datasets with poten-tially inconsistent taxonomies.
In this paper, we present a simple method for training a uniﬁed detector on multi-ple large-scale datasets. We use dataset-speciﬁc training protocols and losses, but share a common detection archi-tecture with dataset-speciﬁc outputs. We show how to au-tomatically integrate these dataset-speciﬁc outputs into a common semantic taxonomy.
In contrast to prior work, our approach does not require manual taxonomy reconcilia-tion. Experiments show our learned taxonomy outperforms a expert-designed taxonomy in all datasets. Our multi-dataset detector performs as well as dataset-speciﬁc models on each training domain, and can generalize to new unseen dataset without ﬁne-tuning on them. Code is available at https://github.com/xingyizhou/UniDet. 1.

Introduction
Computer vision aims to produce broad, general-purpose perception systems that work in the wild. Yet object de-tection is fragmented into datasets [18, 22, 24, 33] and our models are locked into the corresponding domains.
This fragmentation brought rapid progress in object detec-tion [5, 10, 20, 31, 39, 45] and instance segmentation [14], but comes with a drawback. Single datasets are limited in both image domains and label vocabularies and do not yield general-purpose recognition systems. Can we allevi-ate these limitations by unifying diverse detection datasets?
In this paper, we ﬁrst make training an object detector on a collection of disparate datasets as straightforward as train-ing on a single one. Different datasets are usually trained under different training losses, data sampling strategies, and schedules. We show that we can train a single detector with separate outputs for each dataset, and apply dataset-speciﬁc supervision to each. Our training mimics training parallel dataset-speciﬁc models with a common network. As a re-sult, our single detector takes full advantages of all training data, performs well on training domains, and generalizes
Figure 1. Different datasets span diverse semantic and visual do-mains. We learn to unify the label spaces of multiple datasets and train a single object detector that generalizes across datasets. better to new unseen domains. However, this detector pro-duces duplicate outputs for classes that occur in multiple datasets.
A core challenge is integrating different datasets into a common taxonomy, and training a detector that reasons about general objects instead of dataset-speciﬁc classes.
Traditional approaches create this taxonomy by hand [19, 47], which is both time-consuming and error-prone. We present a fully automatic way to unify the output space of a multi-dataset detection system using visual data only. We use the fact that object detectors for similar concepts from different datasets ﬁre on similar novel objects. This allows us to deﬁne the cost of merging concepts across datasets, and optimize for a common taxonomy fully automatically.
Our optimization jointly ﬁnds a uniﬁed taxonomy, a map-ping from this taxonomy to each dataset, and a detector over the uniﬁed taxonomy using a novel 0-1 integer pro-gramming formulation. An object detector trained on this uniﬁed taxonomy has a large, automatically constructed vo-cabulary of concepts from all training datasets.
We evaluate our uniﬁed object detector at an unprece-dented scale. We train a uniﬁed detector on 3 large and di-verse datasets: COCO [22], Objects365 [33], and OpenIm-ages [18]. For the ﬁrst time, we show that a single detector performs as well as dataset-speciﬁc models on each indi-vidual dataset. A uniﬁed taxonomy further improves this detector. Crucially, we show that models trained on diverse training sets generalize to new domains without retraining, and outperform single-dataset models. 2.