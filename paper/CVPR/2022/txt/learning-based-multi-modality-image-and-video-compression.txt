Abstract
Multi-modality (i.e., multi-sensor) data is widely used in various vision tasks for more accurate or robust percep-tion. However, the increased data modalities bring new challenges for data storage and transmission. The exist-ing data compression approaches usually adopt individual codecs for each modality without considering the corre-lation between different modalities. This work proposes a multi-modality compression framework for infrared and visible image pairs by exploiting the cross-modality redun-dancy. Specifically, given the image in the reference modal-ity (e.g., the infrared image), we use the channel-wise align-ment module to produce the aligned features based on the affine transform. Then the aligned feature is used as the context information for compressing the image in the cur-rent modality (e.g., the visible image), and the correspond-ing affine coefficients are losslessly compressed at negligi-ble cost. Furthermore, we introduce the Transformer-based spatial alignment module to exploit the correlation between the intermediate features in the decoding procedures for dif-ferent modalities. Our framework is very flexible and eas-ily extended for multi-modality video compression. Experi-mental results show our proposed framework outperforms the traditional and learning-based single modality com-pression methods on the FLIR and KAIST datasets. 1.

Introduction
In several practical vision applications (e.g., autonomous driving), cameras from different modalities such as visible or infrared imaging cameras are often jointly used for var-ious computer vision tasks by exploiting the complemen-tary characteristic. For example, the visible (RGB) cameras can often provide continuous, high-resolution color images but may not work well for extreme-low lighting scenarios, which is precisely what infrared cameras can help. At the same time, infrared cameras are easily disturbed by abnor-mal heat sources, but the drawback can be compensated by using visible cameras. However, these multiple modalities
Jing Geng is the corresponding author.
Figure 1. The comparison between video compression, stereo compression and multi-modality compression.
Our multi-modality compression approach uses the cross-modality infrared image to facilitate the compression of visible image. visual analysis approaches [10, 13, 25, 26, 47, 48] will in-crease the storage and transmission costs as more images from different modalities are transmitted to the decoder side for visual analysis. Therefore, how to design an efficient compression method for multi-modality visual data is a new and challenging research problem.
In the past decades, a lot of traditional and learning-based compression methods [1,3,5,6,9,30,32,34,36–38,42] have been proposed for image or video compression. How-ever, most existing works focused on single-modality image compression without considering the correlation between different modalities. Due to the strong correlation between the images from different modalities, we cannot use the existing single-modality compression methods to fully ex-ploit the compression redundancy. One of the most related research topics is stereo image compression, where cross-view redundancy is exploited by using various view align-ment approaches. However, compared with the stereo im-ages that share similar distribution, the intensity of different modality images may be quite different (see Fig. 1). There-fore, the commonly used alignment techniques like block-based motion/disparity estimation [19] or homograph trans-form [14] are not feasible enough for multi-modality com-pression. Moreover, considering that the multi-modality data like infrared and visible image pairs represent the same scene in different perspectives, the compression for pixel-wise motion/disparity information from most existing esti-mation approaches will consume a large number of bits for
compression, which is too expensive. Therefore, it is non-trivial to develop a new framework for multi-modality data compression.
In this paper, we propose a learning-based multi-modality compression framework for the infrared and vis-ible image pairs by exploiting the cross-modality redun-dancy in the feature space. Considering the explicit align-ment of different modalities is very difficult and estimated motion/disparity information also requires a lot of transmis-sion bitrates, we use the efficient affine transform and atten-tion mechanism to achieve channel-wise and spatial-wise feature alignment, respectively. Specifically, take the com-pression procedure of visible image (i.e., RGB image) as an example, based on the extracted features from the decoded infrared and the original visible images, the affine transfor-mation coefficients are estimated, which can be transmitted to the decoder side at marginal bandwidth cost. Then we achieve the channel-wise feature alignment based on the affine transform, and the corresponding transformed fea-tures from the infrared modality are used as the conditional context for compressing the visible image. Furthermore, we leverage the correlation of intermediate features from different modalities in the decoding procedure through the spatial-wise alignment module. Our module is integrated into the visible image decoder and will spatially warp the intermediate features from the reference modality to gener-ate the aligned feature, which is used to further reduce the cross-modality redundancy.
The proposed framework is very flexible, and the im-age from one modality can be easily used as the reference for image compression from another modality. And it can also be easily extended for multi-modality video compres-sion. Experimental results show that the proposed method achieves better compression performance on several bench-mark datasets when compared with the single-modality im-age and video compression approaches. The contributions of our framework are summarized as follows,
• We propose a learning-based framework to compress image pairs from different modalities by exploiting the cross-modality redundancy. As far as we know, it is the first end-to-end optimized framework to compress visible-infrared image pairs.
• Our framework introduces the channel-wise and spatial-wise alignment modules to effectively exploit the correla-tions between different modalities in the feature space.
• The proposed framework is very flexible and can be ex-tended for multi-modality video compression. Exper-imental results on several datasets demonstrate the ef-fectiveness of the proposed multi-modality image/video compression framework. 2.