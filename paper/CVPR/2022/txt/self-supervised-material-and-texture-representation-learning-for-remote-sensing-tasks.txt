Abstract
Self-supervised learning aims to learn image feature rep-resentations without the usage of manually annotated la-bels. It is often used as a precursor step to obtain useful ini-tial network weights which contribute to faster convergence and superior performance of downstream tasks. While self-supervision allows one to reduce the domain gap between supervised and unsupervised learning without the usage of labels, the self-supervised objective still requires a strong inductive bias to downstream tasks for effective transfer learning. In this work, we present our material and texture based self-supervision method named MATTER (MATerial and TExture Representation Learning), which is inspired by classical material and texture methods. Material and texture can effectively describe any surface, including its tactile properties, color, and specularity. By extension, ef-fective representation of material and texture can describe other semantic classes strongly associated with said mate-rial and texture. MATTER leverages multi-temporal, spa-tially aligned remote sensing imagery over unchanged re-gions to learn invariance to illumination and viewing angle as a mechanism to achieve consistency of material and tex-ture representation. We show that our self-supervision pre-training method allows for up to 24.22% and 6.33% per-formance increase in unsupervised and fine-tuned setups, and up to 76% faster convergence on change detection, land cover classification, and semantic segmentation tasks. Code and dataset: https://github.com/periakiva/
MATTER. 1.

Introduction
Automated understanding of remote sensing imagery has been a long standing goal of the computer vision commu-nity.
Its broad applicability has driven research and de-velopment in construction phase detection [23], infrastruc-ture mapping [36, 55, 71, 100], land use monitoring [41], post natural disaster damage assessment [42, 89, 97], urban 3D reconstruction [39, 57], population migration predic-tion [19], and climate change tracking [79]. Most of those methods require some degree of annotation effort, which is often expensive and/or time consuming. Satellite imagery is increasingly plentiful and accessible, with hundreds of satellites collecting images on a daily basis [1, 35, 81, 94].
However, annotating land cover, change, or similar labels often requires domain knowledge and/or extreme attention to detail, as labels in remote sensing imagery cover more numerous and smaller objects seen from unfamiliar view points. As a result, annotators require more domain exper-tise compared to standard benchmark datasets such as Pas-cal VOC [38], COCO [61], or similar.
Recent work in self-supervised learning aims to allevi-ate the requirement of labeled data by either detecting self-applied transformations, such as color or rotation change, or implicit metadata information, such as temporal order or geographical location. Those objectives are often achieved using contrastive learning methods [17, 45, 53], in which the distance between feature representations of original and transformed images is minimized. More advanced con-trastive methods use triplet loss [10, 84] or quadruplet loss
[18] which also include negative examples with which the distance between feature representations is maximized. De-spite filling a significant need in the remote sensing do-main, these approaches have been yet to be thoroughly investigated. Even methods that utilize contrastive ap-proaches, such as SeCo [68] and the work of Ayush et al.
[4], which learn seasonal change invariance or geographic-location consistency, still show weaker transfer-ability to downstream task learning, as demonstrated by inferior per-formance and convergence speeds shown in Tab. 1, 3.
Instead, we hypothesize that material and texture have a strong inductive bias to most downstream remote sensing tasks, with pre-training of surface representation to improve performance and convergence speeds (measured in epochs) for those tasks. Consider the task of change detection in re-mote sensing imagery: when semantic class changes (i.e., soil to building, or forest to soil), change can also be ex-pected in materials and texture, demonstrating the high cor-relation between material and texture and the change detec-tion task. We show the effectiveness of our self-supervised pre-trained features in both raw and fine-tuned forms, ob-taining state-of-the-art (SOTA) performance in change de-tection (unsupervised and fine-tuned), land cover segmenta-tion (fine-tuned), and land cover classification (fine-tuned).
Here, we propose a novel self-supervised material and texture representation learning method which is inspired by classical and modern texton filter banks [58, 87, 113]. Tex-tons [52, 58, 66] refer to the description of micro-structures in images often used to describe material and texture con-sistency [25, 27, 58, 101, 108]. Note that literature has only loosely defined what material, structures, texture, and sur-face refer to. Here, we define material as any single or combination of elements (soil, concrete, vegetation, etc.) corresponding to some multi-spectral signature, structures as gradients in intensity, texture as spatial distribution of structures, and surface as the combination of material and texture. Note that here we define the physical surface, rather than the geometric or algebraic surface, as described by its material and textural properties. By extension, we aim to jointly describe combinations of materials and textures in a single objective. For example, within a given image patch, a mixture of grass and concrete should be repre-sented differently than patches with grass or concrete sep-arately.
In this example, the grass-concrete mixture may be associated to both grass and concrete material classes.
To that end, we learn surface representations that describe the affinity, represented as residuals [48], to all pre-defined surface classes, represented as clusters. We achieve this by contrastively learning the similarity between the residuals of multi-temporal, spatially aligned imagery of unchanged regions to obtain consistent material and texture represen-tations, regardless of illumination or viewing angle. This framework acts as a pre-training stage for downstream re-mote sensing tasks.
Overall, our contributions are: 1) We present a novel ma-terial and texture based approach for self-supervised pre-training to generate features with high inductive bias for downstream remote sensing tasks. We propose a texture refinement network to amplify low level features and adapt residual cluster learning to characterize mixed materials and texture patches in a self-supervised, contrastive learning framework. 2) We achieve SOTA performance on unsuper-vised and supervised change detection, semantic segmen-tation, and land cover classification using our pre-trained network. 3) We provide our curated multi-temporal, spa-tially aligned, and atmospherically corrected remote sens-ing imagery dataset, collected over unchanged regions used for self-supervised learning. 2.