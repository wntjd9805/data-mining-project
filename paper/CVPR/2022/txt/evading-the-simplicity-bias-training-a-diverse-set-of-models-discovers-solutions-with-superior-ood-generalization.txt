Abstract
Training set
OOD Test set
Neural networks trained with SGD were recently shown to rely preferentially on linearly-predictive features and can ignore complex, equally-predictive ones. This simplicity bias can explain their lack of robustness out of distribution (OOD). The more complex the task to learn, the more likely it is that statistical artifacts (i.e. selection biases, spurious correlations) are simpler than the mechanisms to learn.
We demonstrate that the simplicity bias can be mitigated and OOD generalization improved. We train a set of similar models to fit the data in different ways using a penalty on the alignment of their input gradients. We show theoretically and empirically that this induces the learning of more com-plex predictive patterns.
OOD generalization fundamentally requires information beyond i.i.d. examples, such as multiple training environ-ments, counterfactual examples, or other side information.
Our approach shows that we can defer this requirement to an independent model selection stage. We obtain SOTA re-sults in visual recognition on biased data and generaliza-tion across visual domains. The method – the first to evade the simplicity bias – highlights the need for a better under-standing and control of inductive biases in deep learning. 1.

Introduction
Inductive biases in deep learning. At the core of every learning algorithm are a set of inductive biases [45]. They define the learned function outside of training examples and they allow extrapolation1 to novel test points. Deep neu-ral networks are remarkably effective because their induc-tive biases happen to reflect properties of real-world data, 1 Contrary to popular belief, deep neural networks rarely perform interpo-lation even in i.i.d. settings. In high dimensions, test points are extremely unlikely to lie within the convex hull of training points [24, 34]. (a) ImageNet-9 [83] (b) MNIST/CIFAR Collages [70]
Figure 1. Training data often contains multiple predictive patterns. (a) In ImageNet-9, bird shapes and blue backgrounds are both pre-dictive of the bird label. (b) In MNIST/CIFAR collages, both parts are equally predictive of training labels. Neural networks display a simplicity bias: they latch on the MNIST digit for example, and completely ignore the CIFAR part. We show that we can mitigate the simplicity bias by training a collection of models, allowing us to discover a diverse set of predictive patterns. although the reasons are still poorly understood [87].
In particular, the simplicity bias [27, 47, 51, 57, 70] has been proposed as a reason of their success. It makes networks trained with SGD2 represent preferentially simple, approx-imately piecewise linear functions.3 But the simplicity bias can also prevent the learning of complex patterns that are the actual mechanisms of the task of interest. This effect is problematic when the learned simple patterns correspond to spurious correlations a.k.a. statistical shortcuts [17]. In image recognition, an example of a shortcut is to use the 2 The simplicity bias is not a property of neural networks themselves, but also of their training with SGD, since it is possible to manually construct networks with arbitrarily poor generalization [87] i.e. no simplicity bias. 3 We adopt the definition of simplicity of a feature from [70]: it is the min-imum number of linear pieces in the decision boundary that achieves op-timal classification accuracy using this feature. The definition naturally extends to the simplicity of a model implementing this decision boundary.
background rather than the shape of the object. In natural language understanding, an example is to use the presence of certain words rather than the overall meaning of a sen-tence. These shortcuts are inevitable byproducts the data collection process e.g. from selection biases. They are in-creasingly problematic as tasks tackled with deep learning grow in complexity. The mechanisms to learn are more and more likely to be overshadowed by simpler spurious pat-terns.
Role of inductive biases in OOD generalization. OOD
Generalization or strong generalization is the capability of making accurate predictions under arbitrary covariate shifts.
To achieve this, a model must learn and reflect the intrin-sic (i.e. causal) mechanisms of the task of interest.4 For example, recognizing objects in arbitrary scenes requires a model to learn about their shape and details.
It can-not rely solely on the background or contextual cues (Fig-ure 1a) . OOD Generalization fundamentally requires extra information beyond i.i.d. training examples [6, 68]. Exist-ing methods use side information such as multiple training environments [1, 12, 56], counterfactual examples [25, 73], or non-stationary time series [23,30,58]. Importantly, OOD generalization is not achievable only through regularizers, network architectures, or unsupervised control of inductive biases [6]. To make this limitation intuitive, consider the task of image recognition in Figure 1. Should a bird la-bel result from a bird shape or from a blue sky ? If shape and background are equally predictive of training labels, the data simply lacks the information to prefer one over the other (i.e. the task is underspecified: the same data could support a task where the labels relate to the background and not the objects). This is where a learning algorithm’s in-ductive biases come into play, possibly detrimentally. The simplicity bias favors the most linearly-predictive patterns, but these may be spurious. While existing methods attempt to integrate extra information during training, we show this can be deferred to a model selection stage.
This study. We seek to control the simplicity bias of neu-ral networks and investigate benefits in OOD generaliza-tion. Variations of architectures, hyperparameters, or ran-dom seeds have no effect on the simplicity bias. Instead, we train a collection of similar models to fit the training data in different ways. Each model is optimized for stan-dard empirical risk minimization (ERM) while a regular-izer encourages diversity across the collection.
It pushes each model to rely on different patterns in the data, includ-ing complex ones that are otherwise ignore because of the simplicity bias. Identifying a model with good OOD per-4 OOD Generalization goes beyond the in-domain (ID) generalization of classical learning theory. Perfect ID generalization (i.e. reaching Bayes error rate on a test set from the same distribution as the training data) is achievable with infinite training data, but the predictions may rely en-tirely on spurious correlations (e.g. recognizing birds from blue skies). formance is reduced to an independent model selection step that can use any type of side information such as those men-tioned above.5
Applicability of our method. We use three image recogni-tion datasets to demonstrate improvements in generalization relevant to computer vision. Issues with OOD generaliza-tion are also root causes of adversarial vulnerabilities [26], some model biases [49,66], and poor cross-domain/-dataset transfer [76]. Potential benefits in these areas remain to be investigated. Kariyappa et al. [32] already demonstrated improved adversarial robustness by increasing diversity in an ensemble with a method similar to ours.
Summary of contributions. 1. We review the fundamental requirements for OOD gen-eralization and derive a rationale for addressing general-ization during model selection rather than training. 2. We describe a method to overcome the simplicity bias by learning a collection of diverse predictors. 3. We demonstrate these benefits on existing benchmarks. (a) A new capability to learn multiple predictive pat-terns otherwise ignored because of the simplicity bias (multi-dataset collages [70]). (b) Improved activity recognition after training on visually-biased data (Biased Activity Recognition dataset [48]). (c) Improved object recognition across visual domains (PACS dataset [36]).
Far from a complete solution to OOD generalization, this paper highlights the need for a better understanding and control of inductive biases in deep learning. 2.