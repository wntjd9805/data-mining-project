Abstract
Accurately predicting the future motions of surrounding trafﬁc agents is critical for the safety of autonomous ve-hicles. Recently, vectorized approaches have dominated the motion prediction community due to their capability of capturing complex interactions in trafﬁc scenes. How-ever, existing methods neglect the symmetries of the prob-lem and suffer from the expensive computational cost, fac-ing the challenge of making real-time multi-agent motion prediction without sacriﬁcing the prediction performance.
To tackle this challenge, we propose Hierarchical Vector
Transformer (HiVT) for fast and accurate multi-agent mo-tion prediction. By decomposing the problem into local con-text extraction and global interaction modeling, our method can effectively and efﬁciently model a large number of agents in the scene. Meanwhile, we propose a translation-invariant scene representation and rotation-invariant spa-tial learning modules, which extract features robust to the geometric transformations of the scene and enable the model to make accurate predictions for multiple agents in a single forward pass. Experiments show that HiVT achieves the state-of-the-art performance on the Argoverse motion forecasting benchmark with a small model size and can make fast multi-agent motion prediction. 1.

Introduction
Navigating through dynamic environments in a safe ma-neuver is a critical mission of autonomous vehicles. To this end, autonomous vehicles need to understand the sur-roundings and anticipate the future on the road. However, it is challenging to accurately predict the future motions of nearby trafﬁc agents, such as vehicles, bicycles, and pedes-trians, whose goals or intents may be unknown. In multi-agent trafﬁc scenarios, an agent’s behavior is shaped by complex interactions with other agents. Such interactions further intertwine with the map-dependent trafﬁc regula-tions, making it extremely difﬁcult to understand the diverse behavior of multiple agents in the scene.
Recently, learning-based methods have demonstrated their effectiveness in the motion prediction task [9, 12, 31,
Inspired by the progress in computer vision, 32, 36, 49]. some works rasterize the scenes into bird’s eye view im-ages and apply CNNs to make predictions [9, 12, 25]. Al-though these approaches are easy to implement with off-the-shelf image models, they are computationally expensive and have limited receptive ﬁelds. Given these limitations, recent works [17, 31, 49] employ a vectorized approach for more compact scene representations, which extracts a set of vectors or points from the trajectories and the map ele-ments. The scenes are then processed by graph neural net-works [6, 20, 29], Transformers [46], or point cloud mod-els [39, 40, 47] to learn the relationships among vectorized entities such as trajectory waypoints and lane segments.
Existing vectorized approaches, however, are challenged by the need to make real-time motion predictions in rapidly changing trafﬁc conditions. Since vectorized methods are generally not robust to translation and rotation of the refer-ence frame, to mitigate this problem, recent research nor-malizes the scene to be centered at the target agent and to be aligned with the target agent’s heading [17, 31, 49].
This remedy becomes problematic when a large number of agents in the scene need to be predicted, owing to the expen-sive cost of re-normalizing the scene and re-computing the scene features for each target agent. Further, existing works model all-to-all relationships in the space and the time di-mensions to capture ﬁne-grained interactions among the vectorized entities [38, 51], which inevitably leads to pro-hibitive computation with the increase of entities. As mak-ing accurate predictions in real time is critical for the safety of autonomous driving, we are thus motivated to push the state-of-the-art by developing a new framework to achieve faster and more accurate multi-agent motion prediction.
In a nutshell, our approach exploits the symmetries and the hierarchical structure in the problem of multi-agent mo-tion prediction. We frame the motion prediction task in multiple stages and hierarchically model the relationships
In the ﬁrst between entities based on Transformers [46]. stage, our framework avoids expensive all-to-all interaction modeling and extracts context features only locally. Specif-ically, we divide the scene into a set of local regions, where each local region is centered at one modeled agent. For each agent-centric local region, we extract context features from the local vectorized entities which contain rich information related to the central agent. In the second stage, to com-pensate for the restricted local receptive ﬁelds and capture long-range dependencies in the scene, we perform global message passing among agent-centric local regions by em-powering the Transformer encoder with the geometric re-lationships between local reference frames. Finally, given the local and the global representations, a decoder produces future trajectories for all agents in a single forward pass. To further leverage the symmetries of the problem, we employ a scene representation that is agnostic about the translation of the global coordinate frame, in which we use relative po-sitions to characterize all vectorized entities. Based on this scene representation, we introduce rotation-invariant cross-attention modules for spatial learning, which can learn local and global representations that are invariant to the rotation of the scene.
Our approach has the following clear advantages. First, by decomposing the problem into local context extraction and global interaction modeling, our approach can progres-sively aggregate information at different scales and model a large number of entities in the scene with high efﬁ-ciency. Second, our method can learn representations robust to translation and rotation of the inputs via a translation-invariant scene representation and rotation-invariant spatial learning modules. Third, our model can make faster and more accurate predictions with much fewer parameters than the state-of-the-art approaches. We validate all the above advantages via extensive experiments on large-scale driving data. Our code will be publicly available. 2.