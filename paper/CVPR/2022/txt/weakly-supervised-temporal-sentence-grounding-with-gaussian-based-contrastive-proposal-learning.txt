Abstract
Temporal sentence grounding aims to detect the most salient moment corresponding to the natural language query from untrimmed videos. As labeling the temporal boundaries is labor-intensive and subjective, the weakly-supervised methods have recently received increasing atten-tion. Most of the existing weakly-supervised methods gen-erate the proposals by sliding windows, which are content-independent and of low quality. Moreover, they train their model to distinguish positive visual-language pairs from negative ones randomly collected from other videos, ignor-ing the highly confusing video segments within the same video.
In this paper, we propose Contrastive Proposal
Learning(CPL) to overcome the above limitations. Specifi-cally, we use multiple learnable Gaussian functions to gen-erate both positive and negative proposals within the same video that can characterize the multiple events in a long video. Then, we propose a controllable easy to hard neg-ative proposal mining strategy to collect negative samples within the same video, which can ease the model opti-mization and enables CPL to distinguish highly confusing scenes. The experiments show that our method achieves state-of-the-art performance on Charades-STA and Activi-tyNet Captions datasets. The code and models are available at https://github.com/minghangz/cpl. 1.

Introduction
Temporal sentence grounding aims at localizing the start and end time of the moment described by a given free-form natural language query in untrimmed videos. Au-tomatic temporal sentence grounding enables us to effi-ciently find the video moment of interest rather than going through the whole video, which has broad application po-tential in video surveillance [6], video summarization [20],
*Corresponding author
Figure 1. (a) Existing methods generate proposals by sliding win-dow and focus on distinguishing negative proposals from other videos. (b) We use multiple learnable Gaussian functions to gen-erate both positive and negative proposals to characterize the tem-poral structure of events. Our negative proposals are in the same video and collected from easy to hard. etc. Fully supervised temporal sentence grounding has witnessed tremendous achievements recently, however, it needs laborious manual annotations of temporal boundaries for every query thus limiting its scalability and practicabil-ity in real-world applications. Therefore, the weakly super-vised learning schemes, where only the video and natural language query are required during training, have gained more attention due to their low annotation cost and reason-able efficiency.
Existing weakly supervised solutions employ either the multiple instance learning (MIL) based or reconstruction-based paradigms. Specifically, MIL-based methods [11, 12, 19, 21] normally define matched and mismatched video-language pairs as positive and negative samples, and learn the latent cross-modal semantic space by aligning the video-level visual-textual relationships. Reconstruction-based method [18, 24, 39] solves the task through joint learning with the reconstruction loss, assuming that the proposals
that match the text should best reconstruct the entire query.
However, both paradigms have the following limitations:
Firstly, most existing methods generate the same propos-als for all samples via sliding window (shown in Fig. 1(a)), regardless of their contents and difficulty, which is ineffi-cient and of low quality. CNM [39] proposes to use sin-gle learnable Gaussian mask as the positive proposal which can characterize the inherent temporal structure of an event.
However, an untrimmed long video usually comprises sev-eral events and these events often contain similar charac-ters and backgrounds. This makes the model easy to op-timize on some sub-optimal solutions when only predict-ing one positive proposal, resulting in a reduction in the recall rate. CNM [39] directly uses one minus the posi-tive Gaussian mask as negative mask, which is unrealistic to describe the temporal structure of negative events and is easily distinguished by the model. Secondly, most exist-ing methods heavily depend on the quality of randomly se-lected negative samples (other unpaired videos), as shown in Fig. 1(a), which are often easy to distinguish and can-not provide strong supervision signals. However, what the model needs for temporal sentence grounding is to distin-guish the highly confusing video segments within the same video (e.g. a man in gloves and a man puts on gloves as shown in Fig. 1(b)). However, directly using the video seg-ments outside the positive proposals as negative proposals will harm the model training during early training stage due to some misidentified negative proposals.
To address the above limitations, we introduce a novel weakly supervised method namely Contrastive Proposal
Learning(CPL), by generating multiple content-dependent proposals and mining negative samples from easy to hard within the same video. On the one hand, to characterize the multiple events in a long video, we use multiple learnable
Gaussian functions to generate both positive (green curve in Fig. 1(b)) and negative (orange curves in Fig. 1(b)) pro-posals, where the negative proposals should be out of the positive ones and do not cover the corresponding positive proposals 1. Moreover, to distinguish the positive from neg-ative proposals in each video, we introduce the entire video as a reference point, as it contains both the ground truth and a large amount of redundant information. We require that the semantic alignment between the positive proposal and the query is expected to be higher than that of the en-tire video, while the semantic alignment of negative pro-posals should be lower. On the other hand, in contrast to learning from randomly selected negative samples from un-paired videos, we propose a controllable easy-to-hard neg-ative proposal mining strategy. We collect negative propos-als within the same video and enforce the negative propos-1We plot a single positive proposal in Fig. 1(b) as an example, prac-tically, we generate multiple positive proposals via Gaussian functions to improve the recall rate. als further away from the positive ones at the early training stages while proposals closer to the positive are learned at later stages. Because we observe that the negative propos-als closer to the positive proposal are often harder to dis-tinguish than those further away, mainly due to the smooth transition of events (similar background and semantics as shown in Fig. 1(b)). This dynamic curriculum strategy to mine samples can gradually reduce the ambiguity and thus facilitate to learn reliable intra-video samples and ease the model optimization.
Our contributions are summarized as follows: (1) We propose to use multiple Gaussian functions to generate both positive and negative proposals from the same video. By introducing the entire video as a reference point, our pro-posal generation is content-dependent and efficient. (2) We propose a controllable Easy to Hard Negative sample min-ing strategy to collect negative proposals within the video and ease the model optimization. This enables our net-work to distinguish highly confusing scenes. (3) Exper-iments on Charades-STA [10] and ActivityNet Captions datasets [2, 15] demonstrate our method significantly out-performs existing weakly supervised methods. 2.