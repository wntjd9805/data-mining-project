Abstract
Federated Learning (FL) fuses collaborative models from local nodes without centralizing users’ data. The per-mutation invariance property of neural networks and the non-i.i.d. data across clients make the locally updated parameters imprecisely aligned, disabling the coordinate-based parameter averaging. Traditional neurons do not ex-plicitly consider position information. Hence, we propose
Position-Aware Neurons (PANs) as an alternative, fusing position-related values (i.e., position encodings) into neu-ron outputs. PANs couple themselves to their positions and minimize the possibility of dislocation, even updating on heterogeneous data. We turn on/off PANs to disable/enable the permutation invariance property of neural networks.
PANs are tightly coupled with positions when applied to
FL, making parameters across clients pre-aligned and fa-cilitating coordinate-based parameter averaging. PANs are algorithm-agnostic and could universally improve existing
FL algorithms. Furthermore, “FL with PANs” is simple to implement and computationally friendly. 1.

Introduction
Federated Learning (FL) [13, 42] generates a global model via collaborating with isolated clients for privacy protection and efﬁcient distributed training, generally fol-lowing the parameter server architecture [6, 21]. Clients update models on their devices using private data, and the server periodically averages these models for multiple com-munication rounds [27]. The whole process does not trans-mit users’ data and meets the basic privacy requirements.
Represented by FedAvg [27], many FL algorithms ag-gregate local parameters via a simple coordinate-based av-eraging [22–25] These algorithms have two kinds of draw-backs. First, as traditional neurons are unaware of their positions, neural networks have the permutation invariance
Figure 1. Left: Position-Aware Neurons (PANs). We fuse equal/varied position encodings to neurons’ outputs, PANs are turned off/on, and the shufﬂed networks make the same/different predictions, the permutation invariance property is en-abled/disabled. Right: applying PANs to FL. Neurons are coupled with their positions for pre-alignment. i.e., property, implying that hidden neurons could be dislocated during training without affecting the local performances.
Second, the samples across clients are non-independent and identically distributed (non-i.i.d.) [11], which could exacer-bate the permutation of neural networks during local train-ing, making local models misaligned and leading to weight divergence [47]. These reasons degrade the performance of coordinate-based parameter averaging.
Recently, a series of works utilize various matching tech-niques to align neurons, such as Bayesian nonparametric learning [38, 44, 45] and optimal transport [2, 33]. First, these methods are too complex to implement. Second, they solve the misalignment problem after ﬁnishing local up-dates and hence belong to post-processing strategies that need additional computation budgets. Fed2 [43] pioneers a novel aspect via designing feature-oriented model struc-tures following a pre-aligned manner. However, it has to carefully customize the network architecture and only stays
at the group level of pre-alignment. By contrast, we explore a more straightforward and general technique to pre-align neurons during local training procedures.
Our work mainly focuses on solving the non-i.i.d. chal-lenge in FL, more speciﬁcally, seeking solutions via limit-ing the permutation invariance property of neural networks.
We ﬁrst summarize the above analysis: the permutation in-variance property of neural networks leads to neuron mis-alignment across local models. The more heterogeneous the data, the more serious the misalignment is. Hence, our mo-tivation is intuitive: could we design a switch to control the permutation invariance property of neuron networks? We propose Position-Aware Neurons (PANs) as the solution, which couple neurons with their positions. Speciﬁcally, for each neuron (channel for ConvNet [10, 17, 32]), we add or multiply a position-related value (i.e., position encoding) to its output. We introduce a hyper-parameter to turn on/off the
PANs, and correspondingly, to disable/enable the permu-tation invariance property of neural networks. PANs bind neurons in their positions, implicitly pre-aligning neurons across clients even faced with non-i.i.d. data. From another aspect, PANs could keep some consistent ingredients in the forward and backward pass across local models, which could reduce the weight divergence. Overall, appropriate
PANs facilitate the coordinate-based parameter averaging in FL. Replacing traditional neurons with PANs is simple to implement and computationally friendly, which is universal to various FL algorithms. Contributions can be briefed as: (1) proposing PANs to disable/enable the permutation in-variance property of deep networks; (2) applying PANs to
FL, which binds neurons in positions and pre-aligns param-eters for better coordinate-wise parameter averaging. 2.