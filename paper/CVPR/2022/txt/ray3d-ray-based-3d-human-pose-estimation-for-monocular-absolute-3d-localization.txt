Abstract
In this paper, we propose a novel monocular ray-based 3D (Ray3D) absolute human pose estimation with cali-brated camera. Accurate and generalizable absolute 3D human pose estimation from monocular 2D pose input is an ill-posed problem. To address this challenge, we con-vert the input from pixel space to 3D normalized rays.
This conversion makes our approach robust to camera in-trinsic parameter changes. To deal with the in-the-wild camera extrinsic parameter variations, Ray3D explicitly takes the camera extrinsic parameters as an input and jointly models the distribution between the 3D pose rays and camera extrinsic parameters. This novel network de-sign is the key to the outstanding generalizability of Ray3D approach.
To have a comprehensive understanding of how the camera intrinsic and extrinsic parameter varia-tions affect the accuracy of absolute 3D key-point local-ization, we conduct in-depth systematic experiments on three single person 3D benchmarks as well as one syn-thetic benchmark. These experiments demonstrate that our method signiﬁcantly outperforms existing state-of-the-art models. Our code and the synthetic dataset are available at https://github.com/YxZhxn/Ray3D. 1.

Introduction
Accurate monocular 3D human pose estimation has found its wide applications in augmented reality [24], human-object interaction [6], and video action recogni-tion [41]. While the problem has been extensively studied in recent years, it’s a well-known ill-posed problem [23] with limited generalization capability. The problem becomes even more difﬁcult when absolute 3D human pose estima-tion in a metric space is required, as knowing exactly where a human joint is in the World Coordinate System (WCS) is much more challenging than estimating the relative 3D off-set of that joint from a reference point. While being more challenging, knowing absolute 3D poses is more desirable (a) (b)
Figure 1. As shown in (a), if both the body size and the distance to camera are scaled up by twice, the projected 2D keypoints lo-cations remain the same. Same phenomenon is observed in (b), where both the focal length and 3D distance are doubled. Z1 and
Z2 refer to the distance from the person to the camera, H1 and H2 represent the height of camera from the ground plane. S1 and S2 are the scale of the person. f1 and f2 represent focal length of the camera. Such ambiguity is discussed in [4] as well. than the root-relative 3D poses in the real-world applica-tions. For instance, unmanned store requires to detect the merchandise picked up by the customer, which relies on ac-curate hand localization in world coordinate system.
A key-point’s 2D pixel location is jointly determined by the scale of person’s body ﬁgure, camera intrinsic param-eters, camera extrinsic parameters and 3D position in the world coordinate system. These factors introduce ambigui-ties for 3D pose estimation. For instance, as shown in Fig-ure 1 (a), if both the body size and the distance to camera are scaled up by twice, the projected 2D key-points loca-tions remain the same. Similarly, if both the focal length and 3D distance are doubled, the 2D key-points keep the same, as illustrated in Figure 1 (b). Typically, there are more than one conﬁguration of 3D key-points that can generate the same observation of 2D key-points in the image plane.
Thus, naively learning a model to map from 2D pixel loca-tions to 3D world locations is arguably prone to failure.
To resolve these ambiguities, a number of monocular 3D human estimation approaches have been proposed [4,10,29, 32, 44, 47]. These methods can be mainly categorized into two groups, i.e., lifting methods and image based meth-ods. Lifting methods [3, 8, 11, 25, 32, 46, 50] take the 2D human poses as input and lift the 2D pose to 3D pose. A few lifting methods normalize the input according to image resolution [32], and camera principal point [4]. While these normalization schemes improve the generalization ability to some extent, they fail to fully resolve the ambiguity due to variation in camera intrinsic parameters. On the other hand, image based approaches [2, 10, 16, 20, 22, 29, 43, 49] esti-mate the 3D root position based on the prior about the body size. In contrast, [29, 47] rely on image-based human depth estimation for absolute root-keypoint localization. The is-sue with these learning-based depth estimation approaches is lack of sufﬁcient training data with viewpoint variations.
For instance, the model trained with front-view viewpoint may not generalize well to cameras with large pitch value.
Moreover, they fail to fully address the aforementioned am-biguities.
To address the challenges more effectively, we propose our Ray3D method. Firstly, in order to have an intrinsic-parameter-invariant representation, we convert the 2D key-points in a pixel space to 3D rays in a normalized 3D space.
With this simple design, our Ray3D approach achieves sta-ble performance regardless of camera intrinsic parameter changes. Inspired by Videopose [32] and RIE [34], we fuse 3D rays from consecutive frames by using temporal convo-lution in order to further resolve the ambiguity introduced by occlusion and to improve accuracy. This temporal fusion mechanism stabilizes the output and generates more accu-rate 3D locations. Secondly, we jointly embed the camera extrinsic parameters into the network. Camera extrinsic pa-rameters contain essential information for accurate 3D hu-man pose estimation. Arguably, exploiting camera extrinsic parameters is the only way to resolve the human body part size ambiguity. For instance, in Fig. 1 (a), if the camera’s height is known to be close to H1, we can safely elimi-nate incompatible hypotheses like the 3D pose with S2/H2.
Therefore, it’s essential to incorporate the camera extrinsic parameters into the network for accurate absolute localiza-tion. To our best knowledge, none of the existing learning-based 3D human pose estimation approaches explicitly uti-lize these information. In contrast, we directly take camera height and camera pitch value as input, and learn an inde-pendent camera embedding through a Multi-Layer Percep-tron (MLP). This camera embedding is then concatenated with temporally fused ray features for 3D pose estimation.
To understand and diagnose the absolute 3D pose esti-mators, we conduct a series of comprehensive and system-atic experiments. Speciﬁcally, we explicitly benchmark the robustness of the approaches against focal length, princi-pal point, camera pitch angle, camera height, camera yaw angle, body ﬁgure size variations on synthetic dataset. Fur-thermore, we evaluate generalization capability of these ap-proaches on three single person benchmarks.
To summarize, the proposed method makes the follow-ing contributions,
• We convert the input space from 2D pixel space to 3D rays in a normalized coordinate system. This simple design effectively normalizes away the variations in-troduced by the camera intrinsic parameter changes as well as the camera pitch angle changes.
• We present a novel and simple network which learns a camera embedding using the camera extrinsic parame-ters, and jointly models the distribution of camera ex-trinsic parameters and 3D rays.
• We provide a comprehensive and systematic bench-marking of existing 3D approaches in terms of robust-ness against camera pose variations, as well as cross-dataset generalization.
• Experiments on three real benchmark datasets and one synthetic dataset clearly demonstrates the advantages of our Ray3D approach. 2.