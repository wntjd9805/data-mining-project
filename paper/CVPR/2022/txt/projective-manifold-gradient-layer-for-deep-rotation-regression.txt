Abstract
Regressing rotations on SO(3) manifold using deep neu-ral networks is an important yet unsolved problem. The gap between the Euclidean network output space and the non-Euclidean SO(3) manifold imposes a severe challenge for neural network learning in both forward and back-ward passes. While several works have proposed differ-ent regression-friendly rotation representations, very few works have been devoted to improving the gradient back-propagating in the backward pass. In this paper, we pro-pose a manifold-aware gradient that directly backpropa-gates into deep network weights. Leveraging Riemannian optimization to construct a novel projective gradient, our proposed regularized projective manifold gradient (RPMG) method helps networks achieve new state-of-the-art perfor-mance in a variety of rotation estimation tasks. Our pro-posed gradient layer can also be applied to other smooth manifolds such as the unit sphere. Our project page is at https://jychen18.github.io/RPMG. 1.

Introduction
Estimating rotations is a crucial problem in visual per-ception that has broad applications, e.g., in object pose esti-mation, robot control, camera relocalization, 3D reconstruc-tion and visual odometry [8, 12, 15, 21, 34]. Recently, with the proliferation of deep neural networks, learning to ac-curately regress rotations is attracting more and more atten-tion. However, the non-Euclidean characteristics of rotation space make accurately regressing rotation very challenging.
As we know, rotations reside in a non-Euclidean man-ifold, SO(3) group, whereas the unconstrained outputs of neural networks usually live in Euclidean spaces. This gap between the neural network output space and SO(3) man-ifold becomes a major challenge for deep rotation regres-sion, thus tackling this gap becomes an important research topic. One popular research direction is to design learning-friendly rotation representations, e.g., 6D continuous rep-†: He Wang is the corresponding author (hewang@pku.edu.cn). resentation from [42] and 10D symmetric matrix represen-tation from [26]. Recently, Levinson et al. [24] adopted the vanilla 9D matrix representation discovering that simply replacing the Gram-Schmidt process in the 6D representa-tion [42] with symmetric SVD-based orthogonalization can make this representation superior to the others.
Despite the progress on discovering better rotation rep-resentations, the gap between a Euclidean network output space and the non-Euclidean SO(3) manifold hasn’t been completely filled. One important yet long-neglected prob-lem lies in optimization on non-Euclidean manifolds [1]: to optimize on SO(3) manifold, the optimization variable is a rotation matrix, which contains nine matrix elements; if we naively use Euclidean gradient, which simply computes the partial derivatives with respect to each of the nine ma-trix elements, to update the variable, this optimization step will usually lead to a new matrix off SO(3) manifold. Un-fortunately, we observe that all the existing works on rota-tion regression simply rely upon vanilla auto-differentiation for backpropagation, exactly computing Euclidean gradient and performing such off-manifold updates to predicted ro-tations. We argue that, for training deep rotation regression networks, the off-manifold components will lead to noise in the gradient of neural network weights, hindering network training and convergence.
To tackle this issue, we draw inspiration from differen-tial geometry, where people leverage Riemannian optimiza-tion to optimize on the non-Euclidean manifold, which finds the direction of the steepest geodesic path on the manifold and take an on-manifold step. We thus propose to lever-age Riemannian optimization and delve deep into the study of the backward pass. Note that this is a fundamental yet currently under-explored avenue, given that most of the ex-isting works focus on a holistic design of rotation regres-sion that is agnostic to forward/backward pass. However, incorporating Riemannian optimization into network train-ing is highly non-trivial and challenging. Although meth-ods of Riemannian optimization allow for optimization on
SO(3) [5, 29], matrix manifolds [1] or general Riemannian manifolds [32,40], they are not directly applicable to update the weights of the neural networks that are Euclidean. Also,
approaches like [16] incorporate a Riemannian distance as well as its gradient into network training, however, they do not deal with the representation issue.
In this work, we want to propose a better manifold-aware gradient in the backward pass of rotation regression that directly updates the neural network weights. We begin by taking a Riemannian optimization step and computing the difference between the rotation prediction and the up-dated rotation, which is closer to the ground truth. Back-propagating this ”error”, we encounter the mapping func-tion (or orthogonalization function) that transforms the raw network output to a valid rotation. This projection, which can be the Gram-Schmidt process or SVD orthogonaliza-tion [24], is typically a many-to-one mapping. This non-bijectivity provides us with a new design space for our gra-dient: if we were to use a gradient to update the raw output rotation, many gradients would result in the same update in the final output rotation despite being completely different for backpropagating into the neural network weights. Now the problem becomes: which gradient is the best for back-propagation when many of them correspond to the same up-date to the output?
We observe that this problem is somewhat similar to some problems with ambiguities or multi-ground-truth is-sues. One example would be the symmetry issue in pose estimation: a symmetric object, e.g. a textureless cube, ap-pears the same under many different poses, which needs to be considered when supervising the pose predictions.
For supervising the learning in such a problem, Wang et. al. [36] proposed to use min-of-N loss [13], which only pe-nalizes the smallest error between the prediction and all the possible ground truths. We therefore propose to find the gra-dient with the smallest norm that can update the final output rotation to the goal rotation. This back-projection process involves finding an element closest to the network output in the inverse image of the goal rotation and projecting the net-work output to this inverse image space. We therefore coin our gradient projective manifold gradient. One thing to note is that this projective gradient tends to shorten the network output, causing the norms of network output to vanish. To fix this problem, we further incorporate a simple regular-ization into the gradient, leading to our full solution regu-larized projective manifold gradient (RPMG).
Note that our proposed gradient layer operates on the raw network output and can be directly backpropagated into the network weights. Our method is very general and is not tied to a specific rotation representation. It can be coupled with different non-Euclidean rotation representations, including quaternion, 6D representation [42], and 9D rotation matrix representation [24], and can even be used for regressing other non-manifold variables. tion regression: 3D object pose estimation from 3D point clouds/images, rotation estimation problems without using ground truth rotation supervisions, and please see supple-mentary material Section 5 for more experiments on cam-era relocalization. Our method demonstrates significant and consistent improvements on all these tasks and all different rotation representations tested. Going beyond rotation esti-mation, we also demonstrate performance improvements on regressing unit vectors (lie on a unit sphere) as an example of an extension to other non-Euclidean manifolds.
We summarize our contribution as below:
• We propose a novel manifold-aware gradient layer, namely RPMG, for the backward pass of rotation re-gression, which can be applied to different rotation representations and losses and used as a “plug-in” at no actual cost.
• Our extensive experiments over different tasks and ro-tation representations demonstrate the significant im-provements from using RPMG.
• Our method can also benefit regression tasks on other manifolds, e.g. S 2. 2.