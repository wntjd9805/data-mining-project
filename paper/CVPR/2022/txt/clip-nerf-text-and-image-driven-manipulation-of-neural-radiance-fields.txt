Abstract
We present CLIP-NeRF, a multi-modal 3D object manip-ulation method for neural radiance ﬁelds (NeRF). By lever-aging the joint language-image embedding space of the recent Contrastive Language-Image Pre-Training (CLIP) model, we propose a uniﬁed framework that allows manip-ulating NeRF in a user-friendly way, using either a short text prompt or an exemplar image. Speciﬁcally, to com-bine the novel view synthesis capability of NeRF and the controllable manipulation ability of latent representations from generative models, we introduce a disentangled condi-tional NeRF architecture that allows individual control over both shape and appearance. This is achieved by performing the shape conditioning via applying a learned deformation
ﬁeld to the positional encoding and deferring color condi-tioning to the volumetric rendering stage. To bridge this disentangled latent representation to the CLIP embedding, we design two code mappers that take a CLIP embedding as input and update the latent codes to reﬂect the targeted editing. The mappers are trained with a CLIP-based match-ing loss to ensure the manipulation accuracy. Furthermore, we propose an inverse optimization method that accurately projects an input image to the latent codes for manipulation to enable editing on real images. We evaluate our approach by extensive experiments on a variety of text prompts and exemplar images and also provide an intuitive interface for interactive editing. 1.

Introduction
With the explosive growth of 3D assets, the demand for manipulating 3D content to achieve versatile re-creation is rising rapidly. While most existing 3D editing methods op-erate on explicit 3D representations [7, 15, 45], the recent advances of implicit volumetric representations in capturing
*† Jing Liao is the corresponding author. Our project page is https:
//cassiepython.github.io/clipnerf/ and rendering dedicated 3D structures [9, 14, 16, 23, 27, 34] have motivated the research to beneﬁt the manipulation from such representations. Among these works, neural ra-diance ﬁelds (NeRF) [23] utilize a volume rendering tech-nique to render neural implicit representations for high-quality novel view synthesis, providing an ideal represen-tation for 3D content.
Editing NeRF (e.g., deforming the shape or changing the appearance color), however, is an extremely challenging task. First, since NeRF is an implicit function optimized per scene, we cannot directly edit the shape using the in-tuitively tools for the explicit representations [35, 40–42].
Second, unlike image manipulation where the single-view information is enough to guide the editing [20, 43, 44], the multi-view dependency of NeRF makes the manipulation way more difﬁcult to control without the multi-view infor-mation. More recent works propose conditional NeRF [36], which trains NeRF on one category of shapes and enables manipulation via latent space interpolations utilizing the pre-trained models. Based on the conditional NeRF, Edit-NeRF [21] takes the ﬁrst step to edit the shape and color of
NeRF given user scribbles. However, due to its limited ca-pacity in shape manipulation, only adding or removing local parts of the object is allows. In addition to achieving more compelling and complicated manipulation, we seek to edit
NeRF in more intuitive ways, such as using a text prompt or a single reference image.
In this paper, we explore how to individually manip-ulate the shape and the appearance of NeRF based on a text prompt or a reference image in a uniﬁed framework.
Our framework is built on a novel disentangled conditional
NeRF architecture, which is controlled by the latent space disentangled into a shape code and an appearance code. The shape code guides the learning of a deformation ﬁeld to warp the volume to a new geometry, while the appearance code allows controlling the emitted color of volumetric ren-dering. Based on our disentangled NeRF model, we take advantage of the recently proposed Contrastive Language-Image Pre-training (CLIP) model [33] to learn two code mappers, which map CLIP features to the latent space to 1
manipulate the shape or appearance code. Speciﬁcally, given a text prompt or an exemplar image as our condition, we extract the features using the pre-trained CLIP model, feed the features into the code mappers, and yield local dis-placements in the latent space to edit the shape and appear-ance codes to reﬂect the edit. We design the CLIP-based loss to enforce the CLIP space consistency between the in-put constraint and the output renderings, thus supporting high-resolution NeRF manipulation. Additionally, we pro-pose an optimization-based method for editing a real image by inversely optimizing its shape and appearance codes.
To sum up, we make the following contributions:
• We present the ﬁrst text-and-image-driven manipula-tion method for NeRF, using a uniﬁed framework to provide users with ﬂexible control over 3D content us-ing either a text prompt or an exemplar image.
• We design a disentangled conditional NeRF architec-ture by introducing a shape code to deform the vol-umetric ﬁeld and an appearance code to control the emitted colors.
• Our feedforward code mappers enable the fast infer-ence for editing different objects in the same cat-egory compared to the optimization-based editing method [21].
• We propose an inversion method to infer the shape and appearance codes from a real image, allowing editing the shape and appearance of the existing data. 2.