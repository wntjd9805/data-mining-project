Abstract
Defense models against adversarial attacks have grown significantly, but the lack of practical evaluation methods has hindered progress. Evaluation can be defined as look-ing for defense models’ lower bound of robustness given a budget number of iterations and a test dataset. A practical evaluation method should be convenient (i.e., parameter-free), efficient (i.e., fewer iterations) and reliable (i.e., ap-proaching the lower bound of robustness). Towards this tar-get, we propose a parameter-free Adaptive Auto Attack (A3) evaluation method which addresses the efficiency and relia-bility in a test-time-training fashion. Specifically, by observ-ing that adversarial examples to a specific defense model follow some regularities in their starting points, we design an Adaptive Direction Initialization strategy to speed up the evaluation. Furthermore, to approach the lower bound of robustness under the budget number of iterations, we pro-pose an online statistics-based discarding strategy that au-tomatically identifies and abandons hard-to-attack images.
Extensive experiments on nearly 50 widely-used defense models demonstrate the effectiveness of our A3. By consum-ing much fewer iterations than existing methods, i.e., 1/10 on average (10× speed up), we achieve lower robust accu-racy in all cases. Notably, we won first place out of 1681 teams in CVPR 2021 White-box Adversarial Attacks on De-fense Models competitions with this method. Code is avail-able at: https://github.com/liuye6666/adaptive auto attack 1.

Introduction
Despite the breakthroughs for a wide range of fields, deep neural networks (DNNs) [21, 23, 42, 52, 57] have been shown high vulnerabilities to adversarial examples. For instance, inputs added with human-imperceptible pertur-*Corresponding author bations can deceive DNNs to output unreasonable predic-tions [4, 11, 14–16, 30, 33, 50, 60–62]. To tackle this issue, various adversarial defense methods [9,17,18,49] have been proposed to resist against malicious perturbations. Unfortu-nately, these defense methods could be broken by more ad-vanced attack methods [7, 12, 44, 45], making it difficult to identify the state-of-the-art. Therefore, we urgently need a practical evaluation method to judge the adversarial robust-ness of different defense strategies.
Robustness evaluation can be defined as looking for de-fense models’ lower bound of robustness given a budget number of iterations and a test dataset [7]. White-box adversarial attacks on defense models are crucial for test-ing adversarial robustness. Among these methods, widely-used random sampling has been proven effective in gen-erating diverse starting points for attacks in a large-scale study [7, 20, 31, 43].
In general, there are two kinds of random sampling strategies. From the perspective of in-put space, given an original image x of label y and a ran-dom perturbation ζ sampled from uniform distributions, the starting point is xst = x + ζ, e.g., Projected Gradient De-scent (PGD) [31]. From the perspective of output space, given a classifier f and a randomly sampled direction of diversification wd, evaluators generate starting points by
⊺ maximizing the change of output, i.e., w df (x). Intuitively, random sampling strategy is sub-optimal since it is model-agnostic.
Comprehensive statistics on random sampling are con-ducted to verify whether it is sub-optimal. In other words, we want to study whether adversarial examples, i.e., im-ages that successfully fooling the victim’s models, to a spe-cific defense model follow some regularities in their start-ing points. Statistic results on input space show that adver-sarial examples’ starting points are actually random. This is reasonable because starting points are highly dependent on their corresponding input images, and input images are randomly distributed in high-dimensional space. Different from input space, statistics results in output space show
that the direction of diversification wd to a specific defense model follows some regularities. Specifically, wd is not uniformly distributed but with a model-specific bias in the positive/negative direction. Therefore, random sampling in the output space cannot obtain a good starting point, which may slow down the evaluation. To speed up the evaluation, we propose an Adaptive Direction Initialization (ADI) strat-egy in this paper. ADI firstly adopts an observer to record the direction of diversification of adversarial examples at the first restart. Then, based on these directions, ADI in-troduces a novel way to generate better staring points than random sampling for the following restarts.
In addition to using ADI to accelerate robustness evalu-ation, we design another strategy named online statistics-based discarding for improving the reliability of existing methods. Currently, the na¨ıve iterative strategy that treats all images evenly and allocates them the same iterations is widely applied to robustness evaluation [6, 7, 20, 31, 32, 43, 51]. However, this strategy is unreasonable because it pays unnecessary efforts to perturb hard-to-attack images. Intu-itively, given the budget number of iterations, the more ex-amples we successfully attack, the closer robustness to the lower bound we obtain. Therefore, the number of iterations assigned to hard-to-attack images is a lower priority. Based on our observation that loss values can roughly distinguish the difficulty of attacks, we propose an online statistics-based discarding strategy that automatically identifies and abandons hard-to-attack images. Specifically, we stop per-turbing images with considerable difficulties at the begin-ning of every restart. For remaining images, the same num-ber of iterations are allocated to them. Obviously, online statistics-based discarding strategy makes full use of the number of iterations and increases the chance of perturbing images to adversarial examples. We can further approach the lower bound of robustness based on this reliable strat-egy. Essentially, speeding up the evaluation is also closely related to improving the reliability because the saved itera-tions can be used to attack easy-to-attack examples, result-ing in lower robust accuracy. By incorporating the above two strategies, a practical evaluation method Adaptive Auto
Attack (A3) is proposed.
To sum up, our main contributions are three-fold: 1)
Based on comprehensive statistics, we propose an adaptive direction initialization (ADI) strategy which generates bet-ter starting points than random sampling to speed up the robustness evaluation. 2) We propose an online statistics-based discarding strategy that automatically identifies and abandons hard-to-attack images to approach further the lower bound of robustness under the budget number of it-erations. 3) Extensive experiments demonstrate the effec-tiveness and reliability of the method. Particularly, we ap-ply A3 to nearly 50 widely-used defense models, without parameters adjustment, our method achieves a lower robust accuracy and a faster evaluation. 2.