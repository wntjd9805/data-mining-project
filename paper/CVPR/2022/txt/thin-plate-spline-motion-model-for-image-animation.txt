Abstract
Image animation brings life to the static object in the source image according to the driving video.
Recent works attempt to perform motion transfer on arbitrary ob-jects through unsupervised methods without using a priori knowledge. However, it remains a significant challenge for current unsupervised methods when there is a large pose gap between the objects in the source and driving images.
In this paper, a new end-to-end unsupervised motion trans-fer framework is proposed to overcome such issues. Firstly, we propose thin-plate spline motion estimation to produce a more flexible optical flow, which warps the feature maps of the source image to the feature domain of the driving image. Secondly, in order to restore the missing regions more realistically, we leverage multi-resolution occlusion masks to achieve more effective feature fusion. Finally, ad-ditional auxiliary loss functions are designed to ensure that there is a clear division of labor in the network modules, encouraging the network to generate high-quality images.
Our method1 can animate a variety of objects, including talking faces, human bodies, and pixel animations. Experi-ments demonstrate that our method performs better on most benchmarks than the state of the art with visible improve-ments in motion-related metrics. 1.

Introduction
Image animation (Fig. 1) transfers the motion of the ob-ject in the driving video to the static object in the source image, which is widely used for video conferencing [31], movie effects [21] and entertainment videos. It can stimu-late people’s creativity to create more interesting works.
Researches have been done on motion transfer by using a priori knowledge of objects, such as 3D models, landmarks, domain labels [6,9,11,18,23,27,30,35,37]. However, these approaches, which rely on labeled data, only work for spe-cific objects, such as faces [9, 11, 30, 35] and human bod-It is costly to obtain such labeled ies [6, 18, 23, 27, 37]. 1Our source code is publicly available: https://github.com/yoyo-nb/Thin-Plate-Spline-Motion-Model.
Figure 1. Example animations generated by our method trained on different datasets. data or pre-trained keypoint extractors. Therefore, these ap-proaches cannot be applied to objects without labeled data.
Recently, some unsupervised motion transfer methods have been proposed that do not require a priori knowledge of objects [25, 26, 28, 32]. These methods use two frames sampled from a video for training, where one frame is used as the source image to reconstruct the other frame as the driving image. And the methods are optimized using recon-struction losses to learn the motion representations. Some unsupervised methods [25, 26, 28] divide motion transfer into two steps. First, an optical flow is estimated using the motion representation that warps the feature maps of the source image to the feature domain of the driving im-age. Second, an occlusion mask is predicted to indicate the missing regions of the warped feature maps, which are then inpainted in the network. Experiments have shown that un-supervised methods can perform motion transfer on various objects [25, 26, 28].
However, there are still some challenges with the unsu-pervised methods. First, the motion representation is not flexible enough, making it difficult for the network to learn the large pose gap between the objects in the source and driving images during training. This deficiency results in large discrepancies between the warped feature maps and the feature domain of the driving image. Moreover, the area of the occlusion mask will increase, making motion transfer
too dependent on the inpainting capability of the network, which leads to the second problem: inadequate inpainting capability of the network. Previous works [25, 26, 28] did not take full advantage of features at different scales to in-paint the missing regions, so it is difficult to generate more realistic images.
Some unsupervised methods [26, 28] improve the qual-ity of the animation by combining local affine transforma-tions to estimate the motion. However, the affine trans-formation is linear, which makes it difficult to represent complex motions.
In fact, the motions of objects are of-ten not linear locally (for example, when people open their mouths, their lips are curved). To overcome this, we in-troduce a more flexible nonlinear transformation, thin-plate spline (TPS) transformation, to approximate the motion and propose a new end-to-end unsupervised motion trans-fer framework. First, we predict several sets of keypoints to generate TPS transformations and combine them with the affine background transformation [28] to estimate the optical flow. Furthermore, we perform dropout for mul-tiple TPS transformations during the early stage of train-ing so that each TPS transformation contributes to the es-timated optical flow. TPS motion estimation makes the es-timated optical flow more flexible, stable and robust than previously estimated [26,28]. Second, we predict occlusion masks for each layer of warped feature maps, making the feature maps have a different focus for more efficient fea-ture fusion. Finally, we design the auxiliary loss functions to make each module have a clearer division of labor, en-couraging the network to generate high-quality images. The proposed framework approximates the motion more accu-rately and has a stronger inpainting capability. To summa-rize, the main contributions are as follows:
• We present TPS motion estimation to approximate the motion from the source image to the driving image. In addition, we perform dropout on multiple TPS trans-formations before combining them during the early stage of training.
• We propose a new end-to-end unsupervised motion transfer framework. It warps the feature maps of the source image using the estimated optical flow and then leverages multi-resolution occlusion masks to indicate the missing regions for inpainting.
• Experiments demonstrate that our method outperforms previous unsupervised motion transfer methods on var-ious datasets, including talking faces, taichi videos,
TED-talk videos and pixel animations. In particular, there is a visible improvement in motion-related met-rics. 2.