Abstract
Referring video object segmentation aims to predict fore-ground labels for objects referred by natural language ex-pressions in videos. Previous methods either depend on 3D
ConvNets or incorporate additional 2D ConvNets as en-coders to extract mixed spatial-temporal features. However, these methods suffer from spatial misalignment or false dis-tractors due to delayed and implicit spatial-temporal inter-action occurring in the decoding phase. To tackle these limitations, we propose a Language-Bridged Duplex Trans-fer (LBDT) module which utilizes language as an interme-diary bridge to accomplish explicit and adaptive spatial-temporal interaction earlier in the encoding phase. Con-cretely, cross-modal attention is performed among the tem-poral encoder, referring words and the spatial encoder to aggregate and transfer language-relevant motion and ap-pearance information. In addition, we also propose a Bi-lateral Channel Activation (BCA) module in the decoding phase for further denoising and highlighting the spatial-temporal consistent features via channel-wise activation.
Extensive experiments show our method achieves new state-of-the-art performances on four popular benchmarks with 6.8% and 6.9% absolute AP gains on A2D Sentences and
J-HMDB Sentences respectively, while consuming around 7× less computational overhead 1. 1.

Introduction
Referring video object segmentation (RVOS), which aims to segment the target object referred by a natural lan-guage expression in video frames, is an emerging task at the intersection of computer vision and natural language processing. Different from semi-automatic video object segmentation (SVOS) [3, 7, 31, 38], where the target ob-*Corresponding author 1https://github.com/dzh19990407/LBDT
Illustration of our main idea. (Top) For tempo-Figure 1. ral→language→spatial transfer, the referring words (e.g., “jump-ing”) can aggregate language-relevant motion information from the temporal features, which can help the spatial encoder recog-nize correct actions. (Bottom) For spatial→language→temporal transfer, the referring words (e.g., “brown”) can aggregate the language-relevant appearance information from the spatial fea-tures, which helps the temporal encoder remove the disturbance of background motion (e.g., the rotating wheel). ject is referred by the manually annotated mask in the first frame, RVOS is more challenging for identifying the targets due to the variance of free-form expressions. Providing a more natural way for human-computer interaction, RVOS opens up a wide range of applications including language-based video editing [5], language-guided video summariza-tion [29], and video question answering [14, 39], etc.
The keys to solving RVOS are spatial-temporal interac-tion and cross-modal alignment [12, 41]. Existing methods mainly focus on the latter and design several mechanisms (e.g., cross-modal attention [30, 37], capsule routing [25], and dynamic convolution [6, 36]) to mine the semantic correspondence between vision and language modalities.
However, all these methods have limitations on spatial-temporal interaction due to the reliance on 3D ConvNets (e.g., I3D [1]). Concretely, since the poses and locations of moving objects vary in adjacent frames, aggregating spatially misaligned multi-frame features via 3D operators (e.g., 3D convolution and 3D pooling) may confuse the original appearance information in the target frame, lead-ing to inaccurate segmentation results.
To alleviate this phenomenon, CSTM [12] introduces an additional 2D spatial encoder (e.g., ResNet [8]) to extract undisturbed appearance information of the target frame, which is fused with features of the temporal encoder in the later decoding phase. However, the spatial encoder of
CSTM lacks motion information since it doesn’t explicitly interact with the temporal encoder, making it hard to dis-tinguish among objects with similar appearances while per-forming different actions. Thus, it tends to generate high responses on false objects and introduces noises inevitably.
In this paper, we argue that an explicit interaction be-tween spatial and temporal features should be established earlier in the encoding phase, forming a more sufficient and effective information exchange process between encoders.
Moreover, naive spatial-temporal interaction still tends to introduce noises due to redundant information contained in language-irrelevant distractors. Therefore, we believe the language expression can be exploited as the medium to bridge spatial and temporal interaction, where only language-relevant information can be transferred between encoders for valid context aggregation. To this end, we pro-pose a novel Language-Bridged Duplex Transfer (LBDT) module for effective spatial-temporal interaction in the en-coding phase. As illustrated in Figure 1, motion information from the temporal encoder is first aggregated to the refer-ring words by cross-modal attention. Then, the spatial en-coder can obtain language-relevant motion clues from the referring words by reversed cross-modal attention, which assists in identifying the referred object by recognizing cor-rect actions (Figure 1 top). Similarly, appearance infor-mation from the spatial encoder is also transferred to the temporal encoder through the language bridge, which facil-itates the temporal encoder to distinguish language-relevant foreground objects from complex backgrounds (Figure 1 bottom). In addition, we also remove the dependence on 3D ConvNets and approximate motion information with frame difference processed by a 2D ConvNet. By this means, the model complexity is significantly reduced as 2D
ConvNet occupies nearly 30× less computational overhead compared to 3D ConvNet (e.g., 3.6 vs. 107.9 GFLOPs) [2].
To exploit rich multi-scale contexts of hierarchical vi-sual features for finer mask predictions, we also propose a
Bilateral Channel Activation (BCA) module to adjust dif-ferent feature channels in the decoding phase. Concretely, we first upsample and add multi-level features together in temporal and spatial decoders respectively to obtain the de-coded features, on which linguistic features are utilized to filter out language-irrelevant motion and appearance infor-mation by channel-wise activation. Meanwhile, the global contexts of the decoded features are further extracted to ac-tivate the spatial-temporal consistent channels for highlight-ing features of the referred object.
In a nutshell, our contributions are three-fold: 1) We pro-pose a Language-Bridged Duplex Transfer (LBDT) mod-ule to conduct spatial-temporal interaction explicitly be-tween two independent 2D ConvNets in the encoding phase for RVOS, where we use referring words as the medium to transfer language-relevant motion and appearance infor-mation. 2) In the decoding phase, we propose a Bilateral
Channel Activation (BCA) module to obtain the language-denoised spatial-temporal consistent features for segment-ing the referred object. 3) Extensive experiments show that our proposed method outperforms previous methods on four popular RVOS benchmarks, with significant AP gains of 6.8% on A2D Sentences and 6.9% on J-HMDB Sentences, while consuming around 7× less computational overhead. 2.