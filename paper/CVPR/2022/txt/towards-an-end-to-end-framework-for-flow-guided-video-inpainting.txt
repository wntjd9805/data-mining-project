Abstract
Optical ﬂow, which captures motion information across frames, is exploited in recent video inpainting methods through propagating pixels along its trajectories. However, the hand-crafted ﬂow-based processes in these methods are applied separately to form the whole inpainting pipeline.
Thus, these methods are less efﬁcient and rely heavily on the intermediate results from earlier stages.
In this pa-per, we propose an End-to-End framework for Flow-Guided
Video Inpainting (E2FGVI) through elaborately designed three trainable modules, namely, ﬂow completion, feature propagation, and content hallucination modules. The three modules correspond with the three stages of previous ﬂow-based methods but can be jointly optimized, leading to a more efﬁcient and effective inpainting process. Experimen-tal results demonstrate that the proposed method outper-forms state-of-the-art methods both qualitatively and quan-titatively and shows promising efﬁciency. The code is avail-able at https://github.com/MCG-NKU/E2FGVI. 1.

Introduction
Video inpainting aims to ﬁll up the “corrupted” regions with plausible and coherent content throughout video clips.
It is widely applied to real-world applications such as ob-ject removal [16], video restoration [28], and video com-pletion [7, 39]. Despite the signiﬁcant progress made in im-age inpainting [42, 59, 60], video inpainting remains full of challenges due to the complex video scenarios and deteri-orated video frames. Directly performing image inpainting on each frame independently tends to generate temporally inconsistent videos and results in severe artifacts. Both spa-tial structure and temporal coherence are required to be con-sidered in high-quality video inpainting. Recent progress in deep learning motivates researchers to exploit more effec-tive solutions [7, 8, 17, 23, 28, 33, 38, 49, 56, 62].
Among them, typical ﬂow-based methods [17, 56] con-sider video inpainting as a pixel propagation problem to naturally preserve the temporal coherence. As shown in
∗Equal contribution
†C.L. Guo is the corresponding author. (a) (b)
Figure 1. (a) The general pipelines of ﬂow-based methods [17,56] and ours. While previous ﬂow-based methods conduct the three stages separately, our corresponding modules work in an end-to-end manner. (b) A qualitative comparison of our approach with a state-of-the-art ﬂow-based method FGVC [17]. Due to the error accumulation and ignoring temporal information during content hallucination, FGVC fails to generate faithful and temporally con-sistent results compared with our method.
Fig. 1 (a), these methods can be decomposed into three (1) Flow completion: The estimated inter-related stages. optical ﬂow needs to be completed ﬁrst because the ab-sence of ﬂow ﬁelds in corrupted regions will inﬂuence the latter processes. (2) Pixel propagation: They ﬁll the holes in corrupted videos by bidirectionally propagating pixels in the visible areas with the guidance of the completed opti-cal ﬂow. (3) Content hallucination: After propagation, the remaining missing regions can be hallucinated by a pre-trained image inpainting network [59, 60].
Unfortunately, even though impressive results can be ob-tained, the whole ﬂow-based inpainting process must be carried out separately as many hand-crafted operations (e.g.,
Poisson blending, solving sparse linear equations, and in-dexing per-pixel ﬂow trajectories) are involved in the ﬁrst two stages. The isolated processes raise two main problems.
One is that the errors that occur at earlier stages would be
accumulated and ampliﬁed at subsequent stages, which fur-ther inﬂuences the ﬁnal performance signiﬁcantly. Specif-ically, the inaccurate ﬂow estimation would mislead the propagation of pixels and further confuse the stage of con-tent hallucination, producing unfaithful inpainting results.
Second, these complex hand-designed operations only can be processed without GPU acceleration. The whole proce-dure of inferring video sequences, therefore, is very time-consuming. Taking DFVI [56] as an example, completing one video with the size of 432 × 240 from DAVIS [44], which contains about 70 frames, needs about 4 minutes1, which is unacceptable in most real-world applications. Be-sides, except for the above-mentioned drawbacks, only us-ing a pretrained image inpainting network at the content hallucination stage ignores the content relationships across temporal neighbors, leading to inconsistent generated con-tent in videos (see Fig. 1 (b)).
To address these ﬂaws, in this paper, we carefully design three trainable modules, including (1) ﬂow completion, (2) feature propagation, and (3) content hallucination modules which simulate corresponding stages in ﬂow-based methods and further constitute an End-to-End framework for Flow-Guided Video Inpainting (E2FGVI). Such close collabora-tion between the three modules alleviates the excessive de-pendence of intermediate results in the previously indepen-dently developed system [17, 23, 26, 56, 66] and works in a more efﬁcient manner.
To be speciﬁc, for the ﬂow completion module, we di-rectly employ it on the masked videos for one-step com-pletion instead of multiple complex steps. For the feature propagation module, in contrast to the pixel-level propaga-tion, our ﬂow-guided propagation process is conducted in the feature space with the assistance of deformable convolu-tion. With more learnable sampling offsets and feature-level operations, the propagation module releases the pressure of inaccurate ﬂow estimation. For the content hallucination module, we propose a temporal focal transformer to effec-tively model long-range dependencies on both spatial and temporal dimensions. Both local and non-local temporal neighbors are considered in this module, leading to more temporally coherent inpainting results.
Experimental results demonstrate that our framework en-joys the following two strengthens:
• State-of-the-art accuracy: Taking comparisons with previous state-of-the-art (SOTA) methods, the pro-posed E2FGVI achieves signiﬁcant improvements on two common distortion-oriented metrics (i.e., PSNR and SSIM [52]), one popular perception-oriented in-dex (i.e., VFID [50]), and one temporal consistency measurement (i.e., Ewarp [25]).
• High efﬁciency: Our method processes 432 × 240 1We test it on Intel(R) Core(TM) i7-6700K CPU with a single NVIDIA
Titan Xp GPU. videos at 0.12 seconds per frame on a Titan Xp GPU, which is nearly 15× faster than previous ﬂow-based methods. In contrast to methods that also can be end-to-end deployed, our method shows comparable infer-ence time. Besides, our method has the lowest com-putational complexity (FLOPs) among all compared
SOTA methods.
We hope the proposed end-to-end framework with the aforementioned advantages could serve as a strong baseline for the video inpainting community. 2.