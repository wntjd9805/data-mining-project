Abstract
The goal of this paper is to learn strong lip reading models that can recognise speech in silent videos. Most prior works deal with the open-set visual speech recogni-tion problem by adapting existing automatic speech recog-nition techniques on top of trivially pooled visual features.
Instead, in this paper, we focus on the unique challenges en-countered in lip reading and propose tailored solutions. To this end, we make the following contributions: (1) we pro-pose an attention-based pooling mechanism to aggregate visual speech representations; (2) we use sub-word units for lip reading for the ﬁrst time and show that this allows us to better model the ambiguities of the task; (3) we pro-pose a model for Visual Speech Detection (VSD), trained on top of the lip reading network. Following the above, we obtain state-of-the-art results on the challenging LRS2 and LRS3 benchmarks when training on public datasets, and even surpass models trained on large-scale industrial datasets by using an order of magnitude less data. Our best model achieves 22.6% word error rate on the LRS2 dataset, a performance unprecedented for lip reading models, sig-niﬁcantly reducing the performance gap between lip read-ing and automatic speech recognition. Moreover, on the
AVA-ActiveSpeaker benchmark, our VSD model surpasses all visual-only baselines and even outperforms several re-cent audio-visual methods. 1.

Introduction
Lip reading, or visual speech recognition, is the task of recognising speech from silent video. It has many practical applications which include improving speech recognition in noisy environments, enabling silent dictation, or dubbing and transcribing archival silent ﬁlms [25]. It also has impor-tant medical applications, such as helping speech impaired individuals, e.g. people suffering from Lou Gehrig’s disease speak [54], or enabling people with aphonia (loss of voice) to communicate just by using lip movements.
Lip reading and audio-based automatic speech recog-nition (ASR) both have the common goal of transcribing speech, however, they differ regarding the input: while in
ASR the input signal is an audio waveform, in essence, a one-dimensional time series, lip reading has to deal with high-dimensional video inputs that have both temporal and spatial complexity. This makes training large end-to-end models harder due to GPU memory and computation con-straints. Furthermore, understanding speech from visual in-formation alone is challenging due to the inherent ambigu-ities present in the visual stream, i.e. the existence of ho-mophemes where different characters that are visually in-distinguishable (e.g. ‘pa’, ‘ba’, and ‘ma’). That lip reading is a much harder task is also supported by the fact that al-though humans can understand speech reasonably well even in the presence of noise and across a variety of accents, they perform relatively poorly on lip reading [7, 16].
Designing a lip reading model requires both a visual component – mouth movements need to be identiﬁed – as well as a temporal sequence modelling component, which typically involves learning a language model that can re-solve ambiguities in individual lip shapes. Recent develop-ments in deep learning models and the availability of large-scale annotated datasets has led to breakthroughs surpassing human performance [16]. However, most of these works have taken the approach of adapting techniques used for
ASR and machine translation, without catering to the par-ticularities of the vision problem.
The conjecture in this paper is that the performance of lip reading, in terms of both accuracy and data efﬁciency, can be improved if the model is designed from the start tak-ing into account the peculiarities of the visual, rather than the audio domain. To this end, we consider both the visual encoding and the text tokenisation.
Visual encoding. Our ﬁrst contribution is the design of a novel visual backbone for lip reading. The spatiotemporal complexity in lip reading requires dealing with problems such as tracking the mouth in moving talking heads. This is usually achieved with complicated pre-processing pipelines based on facial landmarks. However, those are sub-optimal in many cases. For example, landmarks don’t work well in
proﬁle views [26]. Moreover, it is unclear what is the opti-mal region-of-interest for lip reading: it has been shown that besides the lips, other parts of the face, e.g. the cheeks, may also contain useful discriminative information [68]. Also, this region-of-interest can vary drastically in terms of scale, aspect ratio across identities and utterances. Thus, in this work, we propose an end-to-end trainable attention-based pooling mechanism that learns to track and aggregate the lip movement representations, resulting in a signiﬁcant per-formance boost.
Text tokenisation. Lip reading methods most commonly output character-level tokens. This output representation however is sub-optimal as characters are sometimes more
ﬁne-grained than the input, with multiple characters cor-responding to a single video frame. Furthermore, char-acters do not encode any prior knowledge about the lan-guage, which leads to higher dependency on the decoder’s language modeling capacity that must also ‘learn to read’.
In this work, we instead use sub-word tokens (word-pieces) which not only match with multiple adjacent frames but are also semantically meaningful for learning a language easily.
Word-pieces result in much shorter (than character) output sequences which greatly reduces the run-time and memory requirements. They also provide a language prior, reducing the language modelling burden of the model. We experi-mentally compare character and word-piece tokenization to justify this choice.
Visual Speech Detection. One issue with performing lip reading inference on real-world silent videos is that, since there is no audio track, there is no automated procedure for cropping out the clips where the person is speaking. ASR models use Voice Activity Detection (VAD) as a key pre-processing step, but this is clearly not applicable for silent videos. Here, the parts of a video containing speech have to be determined using the video input alone; in other words, by performing Visual Speech Detection (VSD). This can be very useful e.g. for running inference on silent movies.
Among other ﬁndings in this work, we show that it is pos-sible to train a strong VSD model on top of our pre-trained lip reading encoder.
Other downstream tasks. Besides improving performance on the sentence-level lip reading task itself, obtaining im-proved lip movement representations can have a broader im-pact, as those are often used for other related downstream tasks – e.g. sound source separation [19], visual keyword spotting [42], and visual language identiﬁcation [4].
In summary, we make the following three contribu-tions: (i) a visual backbone architecture using attention-based pooling on the spatial feature map; (ii) the use of sub-word units, rather than characters for the language tokens; and (iii) a strong Visual Speech Detection model, directly trained on top of the lip reading encoder.
In the experiments we show the beneﬁts of (i) and (ii) on improving lip reading performance, and we also intro-duce a two-stage training protocol that simpliﬁes the cur-riculum used in prior works. As will be seen, with these design choices and training methodology, the performance of our best models exceeds prior work on standard evalua-tion benchmarks, and even outperforms proprietary models that use an order of magnitude more data for training. Sim-ilarly, we show the beneﬁt of (i) and the lip reading encoder to our visual speech detection model that is far superior to previous methods on a standard evaluation benchmark.
We discuss potential ethical concerns and limitations of our work in the arXiv version. Please check our project page for video examples, code, and pre-trained models. 2.