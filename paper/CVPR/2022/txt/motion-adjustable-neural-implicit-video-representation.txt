Abstract
Implicit neural representation (INR) has been suc-cessful in representing static images. Contemporary image-based INR, with the use of Fourier-based posi-tional encoding, can be viewed as a mapping from si-nusoidal patterns with different frequencies to image content. Inspired by that view, we hypothesize that it is possible to generate temporally varying content with a single image-based INR model by displacing its input sinusoidal patterns over time. By exploiting the rela-tion between the phase information in sinusoidal func-tions and their displacements, we incorporate into the conventional image-based INR model a phase-varying positional encoding module, and couple it with a phase-shift generation module that determines the phase-shift values at each frame. The model is trained end-to-end on a video to jointly determine the phase-shift values at each time with the mapping from the phase-shifted sinusoidal functions to the corresponding frame, en-abling an implicit video representation. Experiments on a wide range of videos suggest that such a model is capable of learning to interpret phase-varying posi-tional embeddings into the corresponding time-varying content. More importantly, we found that the learned phase-shift vectors tend to capture meaningful temporal and motion information from the video. In particular, manipulating the phase-shift vectors induces meaning-ful changes in the temporal dynamics of the resulting video, enabling non-trivial temporal and motion editing effects such as temporal interpolation, motion magnifi-cation, motion smoothing, and video loop detection. 1.

Introduction
Implicit neural representation (INR) has recently emerged as a powerful paradigm for representing visual data [19, 20, 29, 30, 34]. Notably, INR has recently been successfully adopted to represent 2D images for image processing and synthesis [1,5,6]. Image-based INR em-ploys coordinate-based multi-layer perceptron (MLP), typically along with Fourier-based positional encoding, to map 2D pixel coordinates to the corresponding color values. Existing works also studied video-based INR and considered it as a natural extension of their image-based counterpart [15,29]. Such an approach uses time as an additional input coordinate to the coordinate-MLP model, effectively treating a video as a 3D volume without explicitly modeling inherent temporal connec-tion among video frames.
Alternatively, a video is often considered as a se-quence of images evolving over time in computer vision research [23,32]. This work explores a video-based INR from that perspective. We investigate if it is possible to leverage an image-based INR to generate temporally varying video content motivated by two observations.
First, image-based INR, with the use of Fourier-based positional encoding [34], operates as a mapping from sinusoidal patterns of different frequencies to 2D image content. Varying the input sinusoids would necessarily cause the generated output to vary accordingly. There-fore, in principle a time-evolving image sequence can be generated from a single image-based INR by varying its sinusoidal functions over time. Second, displacements of sinusoidal functions can be modeled mathematically by the shifts in their phase angles. Time-varying sinu-soids can therefore be achieved by assigning different phase shifts at different times.
We develop an implicit neural representation for videos based on these observations. We model the pixel generation process in a frame-wise manner with an image-based INR, and leverage the phase infor-mation in its positional encoding to generate tempo-rally varying video content. Our model consists of two components, a frame generation module and a phase-shift generation module. Our frame generation module maps each pixel coordinate c = (x, y) to the color value
Mf (c) at the corresponding coordinates in the image plane. This frame generation module is a standard image-based INR model with a minimal yet important modification to its positional encoding (PE) operation.
Different from a standard INR, each sinusoidal func-tion in our PE is not static but to be shifted at each time t by a phase-shift vector ϕ(t). The mapping ϕ is generated by the phase-shift generation module Mp, jointly trained end-to-end with Mf to fit the input video. After training, Mp can provide the per-frame phase-shift vector at each corresponding frame in the video. Those learned phase-shift vectors can be exter-nally manipulated before entering the frame generation stage, potentially enabling new generated content with modified dynamics. That makes our neural implicit video representation motion-adjustable.
With the proposed neural implicit video representa-tion, we center our study around two questions. First, can the model learn to fit a video? Compared to a standard INR approach where the spatial coordinate encodings are fixed across frames, the input coordi-nate encodings to our frame generation model con-stantly change from frame to frame, making it more challenging to memorize pixel value at each location.
Second, does the learned phase space have any mean-ingful structures? As the image content at each time is associated with a phase-shift vector, it is inter-esting to see whether manipulating the learned per-frame phase-shift sequence can result in meaningful changes in the generated video. Our experiments on diverse video content suggest positive answers. We found that the model can learn to interpret the learned phase-varying positional encoding into the correspond-ing time-varying video content. Interestingly, we found that the resulting phase space corresponds to meaning-ful information in the video. Manipulating the gener-ated phase-shift vectors can enable different temporal-dynamics effects such as temporal interpolation, mo-tion magnitude adjustment, and motion filtering from the video as shown in Figure 1.
This paper makes the following contributions.
• We introduce a motion-adjustable neural implicit video representation.
Instead of treating the time dimension equally as the spatial dimensions, our representation maps time to a driving signal to modulate the frame-generation process, effectively adapting regular image-based INR to generate tem-porally varying video content.
• We report the interesting finding that the phase in-formation in Fourier-based positional encoding can be flexibly leveraged to capture temporal dynamics in a video. Our work adds to the growing litera-ture on the use of Fourier-based positional encoding in INR, complementing prior works that study the roles of frequency information in Fourier-based po-sitional embeddings.
• We experiment on a variety of real-world videos and demonstrate that our neural implicit video repre-sentation can not only represent a video but can also allow for modifying certain temporal-dynamics aspects of the video content, enabling a motion-adjustable neural implicit video representation po-tentially useful for video processing applications. 2.