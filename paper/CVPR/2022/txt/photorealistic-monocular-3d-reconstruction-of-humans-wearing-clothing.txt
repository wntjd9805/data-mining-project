Abstract
We present PHORHUM, a novel, end-to-end trainable, deep neural network methodology for photorealistic 3D hu-man reconstruction given just a monocular RGB image.
Our pixel-aligned method estimates detailed 3D geometry and, for the first time, the unshaded surface color together with the scene illumination. Observing that 3D supervision alone is not sufficient for high fidelity color reconstruction, we introduce patch-based rendering losses that enable reli-able color reconstruction on visible parts of the human, and detailed and plausible color estimation for the non-visible parts. Moreover, our method specifically addresses method-ological and practical limitations of prior work in terms of representing geometry, albedo, and illumination effects, in an end-to-end model where factors can be effectively dis-entangled.
In extensive experiments, we demonstrate the versatility and robustness of our approach. Our state-of-the-art results validate the method qualitatively and for dif-ferent metrics, for both geometric and color reconstruction. 1.

Introduction
We present PHORHUM, a method to photorealistically reconstruct the 3D geometry and appearance of a dressed person as photographed in a single RGB image. The pro-duced 3D scan of the subject not only accurately resem-bles the visible body parts but also includes plausible ge-ometry and appearance of the non-visible parts, see fig. 1. 3D scans of people wearing clothing have many use cases and demand is currently rising. Applications like immer-sive AR and VR, games, telepresence, virtual try-on, free-viewpoint photo-realistic visualization, or creative image editing would all benefit from accurate 3D people models.
The classical way to obtain models of people is to automat-ically scan using multi-camera set-ups, manual creation by an artist, or a combination of both as often artists are em-ployed to ‘clean up’ scanning artifacts. Such approaches are difficult to scale, hence we aim for alternative, automatic solutions that would be cheaper and easier to deploy.
Prior to us, many researchers have focused on the prob-lem of human digitization from a single image [5,15,16,18,
Figure 1. Given a single image, we reconstruct the full 3D geom-etry – including self-occluded (or unseen) regions – of the pho-tographed person, together with albedo and shaded surface color.
Our end-to-end trainable pipeline requires no image matting and reconstructs all outputs in a single step. 35, 36, 41]. While these methods sometimes produce aston-ishingly good results, they have several shortcomings. First, the techniques often produce appearance estimates where shading effects are baked-in, and some methods do not pro-duce color information at all. This limits the usefulness of the resulting scans as they cannot be realistically placed into a virtual scene. Moreover, many methods rely on multi-step pipelines that first compute some intermediate representa-tion, or perceptually refine the geometry using estimated normal maps. While the former is at the same time imprac-tical (since compute and memory requirements grow), and potentially sub-optimal (as often the entire system cannot be trained end-to-end to remove bias), the latter may not be useful for certain applications where the true geometry is needed, as in the case of body measurements for virtual try-on or fitness assessment, among others. In most exist-ing methods color is exclusively estimated as a secondary step. However, from a methodological point of view, we argue that geometry and surface color should be computed simultaneously, since shading is a strong cue for surface ge-ometry [17] and cannot be disentangled.
Our PHORHUM model specifically aims to address the above-mentioned state of the art shortcomings, as sum-marised in table 1. In contrast to prior work, we present an end-to-end solution that predicts geometry and appear-e n d -to -e n d train a ble dista n c e s retu rn s c olo r tru e s u rfa c e n o r m als retu rn s s h a din g retu rn s alb e d o sig n e d m a s k n e e d e d
✗
✗
✗
✗
✗
✓
✗
✗
✗
✗
✗
✓
✓
✗
✗
✓
✓
✓
✗
✗
✗
✗
✗
✓
✗
✗
✗
✗
✗
✓
✓
✓
✓
✗
✗
✓ n o
✗
PIFu [35]
✓ PIFuHD [36]
✗
Geo-PIFu [15]
✗
Arch [18]
✗
Arch++ [16]
✓ PHORHUM (ours)
Table 1. Overview of the properties of single image 3D human reconstruction methods. Our method is the only one predicting albedo surface color and shading. Further, our method has the most practical training set-up, does not require image matting at test-time, and returns signed distances rather than binary occu-pancy – a more informative representation. ance as a result of processing in a single composite net-work, with inter-dependent parameters, which are jointly estimated during a deep learning process. The appearance is modeled as albedo surface color without scene specific il-lumination effects. Furthermore, our system also estimates the scene illumination which makes it possible, in princi-ple, to disentangle shading and surface color. The predicted scene illumination can be used to re-shade the estimated scans, to realistically place another person in an existing scene, or to realistically composite them into a photograph.
Finally, we found that supervising the reconstruction using only sparse 3D information leads to perceptually unsatisfac-tory results. To this end, we introduce rendering losses that increase the perceptual quality of the predicted appearance.
Our contributions can be summarised as follows:
- We present an end-to-end trainable system for high quality human digitization
- Our method computes, for the first time, albedo and shading information
- Our rendering losses significantly improve the visual fidelity of the results learning questioned the need for video. First, hybrid recon-struction methods based on a small number of images have been presented [3, 7]. Shortly after, approaches emerged to predict 3D human geometry from a single image. Those methods can be categorized by the used shape representa-tion: voxel-based techniques [19, 41, 49] predict whether a given segment in space is occupied by the 3D shape. A common limitation is the high memory requirement result-ing in shape estimates of limited spatial resolution. To this end, researchers quickly adopted alternative representations including visual hulls [30], moulded front and back depth maps [13, 38], or augmented template meshes [5]. Another class of popular representations consists of implicit function networks (IFNs). IFNs are functions over points in space and return either whether a point is inside or outside the predicted shape [10, 26] or return its distance to the clos-est surface [31]. Recently IFNs have been used for vari-ous 3D human reconstruction tasks [11, 12, 14, 29] and to build implicit statistical human body models [6, 27]. Neu-ral radiance fields [28] are a related class of representations specialized for image synthesis that have also been used to model humans [24, 32, 42]. Saito et al. were the first to use IFNs for monocular 3D human reconstruction. They proposed an implicit function conditioned on pixel-aligned features [35, 36]. Other researchers quickly adopted this methodology for various use-cases [15, 23, 44, 48]. ARCH
[18] and ARCH++ [16] also use pixel-aligned features but transform information into a canonical space of a statistical body model. This process results in animatable reconstruc-tions, which comes, however, at the cost of artifacts that we will show. In this work, we also employ pixel-aligned features but go beyond the mentioned methods in terms of reconstructed surface properties (albedo and shading) and in terms of the quality of the 3D geometry. Also related is
H3D-Net [34], a method for 3D head reconstruction, which uses similar rendering losses as we do, but requires three images and test-time optimization.
In contrast, we work with a monocular image, purely feed-forward.
- Our results are more accurate and feature more detail 3. Method than current state-of-the-art 2.