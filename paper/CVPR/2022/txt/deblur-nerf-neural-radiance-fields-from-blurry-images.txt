Abstract
Neural Radiance Field (NeRF) has gained considerable attention recently for 3D scene reconstruction and novel view synthesis due to its remarkable synthesis quality. How-ever, image blurriness caused by defocus or motion, which often occurs when capturing scenes in the wild, signifi-cantly degrades its reconstruction quality. To address this problem, We propose Deblur-NeRF, the first method that can recover a sharp NeRF from blurry input. We adopt an analysis-by-synthesis approach that reconstructs blurry views by simulating the blurring process, thus making NeRF robust to blurry inputs. The core of this simulation is a novel Deformable Sparse Kernel (DSK) module that mod-els spatially-varying blur kernels by deforming a canoni-cal sparse kernel at each spatial location. The ray ori-gin of each kernel point is jointly optimized, inspired by the physical blurring process. This module is parameter-ized as an MLP that has the ability to be generalized to various blur types. Jointly optimizing the NeRF and the
DSK module allows us to restore a sharp NeRF. We demon-*Author did this work during the internship at Tencent AI Lab. strate that our method can be used on both camera mo-tion blur and defocus blur: the two most common types of blur in real scenes. Evaluation results on both syn-thetic and real-world data show that our method outper-forms several baselines. The synthetic and real datasets along with the source code is publicly available at https:
//limacv.github.io/deblurnerf/. 1.

Introduction
Tremendous progress has been witnessed in the past few years in novel view synthesis, where an intermediate 3D representation is reconstructed from sparse input views to interpolate or extrapolate arbitrary novel views. Recently,
NeRF [22] emerged as an effective scene representation that achieves photorealistic rendering results. It models a static scene as a continuous volumetric function that maps 3D lo-cation and 2D view direction to color and density. This function is parameterized as a multilayer perceptron (MLP), and its output can be rendered by volume rendering tech-niques in a differentiable manner.
To reconstruct a NeRF, several images from different views are needed. While the original approach for train-ing NeRF works well when these images are well captured and calibrated, it would produce obvious artifacts when blur occurs. For example, when using a long exposure setting to capture a low-light scene, the images are more sensitive to camera shake, resulting in camera motion blur. Further-more, defocus blur is inevitable when capturing scenes with large depth variation using a large aperture. These blurs will significantly decrease the quality of the reconstructed
NeRF, resulting in artifacts in the rendered novel views.
Many works have recently been proposed to tackle ab-normal input while training NeRF. NeRF-W [20] focuses on images with illumination change and moving objects.
Mip-NeRF [1] improves the NeRF when the input spans different scales. Distortion in input is considered and cal-ibrated simultaneously in SCNeRF [11]. To the best of our knowledge, none has considered addressing the prob-lem of training NeRF from blurry input images. One so-lution is to first deblur the input in image space and then train the NeRF with deblurred images, which we refer to as the image-space baseline. This baseline improves the novel view synthesis quality of NeRF to some extend by utilizing recent single image or video deblurring methods.
However, the single-image deblur methods fail to aggregate information from neighbor views and can not guarantee the multi-view consistent result. Video-based methods manage to take multi-frame into consideration, usually relying on image space operations such as optical flows and feature correlation volumes. However, these methods fail to exploit the 3D geometry of the scene, leading to inaccurate corre-spondences across views, especially when they have a large baseline. On the contrary, our method deblurs by aggregat-ing information from all observations with full awareness of the 3D scene.
In this paper, we propose Deblur-NeRF, an effective framework that explicitly models the blurring process in the network, and is capable of restoring a sharp NeRF from blurry input. We model the blurring process by convolv-ing a clean image using a blur kernel similar to blind de-convolution methods [2]. A novel deformable sparse kernel (DSK) module is proposed to model the blur kernel inspired by the following observations. First, convolving with dense kernels is infeasible for scene representations such as NeRF due to the dramatic increase in computation and memory usage during rendering. To address this, DSK uses sparse rays to approximate the dense kernel. Second, we show that the actual blurring process involves combining rays from different origins, which motivates us to jointly optimize the ray origins. Finally, to model spatially-varying blur kernels, we deform a canonical sparse kernel at each 2D spatial loca-tion. The deformation is parameterized as an MLP that can be generalized to different types of blur. During training, we jointly optimize the DSK and a sharp NeRF with only blurry input as supervision, while in the inference stage, clear novel views can be rendered by removing the DSK.
We conduct extensive experiments on both synthetic and real datasets with two types of blur: camera motion blur and defocus blur. Results show that the proposed method out-performs the original NeRF and image-space baselines (i.e., combining NeRF with the state-of-the-art image or video deblurring methods), for these two blur types, as shown in
Fig. 1 and the experiments section. Our contributions can be summarized as follows:
• We propose the first framework that can reconstruct a sharp NeRF from blurry input.
• We propose a deformable sparse kernel module that enables us to effectively model the blurring process and is generalizable for different types of blur.
• We analyze the physical blurring process and extend the 2D kernel to 3D space by considering the transla-tion of the ray origin for each kernel point. 2.