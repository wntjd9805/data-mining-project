Abstract
How to achieve better results with fewer labeling costs remains a challenging task. In this paper, we present a new active learning framework, which for the first time incor-porates contrastive learning into recently proposed one-bit supervision. Here one-bit supervision denotes a simple Yes or No query about the correctness of the model’s prediction, and is more efficient than previous active learning methods requiring assigning accurate labels to the queried samples.
We claim that such one-bit information is intrinsically in accordance with the goal of contrastive loss that pulls pos-itive pairs together and pushes negative samples away. To-wards this goal, we design an uncertainty metric to actively select samples for query. These samples are then fed into different branches according to the queried results. The
Yes query is treated as positive pairs of the queried cate-gory for contrastive pulling, while the No query is treated as hard negative pairs for contrastive repelling. Addition-ally, we design a negative loss that penalizes the negative samples away from the incorrect predicted class, which can be treated as optimizing hard negatives for the correspond-ing category. Our method, termed as ObCP, produces a more powerful active learning framework, and experiments on several benchmarks demonstrate its superiority. 1.

Introduction
Active learning is particularly useful in many modern machine learning systems, where data may be sufficient, but labels are scarce or expensive to obtain [12, 14, 24, 25].
The key hypothesis of active learning is that one can build a good predictive model with less labeled samples if a model already knows which samples should be labeled to help im-prove predictive performance. Generally, an active learning algorithm iteratively selects samples to label, based on the prediction results of current model. Once the samples are labeled, they are added to the training set, and such process
*Corresponding author
Figure 1. Illustration of differences between conventional active learning and one-bit supervision. The latter leverages a simple yes or no query about the correctness of the model’s prediction instead of assigning an accurate label. In this paper, we adopt one-bit supervision in our active learning framework and combine it with contrastive learning for jointly optimization. continues till the labeled budget is used up. However, con-ventional active learning strategies are usually constrained by annotating samples with accurate labels, i.e., which exact class one sample belongs to. While it is difficult for the la-beler to memorize and distinguish all categories especially when the number of categories scales up, like ImageNet [7] which consists of 1K categories.
Recently, a novel active annotation method called one-bit supervision [18] is proposed, which labels samples with one bit information by answering a simple yes-or-no ques-tion, i.e., whether an image belongs to a specified class c, this method can facilitate the learned procedure more effi-ciently under the same amount of supervision. For exam-ple, if we can get the model’s prediction on unlabeled data, we only need to inquiry the labeler “Does the model pre-dict this sample right?” It is a more efficient query strategy compared with traditional labeling procedure in terms of bit information, since for a C-classes classification problem, accurately labeling a sample needs log2 C bits of informa-tion, while the one-bit query only needs one bit. This means that with the same amount of annotations, we are able to query more samples to get more information accordingly.
The differences between conventional active learning and the one-bit method are shown in Figure 1. However, previ-ous one-bit method [18] only makes use of the correct pre-dictions like conventional supervised learning, while does not take full advantage of the information such as negative query, and hence limits its performance.
The one-bit supervision returns Yes or No results, based on this, we are able to obtain information whether the two samples (one is the queried sample, and the other is a ran-dom sample from the queried category) are from the same category or not, which is reminiscent of recent contrastive learning strategy [15]. In contrastive learning, the goal is to learn an encoder that is able to map positive pairs to similar representations while pushing away those negative samples in the embedding space. We claim that contrastive loss is intrinsically in accordance with the one-bit query, and we are able to simply treat the Yes query as positive pairs, while those No query as negative pairs. In this way, we are able to take full advantages of the queried results. Towards this goal, we develop a novel active learning approach that combines semi-supervised learning and contrastive learning with efficient one-bit supervision.
Specifically, firstly, we jointly train the model via com-bining supervised cross-entropy loss with contrastive loss, and obtain a pre-trained model for the next stage one-bit query. Then we develop an uncertainty measurement met-ric, which is based on the variance of the model’s prediction during the training process, to help decide which samples to query. According to the queried results, for the correct prediction with Yes query, we extend the contrastive loss function to allow for multiple positive samples during each forward propagation so that images with the same ground truth label will be pulled together for compact representa-tion. While for the incorrect prediction with No query re-sults, we integrate this incorrect negative label information into contrastive learning to help these samples keep away from their queried class. Moreover, we design a negative loss that penalizes the negative samples away from the in-correct prediction class, which can be treated as optimiz-ing hard negatives for the corresponding category. Integrat-ing one-bit supervision into contrastive learning produces a much more powerful framework, and experimental results on several well-known datasets demonstrate its superiority.
Overall, we summarize our contributions as follows:
• We present a novel active learning framework, which combines contrastive learning with one-bit supervision for the first time, and take full advantages of the su-pervised information. We hope that such framework would shed light on active learning community and of-fer one possible direction for future research.
• We achieve significant leading performance on several widely used image classification benchmarks. Espe-cially on ImageNet, with only 10% labels in terms of bit information, ObCP exceeds previous state-of-the-art that even uses 30% labels. 2.