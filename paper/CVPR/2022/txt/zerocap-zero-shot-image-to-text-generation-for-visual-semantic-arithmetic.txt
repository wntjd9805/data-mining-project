Abstract
Recent text-to-image matching models apply contrastive learning to large corpora of uncurated pairs of images and sentences. While such models can provide a power-ful score for matching and subsequent zero-shot tasks, they are not capable of generating caption given an image. In this work, we repurpose such models to generate a descrip-tive text given an image at inference time, without any fur-ther training or tuning step. This is done by combining the visual-semantic model with a large language model, ben-eﬁting from the knowledge in both web-scale models. The resulting captions are much less restrictive than those ob-tained by supervised captioning methods. Moreover, as a zero-shot learning method, it is extremely ﬂexible and we demonstrate its ability to perform image arithmetic in which the inputs can be either images or text and the output is a sentence. This enables novel high-level vision capabili-ties such as comparing two images or solving visual anal-ogy tests. Our code is available at: https://github. com/YoadTew/zero-shot-image-to-text. 1.

Introduction
Deep learning has led to at least three major revolutions in computer vision: (i) machines that achieve, in multiple domains, what is considered a human level of performance earlier than anticipated [39, 64], (ii) effective transfer learn-ing, which supports rapid modeling of new domains [74], and (iii) a leap in unsupervised learning through the use of adversarial and self-supervised learning [9, 23].
A fourth revolution that is currently taking place is that of zero-shot learning. A seminal work by OpenAI presented the transformer-based [65] GPT-3 model [6]. This model is trained on extremely large text corpora and can then gener-ate text given a prompt. If the prompt contains an instruc-tion, GTP-3 can often carry it out. For example, given the prompt “Translate English to French: typical → typique, house → . . . ” would generate the word “maison.”
Impressive zero-shot capability was later on demon-strated, also by OpenAI, in computer vision. While state-of-the-art computer vision models are often trained as task-speciﬁc models that infer a ﬁxed number of labels, Rad-ford et al. [55] have presented the CLIP image-text trans-former model, which can perform tens of downstream tasks, without further training, with an accuracy comparable to the state of the art. This is done by selecting, given an image, the best match out of sentences of the form “This is an im-age of X.” Subsequently, Ramesh et al. [57] presented a bi-modal Transformer termed DALL-E, which generates im-ages that match a given description in unseen domains with unprecedented performance.
In this work, we employ CLIP to perform the inverse task of DALL-E, namely zero-shot image captioning. Given an image, we employ CLIP together with the GPT-2 language model [56] (we do not have access to GPT-3) to generate a textual description of the input image. This adds a new image-analysis capability to CLIP, beyond the ﬁxed-prompt zero-shot learning demonstrated by Radford et al.
As a zero-shot method, our approach does not involve any training. One can argue that the underlying CLIP model is trained with exactly the same type of supervision that im-age captioning methods [62, 76] are trained on, i.e., pairs of matching images and captions. However, image caption-ing methods are trained from curated sources, such as MS-COCO [43] or Visual Genome [38], while CLIP is trained on WebImageText (WIT), which is an automatically col-lected web-scale dataset. Previous attempts to train a cap-tioning model on WIT have led to poor performance in rec-ognizing the objects in the image, see Sec. 2.
As a result of the difference in both methodology and un-derlying data, the captions produced by our method are very different from those obtained by the supervised caption-ing methods. While supervised methods can mimic human annotators and provide similar sentences, in terms of con-ventional NLP metrics (such as BLEU [53]) to the ground truth sentences, our results exhibit much more freedom and match the image better in the visual-semantic CLIP em-bedding space (ours is optimized for this). Moreover, the semantic knowledge incorporated into CLIP and GPT-2 is manifested in the resulting caption, see Fig. 1.
In addition to the different nature of the obtained cap-tions, our method is also more ﬂexible, since all the com-puting occurs at inference time. Speciﬁcally, we show the
Real-world knowledge
Diversity
OCR
Visual-Semantic Arithmetics
CLIP-VL: A blue hat sitting on top of a black table.
VinVL: A police hat sitting on top of a toilet.
Ours: A promotional cap from the
Toronto Blue Jays 10/09 season.
CLIP-VL: A living room with a television and a book shelf.
VinVL: A living room with a television and pictures on the wall.
Ours: A room dedicated to games and other forms of entertainment that were popular in the late 90s.
CLIP-VL: A birthday cake with candy on top of it.
VinVL: A blue cake with candy on top of it.
Ours: Sean's truck cake. ( (
)
A young queen. 
)
A cow's milk. 
Figure 1. Our novel captioning method ZeroCap exhibits real-world knowledge, generates text that is more diverse and less scripted than existing methods, can address the written content of an image, and can perform visual-semantic arithmetic. ability to perform semantic analysis in image space by us-ing a new form of arithmetic. A well-known example for concept arithmetic in NLP is that of retrieving the word
‘queen’ as the closest word, in the embedding space, to the equation involving the embedding vectors associated with
‘king,’ ‘man,’ and ‘woman,’ after subtracting the 2nd from the 1st and adding the 3rd. We present the novel ability to do the same, only with images instead of words, such that the result is generated as a short sentences, and not just a word, see Fig. 1.
As a corollary, we can, for example, ask what the dif-ference is between two scenes. This ability to compare two images semantically is a novel computer vision capability, which further demonstrates the power of zero-shot learning. 2.