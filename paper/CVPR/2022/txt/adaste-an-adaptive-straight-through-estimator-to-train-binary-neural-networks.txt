Abstract
We propose a new algorithm for training deep neural networks (DNNs) with binary weights.
In particular, we first cast the problem of training binary neural networks (BiNNs) as a bilevel optimization instance and subsequently construct flexible relaxations of this bilevel program. The resulting training method shares its algorithmic simplicity with several existing approaches to train BiNNs, in partic-ular with the straight-through gradient estimator success-fully employed in BinaryConnect and subsequent methods.
In fact, our proposed method can be interpreted as an adap-tive variant of the original straight-through estimator that conditionally (but not always) acts like a linear mapping in the backward pass of error propagation. Experimental re-sults demonstrate that our new algorithm offers favorable performance compared to existing approaches.1 1.

Introduction
Deploying deep neural networks (DNNs) to computing hardware such as mobile and IoT devices with limited com-putational and storage resources is becoming increasingly relevant in practice, and hence training methods especially dedicated to quantized DNNs have emerged as important research topics in recent years [9]. In this work, we are par-ticularly interested in the special case of DNNs with binary weights limited to {+1, −1}, since in this setting the com-putations at inference time largely reduce to sole additions and subtractions. Very abstractly, the task of learning in such binary weight neural networks (BiNNs) can be formu-lated as an optimization program with binary constraints on the network paramters, i.e., minw ℓ(w)
= minw∈{−1,1}d E(x,y)∼pdata [ψ(f (x, w), y)] , s.t. w ∈ {−1, 1}d, (1) (2) 1This work was partially supported by the Wallenberg AI, Autonomous
Systems and Software Program (WASP) funded by the Knut and Alice
Wallenberg Foundation. where d is the dimensionality of the underlying parame-ters (i.e. all network weights), pdata is the training distribu-tion and ψ is the training loss (such as the cross-entropy or squared Euclidean error loss). f (x; w) is the prediction of the DNN with weights w for input x.
In practice, one needs to address problem settings where the parameter dimension d is very large (such as deep neural networks with many layers). However, address-ing the binary constraints in the above program is a chal-lenging task, which is due to the combinatorial and non-differentiable nature of the underlying optimization prob-lem. In view of large training datasets, (stochastic) gradient-based methods to obtain minimizers of (1) are highly prefer-able. Various techniques have been proposed to address the above difficulties and convert (1) into a differentiable sur-rogate. The general approach is to introduce real-valued
“latent” weights θ ∈ Rd, from which the effective weights w = sgn(θ) are generated via the sign function (or a dif-ferentiable surrogate thereof). One of the simplest and nevertheless highly successful algorithms to train BiNNs termed BinaryConnect [10] is based on straight-through estimators (STE), which ignore the sign mapping entirely when forming the gradient w.r.t. the latent weights θ (and therefore the update of θ is based on ∇wℓ(w) instead of
∇θℓ(sgn(θ))). Although this appears initially not justified,
BinnaryConnect works surprisingly well and is still a valid baseline method for comparison. More recently, the flex-ibility in choosing the distance-like mapping leveraged in the mirror descent method [30] (and in particular the en-tropic descent algorithm [7]) provides some justification of
BinaryConnect-like methods [3] (see also Sec. 3.2).
In this work, we propose a new framework for training binary neural networks.
In particular, we first formulate the training problem shown in (1) as a bilevel optimiza-tion task, which is subsequently relaxed using an optimal value reformulation. Further, we propose a novel scheme to calculate meaningful gradient surrogates in order to up-date the network parameters. The resulting method strongly resembles BinaryConnect but leverages an adaptive variant the sign func-of the straight-through gradient estimator:
Figure 1. Adaptive straight-through estimation illustrated when s = tanh. ℓ′ is the incoming back-propagated error signal. Left: θ ≈ 0.
The finite difference slope ( ˆw − w∗)/β matches the derivative of tanh very well. Middle: θ ≪ 0 ∧ ℓ′ < 0. A nearly vanishing derivative of tanh is boosted and tanh becomes “leaky.” Right: θ ≪ 0 ∧ ℓ′ > 0. No gradient “boosting” in this case. The case θ ≫ 0 is symmetrical. tion is conditionally replaced by a suitable linear but data-dependent mapping. Fig. 1 illustrates the underlying princi-ple for the tanh mapping: depending on the incoming error signal, vanishing gradients induced by tanh are condition-ally replaced by non-vanishing finite-difference surrogates.
We finally point out that our proposed method can be cast as a mirror descent method using a data-dependent and vary-ing distance-like mapping. 2.