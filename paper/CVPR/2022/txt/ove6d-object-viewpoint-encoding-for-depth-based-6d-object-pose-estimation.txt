Abstract
This paper proposes a universal framework, called
OVE6D, for model-based 6D object pose estimation from a single depth image and a target object mask. Our model is trained using purely synthetic data rendered from ShapeNet, and, unlike most of the existing methods, it generalizes well on new real-world objects without any fine-tuning. We achieve this by decomposing the 6D pose into viewpoint, in-plane rotation around the camera optical axis and transla-tion, and introducing novel lightweight modules for estimat-ing each component in a cascaded manner. The resulting network contains less than 4M parameters while demon-strating excellent performance on the challenging T-LESS and Occluded LINEMOD datasets without any dataset-specific training. We show that OVE6D outperforms some contemporary deep learning-based pose estimation meth-ods specifically trained for individual objects or datasets with real-world training data. The implementation is avail-able at https://github.com/dingdingcai/OVE6D-pose. 1.

Introduction
The 6D pose of an object refers to a geometric mapping from the object coordinate system to the camera reference frame [18, 22]. Most commonly, this transformation is de-fined in terms of 3D rotation (object orientation) and 3D translation (object location). The ability to infer the object pose is an essential feature for many applications interact-ing with the environment. For instance, in robotic manipu-lation [10] and augmented reality [30], the pose is needed for grasping or realistically rendering artificial objects.
In recent works, the object pose estimation problem is commonly approached by either establishing local corre-spondences between the object 3D model and the observed data [16, 17, 36], or via direct regression [6, 39].
In both cases, the inference models are often optimized and stored separately for each object instance. Such approach quickly turns intractable as the number of object instances grows.
Meanwhile, some existing works [49, 59] consider build-ing a single model for multiple objects. However, to retain
Figure 1. A) We propose a single universal pose estimation model (called OVE6D) that is trained using more than 19,000 synthetic objects from ShapeNet. B) The pre-trained model is applied to encode the 3D mesh model of the target object (unseen during the training phase) into a viewpoint codebook. C) At the inference time, OVE6D takes a depth image, an object segmentation mask, and an object ID as an input, and estimates the 6D pose of the tar-get object using the corresponding viewpoint codebook. New ob-ject can be added by simply encoding the corresponding 3D mesh model and including it into the codebook database (B). the performance, the model requires expensive re-training every time a new object instance is added to the database.
In addition, most of the best-performing methods need an-notated real-world training data, which is laborious to ob-tain. Although some works [24, 41, 43] consider using syn-thetic examples in training, they suffer from noticeable per-formance degradation due to the domain gap.
An alternative approach, called LatentFusion, was pro-posed in [32]. In this work, they first reconstruct a latent 3D object model from a small set of reference views, and later use the model to infer the 6D pose of the corresponding ob-ject from the input image. The main advantage is the ability
only once and the resulting parameters remain fixed in later stages. Second (Fig. 1 B), we convert the 3D mesh mod-els of the target objects into viewpoint codebooks. The conversion is performed once for each object and it takes roughly 30 seconds per instance. Finally (Fig. 1 C), the 6D pose is inferred from the input depth image and object seg-mentation mask. The complete OVE6D model contains less than 4M parameters and requires roughly 50ms to infer the pose for a single object. New, previously unseen, object can be added by simply encoding the corresponding 3D mesh model as in the second stage.
The core of OVE6D is a depth-based object viewpoint encoder that captures the object viewpoint into a feature vector. The encoded representations are trained to be in-variant to the in-plane rotation around the camera optical axis, but to be sensitive to the camera viewpoint, as illus-trated in Figure 2. At the inference time, we first utilize the viewpoint encodings to determine the camera viewpoint, and subsequently estimate the remaining pose components (camera in-plane rotation and object 3D position) condi-tioned on the obtained viewpoint. The cascaded pipeline allows compact architectures for each sub-task and enables efficient training using thousands of synthetic objects.
To summarize, our key contributions are: 1) We propose a cascaded object pose estimation framework, which gen-eralizes to previously unseen objects without additional pa-rameter optimization. 2) We propose a viewpoint encoder that robustly captures object viewpoint while being insen-sitive to the in-plane rotations around the camera optical axis. 3) We demonstrate the new state-of-the-art results on
T-LESS [21], without using any images from the dataset to train our model. 2.