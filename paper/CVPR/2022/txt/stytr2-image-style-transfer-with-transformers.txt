Abstract
The goal of image style transfer is to render an image with artistic features guided by a style reference while maintaining the original content. Owing to the locality in convolutional neural networks (CNNs), extracting and maintaining the global information of input images is difﬁcult. Therefore, traditional neural style transfer methods face biased content representation. To address this critical issue, we take long-range dependencies of input images into account for image style transfer by proposing a transformer-based approach called StyTr2.
In contrast with visual transformers for other vision tasks, StyTr2 contains two different transformer encoders to generate domain-speciﬁc sequences for content and style, respectively. Following the encoders, a multi-layer transformer decoder is adopted to stylize the content sequence according to the style sequence.
We also analyze the deﬁciency of existing positional encoding methods and propose the content-aware positional encoding (CAPE), which is scale-invariant and more suitable for image style transfer tasks. Qualitative and quantitative experiments demonstrate the effectiveness of the proposed
StyTr2 compared with state-of-the-art CNN-based and ﬂow-based approaches. Code and models are available at https://github.com/diyiiyiii/StyTR-2. 1.

Introduction
Image style transfer is an interesting and practical research topic that can render a content image using a referenced style image. Based on texture synthesis, tra-ditional style transfer methods [5, 18] can generate vivid stylized images, but are computationally complex due to the formulation of stroke appearance and painting process.
Afterward, researchers focus on neural style transfer based on convolutional neural networks (CNNs). Optimization-based style transfer methods [19, 31, 47] render the input content images with learned style representation iteratively.
Following the encoder-transfer-decoder pipeline, arbitrary
*Co-corresponding authors (a) conv1_1
{conv1_1,conv2_1}
{conv1_1,...,conv4_1}
Woman with a Hat,
Henri Matisse, 1905. (b) layer_1 layer_2 layer_3
Figure 1. Comparisons of intermediate layers using the leftmost image as the input content and the style reference in a style transfer task. (a) Feature visualizations of a pretrained VGG based on Gatys et al. [19]. (b) Feature visualizations of our transformer decoder. style transfer networks [2, 3, 22, 29, 30, 33, 38, 52, 54] are optimized by aligning second-order statistics of content images to style images and can generate stylized results in a feed-forward manner efﬁciently. However, these methods cannot achieve satisfactory results in some cases due to the limited ability to model the relationship between content and style. To overcome this issue, several recent methods
[13, 14, 35, 39, 63] apply a self-attention mechanism for improved stylization results.
The aforementioned style transfer methods utilize CNNs to learn style and content representations. Owing to the limited receptive ﬁeld of convolution operation, CNNs cannot capture long-range dependencies without sufﬁcient layers. However, the increment of network depth could cause the loss of feature resolution and ﬁne details [24]. The missing details can damage the stylization results in aspects of content structure preservation and style display. As shown in Fig. 1(a), some details are omitted in the process of convolutional feature extraction. An et al. [1] recently show that typical CNN-based style transfer methods are biased toward content representation by visualizing the content leak of the stylization process, i.e., after repeating several rounds of stylization operations, the extracted structures of input
content will change drastically.
With the success of transformer [51] in natural language processing (NLP), transformer-based architectures have been adopted in various vision tasks. The charm of applying transformer to computer vision lies in two aspects. First, it is free to learn the global information of the input with the help of the self-attention mechanism. Thus, a holistic understanding can be easily obtained within each layer.
Second, the transformer architecture models relationships in input shapes [41], and different layers extract similar structural information [46] (see Fig. 1(b)). Therefore, transformer has a strong representation capability to capture precise content representation and avoid ﬁne detail missing.
Thus, the generated structures can be well-preserved.
In this work, we aim to eliminate the biased representation issue of CNN-based style transfer methods and propose a novel image Style Transfer Transformer framework called
StyTr2. Different from the original transformer, we design two transformer-based encoders in our StyTr2 framework to obtain domain-speciﬁc information. Following the encoders, the transformer decoder is used to progressively generate the output sequences of image patches. Furthermore, towards the positional encoding methods that are proposed for NLP, we raise two considerations: (1) different from sentences ordered by logic, the image sequence tokens are associated with semantic information of the image content; (2) for the style transfer task, we aim to generate stylized images of any resolution. The exponential increase in image resolution will lead to a signiﬁcant change of positional encoding, leading to large position deviation and inferior output quality. In general, a desired positional encoding for vision tasks should be conditioned on input content while being invariant to image scale transformation. Therefore, we propose a content-aware positional encoding scheme (CAPE) which learns the positional encoding based on image semantic features and dynamically expands the position to accommodate different image sizes.
In summary, our main contributions include:
• A transformer-based style transfer framework called
StyTr2, to generate stylization results with well-preserved structures and details of the input content image.
• A content-aware positional encoding scheme that is scale-invariant and suitable for style transfer tasks.
• Comprehensive experiments showing that StyTr2 outper-forms baseline methods and achieves outstanding results with desirable content structures and style patterns. 2.