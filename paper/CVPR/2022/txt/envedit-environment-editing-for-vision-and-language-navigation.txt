Abstract
In Vision-and-Language Navigation (VLN), an agent needs to navigate through the environment based on nat-ural language instructions. Due to limited available data for agent training and finite diversity in navigation envi-ronments, it is challenging for the agent to generalize to new, unseen environments. To address this problem, we propose ENVEDIT, a data augmentation method that cre-ates new environments by editing existing environments, which are used to train a more generalizable agent. Our augmented environments can differ from the seen environ-ments in three diverse aspects: style, object appearance, and object classes. Training on these edit-augmented envi-ronments prevents the agent from overfitting to existing en-vironments and helps generalize better to new, unseen en-vironments. Empirically, on both the Room-to-Room and the multi-lingual Room-Across-Room datasets, we show that our proposed ENVEDIT method gets significant im-provements in all metrics on both pre-trained and non-pre-trained VLN agents, and achieves the new state-of-the-art on the test leaderboard. We further ensemble the
VLN agents augmented on different edited environments and show that these edit methods are complementary.1 1.

Introduction
The Vision-and-Language Navigation (VLN) task re-quires an agent to navigate through the environment based on natural language instructions. Existing Vision-and-Language Navigation datasets are usually small in scale and contain a limited number of environments due to the diffi-culty of such data collection. However, navigation environ-ments might differ greatly from each other. For example, indoor navigation environments might differ in the style of the room, the layout of the furniture, and the structure of the entire house. This makes it difficult for the agent to gen-eralize to previously unseen environments. Previous works
[14,21,23,28,40,60,66] have seen that agents perform sub-1Code and data are available at https : / / github . com / jialuli-luka/EnvEdit.
Figure 1. We create synthetic environments through editing the style (a, b), object appearance (c, d), and object classes (e, f) of the training environment. Our synthetic environments serve as environment-level data augmentation during training and help the agent’s generalization to unseen testing environment. stantially worse in unseen environments, and many thought-ful methods [18,22,37,41,56,61,62] have been proposed to solve this generalization problem. One line of the previous work focuses on augmenting the environments to mitigate the environment bias. For example, [56] proposes to drop out environment-level features during training. However, this feature-dropping approach lacks the interpretability of the actually modified environments that the agent learns from to gain better generalizability. [37] takes one step fur-ther in creating in-domain augmentation data by mixing up existing training environments, which effectively reduces generalization error of VLN agents. However, these mixed-up environments did not bring unseen changes or modifica-tions to existing environments, and hence did not break the limitations of existing seen environments, which restricts agent’s generalizability to unseen environments. Thus, in this paper, we propose to create new environments that dif-fer from the original environments in style, appearance, and objects with style transfer and image synthesis approaches.
Another line of work tries to address environment bias by pre-training from large image-text datasets [18, 41], which equips the agent with diverse visual knowledge. Although
promising performance was achieved, even pre-training data in [18], which is an indoor room environment with cap-tions collected from AirBnB, still differs from the VLN task in two ways. In Vision-and-Language Navigation, the agent perceives a panoramic view and receives human-written language instructions, where in [18], the panoramic view is the concatenation of images with similar semantics, and the instruction is a template-based mixing of image descrip-tions. This leads to a domain shift in pre-training data and might not adapt well to the VLN task. Considering the large amount of pre-training data used, the performance gain on the Vision-and-Language Navigation task is still limited.
To address these challenges, in this work, we propose
ENVEDIT: Environment Editing for Vision-and-Language
Navigation. Our approach consists of three stages. In the first stage, we create new environments that maintain most of the semantic information of the original environments while changing the style, appearance, and object classes of the original environment. This constraint enables us to di-rectly adopt the original human-annotated language instruc-tions for the new environments, and avoid generating low-quality synthetic instructions [63]. As illustrated in Fig-ure 1, our generated synthetic environments are mostly con-sistent with the original environments in semantics, but dif-fer greatly in other aspects. For example, the overall style in
Figure 1 (a, b) and object appearance in Figure 1 (c, d) are different, but the semantics of the synthetic environments mostly match the original environments. Meanwhile, our synthetic environments can also moderately differ from the original environments in object semantics (e.g., Figure 1 (e) removes the pictures from the wall). Learning from these synthetic environments could enable the agent to better un-derstand visual semantics and be more robust to appearance changes of objects in different environments. Specifically, we adopt methods from style transfer [26] and image syn-thesis [49] to create new environments.
In style transfer, the newly transferred environment is created with style em-bedding sampled from the learned embedding distribution of artistic paintings. In image synthesis, we generate new environments based on semantic segmentation of the origi-nal environments, which change the appearance of the ob-jects. We further moderately edit the environment seman-tics and change objects (e.g., remove a lamp from the en-vironment) by randomly masking some semantic classes in the semantic segmentation. In the second stage, the agent learns to navigate given natural language instructions from both the original environment and our aforementioned aug-mented environments. In the last stage, we follow the ex-isting instruction-level data augmentation setup in [14, 56], which uses a speaker to generate new instructions for unan-notated paths to fine-tune the agent. But different from [56], our speaker is aware of styles and can generate different in-structions given the style of the environment.
We conduct experiments on both Room-to-Room (R2R) dataset
[2] and the multi-lingual Room-Across-Room (RxR) dataset [30]. Empirical results show that our pro-posed ENVEDIT outperforms all other non-pre-training methods by 1.6% in success rate (SR) and 1.4% in success rate weighted by path length (SPL) on R2R test leaderboard, and 5.3% in normalized Dynamic Time Warping (nDTW) and 8.0% in success rate weighted by normalized Dynamic
Time Warping (sDTW) on RxR test leaderboard. We further show that our proposed approach is beneficial to SotA pre-trained agents. Our ENVEDIT improves the performance by 3.2% in SR and 3.9% in SPL on R2R test leaderboard, and 4.7% in nDTW and 6.6% in sDTW on RxR test leaderboard, achieving the new state-of-the-art for both datasets. Lastly, we ensemble the VLN agents augmented on different edited environments and show that these editing methods are com-plementary to each other. 2.