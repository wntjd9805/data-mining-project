Abstract
Knowledge distillation (KD) has witnessed its powerful capability in learning compact models in object detection.
Previous KD methods for object detection mostly focus on imitating deep features within the imitation regions instead of mimicking classification logit due to its inefficiency in distilling localization information and trivial improvement.
In this paper, by reformulating the knowledge distillation process on localization, we present a novel localization dis-tillation (LD) method which can efficiently transfer the lo-calization knowledge from the teacher to the student. More-over, we also heuristically introduce the concept of valu-able localization region that can aid to selectively distill the semantic and localization knowledge for a certain region.
Combining these two new components, for the first time, we show that logit mimicking can outperform feature imi-tation and localization knowledge distillation is more im-portant and efficient than semantic knowledge for distilling object detectors. Our distillation scheme is simple as well as effective and can be easily applied to different dense ob-ject detectors. Experiments show that our LD can boost the AP score of GFocal-ResNet-50 with a single-scale 1× training schedule from 40.1 to 42.1 on the COCO bench-mark without any sacrifice on the inference speed. Our source code and pretrained models are publicly available at https://github.com/HikariTJU/LD. 1.

Introduction
Localization is a fundamental issue in object detection
[15, 24, 33, 49, 50, 55–57, 61, 68]. Bounding box regression is the most popular manner so far for localization in ob-ject detection [10, 32, 39, 42], where Dirac delta distribution representation is intuitive and popular for years. However, localization ambiguity where objects cannot be confidently located by their edges is still a common issue. For exam-*Equal contribution.
†Corresponding author.
Figure 1. Bottom edge for “elephant” and right edge for “surf-board” are ambiguous. ple, as shown in Fig. 1, the bottom edge for “elephant” and the right edge for “surfboard” are ambiguous to locate.
This issue is even worse for lightweight detectors. One way to alleviate this problem is the knowledge distillation (KD), which, as a model compression technology, has been widely validated to be useful for boosting the performance of the small-sized student network by transferring the generalized knowledge captured by the large-sized teacher network.
Speaking of KD in object detection, previous works
[22, 52, 62] have pointed out that the original logit mimick-ing technique [19] for classification is inefficient as it only transfers the semantic knowledge (i.e., classification), while neglects the importance of localization knowledge distilla-tion. Therefore, existent KD methods for object detection mostly focus on enforcing the consistency of the deep fea-tures between the teacher-student pair, and exploit various imitation regions for distillation [5, 8, 16, 25, 52]. Fig. 2 exhibits three popular KD pipelines for object detection.
However, as the semantic knowledge and the localization one are mixed on the feature maps, it is hard to tell whether it is beneficial to the performance to transfer the hybrid knowledge for each location and which regions are con-ducive to the transfer of a certain type of knowledge.
Motivated by the aforementioned questions, in this pa-per, instead of simply distilling the hybrid knowledge on the feature maps, we propose a novel divide-and-conquer dis-tillation strategy that transfers the semantic and localization knowledge separately. For semantic knowledge, we use the
under the same backbone, neck, and test settings. 2.