Abstract
Despite the success of deep neural networks, there are still many challenges in deep representation learning due to the data scarcity issues such as data imbalance, un-seen distribution, and domain shift. To address the above-mentioned issues, a variety of methods have been devised to explore the sample relationships in a vanilla way (i.e., from the perspectives of either the input or the loss function), fail-ing to explore the internal structure of deep neural networks for learning with sample relationships. Inspired by this, we propose to enable deep neural networks themselves with the ability to learn the sample relationships from each mini-batch. Specifically, we introduce a batch transformer mod-ule or BatchFormer, which is then applied into the batch dimension of each mini-batch to implicitly explore sample relationships during training. By doing this, the proposed method enables the collaboration of different samples, e.g., the head-class samples can also contribute to the learning of the tail classes for long-tailed recognition. Furthermore, to mitigate the gap between training and testing, we share the classifier between with or without the BatchFormer dur-ing training, which can thus be removed during testing. We perform extensive experiments on over ten datasets and the proposed method achieves significant improvements on dif-ferent data scarcity applications without any bells and whis-tles, including the tasks of long-tailed recognition, com-positional zero-shot learning, domain generalization, and contrastive learning. Code is made publicly available at https://github.com/zhihou7/BatchFormer. 1.

Introduction
Figure 1. An illustration of the relationships between different im-ages. Specifically, similar classes tend to share similar parts (e.g., cock, robin, vulture share body shape and claw shape) and trans-ferable augmentation (e.g., angles). Therefore, transferring shared knowledge from head/seen classes to tail/unseen classes can fa-cilitate long-tailed/zero-shot learning. In addition, exploring the invariant features between images belonging to the same class is also helpful for robust representation learning with a few samples. scarcity by exploring the sample relationships has received a lot of attention from the community, especially for the tasks without good training data to guarantee the generaliza-tion, such as long-tailed recognition [65], zero-shot learn-ing [46], and domain generalization [8]. However, it still remains a great challenge to find a unified, flexible, and powerful way to explore the sample relationships for robust representation learning. An intuitive example revealing the effectiveness of sample relationships is shown in Figure 1.
Despite the great success of deep neural networks for representation learning [23, 24], it heavily relies on collect-ing large-scale training data samples, which turns out to be non-trivial in real-world applications. Therefore, how to form robust deep representation learning under the data
Among recent data scarcity learning methods, sample relationships have been intensively explored using an ex-plicit scheme from either regularization [18, 40, 54, 70] or knowledge transfer [63, 66, 78]. Specifically, a sim-ple yet very effective way is to directly generate new data
samples/domains from existing training data [40], such as mixup [70], copy-paste [18], crossgrad [54], and compo-sitional learning [2, 28, 37, 46]. Another way is to trans-fer knowledge between data samples, e.g., 1) transferring meta knowledge between head and tail classes for long-tailed recognition [43, 66]; 2) transferring the knowledge from seen classes to unseen classes for zero-shot learn-ing [46, 67]; and 3) transferring invariant knowledge for domain generalization [1, 31, 49]. However, the above-mentioned methods explore sample relationships from ei-ther the input or output of deep neural networks, failing to enable deep neural networks themselves with the ability to explore sample relationships, i.e., there is no interaction from the view of batch dimension.
Enabling the learning on the batch dimension is not easy for deep neural networks due to the training and inference gap, i.e., we do not always have a mini-batch of data sam-ples during testing. For example, batch normalization re-quires to always keep mini-batch training statistics, where the running mean and variance are then used to normal-ize testing samples [32]. Another example uses the feature memory to keep category centers during training, which is then used to enhance the tail/unseen categories during test-ing [43, 78]. Therefore, to explore sample relationships for robust representation learning, we propose to empower the deep neural networks with structural advances for sample relationship learning. Specifically, we try to capture and model the sample relationships in each mini-batch of train-ing data samples by introducing a transformer into the batch dimension, and we refer to it as the Batch Transformer or
BatchFormer. Furthermore, to mitigate the gap between training and testing, we utilize a shared classifier before and after the BatchFormer module to enforce the batch-invariant learning. By doing this, the BatchFormer module is only required during training, i.e., we do not need to change the inference structures of deep neural networks.
From the perspective of optimization, BatchFormer en-ables the information propagation of all features of the mini-batch samples. Therefore, all samples can contribute to the learning on any object categories, and the insight might be that this implicitly enriches current training samples with hallucinated features from the whole mini-batch (e.g., the shared parts between two categories). For example, in long-tailed recognition, the hallucinated features may improve the feature space of the tail classes. Meanwhile, the loss function also emphasizes on the rare classes via propagat-ing larger gradients of rare classes on other features in the mini-batch.
In particular, we also find that BatchFormer brings two obvious changes to the learned representation, i.e., it effectively facilitates the deep model to learn 1) com-prehensive representations by focusing on almost all differ-ent parts of the object; and 2) invariant representations by focusing on the object itself rather than the complex back-ground cues (See more empirical evaluations in appendix).
In this paper, our main contributions can be summarized as follow: 1) we propose to explore sample relationships from the perspective of the internal structure of deep neu-ral networks; 2) we devise a simple yet effective module termed as BatchFormer, which is a plug-and-play module to explore sample relationships in each mini-batch; and 3) without any bells and whistles, we perform extensive exper-iments to demonstrate the effectiveness of BatchFormer in a variety of visual recognition tasks, including long-tailed recognition, zero-shot learning, domain generalization, and self-supervised representation learning. 2.