Abstract
Stereo matching is a fundamental building block for many vision and robotics applications. An informative and concise cost volume representation is vital for stereo matching of high accuracy and efficiency.
In this paper, we present a novel cost volume construction method which generates attention weights from correlation clues to sup-press redundant information and enhance matching-related information in the concatenation volume. To generate re-liable attention weights, we propose multi-level adaptive patch matching to improve the distinctiveness of the match-ing cost at different disparities even for textureless regions.
The proposed cost volume is named attention concatena-tion volume (ACV) which can be seamlessly embedded into most stereo matching networks, the resulting networks can use a more lightweight aggregation network and meanwhile achieve higher accuracy, e.g. using only 1/25 parameters of the aggregation network can achieve higher accuracy for
GwcNet. Furthermore, we design a highly accurate net-work (ACVNet) based on our ACV, which achieves state-of-the-art performance on several benchmarks. The code is available at https://github.com/gangweiX/ACVNet. 1.

Introduction
Stereo matching which establishes dense correspon-dences between pixels in a pair of rectified stereo images is a key enabling technique for many applications such as robotics, augmented reality, and autonomous driving. De-spite of extensive studies in this field, how to concurrently achieve a high inference accuracy and efficiency is critical for practical applications yet remains challenging.
Recently, convolutional neural networks have exhibited great potential in this field [2, 7, 12, 20]. State-of-the-art
CNN stereo models typically consist of four steps, i.e. fea-ture extraction, cost volume construction, cost aggregation
*Authors contributed equally.
â€ Corresponding author.
Figure 1. Out-Noc error vs. Run-time on the KITTI 2012 leader-board and D1-all error vs. Run-time on the KITTI 2015 leader-board. Our ACVNet, denoted by red stars, achieves competitive performance compared to other state-of-the-art stereo models. and disparity regression. Cost volume which provides ini-tial similarity measures for left image pixels and possible corresponding right image pixels is a crucial step of stereo matching. An informative and concise cost volume repre-sentation from this step is vital for the final accuracy and computational complexity. Learning-based methods ex-plore different cost volume representations. DispNetC [12] computes a single-channel full correlation volume between the left and right feature maps. Such full correlation volume provides an efficient way for measuring similarities, but it loses much content information. GC-Net [9] constructs a 4D concatenation volume by concatenating left and right feature maps along all disparity levels to provide abundant content information. However, the concatenation volume completely ignores similarity measurements, and thus re-quires extensive 3D convolutions for cost aggregation to learn similarity measurements from scratch. To tackle the above drawbacks, GwcNet [7] concatenates the group-wise correlation volume with a compact concatenation volume to encode both matching and content information in the final 4D cost volume. However, the data distribution and charac-teristics of a correlation volume and a concatenation volume are quite different, i.e. the former represents the similarity measurement obtained through dot product, and the latter is the concatenation of the unary features. Simply concatenat-ing the two volumes and regularizing them via 3D convolu-tions can hardly exert the advantages of the two volumes to the full. As a result, GwcNet still requires twenty eight 3D convolutions for cost aggregation.
This work aims to explore a more efficient and effec-tive form of cost volume, which can significantly allevi-ate the burden of cost aggregation and meanwhile achieve the state-of-the-art accuracy. We build our model based on two key observations: first, the concatenation volume con-tains rich but redundant content information; second, the correlation volume which measures feature similarities be-tween left and right images can implicitly reflect relation-ships among neighboring pixels in an image, i.e. nearby pixels which belong to the same class tend to have close similarities. This suggests that utilizing the correlation vol-ume which encodes pixel relationship prior can facilitate a concatenation volume to significantly suppress its redun-dant information and meanwhile maintain sufficient infor-mation for matching in the concatenation volume.
With these intuitions in mind, we propose an attention concatenation volume (ACV) which exploits a correlation volume to generate attention weights to filter concatenation volume (see Figure 2). To have a reliable correlation vol-ume, we propose a novel multi-level adaptive patch match-ing method to produce more accurate similarity measures, which employs multi-size patches with adaptive weights for matching pixels at different feature levels. The ACV can achieve a higher accuracy and meanwhile significantly alle-viate the burden of cost aggregation. Experimental results show that after replacing the combined volume of GwcNet with our ACV, only four 3D convolutions for cost aggrega-tion can achieve better accuracy than GwcNet which em-ploys twenty eight 3D convolutions for cost aggregation.
Our ACV is a general cost volume representation that can be seamlessly integrated into various 3D CNN stereo models for performance improvement. Results show that after ap-plying our method, PSMNet and GwcNet can respectively achieve additional a 42% and 39% accuracy improvement.
Based on the advantages of the proposed ACV, we de-sign an accurate stereo matching network ACVNet, which ranks the 2nd on the KITTI 2012 [5] and KITTI 2015 [13] benchmark, the 2nd on Scene Flow [12], and the 3rd on the
ETH3D [15] benchmark among all the published methods (see Figure 1). It is noteworthy that our ACVNet is the only method that ranks top 3 concurrently on all four datasets above, demonstrating its good generalization ability to var-ious scenes. Regarding the inference speed, our ACVNet is the fastest among the top 10 methods in the KITTI bench-marks. Meanwhile, we also design a real-time version of
ACVNet, named ACVNet-Fast, which outperforms state-of-the-art real-time methods [4, 10, 20, 22]. 2.