Abstract
Despite recent stereo matching networks achieving im-pressive performance given sufficient training data, they suffer from domain shifts and generalize poorly to unseen domains. We argue that maintaining feature consistency between matching pixels is a vital factor for promoting the generalization capability of stereo matching networks, which has not been adequately considered. Here we ad-dress this issue by proposing a simple pixel-wise contrastive learning across the viewpoints. The stereo contrastive fea-ture loss function explicitly constrains the consistency be-tween learned features of matching pixel pairs which are observations of the same 3D points. A stereo selective whitening loss is further introduced to better preserve the stereo feature consistency across domains, which decorre-lates stereo features from stereo viewpoint-specific style in-formation. Counter-intuitively, the generalization of fea-ture consistency between two viewpoints in the same scene translates to the generalization of stereo matching perfor-mance to unseen domains. Our method is generic in nature as it can be easily embedded into existing stereo networks and does not require access to the samples in the target do-main. When trained on synthetic data and generalized to four real-world testing sets, our method achieves superior performance over several state-of-the-art networks. The code is available online1. 1.

Introduction
Estimating depth from images is a fundamental problem in many computer vision applications such as autonomous driving [42] and robot navigation [1]. Stereo matching is a solution to this task, which finds the matching correspon-*Corresponding author: Xiao Bai (baixiao@buaa.edu.cn). 1https://github.com/jiaw-z/FCStereo
Figure 1. Domain generalization performance of PSMNet with and without our method on samples from (a) KITTI, (b) Middle-bury, and (c) ETH3D training sets. All models are trained on the synthetic SceneFlow dataset. dences between stereo image pairs and recovers the depth through triangulation.
Stereo matching is traditionally solved by a matching cost computation process, which usually consists of four steps [38]: matching cost computation, cost aggregation, disparity regression, and disparity refinement. Recently, end-to-end stereo matching networks [4, 15, 21, 30, 57] have been developed based on the cost computation process of traditional methods and achieved state-of-the-art accuracy.
However, the poor generalization performance on unseen domains has been a major challenge for their real-world ap-plications (see Figure 1 for an example).
A common approach to achieve generalization capability is to learn domain-invariant representations [23, 24, 26, 32].
Some stereo matching networks [3, 40, 58] have made at-tempts to tackle this issue by conducting feature-level align-ment to obtain domain-invariant features. These works project the inputs into a domain-invariant feature space, re-ducing the reliance on domain-specific appearance proper-ties and showing more robustness to domain shifts.
Here, we present a weaker constraint, stereo feature con-sistency, for domain generalized stereo networks. For each point in the left image, stereo matching looks for its match-ing one in the right view, which naturally requires robust-ness to viewpoint changes. A domain generalized stereo network is expected to generalize this matching ability to unseen domains, which means, in a nutshell, the general-ization of ”robustness to viewpoint changes”. From this perspective, we believe what a stereo network needs to gen-eralize is the matching relationship, behaving as the fea-ture consistency of paired points. For example, traditional methods, which are largely domain-agnostic [36, 43], com-pute the matching cost directly on RGB images [17]. Al-though image contents differ considerably across different domains, the matching pixels have consistent expressions between the stereo viewpoints in most cases, guarantee-ing stable matching cost computation to produce reliable disparity maps. We further verified this intuition to a toy pipeline that combines a cost volume constructed directly from RGB images with the common PSMNet cost aggre-gation module (cf. Appendix A). Such a simple pipeline with consistent stereo representations also shows a signif-icant improvement in domain generalization performance.
Generally, the appearance inconsistency within a stereo pair is limited to a certain range, thus the matching points being very similar. For example, the corresponding points should share the identical incident light as well as albedo and differ in the shading that appeared in left and right cameras. However, when the learned features are used to construct the cost volume, the feature consistency is not preserved, as shown in Figure 2a. And surprisingly, the features are inconsistent even in the training set, which is contrary to the common intuition that the weight-sharing
Siamese feature extractor has dealt with stereo viewpoint changes and extracted consistent features.
In this paper, we address the domain generalization for stereo matching methods by developing the Feature Con-sistency Stereo networks (FCStereo). Here comes two chal-lenges: (a) obtaining a high feature consistency on the train-ing set and (b) generalizing this consistency across differ-ent domains. We argue that the difficulty of (a) is due to the lack of explicit consistent constraints on features which causes overfitting. We propose the stereo contrastive feature (SCF) loss to encourage the matching points to be close in the representation space. To solve the consistency general-ization problem (b), we utilize a proper normalization oper-ation and constrain the feature statistics. A stereo selective whitening (SSW) loss is further introduced to suppress in-formation that is sensitive to stereo viewpoint changes. Fig-ure 2b illustrates the feature differences in a channel-wise manner and shows the role of the two proposed loss terms.
SCF loss encourages features to be consistent on the train-ing set. However, we see a degradation of consistency on unseen domains. SSW loss yields a relatively lower con-(a) (b)
Figure 2. Analysis about feature consistency of matching points. (a) Evaluation of popular stereo matching backbones on four un-seen domains. (b) Visualization of per-channel feature inconsis-tency. Left-right: PSMNet baseline, with our contrastive loss (C), with our whitening loss (W), and with both. More details about learned feature are shown in Appendix C. sistency compared to the contrastive loss, while the consis-tency is more robust to domain changes. Jointly using both loss terms enables high feature consistency in various do-mains. We apply the proposed method to different stereo matching backbones in the experiment and show a signifi-cant improvement in generalization performance. It demon-strates that the generalization of feature consistency be-tween two viewpoints in the same scene translates to the generalization of stereo matching performance to un-seen domains though appears counter-intuitive. A quali-tative illustration is shown in Figure 1. The main contribu-tions of this paper are as follows:
• We observe that most recent stereo methods learn in-consistent representations for the pairs of matching points, and demonstrate that the generalization perfor-mance of stereo networks can be boosted by maintain-ing a high stereo feature consistency.
• We propose two loss functions, namely the stereo con-trastive feature loss and the stereo selective whitening loss, to encourage the stereo feature consistency across domains. These two losses could be easily embedded in the existing stereo networks.
• Our method is applied to several stereo network archi-tectures and shows a significant improvement in their domain generalization performance.
2.