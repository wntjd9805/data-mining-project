Abstract
We propose a pipeline to generate Neural Radiance
Fields (NeRF) of an object or a scene of a specific class, conditioned on a single input image. This is a challenging task, as training NeRF requires multiple views of the same scene, coupled with corresponding poses, which are hard to obtain. Our method is based on π-GAN, a generative model for unconditional 3D-aware image synthesis, which maps random latent codes to radiance fields of a class of objects. We jointly optimize (1) the π-GAN objective to uti-lize its high-fidelity 3D-aware generation and (2) a carefully designed reconstruction objective. The latter includes an encoder coupled with π-GAN generator to form an auto-encoder. Unlike previous few-shot NeRF approaches, our pipeline is unsupervised, capable of being trained with inde-pendent images without 3D, multi-view, or pose supervision.
Applications of our pipeline include 3d avatar generation, object-centric novel view synthesis with a single input image, and 3d-aware super-resolution, to name a few. 1.

Introduction success
Following the of Neural Radiance
[23], encoding scenes as weights of
Fields (NeRF) multi-layer perceptrons (MLPs) has emerged as a promising research direction. Novel View Synthesis is an important application: given sparse sample views of a scene, the task is to synthesize novel views from unseen camera poses.
NeRF addresses it by encoding color and volume density at each point of the 3D scene into a neural network and uses traditional volume rendering to compose 2D views.
While NeRF is capable of synthesizing novel views with high fidelity, it is often impractical due to being “overfit-ted” to a given scene and requiring multiple views of the scene to train. Several follow-up works attempt to address these limitations via making NeRF generalize to new scenes.
Corresponding author: Shengqu Cai (shecai@ethz.ch)
Code: https://github.com/HexagonPrime/Pix2NeRF
Major progress has been made in training a general NeRF capable of encoding a scene given only one or a handful of views [5, 7, 16, 40, 41, 46]. However, these works are designed to work well only with multi-view images during either training or both training and inference.
One reason why single-shot NeRF, or in general single-shot novel view synthesis is challenging, is the incomplete content information within a single image. For example, given a frontal image of a car, there is very little informa-tion to infer a novel view from the back directly. Bringing back the traditional inverse graphics and 3D reconstruction pipelines, [44] addresses this issue by making an additional assumption on the symmetry of the scene to interpolate po-tentially missing geometry information within a single image.
However, this technique is limited to scenes where symmetry can be introduced and does not tackle the general case.
Therefore, a natural follow-up question is how does a human brain address such a challenging task? One of the approaches we use unconsciously is learning a prior implicit model for object categories and mapping what we observe to the learned model. This line of thinking is already explored in prior works [40, 46]. An essential part missing from these works is ensuring that novel views also meet our expectation of the object class, and due to the lack of supervision from a sole image, this is normally done via imagination.
One of the closest forms of imagination developed by the machine learning community is Generative Adversarial
Networks [13]. GANs have been very successful in im-age synthesis and transformation. Beyond 2D, studies have shown GAN’s capability of synthesizing 3D content [24] from natural images. This suggests another approach to ad-dress 3D reconstruction without multi-view images via 3D
GAN inversion. Such a strategy bypasses the problem of missing information within one sole image due to GAN’s adversarial training. Existing works [31, 47] utilize such a method based on HoloGAN [24], StyleGAN [47], and oth-ers, but one of the drawbacks naturally from these 3D-aware generative models is their relatively weak 3D consistency.
With the rapid increase of NeRF [23] popularity, cor-responding generative models are also gaining attention.
GRAF [35] and π-GAN [2] follow traditional GAN settings by mapping latent codes to category-specific radiance fields.
These generative models typically have high 3D consistency due to the built-in volumetric rendering design. This obser-vation suggests the possibility of few-shot 3D reconstruction using adversarial training and radiance fields.
In this paper, we formulate the task of translating an input image of a given category to NeRF as an end-to-end pipeline termed Pix2NeRF (Fig. 1). The method can perform novel view synthesis given a single image, without the need of pre-training, annotation, or fine-tuning. Pix2NeRF can be trained with natural images – without explicit 3D supervision, in an end-to-end fashion. Inspired by prior works [31, 40, 46], we introduce an encoder mapping a given image to a latent space. We jointly optimize several objectives. First, we train
π-GAN and the added encoder to map generated images back to the latent space. Second, we adapt the encoder coupled with π-GAN’s generator to form a conditional GAN, trained with both adversarial and reconstruction loss. We show that merely doing π-GAN inversion is challenging and insufficient to complete our goal, and adaptation is important for calibrating learned representations of the encoder and generator. Our framework is able to instantiate NeRF in a single shot manner while naturally preserving the ability to synthesize novel views with high fidelity, comparable to state-of-the-art generative NeRF models.
Contributions. – We propose Pix2NeRF, the first unsupervised single-shot
NeRF model, that can learn scene radiance fields from images without 3D, multi-view, or pose supervision. – Our pipeline is the first work on conditional GAN-based
NeRF, or in general, NeRF-based GAN inversion. We expect our pipeline to become a strong baseline for future works towards these research directions. – We demonstrate the superiority of our method compared with naive GAN inversion methods and conduct an exten-sive ablation studies to justify our design choices. 2.