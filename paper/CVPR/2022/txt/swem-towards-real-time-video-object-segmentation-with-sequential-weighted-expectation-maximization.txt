Abstract inference.
Matching-based methods, especially those based on space-time memory, are significantly ahead of other solu-tions in semi-supervised video object segmentation (VOS).
However, continuously growing and redundant template features lead to an inefficient
To alleviate this, we propose a novel Sequential Weighted Expectation-Maximization (SWEM) network to greatly reduce the re-dundancy of memory features. Different from the previ-ous methods which only detect feature redundancy between frames, SWEM merges both intra-frame and inter-frame similar features by leveraging the sequential weighted EM algorithm. Further, adaptive weights for frame features en-dow SWEM with the flexibility to represent hard samples, improving the discrimination of templates. Besides, the pro-posed method maintains a fixed number of template features in memory, which ensures the stable inference complexity of the VOS system. Extensive experiments on commonly used
DAVIS and YouTube-VOS datasets verify the high efficiency (36 FPS) and high performance (84.3% J &F on DAVIS 2017 validation dataset) of SWEM. 1.

Introduction
Semi-supervised video object segmentation (VOS) has seized great interest recent years [3, 6, 10, 16, 18, 21, 25, 28, 32, 33, 37, 38, 42, 43, 47, 49, 51] in the computer vision com-munity. It aims to segment the objects of interest from the background in a video, where only the mask annotation of the first frame is provided during testing. A group of early methods concentrate on on-line fine-tuning [2, 3, 19, 29, 30] with the first annotated frame. However, these method tends to suffer from model degradation caused by target appear-*Work done during an internship at Tencent AI Lab
†Corresponding Author (a) STM (b) SWEM
Figure 1. Instead of storing all past frames features as memory, just like STM [32] and following methods [6, 16, 37, 38] do, our
SWEM sequentially updates a compact set of bases with a fixed size, greatly reducing the inter-frame and intra-frame redundancy. ance changes as video goes on. Besides, propagation-based methods use masks computed in previous frames to esti-mate masks in the current frame [7, 33, 45, 48], which is, however, vulnerable to occlusions and rapid motion.
Recently, matching-based VOS methods [5, 6, 16, 17, 25, 28, 37, 38, 42, 43, 49, 52] have achieved striking perfor-mance. Such matching-based methods first exploit previ-ous frames to construct target templates, and then calculate the pixel-level correlations between the new coming frame embeddings and the target templates to perform the seg-mentation. As seen in Figure 1, the Space-Time Memory
Network (STM) [32] and the following STM-like methods
[6, 16, 37, 38, 49] leverage memory networks to store tem-plate features every T frames endlessly, which is prone to missing key-frame information and running out of memory for long-term videos. Besides, given that the inter-frame redundancy of video features would harm the efficiency of matching, another group of methods AFB URR [25] and
Swift [43] take advantage of the similarity of inter-frame features to selectively update partial features. Nonethe-less, they all fail to balance the performance and efficiency through a hand-crafted similarity threshold.
Although past efforts have achieved promising results, we argue that both inter-frame redundancy and intra-frame one pose the main obstacles that prevent efficient template
matching. Here comes to a question that can we achieve a real-time VOS system by considering both the inter-frame and intra-frame redundancy simultaneously? In this paper, we will explore its feasibility.
Inspired by the Expectation-Maximization Attention (EMA) [21], we intend to construct a set of low-rank bases for memory features through Expectation-Maximization (EM) [9] iterations. Here, the number of bases is far less than that of image pixels. Thus, bases can be re-garded as a more compact representation, which can greatly reduce the intra-frame redundancy.
Instead of applying
EM directly, we adopt Weighted Expectation-Maximization (WEM) with the predicted mask as the fixed weights to ex-plicitly construct foreground and background bases in each frame. What’s more, we also propose a weighted EM with adaptive weights, which give larger weights for hard sam-ples during generating bases. Here, the hard samples refer to those pixels that are not well expressed by bases, but are important for object segmentation.
WEM can deal with the intra-frame redundancy effec-tively; however, inter-frame one remains unsolved. Apply-ing WEM on a single frame is efficient, but the computa-tion complexity will be dramatically increased if directly applying it to all growing memory features. To further re-duce the inter-frame redundancy, we propose the Sequen-tial Weighted Expectation-Maximization (SWEM), where features of only one frame participate in the EM iterations during the memory updating stage. The memory bases will be updated with the new frame features through similari-ties rather than a simple linear combination. Formally, this updating process is equivalent to a weighted average of all past frame features. As shown in Figure 1, compared with
STM [32] which saves all historical frame information as the memory template of objects, our SWEM only updates a more compact set of bases sequentially, thus greatly reduc-ing the inter-frame and intra-frame redundancy.
Our contributions can be summarized as follows:
• We propose a fast and robust matching-based method for VOS, dubbed Sequential Weighted Expectation-Maximization (SWEM) network, where a set of com-pact bases are constructed and updated sequentially, reducing both the inter- and intra-frame redundancy.
• We introduce an adaptive weights calculation approach for weighted EM, which makes the base features pay more attention to hard samples.
• Without bells and whistles, SWEM reaches a level close to state-of-the-art performance, while maintain-ing an inference speed of 36 FPS. 2.