Abstract
Generalization under distributional shift is an open chal-lenge for machine learning.
Invariant Risk Minimization (IRM) is a promising framework to tackle this issue by ex-tracting invariant features. However, despite the potential and popularity of IRM, recent works have reported nega-tive results of it on deep models. We argue that the fail-ure can be primarily attributed to deep models’ tendency to overfit the data. Specifically, our theoretical analysis shows that IRM degenerates to empirical risk minimization (ERM) when overfitting occurs. Our empirical evidence also provides supports: IRM methods that work well in typ-ical settings significantly deteriorate even if we slightly en-large the model size or lessen the training data. To alle-viate this issue, we propose Bayesian Invariant Risk Min-imization (BIRM) by introducing Bayesian inference into the IRM. The key motivation is to estimate the penalty of
IRM based on the posterior distribution of classifiers (as opposed to a single classifier), which is much less prone to overfitting. Extensive experimental results on four datasets demonstrate that BIRM consistently outperforms the exist-ing IRM baselines significantly. 1.

Introduction
The past decade has witnessed a great success of ma-chine learning technology, boosting the development in computer vision [25, 30], speech recognition [23] and many other areas [6, 19, 35, 55, 56]. However, more in-depth stud-ies have recently revealed the failure of these models due to the existence of spurious features or shortcuts [7,14,20,53];
[7] raised an example: models could rely on the background (pastures or deserts) to distinguish cows and camels. In this case, background, a spurious feature, is non-invariant and can change arbitrarily in different domains.
The common foundation of machine learning based on the i.i.d (independent and identically distributed) assump-tion does not always hold. The Empirical Risk Minimiza-*Equal contribution. This work is supported by GRF 16201320.
†Jointly with Google Research tion (ERM) based models can deteriorate dramatically if the testing distribution is different from the training one. This is also known as the out-of-distribution (OOD) generaliza-tion problem. To relax the i.i.d. assumption, [39] propose to exploit the “invariance principle” to obtain a better OOD generalization ability. The invariance principle aims to uti-lize invariant features that are stable even in the case of dis-tributional shifts. In the aforementioned cow and camel ex-ample [7], the shape of the animal is an invariant feature.
Invariant Risk Minimization (IRM) [4] extends the in-variance principle to neural networks. Specifically, IRM considers that training data is collected from multiple envi-ronments (domains) and the correlation of spurious features with the labels differs in different environments while the correlation of invariant ones remain stable. IRM regular-izes neural networks to extract invariant features and discard spurious features. Hopefully, the model relying on invariant features only can generalize to unseen environments well.
IRM has gained its popularity for its potential and in-spires a line of excellent works [1–3,11,31,40,49,54]. IRM is guaranteed to identify the invariant features given enough environments with linear model [4, 40]. However, recent empirical findings in [24, 33] indicate the ineffectiveness of
IRM methods on deep models. We argue that this failure can be mainly attributed to deep models’ tendency to over-fit data. From a theoretical perspective, we show that IRM can ultimately degenerate to ERM when overfitting occurs.
The theoretical findings are verified by extensive empirical evidence: (1) the model trained by ERM can also minimize
IRM penalty (Section 3.3); (2) IRM-trained model can still contain spurious features while the IRM penalty vanishes (Section 5); (3) the IRM methods deteriorate quickly with an enlarged model or lessened data (Section 5).
Motivated by both the theoretical and empirical findings, we propose Bayesian Invariant Risk Minimization (BIRM) as a Bayesian treatment [8] of IRM to substantially alleviate overfitting, making IRM practical in deep models. Suppose the prediction model consists of a feature extractor and a classifier [16, 17]. Given the learned feature representation,
BIRM estimates the posterior distribution of the classifier (as opposed to a single classifier) for each environment. If
feature representation only contains invariant features, the estimated posterior in each environment should be almost the same. Otherwise, the posterior distribution will differ across environments, and to prevent this from happening, we then introduce an additional penalty term. Compared with existing IRM methods, BIRM estimates the invariance regularization on the posterior distribution of the classifier, which is less prone to overfitting [8].
Contributions.
• We formally identify overfitting as a crucial reason why
IRM fails in large deep models. We provide empirical evidence with supporting theoretical analysis.
• We propose a Bayesian formulation of IRM to alleviate overfitting, along with an efficient algorithm, reducing the chance of failure for deep IRM models significantly.
• We verify the effectiveness of Bayesian IRM with exten-sive experiments and show that our method improves the existing baselines by a large margin. 2.