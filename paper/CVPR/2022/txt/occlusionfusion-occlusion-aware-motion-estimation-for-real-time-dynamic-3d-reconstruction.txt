Abstract
RGBD-based real-time dynamic 3D reconstruction suf-fers from inaccurate inter-frame motion estimation as er-rors may accumulate with online tracking. This problem is even more severe for single-view-based systems due to strong occlusions. Based on these observations, we propose
OcclusionFusion, a novel method to calculate occlusion-aware 3D motion to guide the reconstruction. In our tech-nique, the motion of visible regions is first estimated and combined with temporal information to infer the motion of the occluded regions through an LSTM-involved graph neu-ral network. Furthermore, our method computes the con-fidence of the estimated motion by modeling the network output with a probabilistic model, which alleviates untrust-worthy motions and enables robust tracking. Experimental results on public datasets and our own recorded data show that our technique outperforms existing single-view-based real-time methods by a large margin. With the reduction of the motion errors, the proposed technique can handle long and challenging motion sequences. Please check out the project page for sequence results: https://wenbin-lin.github.io/OcclusionFusion. 1.

Introduction
Dynamic 3D reconstruction has been attracting more and more attention with the development of sensing and com-puting techniques. It aims to reconstruct the shape, motion, and appearance of the recorded objects, and thus enables users to record, edit, animate, and play with real 3D targets for various applications, including 3D design, video games, telecommunications, virtual reality, and augmented reality.
In dynamic 3D reconstruction with RGB-D sensors, the NSFC (No.61727808, 62021002),
*This work was supported by Beijing Natural Science Foundation (JQ19015), the National Key
R&D Program of China (2018YFA0704000, 2019YFB1405703) and
TC190A4DA/3. This work was supported by THUIBCS, Tsinghua Uni-versity and BLBCI, Beijing Municipal Education Commission. Jun-Hai
Yong and Feng Xu are corresponding authors.
Figure 1. Our method can reconstruct dynamic objects in real time. fusion-based works [5, 6, 15, 18, 29] have achieved impres-sive results in recent years and have become a new techno-logical trend with several important features. Firstly, these techniques do not require geometry templates of the target objects but fuse the geometries online with the recording.
Therefore, they can handle various targets, including hu-mans, objects, and even 3D scenes [8, 42]. Secondly, they can handle both rigid and nonrigid motions without requir-ing class-specific motion priors. This is also important to increase the generalization capability of 3D reconstruction.
Thirdly, they can be achieved in real time and with a single consumer sensor, which makes these techniques be easily used by end-users.
In single view-based solutions, there is a significant qual-ity gap between online and offline methods. On the online side, existing methods use iterative geometry fitting [29], sparse image feature matching [18], or photometric con-strains [15] to estimate object motions. However, these real-time techniques cannot give very reliable temporal cor-respondences, and thus the accuracy of motion estimation is limited. With the error accumulation, these techniques tend to fail to track long sequences or challenging motions.
On the other hand, offline methods can build much more accurate temporal correspondences without considering the computation complexity [5, 6], which leads to much better
reconstruction results.
We propose OcclusionFusion, which fills the quality gap between online and offline methods with real-time perfor-mance (Fig. 1) and the state-of-the-art accuracy even com-pared to offline methods. This is accomplished by better and efficiently exploring the spatial and temporal motion priors. For single view-based 3D dynamic reconstruction, occlusion is one key obstacle as the occluded regions need to be reconstructed without motion observations. On the other hand, we know that the occluded regions do not move arbitrarily. The motion of the visible regions and the histor-ical motion information give strong prior knowledge to con-strain the motion estimation of the occluded regions. Based on this observation, we propose to train a neural network to estimate full 3D motions of whole objects including the oc-cluded surfaces using the motions of the visible regions as well as the historical information. With the obtained full ob-ject motion between consecutive frames, we do not require either exhaust correspondence computation like DeepDe-form [6] or long range correspondences between multiple frames like Bozic et al. [5]. Either of them involves heavy computation costs that hinder real-time performance.
To estimate the motion of the occluded regions, we pro-pose to train a light-weight graph neural network. The graph neural network integrates both the motion of the visible re-gion by the graph structure and the historical information by involving a long short-term memory (LSTM) [16] mod-ule. Recent work 4DComplete [22] predicted the motion beyond the observable regions by a 3D convolution-based neural network. However, the 3D convolution module re-quires high computation and memory costs, which prevents their method from achieving real-time performance.
Furthermore, we model the per graph node motion using a Gaussian distribution, which not only improves the ac-curacy of motion prediction but also provides a confidence to aid the reconstruction module. With the confidence, we down-weight untrustworthy motion and improve the robust-ness of the reconstruction system.
In summary, the contributions lie in three aspects:
• We proposed a robust real-time dynamic 3D recon-struction system with a light-weight graph neural net-work for full 3D motion estimation. Various results including the one on public benchmark show that our real-time system outperforms the state-of-the-art of-fline methods.
• The graph neural network involves LSTM structure to leverage both the spatial and temporal information to predict the full object motion accurately and effi-ciently.
• Per node motion confidence is estimated by model-ing the predicted motion using a Gaussian distribution, which gives more information for the reconstruction system to achieve high robustness. 2.