Abstract
Continual learning is a longstanding research topic due to its crucial role in tackling continually arriving tasks. Up to now, the study of continual learning in computer vision is mainly restricted to convolutional neural networks (CNNs).
However, recently there is a tendency that the newly emerg-ing vision transformers (ViTs) are gradually dominating the ﬁeld of computer vision, which leaves CNN-based con-tinual learning lagging behind as they can suffer from se-vere performance degradation if straightforwardly applied to ViTs. In this paper, we study ViT-backed continual learn-ing to strive for higher performance riding on recent ad-vances of ViTs. Inspired by mask-based continual learn-ing methods in CNNs, where a mask is learned per task to adapt the pre-trained ViT to the new task, we propose
MEta-ATtention (MEAT), i.e., attention to self-attention, to adapt a pre-trained ViT to new tasks without sacriﬁc-ing performance on already learned tasks. Unlike prior mask-based methods like Piggyback, where all parameters are associated with corresponding masks, MEAT leverages the characteristics of ViTs and only masks a portion of its parameters. It renders MEAT more efﬁcient and effective with less overhead and higher accuracy. Extensive experi-ments demonstrate that MEAT exhibits signiﬁcant superior-ity to its state-of-the-art CNN counterparts, with 4.0 ∼ 6.0% absolute boosts in accuracy. Our code has been released at https://github.com/zju-vipa/MEAT-TIL. 1.

Introduction
Being capable of tackling everchanging tasks is a favor-able merit in open-world scenarios. Humans excel at solving constantly emerging tasks by associating them with previ-ously learned knowledge. Deep neural networks (DNNs), however, usually suffer from catastrophic forgetting [33] if simply adapted to new tasks due to the differences between tasks in data biases.
†Corresponding author
Figure 1. The proposed MEAT for task continual learning in the
MHSA block with vision transformers. With the increase of new tasks, MEAT dynamically assigns attention masks to generate task-speciﬁc self-attention patterns per task.
Over the past years, large pieces of literature have been devoted to addressing the catastrophic forgetting prob-lem to enable DNNs to master new-arrived tasks in a se-quence [23,27,30,36,38,41,43]. Existing continual learning methods can be broadly categorized into three schools: re-play methods [3,6,19,36,37], regularization methods [21,23, 26, 27, 34, 50, 52] and mask methods [18, 30–32, 41]. Replay methods replay previous task samples, which are stored in raw format or generated with a generative model, to alleviate forgetting while learning a new task. To avoid storing raw inputs, prioritize privacy and alleviate memory requirements, regularization methods introduce a regularization term to consolidate previous knowledge while learning the new task.
Mask methods learn a mask per task to adapt the pre-trained model to the new task for preventing any possible forgetting.
Albeit remarkable progress made in computer vision, most of the aforementioned methods are tailored for CNNs for their dominant performance in the ﬁeld over the past decade. However, the primacy of CNNs in computer vision is
recently challenged by vision transformers (ViTs) [10,28,45], due to the more general-purpose architecture (i.e., bridging the architecture gap between natural language processing and computer vision) and superior performance of ViTs. In contrast to the rapid development of ViTs, prior CNN-based continual learning methods appear a bit outdated as straight-forwardly applying them to ViTs does not take full advantage of the characteristics of transformers.
In this work, we devote ourselves to ViT-backed contin-ual learning to keep pace with the advancement of ViTs.
Speciﬁcally, we ground our proposed method on mask method [30–32, 41] for the following three main reasons: (1) mask methods in fact dedicate different parameters to each task, thus perfectly bypassing the catastrophic forget-ting problem; (2) mask methods are not sensitive to task order, which is a very favorable merit in continual learning; (3) mask methods avoid expensive data storage and exhibit a larger capacity to handle more tasks, which gives them a prominent edge over replay and regularization methods.
Motivated by these attractive advantages, we propose our
ViT-backed mask-based continual learning method, dubbed as MEta-ATtention (MEAT), to further boost the continual learning performance, as illustrated in Figure 1. MEAT inherits all the aforementioned merits, and meanwhile in-troduces the following innovations that distinguish it from prior mask methods: (1) MEAT fully leverages the archi-tectural characteristics of ViTs and introduces the attention to self-attention (where the meta-attention comes) mecha-nism, which is tailored for transformer-based architectures and makes MEAT furthermore effective. (2) Prior methods, like Piggyback [30], require manually setting the threshold hyper-parameter for binarizing the mask. MEAT adopts
Gumbel-softmax trick [20] to resolve the optimization difﬁ-culty of discrete mask values, which relaxes the burden of the hyper-parameter search. (3) Unlike prior mask-based methods where all parameters are assigned to masks, MEAT introduces masks to only a portion of its parameters, which renders it more efﬁcient than prior methods.
To validate the superiority of the proposed method, ex-tensive experiments, including benchmark comparison and ablation study, are conducted on a diverse set of image clas-siﬁcation benchmarks (including ImageNet [8], CUB [47],
Stanford Cars [24], FGVC-Aircraft [29], CIFAR-100 [25],
Sketches [11], WikiArt [40] and Places365 [53]) with vari-ous ViT variants (including DeiT-Ti [45], DeiT-S [45] and
T2T-ViT-12 [49]). Experimental results demonstrate that
MEAT exhibits signiﬁcant superiority to its state-of-the-art
CNN counterparts with 4.0 ∼ 6.0% absolute boosts in ac-curacy, meanwhile consuming much lower storage cost for saving task-speciﬁc masks.
In conclusion, the main contributions of our work are summarized as follows:
• We propose MEAT, the ﬁrst ViT-backed continual learn-ing method to the best of our knowledge, to advance the development of continual learning with ViTs.
• We introduce three innovations into MEAT, including masking partial parameters, avoiding manual hyperpa-rameter setting, and meta-attention mechanism to boost the performance of MEAT.
• Extensive experiments demonstrate that MEAT exhibits signiﬁcant superiority over its state-of-the-art CNN counterparts, meanwhile consuming much lower stor-age costs for saving task masks. 2.