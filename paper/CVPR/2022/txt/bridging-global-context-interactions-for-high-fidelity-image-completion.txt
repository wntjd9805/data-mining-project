Abstract is available at https://github.com/lyndonzheng/TFill.
Bridging global context interactions correctly is impor-tant for high-fidelity image completion with large masks.
Previous methods attempting this via deep or large recep-tive field (RF) convolutions cannot escape from the dom-inance of nearby interactions, which may be inferior.
In this paper, we propose to treat image completion as a di-rectionless sequence-to-sequence prediction task, and de-ploy a transformer to directly capture long-range depen-dence. Crucially, we employ a restrictive CNN with small and non-overlapping RF for weighted token representation, which allows the transformer to explicitly model the long-range visible context relations with equal importance in all layers, without implicitly confounding neighboring to-kens when larger RFs are used. To improve appearance consistency between visible and generated regions, a novel attention-aware layer (AAL) is introduced to better exploit distantly related high-frequency features. Overall, exten-sive experiments demonstrate superior performance com-pared to state-of-the-art methods on several datasets. Code 1.

Introduction
Image completion refers to the task of filling reasonable content with photorealistic appearance into missing regions, conditioned on partially visible information (as shown in
Fig. 1). Earlier methods infer the pixels of missing re-gions by propagating pieces from neighboring visible re-gions [1–3, 9], while more recent ones directly learn to gen-erate content and appearance using deep neural networks
[17, 28, 29, 35–37, 47, 51, 52, 54, 60].
A main challenge in this task is the requirement of bridg-ing and exploiting visible information globally, after it had been degraded by arbitrary masks. As depicted on the left of Fig. 1, when the entire person is masked, the natural ex-pectation is to complete the masked area based on the visi-ble background context. In contrast, on the right of Fig. 1, when large free-form irregular masks cover the main parts but leave the partial information visible, it is necessary but highly challenging to correctly capture long-range depen-dencies between the separated foreground regions, so that the masked area can be completed in not just a photorealis-tic, but also semantically correct, manner.
To achieve this goal, several two-stage approaches [35, 37, 51, 52, 54] have been proposed, consisting of a con-tent inference network and an appearance refinement net-work. They typically infer a coarse image/edge/semantic map based on globally visible context in a first phase, and then fill in visually realistic appearances in a second phase.
However, this global perception is achieved by repeating local convolutional operations, which have several limita-tions. First, due to the translation equivariance, the informa-tion flow tends to be predominantly local, with global infor-mation only shared gradually through heat-like propagation across multiple layers. Second, during inference, the ele-ments between adjacent layers are connected via learned but fixed weights, rather than input-dependent adaptive weight-ings. These issues mean long-distance messages are only delivered inefficiently in a very deep layer, resulting in a strong inclination for the network to fill holes based on nearby rather than distant visible pixels (cf . Fig. 1).
In this paper, we propose an alternative perspective by treating image completion as a directionless sequence-to-sequence prediction task. In particular, instead of model-ing the global context using deeply stacked convolutional layers, we design a new content inference model, called
TFill, that uses a Transformer-based architecture to Fill rea-sonable content into the missing holes. An important in-sight here is that a transformer directly exploits long-range dependencies at every encoder layer through the attention mechanism, which creates an equal flowing opportunity for all visible pixels, regardless of their relative spatial posi-tions (Fig. 4 (c)). This reduces the proximity-dominant in-fluence that can lead to semantically incoherent results.
However, it remains a challenge to directly apply these transformer models to visual generation tasks.
In partic-ular, unlike in NLP where each word is naturally treated as a vector for token embedding [10, 39, 40, 46], it is un-clear what a good token representation should be for a vi-sual task. If we use every pixel as a token, the memory cost will make this infeasible except for very small downsam-pled images [8, 47]. To mitigate this issue, our model em-beds the masked image into an intermediate latent space for token representation, an approach also broadly taken by re-cent vision transformers [6, 12, 49, 62, 64]. However, unlike these models that use conventional CNN-based encoders to embed the tokens, without considering the visible infor-mation flow in image completion, we propose a restrictive
CNN for token representation, which has a profound influ-ence on how the visible information is connected in the net-work. To do so, we ensure the individual tokens represent visible information independently, each within a small and non-overlapping patch, and forces the long-range context relationships between tokens to be explicitly and co-equally perceived in every transformer encoder layer. As a result, each masked pixel will not be gradually affected by neigh-boring visible pixels.
While the proposed transformer-based architecture can achieve better results than state-of-the-art methods [12, 51, 52, 60], by itself it only works for a fixed sequence length because of the position embedding (Fig. 2(a)). To allow our approach to flexibly scale to images of arbitrary sizes, es-pecially at high resolution, a fully convolutional network (Fig. 2(b)) is subsequently applied to refine the visual ap-pearance, building upon the coarse content previously in-ferred. A novel Attention-Aware Layer (AAL) is inserted between the encoder and decoder that adaptively balances the attention paid to visible and generated content, leading to semantically superior feature transfer (Figs. 5 and 9).
We highlight our main contributions as follows: 1) A re-strictive CNN head is introduced for individual weighted to-ken representation, which mitigates the proximity influence when propagating visible information to missing holes. 2)
Through a transformer-based architecture, the long-range interactions between these tokens are explicitly modeled, in which the masked tokens are perceptive of other visible tokens with equal opportunity, regardless of their positions. 3) A novel attention-aware layer with adaptive attention bal-ancing is introduced in a refined stage to obtain higher qual-ity and resolution results. 4) Finally, extensive experiments demonstrate that the proposed model outperforms the exist-ing state-of-the-art image completion models. 2.