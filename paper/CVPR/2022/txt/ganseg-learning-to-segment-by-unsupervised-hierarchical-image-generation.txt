Abstract
Segmenting an image into its parts is a common pre-process for high-level vision tasks such as image editing.
However, annotating masks for supervised training is ex-pensive. Weakly-supervised and unsupervised methods ex-ist, but they depend on the comparison of pairs of im-ages, such as from multi-views, frames of videos, and im-age augmentation, which limit their applicability. To ad-dress this, we propose a GAN-based approach that gener-ates images conditioned on latent masks, thereby alleviating full or weak annotations required by previous approaches.
We show that such mask-conditioned image generation can be learned faithfully when conditioning the masks in a hi-erarchical manner on 2D latent points that deﬁne the po-sition of parts explicitly. Without requiring supervision of masks or points, this strategy increases robustness of mask to viewpoint and object position changes.
It also lets us generate image-mask pairs for training a segmentation net-work, which outperforms state-of-the-art unsupervised seg-mentation methods on established benchmarks. Code can be found at https://github.com/xingzhehe/GANSeg. 1.

Introduction
This paper tackles the problem of unsupervised part seg-mentation. Discovering object parts in images is a fun-damental problem in computer vision as parts provide an intermediate representation that is robust to object appear-ance and pose variation [17, 48]. Many high-level tasks beneﬁt from part representations, such as 3D reconstruc-tion [31, 67], pose estimation [27, 42], and image editing
[14, 66]. Keypoints and part segmentation maps are among the most commonly used forms. However, their supervised training [12, 43, 56] requires pixel-level annotations for ev-ery new application domain since labels hardly generalize to other object categories and the number of parts and their granularity vary across tasks.
On the side of keypoint detection, several unsupervised detectors exist [20, 64] but segmentation methods are still in their infancy [17, 32]. Segmenting parts without pixel-level annotation is difﬁcult because it requires disentangling parts from other parts and the foreground from the back-ground. Existing unsupervised1 methods mainly follow the same strategy as applied for unsupervised keypoint detec-tion [52]. Real images are transformed by an afﬁne map or a thin plate spline to ﬁnd those parts that are equivari-ant under the known deformation. For precise reconstruc-tion they require additional information, such as saliency maps [17] or assume the objects to be consistently cen-tered [32], which is constraining. For example, when ap-plied to face datasets [33], the neck and shoulders are often ignored although part of almost every image.
Our goal is to improve the unsupervised part segmenta-tion task. We propose to ﬁrst train a generative adversar-ial network (GAN) [10] to generate images that are inter-nally conditioned on latent masks. This GAN formulation alleviates the dependency on image pairs and pre-deﬁned image transformation in existing autoencoder networks. In this way, the network learns the part distribution from the dataset instead of from pre-deﬁned image transformations.
Subsequently, we use the generator to synthesize virtually inﬁnite mask-image pairs for training a segmentation net-work. Figure 1 provides an overview of our model.
The key question we address is how to design a GAN that generates images with part segmentation masks that are meaningful, i.e., group pixels into regions that typically move together and have shared appearance across images.
We start from a backbone architecture that is borrowed from supervised segmentation networks [3, 43] and the GAN strategy is inspired by its recent application to unsupervised keypoint detection [14]. Our innovation is the hierarchical generation of the image via multiple abstraction levels, in-cluding the use of masks. In the ﬁrst level, we use Gaussian noise to generate part appearance embeddings and a set of 2D latent points. Unlike [14] which continues straight from points to image generation, we ﬁrst group points to deﬁne the position and the scale of each part. In the second ab-straction level, we use a part-relative positional encoding to 1Most existing literature refers to unsupervised when training on single images without annotation, and self-supervised when training on auxiliary tasks using multi-views or videos. We follow this convention.
Hierarchical Image Generation
Part Segmentation on Real Images
Points
Part Masks Foreground
Image
Train Part Segmentation
Figure 1. GANSeg. A segmentation network (right) is trained on mask-image pairs generated by a new hierarchical image generator (left; from points, to mask, over foreground to the image). It is unsupervised and applies to faces, persons, birds, and ﬂowers. generate 2D feature maps and then generate the mask with a CNN. In the third level, the foreground image is generated from a combination of feature maps with corresponding ap-pearance embeddings. Independently, a background image is generated with a randomized position to disentangle fore-ground and background. Finally the foreground is blended with the background. The generated masks are used here a second time to deﬁne the blend weight.
The key to the success of our GAN framework are sev-eral design choices that preserve translational equivariance of parts [22], which are not applicable to the traditional au-toencoder approaches, as explained in Appendix E. As a result, by not knowing the absolute location, the convolu-tional network is forced to condition the image purely on the spatial extent of the masks; moving the part mask will move the image part. This is a crucial inductive bias in our unsupervised learning. Our contributions are threefold: 1. An unsupervised GAN approach to generate mask-image pairs for training part segmentation; 2. A novel hierarchical image generator, which encour-ages the segmentation of meaningful parts; 3. Alleviating prior assumptions on saliency maps and object position.
Ethics - Risks. GANs can be abused for creating deep fakes. However, our method does not work towards editing nor improving image quality but scene understanding. Our
ﬁnal output is a detector, which can not be abused to gener-ate new images but unwanted surveillance applications are a risk. Beneﬁts. Since our method is entirely unsupervised, it could be applied on objects, animals, or situations that have not yet been labeled. 2.