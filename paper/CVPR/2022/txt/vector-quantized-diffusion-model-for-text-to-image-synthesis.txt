Abstract
We present the vector quantized diffusion (VQ-Diffusion) model for text-to-image generation. This method is based on a vector quantized variational autoencoder (VQ-VAE) whose latent space is modeled by a conditional variant of the recently developed Denoising Diffusion Probabilis-tic Model (DDPM). We find that this latent-space method is well-suited for text-to-image generation tasks because it not only eliminates the unidirectional bias with exist-ing methods but also allows us to incorporate a mask-and-replace diffusion strategy to avoid the accumulation of er-rors, which is a serious problem with existing methods. Our experiments show that the VQ-Diffusion produces signif-icantly better text-to-image generation results when com-pared with conventional autoregressive (AR) models with similar numbers of parameters. Compared with previous
GAN-based text-to-image methods, our VQ-Diffusion can handle more complex scenes and improve the synthesized image quality by a large margin. Finally, we show that the image generation computation in our method can be made highly efficient by reparameterization. With traditional AR methods, the text-to-image generation time increases lin-early with the output image resolution and hence is quite time consuming even for normal size images. The VQ-Diffusion allows us to achieve a better trade-off between quality and speed. Our experiments indicate that the VQ-Diffusion model with the reparameterization is fifteen times faster than traditional AR methods while achieving a bet-ter image quality. The code and models are available at https://github.com/cientgu/VQ-Diffusion. 1.

Introduction
Recent success of Transformer [11, 65] in neural lan-guage processing (NLP) has raised tremendous interest in using successful language models for computer vision tasks. Autoregressive (AR) model [4, 46, 47] is one of the most natural and popular approach to transfer from text-to-*Corresponding author. text generation (i.e., machine translation) to text-to-image generation. Based on the AR model, recent work DALL-E [48] has achieved impressive results for text-to-image generation.
Despite their success, existing text-to-image generation methods still have weaknesses that need to be improved.
One issue is the unidirectional bias. Existing methods pre-dict pixels or tokens in the reading order, from top-left to bottom-right, based on the attention to all prefix pix-els/tokens and the text description. This fixed order intro-duces unnatural bias in the synthesized images because im-portant contextual information may come from any part of the image, not just from left or above. Another issue is the accumulated prediction errors. Each step of the infer-ence stage is performed based on previously sampled to-kens – this is different from that of the training stage, which relies on the so-called “teacher-forcing” practice [15] and provides the ground truth for each step. This difference is important and its consequence merits careful examination.
In particular, a token in the inference stage, once predicted, cannot be corrected and its errors will propagate to the sub-sequent tokens.
We present the vector quantized diffusion (VQ-Diffusion) model for text-to-image generation, a model that eliminates the unidirectional bias and avoids accu-mulated prediction errors. We start with a vector quan-tized variational autoencoder (VQ-VAE) and model its la-tent space by learning a parametric model using a con-ditional variant of the Denoising Diffusion Probabilistic
Model (DDPM) [23, 59], which has been applied to im-age synthesis with compelling results [12]. We show that the latent-space model is well-suited for the task of text-to-image generation. Roughly speaking, the VQ-Diffusion model samples the data distribution by reversing a forward diffusion process that gradually corrupts the input via a fixed Markov chain. The forward process yields a sequence of increasingly noisy latent variables of the same dimen-sionality as the input, producing pure noise after a fixed number of timesteps. Starting from this noise result, the reverse process gradually denoises the latent variables to-wards the desired data distribution by learning the condi-tional transit distribution.
The VQ-Diffusion model eliminates the unidirectional bias. It consists of an independent text encoder and a dif-fusion image decoder, which performs denoising diffusion on discrete image tokens. At the beginning of the infer-ence stage, all image tokens are either masked or random.
Here the masked token serves the same function as those in mask-based generative models [11]. The denoising diffu-sion process gradually estimates the probability density of image tokens step-by-step based on the input text. In each step, the diffusion image decoder leverages the contextual information of all tokens of the entire image predicted in the previous step to estimate a new probability density dis-tribution and use this distribution to predict the tokens in the current step. This bidirectional attention provides global context for each token prediction and eliminates the unidi-rectional bias.
The VQ-Diffusion model, with its mask-and-replace dif-fusion strategy, also avoids the accumulation of errors. In the training stage, we do not use the “teacher-forcing” strat-Instead, we deliberately introduce both masked to-egy. kens and random tokens and let the network learn to pre-dict the masked token and modify incorrect tokens. In the inference stage, we update the density distribution of all to-kens in each step and resample all tokens according to the new distribution. Thus we can modify the wrong tokens and prevent error accumulation. Comparing to the conven-tional replace-only diffusion strategy for unconditional im-age generation [1], the masked tokens effectively direct the network’s attention to the masked areas and thus greatly re-duce the number of token combinations to be examined by the network. This mask-and-replace diffusion strategy sig-nificantly accelerates the convergence of the network.
To assess the performance of the VQ-Diffusion method, we conduct text-to-image generation experiments with a wide variety of datasets, including CUB-200 [66], Oxford-102 [40], and MSCOCO [36]. Compared with AR model with similar numbers of model parameters, our method achieves significantly better results, as measured by both image quality metrics and visual examination, and is much faster. Compared with previous GAN-based text-to-image methods [67, 70, 71, 73], our method can handle more com-plex scenes and the synthesized image quality is improved by a large margin. Compared with extremely large models (models with ten times more parameters than ours), includ-ing DALL-E [48] and CogView [13], our model achieves comparable or better results for specific types of images, i.e., the types of images that our model has seen during the training stage. Furthermore, our method is general and produces strong results in our experiments on both uncon-ditional and conditional image generation with FFHQ [28] and ImageNet [10] datasets.
The VQ-Diffusion model also provides important bene-fits for the inference speed. With traditional AR methods, the inference time increases linearly with the output image resolution and the image generation is quite time consuming even for normal-size images (e.g., images larger than small thumbnail images of 64 × 64 pixels). The VQ-Diffusion provides the global context for each token prediction and makes it independent of the image resolution. This allows us to provide an effective way to achieve a better tradeoff between the inference speed and the image quality by a simple reparameterization of the diffusion image decoder.
Specifically, in each step, we ask the decoder to predict the original noise-free image instead of the noise-reduced image in the next denoising diffusion step. Through ex-periments we have found that the VQ-Diffusion method with reparameterization can be fifteen times faster than AR methods while achieving a better image quality. 2.