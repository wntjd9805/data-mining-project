Abstract
Novel classes frequently arise in our dynamically chang-ing world, e.g., new users in the authentication system, and a machine learning model should recognize new classes without forgetting old ones. This scenario becomes more challenging when new class instances are insufﬁcient, which is called few-shot class-incremental learning (FSCIL). Cur-rent methods handle incremental learning retrospectively by making the updated model similar to the old one. By contrast, we suggest learning prospectively to prepare for future updates, and propose ForwArd Compatible Training (FACT) for FSCIL. Forward compatibility requires future new classes to be easily incorporated into the current model based on the current stage data, and we seek to realize it by reserving embedding space for future new classes. In detail, we assign virtual prototypes to squeeze the embed-ding of known classes and reserve for new ones. Besides, we forecast possible new classes and prepare for the up-dating process. The virtual prototypes allow the model to accept possible updates in the future, which act as proxies scattered among embedding space to build a stronger clas-siﬁer during inference. FACT efﬁciently incorporates new classes with forward compatibility and meanwhile resists for-getting of old ones. Extensive experiments validate FACT’s state-of-the-art performance. Code is available at: https:
//github.com/zhoudw-zdw/CVPR22-Fact 1.

Introduction
Recent years have witnessed the signiﬁcant breakthroughs of deep neuron networks in many vision tasks [14, 19, 36, 40, 54]. However, data often come in stream format [18] with emerging new classes [12, 63, 65] in real-world applications, e.g., new types of products in e-commerce. It requires a model to incorporate new class knowledge incrementally, which is called Class-Incremental Learning (CIL). When updating the model with new classes, a fatal problem occurs, namely catastrophic forgetting [16] — the discriminability of
†Correspondence to: Han-Jia Ye (yehj@lamda.nju.edu.cn)
Figure 1. Top: the setting of FSCIL. We need to maintain a classiﬁer covering all classes, where sessions with non-overlapping classes arrive sequentially. Ample training instances are available in the base session, while only few-shot instances in incremental sessions.
The model should incorporate new classes without forgetting old ones. Bottom: forward compatible training scheme. Different from traditional training paradigm, we reserve the embedding space for new classes in the base session for future possible extensions. old classes drastically declines. How to design effective CIL algorithms to overcome catastrophic forgetting has attracted much interest in the computer vision ﬁeld [22, 25, 48, 50, 60].
Current CIL methods address the scenario where new classes are available with sufﬁcient instances. However, the data collection and labeling cost can be relatively high in many applications. Take a rare bird classiﬁcation model for an example. We can only use few-shot images to train the incremental model since they are hard to collect. This task is called Few-Shot Class-Incremental Learning (FSCIL), which is shown at the top of Figure 1. A model needs to sequentially incorporate new classes with limited instances without harming the discriminability of old classes. Apart from the forgetting problem, it also triggers overﬁtting on few-shot instances. As a result, some algorithms [58, 67] are proposed to solve FSCIL from the few-shot learning per-spective, aiming to alleviate overﬁtting in model updating.
In FSCIL, a sequence of models should work together with harmony, i.e., the updated model should maintain the discriminability of old classes. Such a learning process is similar to software development. The newer version soft-ware should accept the data that worked under the previ-ous version, which is referred to as ‘backward compatibil-ity’ [29,43]. It measures the capability of different systems to work together without adaptation. From this perspective, the ability to overcome forgetting represents model’s backward compatibility — if an updated model is good at classifying former classes, it is compatible with the old model and does not suffer forgetting. Consequently, CIL methods seek to increase backward compatibility by maintaining discrim-inability of old classes, and FSCIL methods achieve this by
ﬁxing embedding module and incorporating new classes.
Current methods concentrate on backward compatibility, which shifts the burden of overcoming forgetting to the later model. However, if the former model works poorly, the lat-ter model would degrade consequently. It is impossible to maintain backward compatibility with limited instances in incremental stages. Take software development for an exam-ple. If an early version is poorly designed, the later version needs to work hard putting patches to maintain backward compatibility. By contrast, a better solution is to consider fu-ture extensions in the early version and reserve the interface in advance. Consequently, another compatibility, namely forward compatibility is more proper for FSCIL, which pre-pares the model for possible future updates.
The model with forward compatibility should be grow-able and provident. On the one hand, growable means the model is aware of the incoming classes in the future and makes room for their embedding space. Hence, the model does not need to squeeze the space of former classes to make room for new ones when updating. Provident indicates the ability to forecast possible future classes. The model should anticipate the future and develop methods to minimize the effects of shocks and stresses of future events. Beneﬁts from forward compatibility, the embedding space of old classes will be more compact, and new classes can be easily matched to the reserved space, as shown in Figure 1.
In this paper, we propose ForwArd Compatible Training (FACT) for FSCIL to prepare the model for future classes.
To make the model growable, we pre-assign multiple virtual prototypes in the embedding space, pretending they are the reserved space. By optimizing these virtual prototypes, we push the instances from the same class to be closer and reserve more spaces for incoming new classes. Moreover, we also generate virtual instances via instance mixture to make the model provident. The virtual instances enable us to reserve the embedding space with explicit supervision.
These reserved virtual prototypes can be seen as informative basis vectors during inference, with which we can build a strong classiﬁcation model incrementally. Vast experiments on benchmark datasets under various settings are conducted, validating the effectiveness of FACT. 2.