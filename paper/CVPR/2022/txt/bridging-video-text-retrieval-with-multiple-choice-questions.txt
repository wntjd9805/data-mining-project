Abstract
Pre-training a model to learn transferable video-text representation for retrieval has attracted a lot of attention in recent years. Previous dominant works mainly adopt two separate encoders for efficient retrieval, but ignore lo-cal associations between videos and texts. Another line of research uses a joint encoder to interact video with texts, but results in low efficiency since each text-video pair needs to be fed into the model.
In this work, we enable fine-grained video-text interactions while maintaining high effi-ciency for retrieval via a novel pretext task, dubbed as Mul-tiple Choice Questions (MCQ), where a parametric mod-ule BridgeFormer is trained to answer the “questions” con-structed by the text features via resorting to the video fea-tures. Specifically, we exploit the rich semantics of text (i.e., nouns and verbs) to build questions, with which the video encoder can be trained to capture more regional content and temporal dynamics. In the form of questions and an-swers, the semantic associations between local video-text features can be properly established. BridgeFormer is able to be removed for downstream retrieval, rendering an ef-ficient and flexible model with only two encoders. Our method outperforms state-of-the-art methods on the popu-lar text-to-video retrieval task in five datasets with different experimental setups (i.e., zero-shot and fine-tune), including
HowTo100M (one million videos). We further conduct zero-shot action recognition, which can be cast as video-to-text retrieval, and our approach also significantly surpasses its counterparts. As an additional benefit, our method achieves competitive results with much shorter pre-training videos on single-modality downstream tasks, e.g., action recogni-tion with linear evaluation. 1.

Introduction
Pre-training a model to learn transferable representa-tions for video-text retrieval requires the understanding of video concepts, text semantics, and the relationships be-tween videos and texts. Existing works for video-text pre-training can be divided into two main categories. “Dual-encoder” methods [6,11,13,22,24,27,30,38,40] (see Fig. 2 (a)) adopt two separate encoders to contrast video-level and sentence-level representations respectively, ignoring the de-tailed local information within each modality and the as-sociations between modalities.
“Joint-encoder” methods
[20, 21, 23, 35, 37, 41] (see Fig. 2 (b)) concatenate texts and videos as inputs to a joint encoder for the interactions between local features of videos and texts, sacrificing the retrieval efficiency (every text-video pair needs to be fed into the encoder during inference) for the benefits of fine-grained feature learning.
To enable fine-grained video-text interactions and mean-while maintaining high retrieval efficiency, we introduce a novel parametric pretext task for video-text pre-training, namely, Multiple Choice Questions (MCQ), which prop-erly bridges texts with videos in all their feature levels. A new module termed BridgeFormer, makes it possible, as illustrated in Fig. 1. Based on the backbone of a “dual-encoder” framework, BridgeFormer is trained to answer the
“questions” generated by the text features via visual reason-ing with the video features. MCQ enhances local feature learning within each modality as well as the fine-grained semantic associations cross modalities, and BridgeFormer can be readily removed when transferring to downstream tasks without the loss of representation discriminativeness.
Specifically, we construct the “questions” by erasing a content phrase from the raw text, and the correct “answer” should be the erased phrase itself. Motivated by the obser-vation that noun and verb phrases in a text carry rich se-mantic information [40], which can reflect the local objects and object motions in the video respectively, we randomly choose nouns or verbs as our content phrases. Bridge-Former is then trained to select the correct answer from multiple choices (all the erased content phrases in a batch) in the form of contrastive learning by resorting to the local features from the video encoder. Such a proxy training ob-jective enforces the video encoder to capture accurate spa-tial content (to answer nouns) and temporal dynamics (to answer verbs), promoting the discriminativeness of the lo-cal features and the semantic associations between the local video patches and the text phrases.
BridgeFormer connects local features of videos and texts in all feature levels (low-, mid-, and high-level), i.e., tak-ing each stage’s features from the video and text encoders as input. The regularization will be directly imposed on the video and text features, which is different from the video-text feature aggregation by the conventional “joint-encoder”. Therefore, the proxy BridgeFormer only serves for the pre-training step and can be seamlessly removed for downstream retrieval, rendering a flexible and efficient model like the conventional “dual-encoder” methods, i.e.,
Figure 2. Comparison between existing paradigms and ours for video-text pre-training. Previous dominant methods either (a) adopt two separate encoders to contrast video-level and sentence-level representations, ignoring local associations between videos and texts, or (b) use a joint encoder to interact fine-grained features of videos and texts through concatenating them as inputs, result-ing in low efficiency for retrieval. (c) We propose a novel pretext task that uses a BridgeFormer to promote local feature learning and fine-grained video-text associations. For downstream retrieval task, the proxy BridgeFormer is removed. the similarity between video and text representations can be directly measured via dot product.
Our contributions are three-fold. (1) We introduce a novel pretext task, Multiple Choice Questions (MCQ), for video-text pre-training to receive the benefits of both “dual-encoder” and “joint-encoder” methods, i.e., enhancing fine-grained semantic associations between video and text fea-tures at the same time preserving high retrieval efficiency. (2) We propose a parametric module, dubbed as Bridge-Former, to realize the pretext task of MCQ, with which the video encoder is trained to be more aware of regional ob-jects and temporal dynamics, and the associations between local video-text features are established. Since the Bridge-Former will be removed on downstream tasks, we do not increase any additional parameters or computational over-head for retrieval compared to vanilla backbones. (3) Ex-tensive results on text-to-video retrieval with different se-tups ( i.e., zero-shot and fine-tune) on five datasets, includ-ing the large-scale HowTo100M [25] (1 million videos), demonstrate the large superiority of our method (see Fig. 3 (a)). Furthermore, we evaluate zero-shot action recogni-tion, which can be cast as a video-to-text retrieval task.
Our method significantly surpasses its competitive counter-parts by a large margin, as demonstrated in Fig. 3 (a). As a bonus, we find our method also benefits single-modality video representations as shown in Fig. 3 (b), where the
video-text tokens, each pair of video and text candidates needs to be fed into the model for similarity calculation dur-ing inference, resulting in extremely low efficiency.
The pretext task of masked word prediction. Previous cross-modality pre-training work [17,23,41] use the pretext of masked word prediction (MWP), which randomly masks a proportion of words in the sentence and regularize the net-work to predict the masked words from a fixed vocabulary under the condition of visual inputs. Our introduced MCQ pretext task differs from MWP in two ways: (1) Predict-ing words in MWP imposes the regularizations on low-level word tokens, which may harm the interacted representation learning since the network also needs to serve as a text de-coder. In contrast, contrasting answers with content phrases in our MCQ focuses on high-level semantics, showing sig-nificantly better results than MWP (will be discussed in ex-periments). (2) MCQ erases noun and verb phrases to con-struct informative questions, which reflects salient semantic information in visual features, while MWP randomly masks words (e.g., function words without content). 3. Method
We adopt the “dual-encoder” structure for video-text pre-training to realize highly efficient retrieval, and propose a new pretext task, Multiple Choice Questions (MCQ), with a parametric module BridgeFormer, to enhance fine-grained semantic associations between videos and texts. In this sec-tion, we first revisit the dual-encoder in Sec. 3.1. We then introduce the pretext task MCQ in Sec. 3.2 and the pre-training objectives in Sec. 3.3. At last, we describe the ar-chitecture of three components including a VideoFormer, a
TextFormer, and a BridgeFormer in Sec. 3.4. 3.1. Dual-encoder for Video-text Pre-training: a revisit
As shown in Fig. 4, we adopt a dual-encoder structure, which consists a VideoFormer for learning video repre-sentations from raw video frame pixels, and a TextFormer for encoding text representations from natural languages.
Given a video and its corresponding text description (e.g.,
“A girl in shorts and a hat is dancing on the green grass”), we first embed their respective representations from Video-Former and TextFormer, which are projected to a common embedding space as fv and ft via two separate linear layers.
The similarity between the video and the text is calculated via the dot product between fv and ft. A contrastive objec-tive [16, 26] is utilized to maximize the similarity between fv and ft of positive pairs while minimizing the similarity between fv and ft of negative pairs (A video and its corre-sponding text description is regarded as a positive pair, and otherwise as a negative pair). The independent dual encoder pathways require only the dot product between video and (a) Comparison between recent video-text pre-Figure 3. training methods for zero-shot text-to-video retrieval on MSR-VTT (R@1), HowTo100M (R@50) and zero-shot action recog-nition (video-to-text retrieval) on HMDB51 (top-1) and UCF101 (top-1). (b) Video length for pre-training and the top-1 accuracy of action recognition with linear evaluation, where “-X” denotes the modality used for pre-training besides videos, i.e., optical flow (OF), motion vector (MV), audio (A), and text (T). top-1 accuracy of action recognition with linear evaluation is reported. Despite those considerably longer videos be-ing used in state-of-the-art pre-training methods (e.g., 11× longer in MMV [2] than ours), our method still compares favorably with them. 2.