Abstract
Transformers have shown great potential in computer vision tasks. A common belief is their attention-based to-ken mixer module contributes most to their competence.
However, recent works show the attention-based module in transformers can be replaced by spatial MLPs and the re-sulted models still perform quite well. Based on this ob-servation, we hypothesize that the general architecture of the transformers, instead of the specific token mixer mod-ule, is more essential to the model’s performance. To verify this, we deliberately replace the attention module in trans-formers with an embarrassingly simple spatial pooling op-erator to conduct only basic token mixing. Surprisingly, we observe that the derived model, termed as PoolFormer, achieves competitive performance on multiple computer vi-∗Work done during an internship at Sea AI Lab. sion tasks. For example, on ImageNet-1K, PoolFormer achieves 82.1% top-1 accuracy, surpassing well-tuned vi-sion transformer/MLP-like baselines DeiT-B/ResMLP-B24 by 0.3%/1.1% accuracy with 35%/52% fewer parameters and 49%/61% fewer MACs. The effectiveness of Pool-Former verifies our hypothesis and urges us to initiate the concept of “MetaFormer”, a general architecture ab-stracted from transformers without specifying the token mixer. Based on the extensive experiments, we argue that
MetaFormer is the key player in achieving superior results for recent transformer and MLP-like models on vision tasks.
This work calls for more future research dedicated to im-proving MetaFormer instead of focusing on the token mixer modules. Additionally, our proposed PoolFormer could serve as a starting baseline for future MetaFormer archi-tecture design.
1.

Introduction
Transformers have gained much interest and success in the computer vision field [3, 8, 43, 53]. Since the semi-nal work of vision transformer (ViT) [17] that adapts pure transformers to image classification tasks, many follow-up models are developed to make further improvements and achieve promising performance in various computer vision tasks [35, 51, 61].
The transformer encoder, as shown in Figure 1(a), con-sists of two components. One is the attention module for mixing information among tokens and we term it as token mixer. The other component contains the remaining mod-ules, such as channel MLPs and residual connections. By regarding the attention module as a specific token mixer, we further abstract the overall transformer into a general archi-tecture MetaFormer where the token mixer is not specified, as shown in Figure 1(a).
The success of transformers has been long attributed to the attention-based token mixer [54]. Based on this common belief, many variants of the attention modules
[13, 21, 55, 66] have been developed to improve the vision transformer. However, a very recent work [49] replaces the attention module completely with spatial MLPs as token mixers, and finds the derived MLP-like model can read-ily attain competitive performance on image classification benchmarks. The follow-up works [25, 34, 50] further im-prove MLP-like models by data-efficient training and spe-cific MLP module design, gradually narrowing the perfor-mance gap to ViT and challenging the dominance of atten-tion as token mixers.
Some recent approaches [31, 38, 39, 44] explore other types of token mixers within the MetaFormer architecture, and have demonstrated encouraging performance. For ex-ample, [31] replaces attention with Fourier Transform and still achieves around 97% of the accuracy of vanilla trans-formers. Taking all these results together, it seems as long as a model adopts MetaFormer as the general architecture, promising results could be attained. We thus hypothesize compared with specific token mixers, MetaFormer is more essential for the model to achieve competitive performance.
To verify this hypothesis, we apply an extremely sim-ple non-parametric operator, pooling, as the token mixer to conduct only basic token mixing. Astonishingly, this termed PoolFormer, achieves competi-derived model, tive performance, and even consistently outperforms well-tuned transformer and MLP-like models, including DeiT
[51] and ResMLP [50], as shown in Figure 1(b). More specifically, PoolFormer-M36 achieves 82.1% top-1 accu-racy on ImageNet-1K classification benchmark, surpassing well-tuned vision transformer/MLP-like baselines DeiT-B/ResMLP-B24 by 0.3%/1.1% accuracy with 35%/52% fewer parameters and 49%/61% fewer MACs. These re-sults demonstrate that MetaFormer, even with a naive token mixer, can still deliver promising performance. We thus ar-gue that MetaFormer is our de facto need for vision models which is more essential to achieve competitive performance rather than specific token mixers. Note that it does not mean the token mixer is insignificant. MetaFormer still has this abstracted component. It means token mixer is not limited to a specific type, e.g. attention. transformers
The contributions of our paper are two-fold. Firstly, we abstract into a general architecture
MetaFormer, and empirically demonstrate that the success of transformer/MLP-like models is largely attributed to the
MetaFormer architecture. Specifically, by only employing a simple non-parametric operator, pooling, as an extremely weak token mixer for MetaFormer, we build a simple model named PoolFormer and find it can still achieve highly com-petitive performance. We hope our findings inspire more future research dedicated to improving MetaFormer instead of focusing on the token mixer modules. Secondly, we eval-uate the proposed PoolFormer on multiple vision tasks in-cluding image classification [14], object detection [33], in-stance segmentation [33], and semantic segmentation [65], and find it achieves competitive performance compared with the SOTA models using sophistic design of token mix-ers. The PoolFormer can readily serve as a good starting baseline for future MetaFormer architecture design. 2.