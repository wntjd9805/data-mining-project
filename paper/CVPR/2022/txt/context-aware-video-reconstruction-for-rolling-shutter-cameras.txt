Abstract
With the ubiquity of rolling shutter (RS) cameras, it is be-coming increasingly attractive to recover the latent global shutter (GS) video from two consecutive RS frames, which also places a higher demand on realism. Existing solu-tions, using deep neural networks or optimization, achieve promising performance. However, these methods generate intermediate GS frames through image warping based on the RS model, which inevitably result in black holes and no-ticeable motion artifacts. In this paper, we alleviate these issues by proposing a context-aware GS video reconstruc-tion architecture. It facilitates the advantages such as oc-clusion reasoning, motion compensation, and temporal ab-straction. Speciﬁcally, we ﬁrst estimate the bilateral motion
ﬁeld so that the pixels of the two RS frames are warped to a common GS frame accordingly. Then, a reﬁnement scheme is proposed to guide the GS frame synthesis along with bi-lateral occlusion masks to produce high-ﬁdelity GS video frames at arbitrary times. Furthermore, we derive an ap-proximated bilateral motion ﬁeld model, which can serve as an alternative to provide a simple but effective GS frame ini-tialization for related tasks. Experiments on synthetic and real data show that our approach achieves superior perfor-mance over state-of-the-art methods in terms of objective metrics and subjective visual quality. Code is available at https://github.com/GitCVfb/CVR. 1.

Introduction
Many modern CMOS cameras equipped with rolling shutter (RS) dominate the consumer photography market due to their low cost and simplicity in design, and are also prevalent in the automotive sector and motion picture indus-try [16, 48, 52, 62]. Within this acquisition mode, pixels on the rolling shutter CMOS sensor plane are exposed from top to bottom in a row-by-row fashion with a constant inter-row delay. This leads to undesirable visual distortions called the RS effect (e.g. wobble, skew) in the presence of fast motion, which is a hindrance to scene understanding and a nuisance in photography. With the increased demand for
*Y. Dai is the corresponding author (daiyuchao@gmail.com).
Input RS
RS 0
RS 1
Ground truth GS t=0 t=0.5 t=1
Figure 1. GS video reconstruction example. The left column shows two input consecutive RS images, and three ground-truth
GS images at time 0, 0.5, and 1, respectively. Rows to the right show ﬁve GS frames (at times 0, 0.25, 0.5, 0.75, 1) extracted by [9] (top) and our method (below), followed by two correspond-ing zoom-in regions. The orange box represents occluded black holes and the red box indicates motion artifacts speciﬁc to mov-ing objects. Our method recovers higher ﬁdelity GS images due to contextual aggregation and motion enhancement. Note that the black image edges by our method are because they are not avail-able in both RS frames (cf . blue circle). Best viewed on Screen. high quality and high framerate video of consumer-grade devices (e.g. tablets, smartphones), video frame interpola-tion (VFI) has attracted increasing attention in the computer vision community. Unfortunately, despite the remarkable success, the currently existing VFI methods [2,18,38,39,56] implicitly assume that the camera employs a global shutter (GS) mechanism, i.e. all pixels are exposed simultaneously.
They are therefore unable to produce satisfying in-between frames with rolling shutter video acquired by e.g. these de-vices in dynamic scenes or fast camera movements, result-ing in RS artifacts remaining [9].
To address this problem, many RS correction methods
[13,17,24,43,55,63] have been actively studied to eliminate the RS effect. In analogy to VFI generating non-existent intermediate GS frames from two consecutive GS frames, recovering the latent intermediate GS frames from two con-secutive RS frames, e.g. [10,24,61,62], serves as a tractable
goal that overcomes the limited acquisition framerate and
RS artifacts of commercial RS cameras. This is signiﬁ-cantly challenging because the output GS frames must fol-low coherence both temporally and spatially. To this end, traditional methods [61, 62] are often based on the assump-tion of constant velocity or constant acceleration camera motion, which struggle to accurately reﬂect the real cam-era motion and scene geometry, resulting in the persistence of ghosting and unsmooth artifacts [9, 24]. Recent deep learning-based solutions have achieved impressive perfor-mance, but they typically can only recover one GS image corresponding to a particular scanline, such as the ﬁrst [10] or central [24,60] scanline, limiting their potentials for view transitions from RS to multiple-GS.
In this paper, we tackle the task of reviving and reliving all latent views of a scene as beheld by a virtual GS cam-era in the imaging interval of two consecutive RS frames.
Therefore, we must jointly deal with VFI and RS correction tasks, i.e. interpolating smooth and trustworthy distortion-free video sequences. It is worth mentioning that the most relevant work to our task is [9], which is dedicated to the geometry-aware RS inversion by warping each RS frame to its corresponding virtual GS counterpart. Nevertheless, as illustrated in Fig. 1, the GS images recovered by [9] still suffers from two limitations:
• Masses of black holes (cf . orange box). This is a com-mon issue for warping-based methods (e.g. [9, 44, 61– 63]) due to the occlusion between the RS and GS im-ages, leading to the possibility of permanent loss of some valuable image contents. To maintain visual con-sistency, a cropping operation is used to discard the re-sulting holes, but may degrade the visual experience.
• Noticeable object-speciﬁc motion artifacts (cf . red box). When recording dynamic scenes, the moving ob-ject violates the constant velocity motion assumption of RS cameras used in [9], resulting in its inability to accurately capture motion boundaries speciﬁc to mov-ing objects. Thus severe motion artifacts are generated.
In contrast, we investigate contextual aggregation and motion enhancement based on the bilateral motion ﬁeld (BMF) to alleviate these issues, which aims to synthesize crisp and pleasing GS video frames by occlusion reason-ing and temporal abstraction. Speciﬁcally, we propose CVR (Context-aware Video Reconstruction architecture), which consists of two stages to recover a faithful and coherent GS video sequence from two input consecutive RS images. In the ﬁrst initialization stage, we adopt a motion interpreta-tion module to estimate the initial bilateral motion ﬁeld, which warps the two RS frames to a common GS version.
We design two schemes to achieve this goal. One is based on [9] which requires a pre-trained encoder-decoder net-work; the other is our proposed approximation of [9], with-out resorting to a deep network. Also, we show that this simple approximation is able to provide a feasible solu-tion for the initial prediction. Afterward, a second reﬁne-ment stage is introduced to handle black holes and ambigu-ous misalignments caused by occlusions and object-speciﬁc motion patterns. As a result of exploiting bilateral motion residuals and occlusion masks, it can guide the subsequent
GS frame synthesis to reason about complex motion proﬁles and occlusions. Furthermore, inspired by [10], we propose a contextual consistency constraint to effectively aggregate the contextual information, such that the unsmooth areas can be enhanced in an adaptive manner. Extensive exper-imental results demonstrate that our method surpasses the state-of-the-art (SOTA) methods by a large margin in re-moving RS artifacts. Meanwhile, our method is capable of generating high-ﬁdelity GS videos.
The main contributions of this paper are three-fold: 1) We propose a simple yet effective bilateral motion ﬁeld approximation model, which serves as a reliable initial-ization for GS frame reﬁnement. 2) We develop a stable and efﬁcient context-aware GS video reconstruction framework, which can reason about complex occlusions, motion patterns speciﬁc to objects, and temporal abstractions. 3) Experiments show that our method achieves SOTA re-sults while maintaining an efﬁcient network design. 2.