Abstract
Estimating the risk level of adversarial examples is es-sential for safely deploying machine learning models in the real world. One popular approach for physical-world at-tacks is to adopt the “sticker-pasting” strategy, which how-ever suffers from some limitations, including difﬁculties in access to the target or printing by valid colors. A new type of non-invasive attacks emerged recently, which attempt to cast perturbation onto the target by optics based tools, such as laser beam and projector. However, the added optical patterns are artiﬁcial but not natural. Thus, they are still conspicuous and attention-grabbed, and can be easily no-ticed by humans. In this paper, we study a new type of op-tical adversarial examples, in which the perturbations are generated by a very common natural phenomenon, shadow, to achieve naturalistic and stealthy physical-world adver-sarial attack under the black-box setting. We extensively evaluate the effectiveness of this new attack on both simu-lated and real-world environments. Experimental results on trafﬁc sign recognition demonstrate that our algorithm can generate adversarial examples effectively, reaching 98.23% and 90.47% success rates on LISA and GTSRB test sets re-spectively, while continuously misleading a moving camera over 95% of the time in real-world scenarios. We also offer discussions about the limitations and the defense mecha-nism of this attack1. 1.

Introduction
In the past few years, we have witnessed the great suc-cess of deep neural networks (DNNs) in a variety of com-puter vision tasks, such as image classiﬁcation, object de-tection, scene segmentation and so on. However, recent studies have revealed that DNNs based models are vulner-able to adversarial examples, even though the added mag-nitude of perturbations is small. In safety sensitive scenar-*Corresponding author 1Our code is available at https://github.com/hncszyq/
ShadowAttack. ios, such as autonomous driving [18] and medical diagno-sis [19], adversarial inputs would enforce a machine learn-ing system to produce erroneous decision, leading to unex-pected situations that may be potentially dangerous.
Estimating when a machine learning model fails to work is of great concern to trustworthy AI. Accordingly, it is im-portant to understand the level of risk of various adversarial examples to the machine learning models. There are nu-merous attack strategies investigated in the literature, which can be in the digital domain, where digital images corre-sponding to a scene are modiﬁed; or in the physical do-main, where perturbations are physically added to the ob-jects themselves [10].
The adversarial attacks in the physical domain receive more attention recently, since they are more practical and challenging. One popular approach for physical-world at-tacks is to adopt the “sticker-pasting” strategy [10], where the adversarial perturbation is printed as a sticker and then attached onto the target object, e.g., a road sign. How-ever, this approach would suffer from a few troubles: 1)
In some cases, it may be impossible to access the target object; 2) The printing of perturbations is imperfect, i.e., some values cannot be reproduced by valid colors in the real world. Some strategies emerged very recently to rem-edy these limitations, which explored a new type of adver-sarial attack—the threats of light—to achieve non-invasive attack [9, 12, 22, 28]. For instance, Duan et al. [9] propose to utilize the laser beam as adversarial perturbation directly rather than crafting adversarial perturbation from scratch, which has been demonstrated to be an effective physical-world attack to DNNs. Gnanasambandam et al. [12] pro-pose to project calculated patterns to alter the appearance of the target objects based on a low-cost projector-camera sys-tem. Experimental results shown in [9, 12] demonstrate the effectiveness of light attacks in both digital- and physical-settings.
A basic principle in adversarial attack is that the care-fully perturbed inputs should cause the network to generate wrong decision while without introducing a visible change to humans. However, the added optical patterns in existing
methods, both laser beam [9] and projected pattern [12], are artiﬁcial but not natural. Thus, they are still conspicu-ous and attention-grabbed, and can be easily noticed by hu-mans. In this paper, we study a new type of optical adversar-ial examples, in which the perturbations are generated by a very common natural phenomenon, shadow, to achieve nat-uralistic and stealthy physical-world adversarial attack. We choose trafﬁc sign recognition as our target task to validate the effectiveness of this attack. It is worth noting that, in the
ﬁeld of computer vision, many methods have been proposed to conduct shadow removal [11, 17, 33], which work from the perspective of image restoration in order to improve the accuracy of the subsequent high-level tasks [34]. We are the
ﬁrst work to reveal that shadow can also become a threat to harm the reliability of machine learning based vision sys-tem. It is a meaningful reminding to prevent such attacks in practical systems.
The contributions of this paper can be highlighted as fol-lows:
• We propose a new light-based physical-world ad-versarial attack under the black-box setting via the very common natural phenomenon—shadow, which is more naturalistic and stealthy.
• We offer feasible optimization strategies to generate digitally and physically realizable adversarial exam-ples perturbed by shadows.
• We provide thorough evaluations conducted on both simulated and real-world environments to demonstrate the effectiveness of our method. We also discuss the limitations and the defense mechanism of this attack. 2.