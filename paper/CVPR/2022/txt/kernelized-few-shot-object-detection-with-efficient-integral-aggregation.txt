Abstract
We design a Kernelized Few-shot Object Detector by leveraging kernelized matrices computed over multiple pro-posal regions, which yield expressive non-linear represen-tations whose model complexity is learned on the ﬂy. Our pipeline contains several modules. An Encoding Network encodes support and query images. Our Kernelized Auto-correlation unit forms the linear, polynomial and RBF ker-nelized representations from features extracted within sup-port regions of support images. These features are then cross-correlated against features of a query image to obtain attention weights, and generate query proposal regions via an Attention Region Proposal Net. As the query proposal regions are many, each described by the linear, polyno-mial and RBF kernelized matrices, their formation is costly but that cost is reduced by our proposed Integral Region-of-Interest Aggregation unit. Finally, the Multi-head Rela-tion Net combines all kernelized (second-order) represen-tations with the ﬁrst-order feature maps to learn support-query class relations and locations. We outperform the state of the art on novel classes by 3.8%, 5.4% and 5.7% mAP on
PASCAL VOC 2007, FSOD, and COCO. 1.

Introduction
CNN object detectors [8, 29–31] require thousands of manually annotated images for training. Their performance drops during adaptation to novel classes if samples are few.
In contrast, Few-shot Learning (FSL) methods rapidly adapt to new visual concepts [39, 41, 43] but off-the-shelf
FSL methods perform classiﬁcation rather than Few-shot
Object Detection (FSOD). As queries in FSOD contain mul-tiple objects of various categories and FSOD detectors have to predict class labels and locations of objects in a query im-age, effective techniques capturing query-support similari-ties across multiple Regions-of-Interest (RoI) are required.
FSOD models [2, 6, 11, 12, 52, 58] are trained with so-called training episodes containing samples of common ob-jects (i.e. base classes). Testing episodes contain support images of rare objects (i.e. novel classes) and query im-ages in which these rare objects must be recognized/local-ized. Fan et al. [6] introduced into FSOD a Region Proposal
Network (RPN), termed Attention RPN (ARPN)1. ARPN cross-correlates average-pooled features from support re-gions with features of the query image, which produces an attention map over the feature tensor of query image. How-ever, average pooling (a ﬁrst-order statistic) retains less in-formation compared to higher-order statistics. PNSD [58] improves [6] by second-order pooling but is limited to so-called linear correlations (autocorrelation matrix). To ad-dress this limitation, we use kernelized covariance matri-ces [57] and Reproducing Kernel Hilbert Space (RKHS) kernels [38] which capture non-linear patterns. Kernels in-duce regularization e.g., an RBF kernel with a small (resp. large) radius captures a complex (resp. simple) decision boundary. However, as generating kernel matrices is com-putationally expensive, they are rarely used in detection.
We propose a novel feature representation which lever-ages the expressiveness and regularization capabilities of kernels, while enjoying an efﬁcient implementation. The key to this efﬁciency is a novel Integral Region-of-Interest
Aggregation (IRA) scheme for fast kernelization. We fur-ther accelerate IRA by count sketching [47], an unsuper-vised dimensionality reduction technique with a favourable property of implicitly performing feature augmentations.
As the variance introduced by sketching is inversely-proportional to its size, it boosts the accuracy and compu-tational speed, as described in Section 4. Our pipeline is shown in Figure 1. Our contributions are listed below: i. We propose two types of kernelized representations used conjointly for FSOD that capture non-linear cor-relation patterns, obtained from candidate regions by computationally efﬁcient Integral Region-of-Interest
Aggregation (IRA). The performance and speed of IRA are boosted with count sketching and its inverse to fa-cilitate the practical use of kernelization in FSOD (gen-erating hundreds of kernels per image).
*Equal contribution. PK is the corresponding author.
Code: https://github.com/ZS123-lang/KFSOD. 1PSND [58] calls this module Hyper Attention RPN (HARPN) but
FSOD-ARPN [6] calls it ARPN. We adopt the ARPN name for brevity.
ii. We equip our network with MLP units which learn the kernel hyper-parameters on the ﬂy to adjust the learning complexity of kernelized representations to the data.
We partially whiten matrices using Spectral Power Nor-malization [16] whose hyper-parameters are learnt via another MLP to extract the most informative features that concentrate along diagonals of kernel matrices. iii. We redesign a Multi-head Relation Network to com-bine the ﬁrst-order spatially-ordered features of support and query regions with the spatially orderless kernel-ized representations that contain higher-order statistics.
Advantages of RKHS kernelization in FSOD. We note that (i) kernels are very good at capturing non-linear re-lationships between feature channels of each candidate bounding box, (ii) kernels factor out spatial order while keeping rich statistics about each region, thus matching sim-ilar objects that vary in physical location, orientation, view-point is easy due to the shift-invariance, (iii) kernels let con-trol the model complexity w.r.t. the region size and visual complexity, (iv) typical FSOD head uses either shift-variant or average pooled representations (we combine both). 2.