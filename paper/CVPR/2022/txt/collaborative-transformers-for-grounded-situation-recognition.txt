Abstract
Grounded situation recognition is the task of predicting the main activity, entities playing certain roles within the activity, and bounding-box groundings of the entities in the given image. To effectively deal with this challenging task, we introduce a novel approach where the two processes for activity classiﬁcation and entity estimation are interactive and complementary. To implement this idea, we propose
Collaborative Glance-Gaze TransFormer (CoFormer) that consists of two modules: Glance transformer for activity classiﬁcation and Gaze transformer for entity estimation.
Glance transformer predicts the main activity with the help of Gaze transformer that analyzes entities and their rela-tions, while Gaze transformer estimates the grounded enti-ties by focusing only on the entities relevant to the activity predicted by Glance transformer. Our CoFormer achieves the state of the art in all evaluation metrics on the SWiG dataset. Training code and model weights are available at https://github.com/jhcho99/CoFormer. 1.

Introduction
Humans make decisions via dual systems of thinking as stated in the cognitive theory by Kahneman [13]. Those two systems are known to work in tandem and complement each other [8, 26]. Consider a comprehensive scene understand-ing task as a speciﬁc example of such decision making. As illustrated in Figure 1, humans cast a quick glance to ﬁgure out what is happening, and slowly gaze at details to analyze which objects are involved and how they are related. These two processes are mutually supportive, e.g., understanding involved objects and their relations leads to more accurate recognition of the event depicted in the scene.
Inspired by this, we propose a collaborative framework which leverages the two processes for Grounded Situation
Recognition (GSR) [27]. GSR is a comprehensive scene un-derstanding task that is recently introduced as an extension of Situation Recognition (SR) [38]. The objective of SR is to produce a structured image summary that describes the main activity and entities playing certain roles within the
Figure 1. Two processes in comprehensive scene understanding.
Glance ﬁgures out what is happening, and Gaze analyzes entities engaged in the main activity and their relations. In our CoFormer, these two processes are interactive and complementary. activity, where the roles are predeﬁned for each activity by a lexical database called FrameNet [7]. In GSR, those in-volved entities are grounded with bounding boxes; Figure 2 presents example results of GSR. Following conventions, we call an activity verb and an entity noun in this paper.
The common pipeline of SR and GSR in the litera-ture [3, 4, 18, 25, 27, 30, 37, 38] resembles the two processes: predicting a verb (Glance), then estimating a noun for each role associated with the predicted verb (Gaze). Regarding this pipeline, correctness of the predicted verb is extremely important since noun estimation entirely depends on the predicted verb. If the result of verb prediction is incorrect, then estimated nouns cannot be correct either because the predicted verb determines the set of roles, i.e., the basis of noun estimation. Moreover, verb prediction is challenging since a verb is highly abstract and situations for the same verb could signiﬁcantly vary as shown in Figure 2. In spite of its importance and difﬁculty, verb prediction has been made in na¨ıve ways, e.g., using a single classiﬁer on top of a convolutional neural network (CNN), which is anal-ogous to Glance only. Existing methods allow Glance to assist Gaze by informing the predicted verb but not vice versa; this could limit the performance of verb prediction, and consequently, that of the entire pipeline.
We resolve the above issue by a collaborative framework that enables Glance and Gaze to interact and complement
(cid:104)human, object, interaction(cid:105). However, it is not straight-forward to evaluate the quality of natural language captions, and the triplets have limited expressive power. To overcome such limitations, Yatskar et al. [38] introduce SR along with the imSitu dataset. SR has more expressive power based on linguistic sources from FrameNet [7], and its quality evalua-tion is straightforward. GSR builds upon SR by additionally estimating bounding-box groundings.
Situation Recognition. Yatskar et al. [38] propose a con-ditional random ﬁeld [16] model, and also present a ten-sor composition method with semantic augmentation [37].
Mallya and Lazebnik [25] employ a recurrent neural net-work to capture role relations in the predeﬁned sequential order. Li et al. [18] propose a gated graph neural network (GGNN) [19] to capture the relations in more ﬂexible ways.
To learn context-aware role relations depending on an in-put image, Suhail and Sigal [30] apply a mixture kernel method to GGNN. Cooray et al. [4] employ inter-dependent queries to capture role relations, and present a verb model which considers nouns from the two predeﬁned roles; they construct a query based on two nouns for verb prediction.
Compared with this, CoFormer considers nouns from all role candidates for accurate verb prediction.
Grounded Situation Recognition. Pratt et al. [27] propose
GSR along with the SWiG dataset, and present two mod-els: Independent Situation Localizer (ISL) and Joint Situa-tion Localizer (JSL). They ﬁrst predict a verb using a single classiﬁer on top of a CNN backbone, then estimate nouns and their groundings. In both models, LSTM [11] produces output features to predict nouns in the predeﬁned sequen-tial order, while RetinaNet [21] estimates their groundings.
ISL separately predicts nouns and their groundings, and JSL jointly predicts them. Cho et al. [3] propose a transformer encoder-decoder architecture, where the encoder effectively captures high-level semantic features for verb prediction and the decoder ﬂexibly learns the role relations. Compared with these models, CoFormer leverages involved nouns and their relations for accurate verb prediction via transformers.
Transformer Architecture. Transformers [31] have driven remarkable success in vision tasks [1, 2, 6, 9, 15, 17, 22, 24].
Dosovitskiy et al. [6] propose a transformer encoder archi-tecture for image classiﬁcation by aggregating image fea-tures using a learnable token in the encoder. Carion et al. [1] present a transformer encoder-decoder architecture for ob-ject detection by predicting a set of bounding boxes using a ﬁxed number of learnable queries in the decoder. Such learnable queries have been widely used to extract features in other transformer architectures [15, 17, 22]. Compared with those transformers, CoFormer employs two learnable tokens which aggregate different kinds of features through self-attentions. In addition, CoFormer constructs a differ-ent number of learnable queries by explicitly leveraging the prediction result obtained by two encoders and a classiﬁer.
Figure 2. Two examples of Grounded Situation Recognition [27].
These show various situations for the same verb. each other. To fully utilize this framework, we propose
Collaborative Glance-Gaze TransFormer (CoFormer) that consists of Glance transformer and Gaze transformer as il-lustrated in Figure 3. Glance transformer predicts a verb by aggregating image features through self-attentions, and
Gaze transformer estimates nouns and their groundings by allowing each role to focus on its relevant image region through self-attentions and cross-attentions. As shown in
Figure 3, there are two steps for Gaze in our CoFormer.
Gaze-Step1 transformer estimates nouns for all role candi-dates and assists Glance transformer for more accurate verb prediction. Meanwhile, Gaze-Step2 transformer estimates a noun and its grounding for each role associated with the predicted verb by exploiting the aggregated image features obtained by Glance transformer.
The collaborative relationship between Glance and Gaze transformers lead to more accurate verb and grounded noun predictions for GSR. In CoFormer, Gaze-Step1 supports
Glance by analyzing involved nouns and their relations, which enables noun-aware verb prediction. Glance assists
Gaze-Step2 by informing the predicted verb, which reduces the role candidates considered in grounded noun prediction.
Contributions. (i) We propose a collaborative framework where the two processes for verb prediction and noun esti-mation are interactive and complementary, which is novel in GSR. (ii) Our method achieves state-of-the-art accuracy in every evaluation metric on the SWiG dataset. (iii) We demonstrate the effectiveness of CoFormer by conducting extensive experiments and provide in-depth analyses. 2.