Abstract
Videos typically record the streaming and continuous vi-sual data as discrete consecutive frames. Since the stor-age cost is expensive for videos of high ﬁdelity, most of them are stored in a relatively low resolution and frame rate. Recent works of Space-Time Video Super-Resolution (STVSR) are developed to incorporate temporal interpo-lation and spatial super-resolution in a uniﬁed frame-work. However, most of them only support a ﬁxed up-sampling scale, which limits their ﬂexibility and appli-cations.
In this work, instead of following the discrete representations, we propose Video Implicit Neural Repre-sentation (VideoINR), and we show its applications for
STVSR. The learned implicit neural representation can be decoded to videos of arbitrary spatial resolution and frame rate. We show that VideoINR achieves competi-tive performances with state-of-the-art STVSR methods on common up-sampling scales and signiﬁcantly outperforms prior works on continuous and out-of-training-distribution scales. Our project page is at here and code is avail-able at https://github.com/Picsart-AI-Research/VideoINR-Continuous-Space-Time-Super-Resolution. 1.

Introduction
We observe the visual world in the form of streaming and continuous data. However, when we record such data with a video camera in a computer, it is often stored with limited spatial resolutions and temporal frame rates. Because of the high cost on recording and storing large time-scales of video data, oftentimes our computer vision system will need to process low-resolution and low frame rate videos. This introduces challenges in recognition systems such as video object detection [53], and we are still struggling at learning to recognize motion and actions from discrete frames [4, 12]. When presenting the video back to humans (e.g., on a TV), it is essential to visualize it in high resolution and
† Corresponding authors.
Figure 1. Video Implicit Neural Representation (VideoINR) maps any 3D space-time coordinate to an RGB value. This nature en-ables extending the latent interpolation space of STVSR from ﬁxed space and time scales to arbitrary frame rate and spatial resolution. high frame rate for user experience. How to recover the low resolution video back to high resolution in space and time becomes an important problem and the ﬁrst step for many downstream applications.
Space-Time Video Super-Resolution (STVSR) ap-proaches [15, 21, 28, 37, 38, 47, 48] are developed to in-crease the spatial resolution and frame rate at the same time given a low-resolution and low frame rate video as the input. Instead of performing super-resolution in space and time separately in two stages, researchers recently pro-pose to simultaneously perform super-resolution in one stage [15, 21, 47, 48]. Intuitively, the aggregated informa-tion in time from multiple frames can reveal missing details for each frame when spatial scaling is applied, and the tem-poral interpolation can be more smooth and accurate given higher and richer spatial representation. The one-stage end-to-end training has shown to unify the beneﬁts from both
sides. While these results are encouraging, most approaches can only perform super-resolution to a ﬁxed space and time scale ratio. poral scales and signiﬁcantly outperforms other methods on out-of-distribution scales.
We highlight our main contributions as follows:
In this paper, instead of super-resolution in a ﬁxed scale, we propose to learn a continuous video representation, which allows to sample and interpolate the video frames in arbitrary frame rate and spatial resolution at the same time.
Our key idea is to learn an implicit neural representation, which is a neural function that takes a space-time coordi-nate as input, and outputs the corresponding RGB value.
Since we can sample the coordinate continuously, the video can be decoded in any spatial resolution and frame rate. Our work is inspired by recent progress on implicit functions for 3D shape representations [10, 13, 14, 26] and image repre-sentations with Local Implicit Image Functions (LIIF) using a ConvNet [7]. Different from images, where interpolation in space can be based on the gradients between pixels, pixel gradients across frames with low frame rates are hard to compute. The network will need to understand the motion of the pixels and objects to perform interpolation, which is hard to model by 2D or 3D convolutions alone.
We propose a novel Video Implicit Neural Representa-tion (VideoINR) as a continuous video representation. In the STVSR task, two low-resolution image frames are con-catenated and forwarded to an encoder which generates a feature map with spatial dimensions. VideoINR then serves as a continuous video representation over the generated fea-ture map. It ﬁrst deﬁnes a spatial implicit neural represen-tation for a continuous spatial feature domain, from which a high-resolution image feature is sampled according to all query coordinates.
Instead of using convolutional opera-tions to perform temporal interpolation, we learn a temporal implicit neural representation to ﬁrst output a motion ﬂow
ﬁeld given the high-resolution feature and the sampling time as inputs. This ﬂow ﬁeld will be applied back to warp the high-resolution feature which will be decoded to the tar-get video frame. Since all the operations are differentiable, we can learn the motion in feature level end-to-end without any extra supervision besides the reconstruction error. To summarize, given the input frames, an encoder generates a feature map, which can be then decoded by VideoINR to arbitrary spatial resolution and frame rate.
In our experiments, we demonstrate that VideoINR can not only represent video in arbitrary space and time reso-lutions on the scales within the training distributions, but also extrapolate to out-of-distribution frame rates and spa-tial resolutions. Given the learned continuous function, in-stead of decoding the whole video each time, it allows the
ﬂexibility to decode only a certain region and time scale when needed. We conduct experiments with Vid4 [23], Go-Pro [29] and Adobe240 [41] datasets. We demonstrate that
VideoINR achieves competitive performances with state-of-the-art STVSR methods on in-distribution spatial and tem-• We propose a novel Video Implicit Neural Representa-tion as a continuous video representation.
• The proposed approach allows for representing videos in arbitrary space and time resolution efﬁciently with one single network.
• VideoINR achieves out-of-distribution generalization and outperforms baselines by a large margin. 2.