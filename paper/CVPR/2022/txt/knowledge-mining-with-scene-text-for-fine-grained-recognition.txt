Abstract
Recently, the semantics of scene text has been proven to be essential in fine-grained image classification. However, the existing methods mainly exploit the literal meaning of scene text for fine-grained recognition, which might be irrel-evant when it is not significantly related to objects/scenes.
We propose an end-to-end trainable network that mines im-plicit contextual knowledge behind scene text image and en-hance the semantics and correlation to fine-tune the image representation. Unlike the existing methods, our model in-tegrates three modalities: visual feature extraction, text se-mantics extraction, and correlating background knowledge to fine-grained image classification. Specifically, we em-ploy KnowBert to retrieve relevant knowledge for seman-tic representation and combine it with image features for fine-grained classification. Experiments on two benchmark datasets, Con-Text, and Drink Bottle, show that our method outperforms the state-of-the-art by 3.72% mAP and 5.39% mAP, respectively. To further validate the effectiveness of the proposed method, we create a new dataset on crowd ac-tivity recognition for the evaluation. The source code and new dataset of this work are available at this repository1. 1.

Introduction
The text conveys the information, knowledge, and emo-tion of human beings as a significant carrier. Texts in natu-ral scene images contain sophisticated semantic information that can be used in many vision tasks such as image classifi-cation, visual search, and image-based question answering.
Several approaches [2,15,22,23,26,39] were proposed to incorporate semantic cues of scene text for image classifi-cation or retrieval and achieved significant performance im-provements. These methods follow a general pipeline that first spots the text by a scene text reading system, then con-verts the spotted word into text features to combine it with image features for the subsequent tasks.
*Authors contribute equally.
†Corresponding author. 1https://github.com/lanfeng4659/KnowledgeMiningWithSceneText
Figure 1. The three images belong to the category of “Soda”. (d) shows the knowledge behind scene text embodied in the image (c) from knowledge base Wikipedia. Each text instance contains one or more entities stored in the knowledge base. The associated descriptions further explain the precise meaning of entity. Only the entities of two text instances are listed for simplicity.
This paper explores how to dig deeper into background knowledge and extract context information of scene text for the fine-grained image classification task. Unlike docu-ment text, in our observation, the natural scene text is often sparse, appearing as a few keywords rather than complete sentences. Moreover, these few keywords may be vague and give no clue to the classification model when their se-mantic cues are not directly related to the precise meaning that the image conveys.
As shown in Fig. 1 (a) and (b), the literal meaning of the keyword “Soda” explicitly expresses that the bottles in the two images belong to the category Soda despite their intra-class visual variance. However, we hardly understand the object in Fig. 1 (c) by solely fetching the semantic cues of scene text. To understand the image certainly, getting more relevant contextual knowledge about the image is crucial.
Therefore, we explore how to dig extra background knowl-edge and mine the contextual information to enhance the correlation between scene text and a picture. For example, the table in Fig. 1 (d) exhibits related information or knowl-edge of scene text embodied in (c). The description of the entity Leninade informs that it is a Soda beverage bottle.
Thus, the knowledge extracted in this manner complements the literal meaning of the raw text and reduces the semantics loss caused by using the literal meaning of scene text only.
Specifically, after extracting the text from the image by a scene text reading system [20, 40], we retrieve relevant knowledge from databases such as ( e.g., WordNet [25] and
Wikipedia) that store rich human-curated knowledge with all possible correlation to the target. As shown in Fig. 1 (d), the possible entities ( e.g., party and political party) can be extracted for the text instance “party” from the knowledge databases. However, all the retrieved contextual knowledge may not necessarily provide helpful semantic cues to un-derstand the visual contents. In order to filter relevant con-textual information from irrelevant, we design an attention module that focuses on very pertinent knowledge for the se-mantics of objects or scenes.
We evaluate the performance of our method on two pub-lic benchmark datasets, Bottles [2] and Con-Text [16]. The results demonstrate the usage of contextual knowledge be-hind scene text can significantly promote fine-grained im-age classification models performances. To further prove the effectiveness of our method, we developed a new dataset consisting of 21 categories and 8785 natural images. Fur-thermore, the dataset mainly focuses on crowd activity, while most images contain multiple scene text instances.
To the best of our knowledge, the existing crowd activ-ity datasets do not contain scene text instances. However, everyday human activities are highly related to scene text presences, for example, procession, exhibitions, press brief-ing, and sales campaigns. This dataset will be a valuable asset for exploring the role of scene text on crowd activity.
In this paper, we propose a method that mines contextual knowledge behind scene text to improve the performance of the multi-modality understanding task. To this end, we de-sign a deep-learning-based architecture that combines three modality features, including visual contents, scene text, and knowledge for fine-grained image recognition. Our method achieves significant improvements and can be applied to other tasks, such as visual grounding [33] and text-visual question answering [3] beyond the fine-grained image clas-sification task. In addition, we propose a new dataset where each image contains multiple scene text instances, which promotes the study of multi-modal crowd activity analysis. ous methods [6, 10] classify objects with only visual cues and aim at finding a discriminative image path. Recently, some approaches have shown a growing interest in employ-ing textual cues to combine the visual cues for this task.
Movshovitz et al. [26] first propose to leverage scene text for the fine-grained image classification task by using the visual cues of scene text. However, extracting robust visual cues of scene text is challenging due to blur and occlusion of text instances. Karaoglu et al. [15] employ the textual cues of scene text as a discriminative signal and combine the vi-sual features that are obtained by the GoogLeNet [38] to distinguish business place. To fully exploit the complemen-tarity of visual information and textual cues, several meth-ods [2, 22] propose to fuse features of the two modalities with an attentional module. Bai et al. [2] propose an at-tention mechanism to select textual features from word em-beddings of recognized words. To overcome optical char-acter recognition errors, Mafla et al. [22] leverage the usage of the PHOC [1] representation to construct a bag of tex-tual words along with the fisher vector [29] that models the morphology of text. Despite the promising progress, the ex-isting methods exploit the literal meaning of scene text and overlook the meaningful human-curated knowledge of text. 2.2. Knowledge-aware Language Models
The pre-trained language models such as ELMo [30] and
BERT [8] are optimized to either predict the next word or some masked words in a given sequence. Petroni et al. [32] find that the pre-trained language models, such as BERT, can recall factual and commonsense knowledge.
Such knowledge is stored implicitly in the parameters of the language model and useful for downstream tasks such as visual question answering [17]. This knowledge is usually obtained either from the latent context representations pro-duced by the pre-trained model or by using the parameters of the pre-trained model to initialize a task-specific model for further fine-tuning. To further enhance the language model awareness of human-curated knowledge better, some works [31, 34] explicitly integrate the knowledge in knowl-edge bases into the pre-trained language model.
In our method, we employ both BERT [8] and KnowBert [31] as a knowledge-aware language model and apply them to extract knowledge features. Although previous methods [36] ex-tract knowledge features from sentences on vision-language tasks, they require the annotation of image-text pairs. 2.