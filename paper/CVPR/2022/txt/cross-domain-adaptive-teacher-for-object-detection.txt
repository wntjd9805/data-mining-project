Abstract
We address the task of domain adaptation in object de-tection, where there is an obvious domain gap between a domain with annotations (source) and a domain of interest without annotations (target). As a popular semi-supervised learning method, the teacher-student framework (a student model is supervised by the pseudo labels from a teacher model) has also yielded a large accuracy gain in cross-domain object detection. However, it suffers from the do-main shift and generates many low-quality pseudo labels (e.g., false positives), which leads to sub-optimal perfor-mance. To mitigate this problem, we propose a teacher-student framework named Adaptive Teacher (AT) which leverages domain adversarial learning and weak-strong data augmentation to address the domain gap. Specifically, we employ feature-level adversarial training in the student model, allowing features derived from the source and target domains to share similar distributions. This process ensures the student model produces domain-invariant features. Fur-thermore, we apply weak-strong augmentation and mutual learning between the teacher model (taking data from the target domain) and the student model (taking data from both domains). This enables the teacher model to learn the knowledge from the student model without being biased to the source domain. We show that AT demonstrates su-periority over existing approaches and even Oracle (fully-supervised) models by a large margin. For example, we achieve 50.9% (49.3%) mAP on Foggy Cityscape (Cli-part1K), which is 9.2% (5.2%) and 8.2% (11.0%) higher than previous state-of-the-art and Oracle, respectively. 1.

Introduction
Developing algorithms that can transfer the knowledge learned from one labeled dataset (i.e., source domain) to another unlabeled dataset (i.e., target domain) becomes in-* Work done during the internship at Meta (Facebook).
Figure 1. The effectiveness of domain loss and weak-strong augmentation on pseudo labeling in Mean Teacher (MT) [40].
The figure shows the false positive ratio on the training set of Cli-part1k (target) with PASCAL VOC as source. We run 5 identical experiments for each setting and plot the error bound accordingly.
Due to inherent domain shift in the Teacher model, it generates noisy pseudo labels without domain loss. The weak-strong aug-mentation is able to stabilize pseudo labeling. creasingly important for object detection. Researchers have proposed various methods, such as domain classifier and ad-versarial learning [10], to address the task of cross-domain adaptation in object detection [2, 3, 14, 32, 39, 42, 44]. Even though these methods have led to accuracy improvement, solely using adversarial learning on the complex recogni-tion task such as object detection is still limited. Hence, there is generally still a large performance gap from the Or-acle model (fully supervision) on the target domain.
To explore the potential of self-training on the unla-beled target domain for improved detection performance, researchers have exploited and extend the teacher-student self-training method from semi-supervised learning to do-main adaptation [40]. These approaches are able to learn
without annotations by typically involving a teacher model to generate pseudo labels to update student model. These methods have led to notable accuracy gains in the domain adaptation scenario. For example, MTOR [1] employs the
Mean Teacher (MT) [40] as its pipeline to identify relations using region-level, inter-graph, and intra-graph consistency.
Unbiased Mean Teacher (UMT) [8] is proposed to aug-ment the teacher-student framework with CycleGAN [43] and achieved further performance improvement.
Despite the accuracy gain, the teacher-student frame-work still face a major challenge upon the settings of domain adaptation: Unlike semi-supervised learning, the pseudo label generated from the teacher model usually con-tains a substantial amount of errors and false positives, as shown in Figure 1. This is because the scenario of domain adaptation typically involves a large domain gap between the labeled data (source domain) and unlabeled data (tar-get domain). The teacher model is trained on, biased to, and only able to capture features precisely on the source domain, hence unable to provide high-quality pseudo la-bels in the target domain. As a result, directly applying the teacher-student framework only leads to sub-optimal adap-tation performance.
To address this problem, we propose a self-training framework named Adaptive Teacher (AT) to mitigate the domain shift and improve the pseudo labeling quality on the target domain leveraging adversarial learning and mu-tual learning. Our model comprises of two separate mod-ules: target-specific Teacher model and cross-domain Stu-dent model. We also apply weak augmentation (only strong augmentation in Student model) and feed images from the target domain into the Teacher model, which we refer to as
“Weak-Strong augmentation”, following Unbiased Teacher (UT) [22]. This allows the teacher model to generate re-liable pseudo labels without being affected by heavy aug-mentation. In addition, to mitigate the domain bias toward source domain in the Student model, we apply adversar-ial learning by introducing a discriminator with gradient reverse layer to align the distribution across two domains in the Student model. With all the techniques, we observe the pseudo label quality improved significantly, as shown in Figure 1, where the false positive ratio is suppressed by up to 35%. This further leads to substantial accuracy gain across all the domain adaptation experiments and outper-forms all existing methods. We summarize the contribu-tions of this paper as follows:
• We demonstrate the limitation of the teacher-student framework in the domain adaptation scenario: The teacher model is biased toward the source domain and only able to produce low-quality pseudo labels on the target domain.
• We propose a novel framework leveraging adversarial learning augmented mutual learning and weak-strong augmentation to address domain shift in cross-domain object detection.
• Our method is able to deal with domain shift and out-perform all existing SOTA by a large margin. For ex-ample, we achieve 50.9% mAP on Foggy Cityscape, which is 9.2% and 8.2% higher than SOTA and Oracle (full supervision). 2.