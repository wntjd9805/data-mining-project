Abstract
Most state-of-the-art instance segmentation methods have to be trained on densely annotated images. While difficult in general, this requirement is especially daunt-ing for biomedical images, where domain expertise is of-ten required for annotation and no large public data collec-tions are available for pre-training. We propose to address the dense annotation bottleneck by introducing a proposal-free segmentation approach based on non-spatial embed-dings, which exploits the structure of the learned embed-ding space to extract individual instances in a differentiable way. The segmentation loss can then be applied directly to instances and the overall pipeline can be trained in a fully-or weakly supervised manner. We consider the challeng-ing case of positive-unlabeled supervision, where a novel self-supervised consistency loss is introduced for the un-labeled parts of the training data. We evaluate the pro-posed method on 2D and 3D segmentation problems in dif-ferent microscopy modalities as well as on the Cityscapes and CVPPP instance segmentation benchmarks, achieving state-of-the-art results on the latter. 1.

Introduction
Instance segmentation is one of the key problems ad-dressed by computer vision. It is important for many ap-plication domains, from astronomy to scene understanding in robotics, forming the basis for the analysis of individual object appearance. Biological imaging provides a partic-ularly large set of use cases for the instance segmentation task, with imaging modalities ranging from natural pho-tographs for phenotyping to electron microscopy for de-tailed analysis of cellular ultrastructure. The segmentation task is often posed in crowded 3D environments or their 2D projections with multiple overlapping objects. Additional challenges – compared to segmentation in natural images – come from the lack of large, publicly accessible, annotated
Correspondence to: anna.kreshuk@embl.de
Code: github.com/kreshuklab/spoco training datasets that could serve for general-purpose back-bone training. Most microscopy segmentation networks are therefore trained from scratch, using annotations produced by domain experts in their limited time.
Over the recent years, several weakly supervised seg-mentation approaches have been introduced to lighten the necessary annotation burden. For natural images, image-level labels can serve as a surprisingly strong supervision thanks to the popular image classification datasets which include images of individual objects and can be used for pre-training [11]. There are no such collections in mi-croscopy (see also Fig. 5 for a typical instance segmenta-tion problem example where image-level labels would be of no help). Semi-supervised instance segmentation meth-ods [4, 3, 9] can create pseudo-labels in the unlabeled parts of the dataset. However, these methods require (weak) an-notation of all the objects in at least a subset of images – a major obstacle for microscopy datasets which often contain hundreds of tightly clustered objects, in 3D.
Figure 1. Differentiable instance selection for non-spatial embed-ding networks. First, we sample an anchor point randomly or guided by the groundtruth instances. Second, we compute a dis-tance map in the embedding space from the anchor point to all image pixels. In the final step, a kernel function (Eq 3) transforms the distance map to the “soft” instance mask.
The aim of our contribution is to address the dense an-notation bottleneck by proposing a different kind of weak supervision for the instance segmentation problem: we re-quire mask annotations for a subset of instances in the im-age, leaving the rest of the pixels unlabeled. This “posi-tive unlabeled” setting has been explored in image classi-fication and semantic segmentation problems [34, 30], but
- to the best of our knowledge - not for instance segmen-tation. Intrinsically, the instance segmentation problem is
Figure 2. Overview of training procedure. Two augmented views of an input image are passed through two embedding networks f (·) and g(·) respectively. Anchor pixels inside labeled objects (blue dots) are sampled and their corresponding instances are extracted as shown in Fig. 1. Discrepancy between extracted objects and groundtruth objects is minimized by the instance-based loss. Another set of anchors (yellow triangles) is sampled exhaustively from the unlabeled region and for each anchor two instances are selected based on the outputs from f (·) and g(·). Discrepancy between instances is minimized using embedding consistency loss. very well suited for positive unlabeled supervision: as we also show empirically (Appendix A.5), sampling a few ob-jects in each image instead of labeling a few images densely exposes the network to a more varied training set with better generalization potential. This is particularly important for datasets with sub-domains in the raw data distribution, as it can ensure all sub-domains are sampled without increasing the annotation time. Furthermore, in crowded microscopy images which commonly contain hundreds of objects, and especially in 3D, dense annotation is significantly more dif-ficult and time consuming than sparse annotation, for the same total number of objects annotated. The main obstacle for training an instance segmentation method on sparse ob-ject mask annotations lies in the assignment of pixels to in-stances that happens in a non-differentiable step which pre-cludes the loss from providing supervision at the level of in-dividual instances. To lift this restriction, we propose a dif-ferentiable instance selection step which allows us to incor-porate any (differentiable) instance-level loss function into non-spatial pixel embedding network [6] training (Fig. 1).
We show that with dense object mask annotations and thus full supervision, application of the loss at the single instance level consistently improves the segmentation accuracy of pixel embedding networks across a variety of datasets. For our main use case of weak positive unlabeled (PU) supervi-sion, we propose to stabilize the training from sparse object masks by an additional instance-level consistency loss in the unlabeled areas of the images. The conceptually simple unlabeled consistency loss, inspired by [21, 46], does not re-quire the estimation of class prior distributions or the prop-agation of pseudo-labels, ubiquitously present in PU and other weakly supervised segmentation approaches [47, 33].
In addition to training from scratch, our approach can de-liver efficient domain adaptation using a few object masks in the target domain as supervision.
In summary, we address the instance segmentation task with a CNN that learns pixel embeddings and propose the first approach to enable training with weak positive unla-beled supervision, where only a subset of the object masks are annotated and no labels are given for the background.
To this end, we introduce: (1) a differentiable instance se-lection step which allows to apply the loss directly to indi-vidual instances; (2) a consistency loss term that allows for instance-level training on unlabeled image regions, (3) a fast and scalable algorithm to convert the pixel embeddings into final instances, which partitions the metric graph derived from the embeddings. We evaluate our approach on natu-ral images (CVPPP [37] , Cityscapes [14]) and microscopy datasets (2D and 3D, light and electron microscopy), reach-ing the state-of-the-art on CVPPP and consistently outper-forming strong baselines for microscopy. On all datasets, the bulk of CNN performance improvement happens after just a fraction of training objects are annotated. 2.