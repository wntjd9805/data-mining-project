Abstract
Open-world instance segmentation is the task of group-ing pixels into object instances without any pre-determined taxonomy. This is challenging, as state-of-the-art meth-ods rely on explicit class semantics obtained from large la-beled datasets, and out-of-domain evaluation performance drops significantly. Here we propose a novel approach for mask proposals, Generic Grouping Networks (GGNs), con-structed without semantic supervision. Our approach com-bines a local measure of pixel affinity with instance-level mask supervision, producing a training regimen designed to make the model as generic as the data diversity allows.
We introduce a method for predicting Pairwise Affinities (PA), a learned local relationship between pairs of pixels.
PA generalizes very well to unseen categories. From PA we construct a large set of pseudo-ground-truth instance masks; combined with human-annotated instance masks we train GGNs and significantly outperform the SOTA on open-world instance segmentation on various benchmarks including COCO, LVIS, ADE20K, and UVO. 1.

Introduction
Instance segmentation is the task of grouping pixels into object instances [22]. In the closed-world setup, the task is to detect and segment objects from a predefined taxon-omy. In contrast, the open-world setting requires segment-ing objects of arbitrary categories. For a model trained in a closed-world setup, this means segmenting not only the
“seen” categories (those presented at training time) but also
the “unseen” categories (not seen during training) [27, 54].
There is generally a large performance gap between the seen and unseen domains. Leading computer vision sys-tems today have tightly coupled recognition and segmenta-tion; these systems are unable to segment out objects that they cannot recognize (e.g. Fig 1 (c)). Comparing Aver-age Recall (AR@100) of Mask R-CNN [22] trained on 80
COCO [33] classes vs a subset of 20 classes, AR@100 of 60 classes out of training taxonomy drops from 49.6% to 19.9% when no mask of these classes is provided in training data. The “unseen” gap remains large if we train on larger taxonomy (e.g., 1,000+ classes in training data) Table 5).
In contrast, humans can readily group and segment objects which they cannot categorize - few of us can identify the 6500 Passerine bird species, but we can readily segment out a perching bird from a tree branch. Or use another often-quoted example: our familiarity with a generic quadruped body plan enables us to segment out horses, donkeys and zebras, and even an okapi when first encountered.
On the other hand, models which were common in com-puter vision in 2000-2015 (e.g., [1, 4, 6, 18, 51, 58]), before deep learning for supervised object detection took off, were quite category-agnostic. They didn’t work as well as, say,
Mask R-CNN on a category for which it has trained, but they worked across the board (e.g. Figure 1 (b)). The goal was to come up with a moderately sized set of object pro-posals which included the true objects. The emphasis was on recall; precision was secondary. MCG [4] is an illustra-tive example. It starts with local grouping which produces a set of elementary regions of coherent color and texture,
“super-pixels”. These typically over-segment objects; e.g. a person might be broken up into a face, a torso, legs, parts of clothing, shadows, etc. MCG then assembles regions into objects by considering various groupings of regions, and ranks them on some “objectness” score. While some learning is involved in both edge detection and objectness ranking, the method works primarily with hand-crafted fea-tures and a small number of parameters, quite unlike the deep learning zeitgeist.
How do we get the best of both worlds? A modern in-stance segmentation system (eg. Mask R-CNN) would do well if given comprehensive training data containing a large number of examples from all visual categories. While we have a practically infinite supply of raw natural images, ob-taining mask annotation is very expensive. Multiple ap-proaches have emerged to handle this data problem. Self-supervised learning [10, 11, 20, 42] is the most well-known; self-learning [46, 47, 59, 62] is another approach, based on the classical idea of adding high-confidence guessed la-bels to previously unlabeled data, and then combining this
“pseudo ground truth” data with real ground truth. We ex-ploit this second strategy.
Our approach begins with a learned pairwise affinity pre-dictor (Figure 2a), followed by a module which extracts and ranks segments (Figure 2b, essentially a very simpli-fied version of MCG [4]). We can run this on any image dataset without using annotations; we extract the highest ranked segments as “pseudo ground truth” candidate ob-jects. This is a large and category-agnostic set; we add it to our (much smaller) datasets of curated annotations, to train a Mask R-CNN instance segmentation module. Ideally this model should become more generic and class-agnostic (Fig-ure 2c). Indeed, this simple approach produces impressive gains compared to closed-world training on the same back-bone (Mask R-CNN) (Table 5, Table 6, Table 7): +11% on VOC to Non-VOC cross-category evaluation, +3.9% on COCO to LVIS cross-category evaluation, +5.8% on
COCO to ADE20K and +5.2% on COCO to UVO.
Our contributions in this paper include:
• A novel approach, Generic Grouping Networks (GGNs), for open-world instance segmentation; GGN exploits additional pseudo ground truth supervision generated from learned pixel-level pairwise affinities.
• Comprehensive ablation experiments which provide insights about GGNs and the problem of open-world instance segmentation.
• GGNs achieve state-of-the-art performance in open-world instance segmentation on various benchmarks including COCO, LVIS, ADE20K, and UVO. 2.