Abstract
Abductive reasoning seeks the likeliest possible expla-nation for partial observations. Although abduction is fre-quently employed in human daily reasoning, it is rarely ex-plored in computer vision literature. In this paper, we pro-pose a new task and dataset, Visual Abductive Reasoning (VAR), for examining abductive reasoning ability of ma-chine intelligence in everyday visual situations. Given an in-complete set of visual events, AI systems are required to not only describe what is observed, but also infer the hypothe-sis that can best explain the visual premise. Based on our large-scale VAR dataset, we devise a strong baseline model,
REASONER (causal-and-cascaded reasoning Transformer).
First, to capture the causal structure of the observations, a contextualized directional position embedding strategy is adopted in the encoder, that yields discriminative represen-tations for the premise and hypothesis. Then, multiple de-coders are cascaded to generate and progressively refine the premise and hypothesis sentences. The prediction scores of the sentences are used to guide cross-sentence information flow in the cascaded reasoning procedure. Our VAR bench-marking results show that REASONER surpasses many fa-mous video-language models, while still being far behind human performance. This work is expected to foster future efforts in the reasoning-beyond-observation paradigm. 1.

Introduction
Abduction · · · consists in studying facts and devising a theory to explain them. – Charles Sanders Peirce (1839 – 1914)
Abductive reasoning [50] was coined by Charles Sanders
Peirce, the founder of American pragmatism, around 1865.
It is inference to the most likely explanation or conclusion for an incomplete set of observations. Abductive reasoning is invariably employed in our everyday life; the generated hypothesis (H) is expected to best explain what happens before, after, or during the observation (O). Fig. 1 gives some examples. If you see O: “the road is wet”, abduction
*Part of this work was done when Chen Liang was an intern at Baidu.
Figure 1. Abductive reasoning is inference to the most likely ex-planation for an incomplete set of observations. will lead you to the best explanation H: “it rained earlier” (i.e., H →O). One morning you find O: “sister leaves home hurriedly”, then you conclude H: “she will be late for class” (i.e., O → H). You see O1: “a boy throws a frisbee out and his dog is running after it”. One minute later you find
O2: “frisbee is in the boy’s hand”. You can imagine H:
“the dog just caught the frisbee back” (i.e., O1 → H → O2).
Although abductive reasoning has long been considered as a core ability of everyday human cognition [39,54,56], it still remains an untouched domain in computer vision literature.
In this article, we propose Visual Abductive Reasoning (VAR), a novel task and dataset for investigating the abduc-tive reasoning ability of AI systems in daily visual situa-tions. In particular, inspired by the recent advance of causal reasoning in NLP community (i.e., abductive text genera-tion [5] and counterfactual story revision [51]), we explore the use of natural language as the expression form to fully capture the complexity of real situations. This also better re-flects the nature of human mind, which relies on linguistic thinking [37, 38]. VAR requires the AI systems to describe the incomplete observation (i.e., visual premise) and write down the hypothesis that can best explain the premise. This allows to thoroughly evaluate the entire abduction proce-dure, as accurate understanding of the premise is the basis of abductive reasoning. Moreover, this can hasten the devel-opment of this new field, by comparing and embracing ideas for a relevant, well-established, yet different task – dense video captioning (DVC) [29]. In contrast to DVC that fo-cuses only on describing the observation, VAR yields a new
visual-linguistic reasoning paradigm – inference beyond ob-servation. Three characteristics make VAR uniquely chal-lenging: i) VAR needs imagination to find hypotheses out-side the observation. ii) VAR seeks to discover the plausi-ble causal structure among the observed events. iii) VAR is more related to the kind of human reasoning in daily situa-tions where the information at hand is often incomplete [25] and absolutely certain conclusions cannot be reached [5,26].
Our dataset is collected to address the characteristics of the VAR task (cf. §3). It contains 9K examples from 3,718 videos. Each example consists of several chronologically-ordered events, most of which are logically related. For each event, abduction oriented description is written by peo-ple, and its role of premise or explanation is also annotated.
When presenting each example to the AI system, the expla-nation event is masked and premise events are visible. The
AI system is required to understand the partial, noisy obser-vations (i.e., premise events) and construct the most plau-sible explanatory hypothesis – accurately describing both the observable premise events and the masked explanation event. The examples are on average 37.8s long, with 4.17 events, and harvested from diversely event-rich sources, i.e.,
YouTube Lifestyle videos, movies and TV shows.
To lay a solid foundation for future efforts, a new model, named REASONER (causal-and-cascaded reasoning Trans-former), is proposed (cf. §4). Specifically, REASONER is building upon a Transformer encoder-decoder architecture.
In the encoder of REASONER, a contextualized directional position embedding strategy is adopted to capture causal de-pendencies among the premise events. Hence the context of the premise events can be gathered in a causality-aware manner, enabling REASONER to learn discriminative repre-sentations for the premise and explanatory hypothesis. Then
REASONER cascades a set of decoders for premise/hypoth-esis sentence production and refinement. For each gener-ated sentence, the associated prediction score is viewed as the confidence and embedded into the next decoder as a signal for inspiring more information to flow from high-confident sentences to the low-confident ones. This leads to a confidence-guided multi-step reasoning strategy, boost-ing the reasoning power of REASONER eventually.
Extensive experimental results are provided in §5. First, to comprehensively probe deep neural models on this chal-lenging task, we establish a group of baselines based on current top-leading DVC models. The benchmarking re-sults on VAR dataset show that REASONER outperforms the best competitor by a large margin, e.g., 33.44 vs 28.71 in terms of BERT-S, but is still far behind human performance (42.96). This shows that VAR is especially challenging for current video-language models. Then a set of user studies and ablative experiments are conducted for a thorough eval-uation. For completeness, we further test REASONER on the DVC task and confirm again its superiority.
Concurrent to us, [16] studies image-based abductive rea-soning: AI systems are required to identify, ground, or com-pare given inferences. Overall, we feel vision-based abduc-tive reasoning is an intriguing problem worthy of exploring. 2.