Abstract
What makes a video task uniquely suited for videos, be-yond what can be understood from a single image? Build-ing on recent progress in self-supervised image-language models, we revisit this question in the context of video and language tasks. We propose the atemporal probe (ATP), a new model for video-language analysis which provides a stronger bound on the baseline accuracy of multimodal models constrained by image-level understanding. By ap-plying this model to standard discriminative video and lan-guage tasks, such as video question answering and text-to-video retrieval, we characterize the limitations and poten-tial of current video-language benchmarks. We find that un-derstanding of event temporality is often not necessary to achieve strong or state-of-the-art performance, even com-pared with recent large-scale video-language models and in contexts intended to benchmark deeper video-level under-standing. We also demonstrate how ATP can improve both video-language dataset and model design. We describe a technique for leveraging ATP to better disentangle dataset subsets with a higher concentration of temporally challeng-ing data, improving benchmarking efficacy for causal and temporal understanding. Further, we show that effectively integrating ATP into full video-level temporal models can improve efficiency and state-of-the-art accuracy.1 1.

Introduction
Videos offer the promise of understanding not only what can be discerned from a single image (e.g. scenes, people, and objects), but also multi-frame event temporality, causal-ity, and dynamics (Figure 1(a)). Correspondingly, there lies a central question at the heart of video research: What makes a video task uniquely suited for videos, beyond what can be understood from a single image?
As a field, video analysis has considered this ques-tion deeply in the context of action classification in videos
[3, 17, 43, 50]. The emergence of strong convolutional mod-1Project website: https://stanfordvl.github.io/atp-revisit-video-lang/ (a) The promise of videos lies in the potential to go
Figure 1. beyond image-level understanding (scenes, people, etc.) to capture event temporality, causality, and dynamics. (b) In this work, we propose an atemporal probe (ATP) model to revisit the video in standard benchmarks [29,53,55] for video question answering and text-to-video retrieval, offering a stronger image-centric baseline and analytical tool. For example, ATP finds non-trivial subsets of
“causal” questions that can be answered with (c) only image-level understanding, rather than (d) full video-level understanding. els for image classification [15] enabled researchers to bet-ter characterize the limits of single-frame understanding for recognizing actions [17, 50]. A key finding from this anal-ysis was that, in many standard video datasets [24, 47] at the time, temporal understanding was simply not required
to perform well on these benchmarks. For example, recog-nizing static scene context like the presence of a pool was sufficient to recognize the “diving” activity from a single frame [31, 50]. The impact of such analysis was tremen-dous: later datasets were designed to capture a richer distri-bution of temporal understanding [6, 13, 46] with better dis-entanglement of such cues [33], and model designs evolved further to better capture the now necessary dynamics to ad-dress these improved tasks [9–11, 30, 51].
Meanwhile, the recent advent of self-supervised image-language models [20, 41] with competitive performance to standard image-classification models [7, 15] means that we have a unique opportunity to reconsider this funda-mental question in the context of standard discriminative video-language tasks, such as video question answering
[29,53,55] and video-language retrieval [16,23,55]. In par-ticular, we can now build beyond prior (video-only) analy-sis work, largely constrained to recognition settings of lim-ited atomic actions in relatively short clips, towards more complex (temporal, causal) event understanding in longer-horizon, multimodal settings where the expressivity of nat-ural language can potentially describe a richer event space.
The primary motivation of our work is to analyze these existing video-language benchmarks by revisiting the video, and derive insights that can help guide the further develop-ment of the field. Our driving question is, to what extent can image-level understanding obtained from a single frame (well-chosen, without temporal context) address the current landscape of video-language tasks? To accomplish this, we make the following key contributions:
First, we introduce the atemporal probe (ATP) model to provide a stronger bound on the capabilities of image-level understanding in video-language settings than traditional random frame and mean pooling baselines [50]. Here, we leverage a frozen self-supervised image-language model (e.g. CLIP [40]) to extract a set of image and language rep-resentations: our ATP model must then learn to select a sin-gle frozen representation corresponding to a single frame, and forward that to the downstream video-language task.
Critically, our framework is constrained to not be capable of reasoning temporally, and its output is ultimately bottle-necked by what a frozen image-language model can discern from an individual, decontextualized video frame.
Second, we apply ATP to analyze a wide range of video-language datasets, focusing primarily on video question an-swering with extensions to text-to-video retrieval (per Fig-ure 1(b)). To our surprise, we find that many standard and recent benchmarks can be potentially well-addressed with single-frame image understanding. In particular, while this was not our primary aim, we find that our learned ATP model is able to outperform recent state-of-the-art video-language models on standard vision-language benchmarks
[16, 23, 29, 53, 55], despite its substantial bottleneck con-straints on model capacity, capability, and inputs. We find that even recent benchmarks that explicitly design for tem-poral and causal understanding (e.g., [53]), can have a non-trivial subset of questions answerable by simple single-frame event recognition. As shown in Figure 1(c), while the question asking “why” an event occurred suggests causal understanding may be needed, our ATP model shows that in practice simple scene and object recognition can ascer-tain the correct answer from a single chosen frame.
Finally, we examine how ATP and the insights it pro-vides can help with improving both dataset and video-level temporal modeling designs. As a case study, we closely ex-amine the NExT-QA benchmark [53]. We find that ATP is able to better identify collections of “causal” and “tempo-ral” questions that cannot be well-addressed with single-frame understanding.
In Figure 1(d), ATP struggles to answer this question since it necessitates multi-event rea-soning across time. By improving the disentanglement of video- and image-level understanding in the benchmark data, we can better understand the progress of state-of-the-art video techniques leveraging motion features and event reasoning architectures over image-centric models, a result that is not as apparent in the original setting. We further val-idate our analysis by training a temporal video-level model on top of our ATP selectors, achieving a new state-of-the-art for this benchmark with improved efficiency. Taken together, our analysis suggests key avenues by which our
ATP technique can guide continued development of video-language datasets and models in future work. 2.