Abstract
ℋ
ℜ
Without access to the training data where a black-box victim model is deployed, training a surrogate model for black-box adversarial attack is still a struggle. In terms of data, we mainly identify three key measures for effective sur-rogate training in this paper. First, we show that leveraging the loss introduced in this paper to enlarge the inter-class similarity makes more sense than enlarging the inter-class diversity like existing methods. Next, unlike the approaches that expand the intra-class diversity in an implicit model-agnostic fashion, we propose a loss function speciﬁc to the surrogate model for our generator to enhance the intra-class diversity. Finally, in accordance with the in-depth ob-servations for the methods based on proxy data, we argue that leveraging the proxy data is still an effective way for surrogate training. To this end, we propose a triple-player framework by introducing a discriminator into the tradi-tional data-free framework. In this way, our method can be competitive when there are few semantic overlaps between the scarce proxy data (with the size between 1k and 5k) and the training data. We evaluate our method on a range of vic-tim models and datasets. The extensive results witness the effectiveness of our method. Our source code is available at https://github.com/xuxiangsun/ST-Data. 1.

Introduction
Over the last decades, we have witnessed the blowout success of Deep Neural Networks (DNNs) in computer vi-sion tasks. Yet they also show pervasive brittleness when they are exposed to adversarial examples [14, 43]. This leaves great safety hazards for the practical deployment of
DNNs. Hence, more threatening adversarial examples had been crafted to push for further research [3, 14, 23, 26, 31]. Most of them assume that the prior information of a victim model (denoted by V) can be accessed, e.g., its in-ternal architecture or training data. Albeit this lenient at-tack scenario (i.e., the white-box setting) can help us to ex-plore the robustness of DNNs, their performance will get
∗Corresponding author.
ℜ
ℋ (a) Samples with insufﬁcient imitation ability (b) Samples with sufﬁcient imitation ability
Figure 1.
Illustration of the location relationship in feature space between (a) Samples with insufﬁcient imitation ability, and (b) Samples with sufﬁcient imitation ability. Here, H de-notes the decision boundary of a model, (cid:2) represents the imitation margin of the synthesized samples. hurt drastically no prior knowledge (i.e., the black-box set-ting) can be accessed. In this case, how to attack DNN has stirred great interest in scholars.
Under the black-box setting, only the input-output feed-back of the victim model V can be accessed. According to the output, the solutions to black-box attack can either be decision-based (only the ﬁnal label can be accessed) or score-based (the output logits can be obtained). Among them, a feasible way is to design effective searching algo-rithms [1, 2, 4–6, 9, 15, 27, 32, 36, 37]. However, with a low query budget, their efﬁciency may be limited exceedingly.
While another intuitive idea is training a local surrogate model (denoted by S) to imitate the remote victim model V, and then craft adversarial examples on the trained model S via existing white-box attacks. However, without the train-ing data of model V, training the model S is still strenuous.
Recent advances [46, 58] pointed out that leveraging the synthesized data is more effective than the real proxy data [33, 34]. Speciﬁcally, [46] mentioned that this may be attribute to the poor diversity of the proxy data with limited size. In a way, it encounters the underﬁtting problem. From this perspective of view, the synthesized data can be served as the proxy data with inﬁnite size and relatively large di-versity. In this case, an important question is: what kind of synthesized data are effective to train the model S?
To deal with this question, the most advanced meth- 
ods [46, 58] enlarge the inter-class diversity of the synthe-sized data. However, as shown in Fig. 1, if the imitation margin of the samples stays away from the decision bound-ary like Fig. 1a, then the surrogate model S can not learn how the victim model V makes decisions. Things will get worse when it comes to the case of the label-only scenario.
Instead, if the synthesized data can lay close to the decision boundary (i.e., with the large inter-class similarity), the im-itation ability will be sufﬁcient, like Fig. 1b. This also can be revealed in Fig. 2 (details will be described in Appendix
C.1), where the boundary loss measures the distance from the synthesized data to the decision boundary. For most cases when the boundary loss is relatively high, the Attack
Success Rate (ASR) of DaST [58] and Knockoff [33] can not grow up sustainably. Compared with them, our method leads to continuous improvement via keeping a lower loss.
Besides, as indicated by [46], the intra-class diversity makes sense for surrogate training. Nonetheless, there is no ex-plicit constraint in [46] speciﬁed to the surrogate model to enlarge the intra-class diversity, which can not promise the effectiveness of the synthesized data to the model S.
Last but not least, according to [33], when the size of the proxy dataset is large enough, it still can work well. Hence, we make a feasible conjecture, i.e., the distribution of the proxy dataset can be roughly divided into two parts, i.e., the
ﬁrst one contains the samples that are useful for surrogate training while the second one does not. Different datasets contain different proportions of valid data. Based on this assumption, we can use the vanilla Generative Adversarial
Networks (GAN) [13, 30, 35] to imitate the distribution of a proxy dataset. As shown in Fig. 2, the synthesized data can still be effective (lower loss and better performance than
DaST and Knockoff). Moreover, it is striking that the ASR of vanilla GAN can even exceed DaST. We think that is due to searching from the whole data space like [46, 58] may be ineffective within the same training budget. Consequently, their performance will get stuck.
Spurred by the above observations and assumptions, we make the following contributions: 1) We propose a triple-player framework to train the surrogate model for the black-box adversarial attack. Speciﬁcally, based on the traditional data-free training framework, we are the ﬁrst to introduce a discriminator to limit the searching space of our gen-erator, which can enhance the training efﬁciency; 2) We identify that enlarging the inter-class similarity makes more sense than the inter-class diversity. Then, a loss function is introduced in this paper to enlarge the inter-class simi-larity of the synthesized data; 3) We propose a new loss function speciﬁed to the surrogate model to boost the intra-class diversity explicitly; 4) We indicate that the effective-ness of our method is competitive when there are few se-mantic overlaps between the scarce proxy data (i.e., with the size between 1k and 5k) and the training data. e t a
R s s e c c u
S k c a t t
A e t a
R s s e c c u
S k c a t t
A
Iterations 1e6 s s o
L y r a d n u o
B s s o
L y r a d n u o
B
Iterations 1e6
Iterations 1e6
Iterations 1e6
Figure 2. Iterations vs. Attack Success Rate (Untargeted) and
Boundary Loss (BL). Here, BL denotes the distance from the synthesized data to the decision boundary. “*-P” denotes the probability-only scenario and “*-L” represents the label-only scenario. To eliminate the inﬂuence of other factors, we replace the generator of DaST with ours, dubbed “DaST*”. 2.