Abstract
Continual learning is a challenging real-world problem for constructing a mature AI system when data are provided in a streaming fashion. Despite recent progress in contin-ual classification, the researches of continual object detec-tion are impeded by the diverse sizes and numbers of ob-jects in each image. Different from previous works that tune the whole network for all tasks, in this work, we present a simple and flexible framework for continual object de-tection via pRotOtypical taSk corrElaTion guided gaTing mechAnism (ROSETTA). Concretely, a unified framework is shared by all tasks while task-aware gates are intro-duced to automatically select sub-models for specific tasks.
In this way, various knowledge can be successively mem-orized by storing their corresponding sub-model weights in this system. To make ROSETTA automatically deter-mine which experience is available and useful, a proto-typical task correlation guided Gating Diversity Controller (GDC) is introduced to adaptively adjust the diversity of gates for the new task based on class-specific prototypes.
GDC module computes class-to-class correlation matrix to depict the cross-task correlation, and hereby activates more exclusive gates for the new task if a significant domain gap is observed. Comprehensive experiments on COCO-VOC,
KITTI-Kitchen, class-incremental detection on VOC and se-quential learning of four tasks show that ROSETTA yields state-of-the-art performance on both task-based and class-based continual object detection. 1 1.

Introduction
Thanks to the development of computer vision and deep learning, a great progress have been made in object detec-*Equal contribution. †Corresponding author. 1Codes are available at: https : / / github . com / dkxocl /
ROSSETA.
Figure 1. Overview of our pRotOtypical taSk corrElaTion guided gaTing mechAnism (ROSETTA) for continual object detection.
Sequential detection tasks share a unified backbone of the gated detector. Knowledge from previous tasks can be stored in the weights of the corresponding sub-models which are activated by the stored gates. Boxes in colors indicate the channels activated by task-aware gates for different tasks. Best viewed in color. tion [31, 32, 41]. The mainstream of existing works typ-ically follows the offline training paradigm: an individ-ual model is trained on a dataset and then evaluated on the test set with similar distribution. Nevertheless, on-line training on streaming data plays a more important role in real-world applications, especially large-scale industrial systems. More significantly, an artificial intelligent system is expected to continually learn different skills, e.g., detect more and more objects in multiple scenarios, resembling the memorizing and learning abilities of humans instead of
learning from scratch every time. A common solution from the machine learning perspective is continual learning (life-long learning) [23, 24, 34, 44], aiming to sequentially solve non-stationary tasks with ideally no performance drop when inferred on the previously seen tasks. Typically, an elastic model is required to achieve the equilibrium between con-tinuously acquiring new knowledge and preserving existing knowledge.
Despite the increasing attention on continual image clas-sification [1,24,44], few studies have been devoted to build-ing a continual object detection [23, 34] framework due to the challenges of preserving the capability of localiz-ing and recognizing multiple objects of diverse scales in streaming tasks. What is noteworthy is that, an image of the new task might simultaneously contain objects of novel classes and previously seen classes. Existing works on continual object detection mostly use knowledge dis-tillation [34, 39, 45, 53] on features of the region proposal network (RPN) to mitigate catastrophic forgetting. Specif-ically, suppose an object detector is trained consecutively on two tasks, i.e., task1 and task2. When training on task2, the distillation-based methods will force the fea-tures of the RPN to be consistent with those produced by the saved model trained on task1. However, such methods suffer from domain shift: distillation over the task2 data captures biased rather than the actual knowledge of task1 due to the unavailability of samples from task1. Another line of researches [23, 34] on continual detection store a small number of samples(exemplars) for reviewing previous knowledge. Nevertheless, replaying exemplars also fails in capturing actual knowledge because not all samples are ac-cessible and the sampling strategy plays a decisive role.
In this work, we explore a different way to solve con-tinual object detection without any exemplar replay: di-rectly storing knowledge via a sparse and dynamic frame-work. As illustrated in Fig. 1, a unified detector is shared by sequential tasks and the task-aware gates are designed to automatically determine which sub-models (channel-level) should be activated for specific tasks. To avoid the difficulty of jointly optimizing binary gates and channel weights, we propose a soften-and-discretize strategy for the gate learn-ing. Specifically, dynamic soft gates are generated during training stage and then discretized to be static binary gates for inference stage. The channels’ weights that have been activated for previous tasks by the binary gates are frozen and stored to keep existing knowledge. Each sub-model can dynamically choose whether to use the frozen channels to boost the learning for the current task. In this manner, the previous knowledge can be unbiasedly stored in the sub-models’ weights and cross-task knowledge can be shared by their overlapped channels. Binary gates are used to retrieve such existing knowledge by activating sub-models’ channel weights.
Although the proposed gating mechanism well preserves the previous knowledge, we observe degradation on the sub-sequent tasks when the domain gaps are significant, e.g.,
KITTI→Kitchen, which have different foreground objects and backgrounds. We attribute this phenomenon to the unawareness of cross-task correlation. Confronted with a relatively large gap, sharing too much previous knowledge would limit the performance gain in subsequent tasks [26] and more exclusive channels should be activated for new tasks. Thus, we propose the Prototypical Task Correlation
Guided Gating Mechanism to achieve a balance between sharing existing knowledge and exploiting exclusive knowl-edge (i.e., identifying which knowledge in the current task is orthogonal to the existing one). Specifically, a task correla-tion guided Gating Diversity Controller (GDC) is proposed to adaptively adjust the diversity of gates for the new task based on class-specific prototypes. GDC computes a cross-task class-to-class prototypical correlation matrix to depict the inter-task affinity and hereby activates more gates for the task2 when the domain gap between task1 and task2 is significant, and vice versa.
To verified its effectiveness, our proposed ROSETTA is evaluated on both task-based [34] and class-based [23] con-tinual object detection scenarios. For task-based settings, our method equipped with Faster R-CNN backbone outper-forms the state-of-the-art benchmarks by 11.8 mAP and 3.4 mAP on COCO and VOC for COCO→VOC, 5.8 mAP and 5.7 mAP on KITTI and Kitchen for KITTI→Kitchen. As for the class-incremental detection, our method surpasses the state-of-the-art method by 2.2 mAP for the “10+10” setting on VOC. Furthermore, comprehensive experiments demonstrate that our proposed gating mechanism success-fully achieve an equilibrium between sharing knowledge and exploiting exclusive knowledge for multiple tasks by capturing their prototypical cross-task correlation. 2.