Abstract
Despite the impressive progress of general face detec-tion, the tuning of hyper-parameters and architectures is still critical for the performance of a domain-speciﬁc face detector. Though existing AutoML works can speedup such process, they either require tuning from scratch for a new scenario or do not consider data privacy. To scale up, we derive a new AutoML setting from a platform perspective.
In such setting, new datasets sequentially arrive at the plat-form, where an architecture and hyper-parameter conﬁg-uration is recommended to train the optimal face detector for each dataset. This, however, brings two major chal-lenges: (1) how to predict the best conﬁguration for any given dataset without touching their raw images due to the privacy concern? and (2) how to continuously improve the
AutoML algorithm from previous tasks and offer a better warm-up for future ones? We introduce “HyperFD”, a new privacy-preserving online AutoML framework for face de-tection. At its core part, a novel meta-feature representation of a dataset as well as its learning paradigm is proposed.
Thanks to HyperFD, each local task (client) is able to effec-tively leverage the learning “experience” of previous tasks without uploading raw images to the platform; meanwhile, the meta-feature extractor is continuously learned to bet-ter trade off the bias and variance. Extensive experiments demonstrate the effectiveness and efﬁciency of our design. 1.

Introduction
Face detection [6, 28, 51, 68, 69] is one of the most fun-damental problems in computer vision. Although, rapid progress has been made lately for the general cases, be-spoken face detection models are still in high-demand for domain-speciﬁc scenarios. This is because, the challenges for detecting faces from an outdoor surveillance camera might be different from a panoramic indoor ﬁsh-eye camera
[13]; likewise, the challenges for detecting occluded faces (e.g., masks [21]) are also quite different from selﬁe faces
*Work done as an intern at MSRA. † Equal contribution.
Figure 1. Overview of HyperFD framework, which aims to build a shared AutoML platform that enables exchanging tuning experi-ence among customers, without access to customers’ raw datasets.
The performance ranker consists of meta-feature extractor and conﬁguration encoder. Both are continuously updated to incor-porate tuning experience on the latest task. captured by cellphone cameras [30]. Therefore, extensive manual parameter tuning and large computing resource is required in order to obtain the best specialized model for each domain. To scale up the scenarios, we see a clear in-dustry demand of building shared AI model training plat-form so as to leverage pretrained representation from other relevant tasks. For example, Microsoft Custom Vision [34] can train specialized object detection models given a set of user-uploaded images for engineers without deep learning background. This, however, comes at the cost of sacriﬁcing the face data privacy, for face detection models. Similarly, other AutoML tools (e.g., NNI [35]) either do not consider the data privacy or still require tuning from scratch for a new scenario, which is not secure and scalable.
The training of domain-speciﬁc face detection for real-world scenarios requires a new problem setup from a platform perspective, where the platform receives new
datasets sequentially, and recommends architecture and hyper-parameter conﬁguration to train the optimal face de-tector for each dataset (corresponds one particular domain).
This problem setting brings two major challenges. The ﬁrst is to effectively predict the best conﬁguration for any in-coming dataset under the constraints that the privacy of their raw images are protected. The second is to leverage the “ex-perience” from previous tasks1 to continuously improve the
AutoML algorithm, such that the platform can better serve future ones.
To tackle the challenge of privacy protection, we derive a new online AutoML paradigm for face detection, which is called “HyperFD”. Speciﬁcally, instead of uploading the raw images to the server, each local client only sends the dataset level representations (called “meta-features” in the following) to the platform and asks for the best conﬁgura-tion including a network architecture and hyper-parameters.
The meta-feature is designed to encode the overall statis-tics and general attributes of a given dataset. The plat-form maintains a learnable performance ranker that se-lects top-k optimal training conﬁgurations from the hyper-parameter/architecture search space based on the dataset meta-feature. Finally, after obtaining the conﬁgurations from the platform, the face detector is then trained locally on each client. In this way, the platform only sees dataset meta-features and the testing performance, which effec-tively protect the training data privacy. Figure 1 gives an overview of our HyperFD framework. Note that, although it is federated, HyperFD conducts the actual training task locally and there is no global aggregation is needed, which is different from traditional federated learning [32].
To tackle the second challenge and make HyperFD more generalizable for unseen scenarios, we ask the meta-feature to be updated continuously with the new dataset, yet prop-erly borrow the “experience” from previously trained tasks.
Due to the fact that there is no way to access the raw data, we integrate a novel meta-feature transformation mod-ule that builds a mapping between the current meta-feature space and previous feature space. Intuitively, this mapping will help make similar distributed historical tasks play more inﬂuence in ranking the ﬁnal conﬁguration for the new task.
To summarize, we make the following contributions:
• We introduce privacy-preserving online AutoML for face detection, which is a new problem setting from platform perspective.
• We propose a novel meta-feature extractor to build bet-ter dataset level representations, which is trained con-tinuously without touching the raw face images.
• Extensive experiments show the superior performance of our approach. We will also release the benchmark and source code to facilitate future research. 1In this paper, we use “task” and “dataset” interchangeably. 2.