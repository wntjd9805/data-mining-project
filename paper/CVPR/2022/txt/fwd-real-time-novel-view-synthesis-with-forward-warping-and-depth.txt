Abstract
Novel view synthesis (NVS) is a challenging task requir-ing systems to generate photorealistic images of scenes from new viewpoints, where both quality and speed are important for applications. Previous image-based rendering (IBR) methods are fast, but have poor quality when input views are sparse. Recent Neural Radiance Fields (NeRF) and generalizable variants give impressive results but are not real-time. In our paper, we propose a generalizable NVS method with sparse inputs, called FWD , which gives high-quality synthesis in real-time. With explicit depth and dif-ferentiable rendering, it achieves competitive results to the
SOTA methods with 130-1000× speedup and better percep-tual quality. If available, we can seamlessly integrate sen-sor depth during either training or inference to improve im-age quality while retaining real-time speed. With the grow-ing prevalence of depths sensors, we hope that methods making use of depth will become increasingly useful. 1.

Introduction
Given several posed images, novel view synthesis (NVS) aims to generate photorealistic images depicting the scene from unseen viewpoints. This long-standing task has appli-cations in graphics, VR/AR, bringing life to still images. It requires a deep visual understanding of geometry and se-mantics, making it appealing to test visual understanding.
Early work on NVS focused on image-based render-ing (IBR), where models generate target views from a set of input images. Light field [39] or proxy geometry (like mesh surfaces) [12, 24, 61, 62] are typically constructed from posed inputs, and target views are synthesized by re-sampling or blending warped inputs. Requiring dense in-put images, these methods are limited by 3D reconstruction quality, and can perform poorly with sparse input images.
Recently, Neural Radiance Fields (NeRF) [48] have be-come the leading methods for NVS, using MLPs to repre-sent the 5D radiance field of the scene implicitly. The color and density of each sampling point are queried from the network and aggregated by volumetric rendering to get the
Figure 1. Real-time Novel View Synthesis. We present a real-time and generalizable method to synthesize images from sparse inputs. NeRF variants model the scene via an MLP, which is queried millions of times during rendering and leads to low speeds.
Our method utilizes explicit depths and point cloud renderers for fast rendering, inspired by SynSin [82]. The model is trained end-to-end with a novel fusion transformer to give high-quality results, where regressed depths and features are optimized for synthesis. pixel color. With dense sampling points and differentiable renderer, explicit geometry isn’t needed, and densities opti-mized for synthesis quality are learned. Despite impressive results, they are not generalizable, requiring MLP fitting for each scene with dense inputs. Also, they are extremely slow because of tremendous MLP query times for a single image.
Generalizable NeRF variants like PixelNeRF [89], IBR-Net [78] and MVSNeRF [9] emerge very recently, synthe-sizing novel views of unseen scenes without per-scene op-timization by modeling an MLP conditioned on sparse in-puts. However, they still query the MLP millions of times, leading to slow speeds. Albeit the progress of accelerat-ing NeRF with per-scene optimization [88, 27, 18], fast and generalizable NeRF variants are still under-explored.
In this paper, we target a generalizable NVS method with sparse inputs, refraining dense view collections. Both real-time speed and high-quality synthesis are expected, allow-ing interactive applications. Classical IBR methods are fast but require dense input views for good results. General-izable NeRF variants show excellent quality without per-scene optimization but require intense computations, lead-ing to slow speeds. Our method, termed FWD, achieves this
target by Forward Warping features based on Depths. 2.