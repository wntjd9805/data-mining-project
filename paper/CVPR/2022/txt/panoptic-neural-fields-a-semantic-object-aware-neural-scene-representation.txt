Abstract 1.

Introduction
We present Panoptic Neural Fields (PNF), an object-aware neural scene representation that decomposes a scene into a set of objects (things) and background (stuff). Each object is represented by an oriented 3D bounding box and a multi-layer perceptron (MLP) that takes position, direc-tion, and time and outputs density and radiance. The back-ground stuff is represented by a similar MLP that addi-tionally outputs semantic labels. Each object MLPs are instance-speciﬁc and thus can be smaller and faster than previous object-aware approaches, while still leveraging category-speciﬁc priors incorporated via meta-learned ini-tialization. Our model builds a panoptic radiance ﬁeld rep-resentation of any scene from just color images. We use off-the-shelf algorithms to predict camera poses, object tracks, and 2D image semantic segmentations. Then we jointly op-timize the MLP weights and bounding box parameters using analysis-by-synthesis with self-supervision from color im-ages and pseudo-supervision from predicted semantic seg-mentations. During experiments with real-world dynamic scenes, we ﬁnd that our model can be used effectively for several tasks like novel view synthesis, 2D panoptic segmen-tation, 3D scene editing, and multiview depth prediction.
The ability to understand the content within an image is an essential task in computer vision, and over time we have witnessed a rapid increase in task complexity. Over a short period of time, we have progressed from the task of identifying the overall presence of objects within an im-age (i.e. classiﬁcation [26] and object detection [17, 20]), to
ﬁne grained pixel-by-pixel classiﬁcation (i.e. semantic seg-mentation [31, 47]), and to the ability to differentiate be-tween object instances of the same class (i.e. panoptic seg-mentation [6, 24]).
However image level representations described above have limited applications. Instead we are interested in full 3D scene understanding which is important for autonomous driving [21], semantic mapping [58], and many other ap-plications involving navigation or operation in the physical world [7]. Given a sequence of RGB images, our goal is to infer: 1) a 3D reconstruction of the observed geometry, 2) a radiance ﬁeld of the scene, 3) a decomposition of the scene into potentially dynamic things (e.g., cars) and background stuff (e.g., grass), 4) a category and instance label for every 3D point, as illustrated in Figure 1.
In recent years, neural 3D scene representations like
NeRF [33] have made signiﬁcant advancements [55, 60].
NeRF represents a scene using a multi-layer perceptron (MLP) that maps positions and directions to densities and radiances which can then be used to synthesize an image from a novel view. However NeRF lacks semantic under-standing and is also not object aware. In this work we ex-plore neural scene representations for semantic 3D scene understanding tasks beyond the usual view synthesis task.
Some recent work augments NeRF to infer semantics
[66], adding an extra head to predict semantic logits for any 3D position along with the usual density/color. Other recent work decomposes a scene into a set of NeRFs as-sociated with foreground objects separated from the back-ground [18, 40, 61]. However, these systems have several limitations in the context of our goals: 1) they do not pro-duce panoptic segmentations, 2) they learn from scratch for every scene; and 3) they share MLPs for multiple objects, which limits their ability to reproduce speciﬁc instances.
We address these issues in our proposed Panoptic Neu-ral Fields (PNF), an object-aware neural scene representa-tion that explicitly decomposes a scene into a set of objects (things) and amorphous stuff background. Each object in-stance is represented by a separate MLP to evaluate the ra-diance ﬁeld within the local domain of a potentially moving and semantically labeled 3D bounding box. The semantic-radiance ﬁeld of the stuff background is also represented by a MLP which includes an additional semantic head. To-gether the stuff and things MLPs jointly deﬁne a panoptic-radiance ﬁeld that describes the density, color, category, and instance label of any 3D point over time.
Our object aware representation makes it possible to de-scribe scenes with multiple moving objects and also paves the way to incorporate constraints that objects of the same
Previous category have similar shape and appearance. object-aware frameworks [40, 61] used a shared MLP with instance-speciﬁc latent codes to incorporate this prior. In our model, each object instance is represented by a sepa-rate MLP that is initialized with a category-speciﬁc prior using meta-learning. The separation of learning of object category priors via meta-learning makes it possible to rep-resent instance-speciﬁc details with smaller MLPs, which speeds inference in scenes with many objects.
Given a collection of images captured from a scene, we employ off-the-shelf algorithms to predict camera param-eters [35] and 2D semantic segmentations [6] for all im-ages, plus a set of 3D object detections with 3D oriented bounding boxes and category labels [41]. We initialize the weights of the MLPs for our panoptic neural ﬁeld model either with object category-speciﬁc meta-learned initializa-tion or simple biased initialization of density activation lay-ers. We then jointly optimize the bounding box and MLP parameters to minimize analysis-by-synthesis style losses that measure differences in color and semantic images syn-thesized with volumetric rendering (as in NeRF [33]). Thus, our approach provides an uniﬁed framework for optimizing 3D shape, appearance, semantics, and object poses all from a set of color images.
We evaluate our method on several scene understanding and synthesis tasks using experiments on the KITTI [15] and KITTI-360 [30] dataset, including 3D panoptic recon-struction, and scene editing. The output panoptic-radiance
ﬁeld can also be used to synthesize 2D image-level outputs like semantic segmentation, panoptic segmentation, depth images, and colored images of both observed and novel views. We demonstrate the utility of the proposed method for these scene understanding tasks, as well as for novel-view synthesis method with movable scene components.
Our contributions can be summarized as follows:
• We propose, to the best of our knowledge, the ﬁrst method that can derive a panoptic-radiance ﬁeld of complex dynamic 3D scenes from images alone.
• Our single uniﬁed model achieves state-of-the-art quality across multiple tasks and benchmarks on
KITTI and KITTI-360 datasets.
• We incorporate object shape and appearance priors via category-speciﬁc meta-learned initialization. This al-lows our object MLPs to be much smaller and faster than previous object-aware representations.
• We jointly optimize all (stuff and things) neural ﬁelds and object poses, allowing our method to cope with noisy object poses and image segmentations. 2.