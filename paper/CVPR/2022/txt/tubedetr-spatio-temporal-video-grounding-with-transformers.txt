Abstract
We consider the problem of localizing a spatio-temporal tube in a video corresponding to a given text query. This is a challenging task that requires the joint and efficient mod-eling of temporal, spatial and multi-modal interactions. To address this task, we propose TubeDETR, a transformer-based architecture inspired by the recent success of such models for text-conditioned object detection. Our model notably includes: (i) an efficient video and text encoder that models spatial multi-modal interactions over sparsely sampled frames and (ii) a space-time decoder that jointly performs spatio-temporal localization. We demonstrate the advantage of our proposed components through an exten-sive ablation study. We also evaluate our full approach on the spatio-temporal video grounding task and demonstrate improvements over the state of the art on the challenging
VidSTG and HC-STVG benchmarks. 1.

Introduction
Grounding natural language in visual content is a fun-damental skill to build powerful and explainable vision and language models. In particular, understanding the associa-tion of language with spatial regions and temporal bound-aries in videos is particularly important to analyze and im-prove multi-modal video models. This goes beyond associ-ating a global visual representation with a textual represen-tation [56,61], as it requires to reason about detailed spatio-temporal visual representations and their association with natural language, as illustrated in Figure 1.
Spatio-temporal video grounding, recently introduced in [100], is an interesting and challenging task that lies at the intersection of visual grounding [33, 58, 72] and tem-poral localization [9, 25, 30]. Given an untrimmed video and a textual description of an object, spatio-temporal video grounding aims at localizing a spatio-temporal tube (i.e., a sequence of bounding boxes) for the target object described by the input text. This task is particularly challenging as 4Czech Institute of Informatics, Robotics and Cybernetics at the Czech
Technical University in Prague.
Figure 1. Spatio-temporal video grounding requires reasoning about space, time, and language. videos are highly diverse and often present challenging sce-narios where different entities have similar appearance or perform similar actions within one scene.
The success of attention-based models in natural lan-guage processing [21, 73] has recently inspired approaches to integrate transformers into computer vision tasks, such as image classification [22], object detection [8], semantic segmentation [52] or action recognition [3, 7, 59, 98]. No-tably, with DETR [8], transformers have shown competitive performance on object detection while removing the need of multiple hand-designed components encoding a prior knowledge about this task. More recently, MDETR [37] has extended this framework for various text-conditioned object detection tasks in the image domain, such as phrase ground-ing, referring expression comprehension and segmentation.
Inspired by these works, and the fact that attention-based architectures are an intuitive choice for modelling multi-modal and spatio-temporal contextual relationships in videos, we develop a transformer encoder-decoder model for spatio-temporal video grounding, as illustrated in Fig-ure 2. While existing approaches for this task rely on pre-extracted object proposals [100], tube proposals [70] or up-sampling layers [66], our architecture simply reasons about abstractions called time queries to jointly perform temporal localization and visual grounding. Our framework enables to use the same representations for both subtasks in order to learn powerful contextualized representations.
More specifically, our architecture includes key compo-nents to jointly model temporal, spatial and multi-modal in-teractions. Our video-text encoder efficiently encodes spa-tial and multi-modal interactions by computing these inter-actions over sparsely sampled frames, and separately re-covers temporally local information with a lightweight fast branch. Our space-time decoder models temporal interac-tions with temporal self-attention layers, and spatial and multi-modal interactions with time-aligned cross-attention layers. Spatio-temporal video grounding is then tackled with multiple heads on top of the decoder outputs, which predict the object boxes and temporal start and end prob-abilities. We conduct various ablation studies, where we notably show the benefit of our video-text encoder in terms of performance-memory trade-off, and the efficiency of our space-time decoder in terms of spatio-temporal grounding results. Finally, we show that our method significantly im-proves over state-of-the-art methods on two benchmarks,
VidSTG [100] and HC-STVG [70].
In summary, our contributions are three-fold: (i) We pro-pose a novel architecture for spatio-temporal video ground-ing that performs this task with a space-time transformer decoder. (ii) We propose a dual-stream encoder that effi-ciently encodes spatial and multi-modal interactions, based on a slow multi-modal stream and a lightweight fast visual (iii) We conduct comprehensive experiments on stream. two benchmarks, VidSTG and HC-STVG, showing the ef-fectiveness of our framework for the spatio-temporal video grounding task. Our approach, referred to as TubeDETR, outperforms all state-of-the-art methods by a large margin.
Code and trained models are publicly available at [1]. 2.