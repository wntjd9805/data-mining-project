Abstract
In this paper, we tackle the problem of category-level 9D pose estimation in the wild, given a single RGB-D frame.
Using supervised data of real-world 9D poses is tedious and erroneous, and also fails to generalize to unseen scenarios.
Besides, category-level pose estimation requires a method to be able to generalize to unseen objects at test time, which is also challenging. Drawing inspirations from traditional point pair features (PPFs), in this paper, we design a novel
Category-level PPF (CPPF) voting method to achieve ac-curate, robust and generalizable 9D pose estimation in the wild. To obtain robust pose estimation, we sample numer-ous point pairs on an object, and for each pair our model predicts necessary SE(3)-invariant voting statistics on ob-ject centers, orientations and scales. A novel coarse-to-fine voting algorithm is proposed to eliminate noisy point pair samples and generate final predictions from the population.
To get rid of false positives in the orientation voting pro-cess, an auxiliary binary disambiguating classification task is introduced for each sampled point pair. In order to de-tect objects in the wild, we carefully design our sim-to-real pipeline by training on synthetic point clouds only, unless objects have ambiguous poses in geometry. Under this cir-cumstance, color information is leveraged to disambiguate these poses. Results on standard benchmarks show that our method is on par with current state of the arts with real-world training data. Extensive experiments further show that our method is robust to noise and gives promising re-sults under extremely challenging scenarios. Our code is available on https://github.com/qq456cvb/CPPF. 1.

Introduction
Estimation of 3D position, orientation and scale of novel objects, namely, category-level 9D pose estimation in the wild, is of great importance in many fields, such as
*Cewu Lu and Weiming Wang are the corresponding authors. Cewu
Lu is member of Qing Yuan Research Institute and MoE Key Lab of Arti-ficial Intelligence, AI Institute, Shanghai Jiao Tong University, China and
Shanghai Qi Zhi institute.
Figure 1. An overview of our proposed voting scheme. Dur-ing training, for each sampled point pairs on synthetic models, we train a set of voting targets with a SE(3) invariant neural network.
When testing, objects are first segmented out, and then for each object we randomly sample some point pairs to vote for the final 9D poses (translation, orientation and scale). robotics [10, 23, 32] and human-object interactions [1, 20].
There are many prior works exploring this direction, but with limitations, though. Some past works [3, 9, 17, 25, 31] have explored the instance-level 6D pose estimation. How-ever, they require exact object models and their sizes before-hand, which is often not realizable in real-world scenarios.
NOCS [32] introduces normalized object coordinate space to give a consistent representation across intra-class objects.
Although it is able to achieve category-level pose estima-tions, it requires real-world pose annotations, which are te-dious and limited by size. Besides, the 3D object scales predicted by NOCS are simple heuristics and prone to ob-ject occlusions, which is inevitable in real world. We doubt if one can leverage a sim-to-real approach that generalize 9D pose estimations from synthetic objects to real world, since ground-truth pose annotations in real-world scenarios are hard to acquire. Gao et al. [10] tries to solve this prob-lem via comparing the appearance of objects of synthetic objects and real objects, and finds a pose that minimize the
difference. Though this method does not require real 9D pose labels, it is erroneous and inferior to NOCS in terms of orientation error, due to the domain gap between synthetic and real RGB images.
Embracing these challenges, in this paper, we propose
Category-level PPF (CPPF) that votes for category-level 9D poses. We draw some inspirations from traditional point pair features (PPFs), and formulate the problem of pose es-timation as a voting process, where each point pair would generate several offsets or relative angles towards ground-truth 9D poses. Next, the pose with the most votes is cast as our final prediction. In contrast to traditional instance-level PPFs, where each pair is matched against an offline database, our method is much faster and able to generalize to unseen objects. To overcome the difficulty in orientation voting, where false positives are generated, an auxiliary bi-nary classification task is introduced.
In order to segment out the point cloud of a real-world target object, we leverage an off-the-shelf or fine-tuned in-stance segmentation model. We argue that instance segmen-tation labels are much cheaper and easier to annotate than 9D poses.
Besides, we develop a two-stage coarse-to-fine voting method, to robustly estimate the object pose when pre-dictions of instance segmentation are not accurate. This method could eliminate noisy point pairs that do not con-tribute to the voting of object pose. Furthermore, our ad-ditional experiments show that when only object bounding boxes are available, our method could still achieve robust and appealing results.
We evaluate our method on the publicly available real-world dataset released by Wang et al. [32] on category-level object pose estimation. Results show that our method beats sim-to-real state of the arts and is comparable to real-In addition, we show that when world training methods. only bounding box detections are provided, our method still gives decent 9D pose predictions. To further evaluate the generalization ability of our method to real-world scenarios, we directly apply our method on SUN RGB-D [27] dataset with zero-shot transfer, which contains much more diverse and complex scenes. Our method also outperforms base-lines by a large margin. In summary, our contribution is:
• We propose a novel category-level voting scheme to extract 9D pose of objects. An auxiliary task is intro-duced to remove the ambiguity in orientation voting.
A coarse-to-fine voting algorithm is proposed to elim-inate noisy point pairs with robust pose predictions.
• We introduce a novel sim-to-real pipeline with care-fully designed point pair features to achieve generaliz-able sim-to-real transfer, with synthetic models only.
• Extensive experiments show that our sim-to-real method is on par with current state-of-the-art meth-ods, which utilize real-world training data. Besides, our model is robust to segmentation errors and could give accurate pose predictions with only bounding box detections. 2.