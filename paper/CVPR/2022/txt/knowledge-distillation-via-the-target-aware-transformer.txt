Abstract
Knowledge distillation becomes a de facto standard to improve the performance of small neural networks. Most of the previous works propose to regress the representa-tional features from the teacher to the student in a one-to-one spatial matching fashion. However, people tend to overlook the fact that, due to the architecture differences, the semantic information on the same spatial location usu-ally vary. This greatly undermines the underlying assump-tion of the one-to-one distillation approach. To this end, we propose a novel one-to-all spatial matching knowledge distillation approach. Specifically, we allow each pixel of the teacher feature to be distilled to all spatial locations of the student features given its similarity, which is gener-ated from a target-aware transformer. Our approach sur-passes the state-of-the-art methods by a significant mar-gin on various computer vision benchmarks, such as Im-ageNet, Pascal VOC and COCOStuff10k. Code is available at https://github.com/sihaoevery/TaT. 1.

Introduction
Knowledge distillation [19, 31] refers to a simple tech-nique to improve the performance of any machine learning algorithms. One common scenario is to distill the knowl-edge from a larger teacher neural network to a smaller student one, such that the performance of student model can be significantly boosted comparing to training the stu-dent model alone. Concretely, people formulate an external loss function that guides the student feature map to mimic teacher’s. Recently, it has been applied to various down-stream applications, such as model compression [42, 48], continual learning [25], and semi-supervised learning [8].
Earlier works only distill the knowledge from the final layer of neural networks, for example, the “logits” in image classification task [1, 19]. Recently, people discover that
§Corresponding Author.
†Equal contribution.
‡Part of the work done when as an intern in DAMO Academy.
Illustration of semantic mismatch. Suppose that
Figure 1. teacher and student are the 3-layers and 2-layers convnets with kernel size 3 × 3 and stride 1 × 1. (a) shows the receptive field of the middle pixel of the final feature map, where the blue box repre-sents the teacher’s receptive field and the orange box is that of the student’s. Since teacher model has more convolutional operations, the resulting teacher feature map has a larger receptive field and thus contains richer semantic information. (b) Hence, directly re-gressing the student’s and teacher’s feature in a one-to-one spatial matching fashion may be suboptimal. (c) We proposed a one-to-all knowledge distillation via a target-aware transformer that can let the teacher’s spatial components be distilled to the entire student feature maps. distilling the intermediate feature maps is a more effective approach to boost the student’s performance. This line of works encourage similar patterns to be elicited in the spa-tial dimensions [36,50], and is constituted as state-of-the-art knowledge distillation approach [7, 22].
To compute the distillation loss of the aforementioned approach, one need to select the source feature map from the teacher and the target feature map from the student, where these two feature maps must have the same spatial dimension. As shown in Figure 1 (b), the loss is computed
in a one-to-one spatial matching fashion, that is formulated as a summation of the distance between the source and the target features at each spatial location. One underlying as-sumption of this approach is the spatial information of each pixel is the same. In practice, this assumption is commonly not valid due to the fact that student model usually has fewer convolutional layers than the teacher. One example is shown in Figure 1 (a), even at the same spatial location, the receptive field of student feature is often significantly smaller than the teacher’s and thus contains less semantic information. In addition, recent works [5, 10, 41, 49] evi-dences the importance of receptive field’s influence on the model representation power. Such discrepancy is a poten-tial reason that the current one-to-one matching distillation leads to sub-optimal results.
To this end, we propose a novel one-to-all spatial match-ing knowledge distillation approach. In Figure 1 (c), our method distills the teacher’s features at each spatial location into all components of the student features through a para-metric correlation, i.e., the distillation loss is a weighted summation of all student components. To model such cor-relation, we formulate a transformer structure that recon-structs the corresponding individual component of the stu-dent features and produces an alignment with the target teacher feature. We dubbed this target-aware transformer.
As such, we use parametric correlations to measure the se-mantic distance conditioned on the representational compo-nents of student feature and teacher feature to control the intensity of feature aggregation, which address the down-side of one-to-one matching knowledge distillation.
As our method computes the correlation between feature spatial locations, it might become intractable when feature maps are large. To this end, we extend our pipeline in a two-step hierarchical fashion: 1) instead of computing cor-relation of all spatial locations, we split the feature maps into several groups of patches, then performs the one-to-all distillation within each group; 2) we further average the fea-tures within a patch into a single vector to distill the knowl-edge. This reduces the complexity of our approach by order of magnitudes.
We evaluate the effectiveness of our method on two pop-ular computer vision tasks, image classification and seman-tic segmentation. On the ImageNet classification dataset, the tiny ResNet18 student can be boosted from 70.04% to 72.41% in terms of the top-1 accuracy, and surpasses the state-of-the-art knowledge distillation by 0.8%. As for the segmentation task on COCOStuff10k, comparing to the pre-vious approaches, our approach is able to boost the compact
MobilenetV2 architecture by 1.75% in terms of the mean intersection of union (mIoU).
Our contributions can be summarized as follows:
• We propose the knowledge distillation via a target-aware transformer, which enables the whole student to mimic each spatial component of the teacher respectively. In this way, we can increase the matching capability and subse-quently improve the knowledge distillation performance.
• We propose the hierarchical distillation to transfer local features along with global dependency instead of the orig-inal feature maps. This allows us to apply the proposed method to applications, which are suffered from heavy computational burden because of the large size of feature maps.
• We achieve state-of-the-art performance compared against related alternatives on multiple computer vision tasks by applying our distillation framework. 2.