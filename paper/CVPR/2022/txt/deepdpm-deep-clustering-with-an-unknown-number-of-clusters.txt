Abstract
Deep Learning (DL) has shown great promise in the un-supervised task of clustering. That said, while in classical (i.e., non-deep) clustering the benefits of the nonparamet-ric approach are well known, most deep-clustering meth-ods are parametric: namely, they require a predefined and fixed number of clusters, denoted by K. When K is un-known, however, using model-selection criteria to choose its optimal value might become computationally expensive, especially in DL as the training process would have to be repeated numerous times. In this work, we bridge this gap by introducing an effective deep-clustering method that does not require knowing the value of K as it infers it during the learning. Using a split/merge framework, a dynamic architecture that adapts to the changing K, and a novel loss, our proposed method outperforms existing nonpara-metric methods (both classical and deep ones). While the very few existing deep nonparametric methods lack scal-ability, we demonstrate ours by being the first to report the performance of such a method on ImageNet. We also demonstrate the importance of inferring K by showing how methods that fix it deteriorate in performance when their assumed K value gets further from the ground-truth one, especially on imbalanced datasets. Our code is available at https://github.com/BGU-CS-VIL/DeepDPM . 1.

Introduction
Clustering is an important unsupervised-learning task where, unlike in the supervised case of classification, class labels are unavailable. Moreover, in the purely-unsupervised (and more realistic) setting this work focuses on, the number of classes, denoted by K, and their relative sizes (i.e., the class weights) are unknown too.
Acknowledgements. This work was supported by the Lynn and William Frankel Center at BGU CS, by the Israeli Council for
Higher Education via the BGU Data Science Research Center, and by Israel Science Foundation Personal Grant #360/21. M.R. was also funded by the VATAT National excellence scholarship for female Master’s students in Hi-Tech-related fields. (a) ImageNet50: The original balanced dataset
——– (b) ImageNet50: An imbalanced dataset
——–
Figure 1. Mean clustering accuracy of 3 runs (± std. dev.) on
ImageNet50. The Ground Truth K is 50. Parametric methods such as K-means, DCN++ (an improved variant of [71]) and SCAN [64], require knowing K. When given a poor estimate of K, they deterio-rate in performance in a balanced dataset (a) and even more so in an imbalanced dataset (b). In contrast, the proposed DeepDPM does not require knowing K (it infers its value; e.g., K = 55.3 ± 1.53 in (a) and 46.3 ± 2.52 in (b)) and yet yields comparable results.
The emergence of Deep Learning (DL) has not skipped clustering tasks. DL methods usually cluster large and high-dimensional datasets better and more efficiently than classi-cal (i.e., non-deep) clustering methods [64, 71]. That said, while in classical clustering it is well understood that non-parametric methods (namely, methods that find K) have advantages over parametric ones (namely, methods that re-quire a known K) [8,57], there are only a few nonparametric deep clustering methods. Unfortunately, the latter are neither scalable nor effective enough. Our work bridges this gap by proposing an effective deep nonparametric method, called
DeepDPM. In fact, even when K is known, DeepDPM still achieves results comparable to leading parametric methods (especially in imbalanced cases) despite their “unfair” ad-vantage; see, e.g., Figure 1 or § 5.
More generally, the ability to infer the latent K has prac-tical benefits, including the following ones. 1) Without a good estimate of K, parametric methods might suffer in per-formance. Figure 1 shows that using the wrong K can have a significant negative effect on parametric methods in both balanced and imbalanced datasets. When the value of K becomes more and more inaccurate, even a State-Of-The-Art (SOTA) parametric deep clustering method, SCAN [64], de-teriorates in performance significantly. 2) Changing K dur-ing training has positive optimization-related implications; e.g., by splitting a single cluster into two, multiple data labels are changed simultaneously. This often translates to large moves on the optimization surface which may lead to conver-gence to better local optima and performance gains [10]; e.g., in § 5 we demonstrate cases where nonparametric methods, ours included, outperform parametric ones even when the latter are given the true K. 3) A common workaround to not knowing K is to use model selection: namely, run a paramet-ric method numerous times, using different K values over a wide range, and then choose the “best” K via an unsuper-vised criterion. That approach, however, besides missing the aforementioned potential gains (not being able to make large moves), does not scale and is usually infeasible for large datasets, especially in DL. Moreover, the negative societal impact of the model-selection approach must be noted as well: training a deep net tens or hundreds of times on a large dataset consumes prohibitively-large amounts of energy. 4)
K itself may be a sought-after quantity of importance.
Bayesian nonparametric (BNP) mixture models, exempli-fied by the Dirichlet Process Mixture (DPM) model, offer an elegant, data-adaptive, and mathematically-principled solu-tion for clustering when K is unknown. However, the high computational cost typically associated with DPM inference is arguably why only a few works tried to use it in conjunc-tion with deep clustering (e.g., [11,66,74]). Here we propose to combine the benefits of DL and the DPM effectively. The proposed method, DeepDPM, uses splits and merges of clus-ters to change K together with a dynamic architecture to ac-commodate for such changes. It also uses a novel amortized inference for Expectation-Maximization (EM) algorithms in mixture models. DeepDPM can be incorporated in deep pipelines that rely on clustering (e.g., for feature learning).
Unlike an offline clustering step (e.g., K-means), DeepDPM is differentiable during most of the training (the exception is during the discrete splits/merges) and thus supports gradi-ent propagation through it. DeepDPM outperforms existing nonparametric clustering methods (both classical and deep ones) across several datasets and metrics. It also handles class imbalance gracefully and scales well to large datasets.
While we focus on clustering and not feature learning, we also show examples of clustering on pretrained features as well as jointly learning features and clustering in an end-to-end fashion. To summarize, our key contributions are: 1) A deep clustering method that infers the number of clus-ters. 2) A novel loss that enables a new amortized inference in mixture models. 3) A demonstration of the importance, in deep clustering, of inferring K. 4) Our method outper-forms existing nonparametric clustering methods and we are the first to report results of a deep nonparametric clustering method on a large dataset such as ImageNet [17]. 2.