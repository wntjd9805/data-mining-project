Abstract
Real-world data often exhibits long tail distributions with heavy class imbalance, where the majority classes can dominate the training process and alter the decision bound-aries of the minority classes. Recently, researchers have in-vestigated the potential of supervised contrastive learning for long-tailed recognition, and demonstrated that it pro-vides a strong performance gain. In this paper, we show that while supervised contrastive learning can help improve performance, past baselines suffer from poor uniformity brought in by imbalanced data distribution. This poor uni-formity manifests in samples from the minority class hav-ing poor separability in the feature space. To address this problem, we propose targeted supervised contrastive learn-ing (TSC), which improves the uniformity of the feature distribution on the hypersphere. TSC ﬁrst generates a set of targets uniformly distributed on a hypersphere. It then makes the features of different classes converge to these distinct and uniformly distributed targets during training.
This forces all classes, including minority classes, to main-tain a uniform distribution in the feature space, improves class boundaries, and provides better generalization even in the presence of long-tail data. Experiments on multi-ple datasets show that TSC achieves state-of-the-art perfor-mance on long-tailed recognition tasks. 1.

Introduction
Real-world data often has a long tail distribution over classes: A few classes contain many instances (head classes), whereas most classes contain only a few instances
For critical applications, such as medi-(tail classes). cal diagnosis, autonomous driving, and fairness, the data are by their nature heavily imbalanced, and the minority classes are particularly important (minority classes can be
Interest in such prob-patients or accidents [41, 49, 51]). lems has motivated much recent research on imbalanced
∗Indicates equal contribution.
Figure 1. Test data feature distribution of (a) k-positive contrastive learning (KCL) and (b) TSC for three classes of CIFAR10 (plane, cat, dog), for different training data imbalance ratios ρ. With high imbalance ratio, class centers learned by KCL exhibit poor unifor-mity while class centers learned by TSC are still uniformly dis-tributed and thus TSC achieves better performance (where Acc refers to Accuracy on test data). classiﬁcation, where the training dataset is imbalanced or long-tailed but the test dataset is equally distributed among classes [4, 24, 46, 49, 50].
Long-tailed and imbalanced datasets pose major chal-lenges for classiﬁcation tasks leading to a signiﬁcant per-formance drop [1, 2, 8, 48, 52]. Techniques such as data re-sampling [1,2,5,40] and loss re-weighting [3,4,9,11,25,26] can improve the performance of tail classes but typically harm head classes [24]. Recently, researchers have inves-tigated the potential of supervised contrastive learning for long-tailed recognition, and demonstrated that it provides a strong performance gain [23]. They further proposed k-positive contrastive learning (KCL), a variant of supervised contrastive learning that yields even better performance on long-tailed datasets.
However, while supervised contrastive learning can be beneﬁcial, applying the contrastive loss (including the KCL loss) to imbalanced data can yield poor uniformity, which hampers performance. Uniformity is a desirable prop-erty [46]; it refers to that in an ideal scenario super-vised contrastive learning should converge to an embedding
where the different classes are uniformly distributed on a hypersphere [19, 46]. Uniformity maximizes the distance between classes in the feature space, i.e., maximizes the margin. As a result, it improves generalizability.
But, when the classes are imbalanced, training naturally puts more weight on the loss of majority classes and less weight on that of minority classes. As a result, the classes are no longer uniformly distributed in the feature space. To illustrate this issue, we consider three classes from CIFAR-10: dog, cat, and plane. We train a KCL model [23] on this data for different imbalance ratios, ρ. For visualiza-tion clarity we use a 2D feature space. As seen in Fig. 1(a), when the classes are balanced (i.e., ρ=1:1:1), the centers of the three classes are uniformly distributed in the KCL fea-ture space.
In contrast, when the imbalance ratio is high (e.g., ρ=100:1:1), the classes with fewer training instances start to collapse into each other, leading to unclear and in-separable decision boundaries, and thus lower performance.
This is because the imbalanced data distribution naturally puts more weight on the uniformity loss between the head class and the tail classes, and less weight on that between the two tail classes, making the distance between head and tail classes much larger than the distance between two tail classes. The more imbalanced the long-tailed data, the more biased and less uniformly distributed the feature space.
One may attempt to ﬁx this problem by oversampling the tail classes or re-weighting the loss function. However, as shown in [24], those methods overﬁt tail classes and im-prove tail-class performance at the expense of head classes, and thus harm the quality of the learned features. Therefore, a method that performs instance-balanced sampling while still being able to learn a uniform feature space is needed.
In this paper, we propose targeted supervised contrastive learning (TSC) for long-tailed recognition. To avoid the fea-ture space being dominated and biased by head classes, we generate the optimal locations of class centers in advance (i.e., off-line). We call these uniformly distributed points class targets. We then devise an online matching-training scheme that performs contrastive training while adaptively matching samples from each class to one of the targets. As shown in Fig. 1(b), TSC learns a class-balanced feature space regardless of the imbalance ratio of the training set.
Note that one cannot simply match any target point with any class. Though the targets are uniformly distributed in the feature space, the distance between two targets can vary widely. For example, if the number of classes in Fig. 1 was 10 instead of 3, then though the targets are uniformly distributed, some targets will be closer to each other than the rest. Thus, our matching-training scheme has to ensure that classes that are semantically close (e.g., cat and dog) converge to nearby targets, and classes that are semantically farther apart converge to relatively distant targets.
We evaluate TSC on long-tailed benchmark datasets in-cluding CIFAR-10-LT, CIFAR-100-LT, ImageNet-LT, and iNaturalist, and show that it improves the state-of-the-art (SOTA) performances on all of them.
To summarize, this paper makes the following contribu-tions:
• It introduces TSC, a novel framework for long-tailed recognition that avoids the feature space being dominated and biased by head classes.
• It empirically shows that supervised contrastive learning baselines can suffer from poor uniformity when applied to long-tailed recognition, which degrades their perfor-mances.
• It shows further that TSC achieves SOTA long-tailed recognition performances on benchmark datasets, demonstrating the effectiveness of the proposed method. 2.