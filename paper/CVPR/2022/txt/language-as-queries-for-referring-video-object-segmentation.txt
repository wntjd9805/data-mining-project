Abstract
Referring video object segmentation (R-VOS) is an emerging cross-modal task that aims to segment the target object referred by a language expression in all video frames.
In this work, we propose a simple and unified framework built upon Transformer, termed ReferFormer. It views the language as queries and directly attends to the most rele-vant regions in the video frames. Concretely, we introduce a small set of object queries conditioned on the language as the input to the Transformer.
In this manner, all the queries are obligated to find the referred objects only. They are eventually transformed into dynamic kernels which cap-ture the crucial object-level information, and play the role of convolution filters to generate the segmentation masks from feature maps. The object tracking is achieved natu-rally by linking the corresponding queries across frames.
This mechanism greatly simplifies the pipeline and the end-to-end framework is significantly different from the previ-ous methods. Extensive experiments on Ref-Youtube-VOS,
Ref-DAVIS17, A2D-Sentences and JHMDB-Sentences show the effectiveness of ReferFormer. On Ref-Youtube-VOS,
ReferFormer achieves 55.6 J &F with a ResNet-50 back-bone without bells and whistles, which exceeds the previ-ous state-of-the-art performance by 8.4 points. In addition, with the strong Video-Swin-Base backbone, ReferFormer achieves the best J &F of 64.9 among all existing meth-ods. Moreover, we show the impressive results of 55.0 mAP and 43.7 mAP on A2D-Sentences and JHMDB-Sentences respectively, which significantly outperforms the previous methods by a large margin. Code is publicly available at https://github.com/wjn922/ReferFormer. 1.

Introduction
Referring video object segmentation (R-VOS) aims to segment the target object in a video given a natural language description. This emerging topic has raised great attention in the research community and is expected to benefit many applications in a friendly and interactive way, e.g., video
Figure 1. Comparison of current referring video object segmenta-tion (R-VOS) pipelines. (a) Bottom-up. (b) Top-down. (c) Ours. editing and video surveillance. R-VOS is more challenging than the traditional semi-supervised video object segmenta-tion [33, 46], because it does not only lack the ground-truth mask annotation in the first frame, but also require the com-prehensive understanding of the cross-modal sources, i.e., vision and language. Therefore, the model should have a strong ability to infer which object is referred and to per-form accurate segmentation.
To accomplish this task, the existing methods can be mainly categorized into two groups: (1) Bottom-up meth-ods. These methods incorporate the vision and language features in a early-fusion manner, and then adopt a FCN
[28] as decoder to generate object masks, as shown in Fig-ure 1(a). (2) Top-down methods. These methods tackle the problem in a top-down perspective and follow a two-stage pipeline. As illustrated in Figure 1(b), they first employ an instance segmentation model to find all the objects in each
frame, and then associate them in the entire video to form the tracklet candidates. Afterwards, they use the expression as the grounding criterion to select the best-matched one.
Although these two streams of methods have demon-strated their effectiveness with promising results, they still have some intrinsic limitations. First, for the bottom-up methods, they fail to capture the crucial instance-level in-formation and do not consider the object association across multiple frames. Therefore, these methods can not provide explicit knowledge for cross-modal reasoning and would encounter the discrepancy of predicted object due to scene changes. Second, although top-down methods have greatly boost the performance over the bottom-up methods, they suffer from heavy workload because of the complex, multi-stage pipeline. Furthermore, the separate optimization on several sub-problems would lead to sub-optimal solution.
These limitations of current methods motivate us to de-sign a simple and unified framework that solves the R-VOS task elegantly. The recent success of Transformer [40] in object detection [4, 55] and video instance segmentation
[15, 43, 44] demonstrates a promising solution. However, it is non-trivial to apply such models to the R-VOS task.
These models [4,55] use a fixed number (e.g., 100) of learn-able queries to detect all the objects in an image. Under this circumstance, it would be confused for the model to distin-guish which object is referred due to the randomness of the expression. Here raises a natural question: ”Is it possible for a unified model to know where to look using queries?”
This work answers the question by proposing the notion of language as queries, as shown in Figure 1(c). We put the linguistic restriction on all object queries and use these con-ditional queries as input for the model. In this manner, the expression will make the queries focus on the referred ob-ject only, and thus greatly reducing the query number (e.g., 5 in our experiments). The next challenge lies in how to decode the object mask from query representations. As the queries contain rich instance characteristics, we view them as instance-aware dynamic kernels to filter out the segmen-tation masks from feature maps.
The unified framework can not only produce the seg-mentation masks for referred objects, but also the classi-fication results and detection boxes. Moreover, the con-ditional queries are linked via instance matching strategy across frames so that the object tracking is achieved natu-rally without post-process. We hope this framework could serve as a strong baseline for R-VOS task.
The main contributions of this work are as follows.
• We propose a simple and unified framework for referring video object segmentation, termed ReferFormer. Given a video clip and the corresponding language expression, our framework directly detects, segments and tracks the referred object in all frames in an end-to-end manner.
• We present the notion of language as queries. We in-troduce a small set of object queries which conditioned on the text expression to attend the referred object only.
These conditional queries are shared across different frames in the initial state and they are transformed into dynamic kernels to filter out the segmentation masks from feature maps. This mechanism provides a new perspec-tive for the R-VOS task.
• We design the cross-modal feature pyramid network (CM-FPN) for multi-scale vision-language fusion, which improves the discriminativeness of mask features for ac-curate segmentation.
• Extensive experiments on Ref-Youtube-VOS, Ref-DAVIS17, A2D-Sentences and JHMDB-Sentences show that ReferFormer outperforms the previous methods on these four benchmarks by a large margin. E.g., on Ref-Youtube-VOS, ReferFormer with a ResNet-50 backbone achieves 55.6 J &F without bells and whistles, showing the significant 8.4 points gain over the previous state-of-the-art methods. And using the strong Video-Swin-Base visual backbone, ReferFormer achieves the impressive results of 64.9 J &F. 2.