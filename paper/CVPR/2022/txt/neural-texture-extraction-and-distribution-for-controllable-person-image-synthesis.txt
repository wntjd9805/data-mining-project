Abstract
We deal with the controllable person image synthesis task which aims to re-render a human from a reference im-age with explicit control over body pose and appearance.
Observing that person images are highly structured, we propose to generate desired images by extracting and dis-tributing semantic entities of reference images. To achieve this goal, a neural texture extraction and distribution opera-tion based on double attention is described. This operation first extracts semantic neural textures from reference fea-ture maps. Then, it distributes the extracted neural textures according to the spatial distributions learned from target poses. Our model is trained to predict human images in ar-bitrary poses, which encourages it to extract disentangled and expressive neural textures representing the appearance of different semantic entities. The disentangled represen-tation further enables explicit appearance control. Neural textures of different reference images can be fused to control the appearance of the interested areas. Experimental com-parisons show the superiority of the proposed model. Code is available at https://github.com/RenYurui/
Neural- Texture- Extraction- Distribution. 1.

Introduction
Synthesizing person images with explicitly controlling the body pose and appearance is an important task with a large variety of applications. Industries such as electronic commerce, virtual reality, and next-generation communica-tion require such algorithms to generate content. Typical examples are shown in Fig. 1. It can be seen that the desired output images are not aligned with the reference images.
Therefore, a fundamental challenge for generating photo-realistic target images is to accurately deform the reference images according to the modifications.
Figure 1. Controllable person image synthesis. Our model can generate realistic images by explicitly controlling the poses and appearance of reference images.
However, Convolutional Neural Networks lack the abil-ity to enable efficient spatial transformation [6, 27]. Build-ing blocks of CNNs process one local neighborhood at a time. To model long-term dependencies, stacks of con-volutional operations are required to obtain large receptive fields. Realistic textures will be “washed away” during the repeating local operations. Flow-based methods [14, 22, 25, 28] are proposed to enable efficient spatial transformation.
These methods predict 2D coordinate offsets assigning a sampling position for each target point. Although realistic textures can be reconstructed, these methods yield notice-able artifacts, which is more evident when complex defor-mations and severe occlusions are observed [21].
Attention mechanism [27, 30, 33] has emerged as an ef-ficient approach to capture long-term dependencies. This operation computes the response of a target position as a weighted sum of all source features. Therefore, it can build dependencies by directly computing the interactions be-tween any two positions. However, in this task, the vanilla attention operation suffers from some limitations. First, since the target images are the deformation results of the sources, each target position is only related to a local source region, which means that the attention correction matrix should be a sparse matrix to reject the irrelevant regions.
Second, the quadratic memory footprint hinders its applica-bility to deform realistic details in high-resolution features.
To deal with these limitations, we introduce an efficient spatial transformation operation. This operation is moti-vated by an intuitive idea: person images can be manipu-lated by extracting and reassembling semantic entities (e.g. face, hair, cloth). To achieve this goal, we propose a Neu-ral Texture Extraction and Distribution (NTED) operation based on double attention [3, 24]. The architecture of this operation is shown in Fig. 2. Specifically, the extraction op-eration is first used to extract neural textures by gathering features obtained from the reference images. Then, the dis-tribution operation is responsible for generating the results by soft selecting the extracted neural textures for each target position according to the learned semantic distribution.
We design a generative neural network by using NTED operations at different scales. This network renders the in-put skeletons by predicting the conditional semantic dis-tributions and reassembling the extracted neural textures.
The experimental evaluation demonstrates photo-realistic results at a high resolution of 512 × 352. The comparison experiments show the superiority of the proposed model. In addition, our model can be further applied for explicit ap-pearance control. Interested semantics can be manipulated by exchanging the corresponding neural textures of differ-ent references. An optimization method is proposed to au-tomatically search for the interpolation coefficients which are further used to fuse the extracted neural textures. Our method enables coherent and realistic results. The main contributions of our paper can be summarized as:
• An intuitive idea for image deformation is provided.
Desired images are generated by extracting and dis-tributing the semantic entities of reference images.
• We implement the proposed idea with a light-weighted and computationally-efficient NTED operation. Ex-periments show the operation as an efficient spatial deformation module. Comprehensive ablation studies demonstrate its efficacy.
• Thanks to the disentangled and expressive neural tex-tures extracted by our model, we can achieve explicit appearance control by interpolating between neural textures of different references. 2.