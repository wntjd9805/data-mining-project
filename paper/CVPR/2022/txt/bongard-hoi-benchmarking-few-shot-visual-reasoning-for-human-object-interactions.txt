Abstract positive examples ride bicycle negative examples
!ride bicycle
A significant gap remains between today’s visual pattern recognition models and human-level visual cognition espe-cially when it comes to few-shot learning and compositional reasoning of novel concepts. We introduce Bongard-HOI, a new visual reasoning benchmark that focuses on compo-sitional learning of human-object interactions (HOIs) from natural images. It is inspired by two desirable characteris-tics from the classical Bongard problems (BPs): 1) few-shot concept learning, and 2) context-dependent reasoning. We carefully curate the few-shot instances with hard negatives, where positive and negative images only disagree on ac-tion labels, making mere recognition of object categories insufficient to complete our benchmarks. We also design multiple test sets to systematically study the generalization of visual learning models, where we vary the overlap of the HOI concepts between the training and test sets of few-shot instances, from partial to no overlaps. Bongard-HOI presents a substantial challenge to today’s visual recog-nition models. The state-of-the-art HOI detection model achieves only 62% accuracy on few-shot binary prediction while even amateur human testers on MTurk have 91% accu-racy. With the Bongard-HOI benchmark, we hope to further advance research efforts in visual reasoning, especially in holistic perception-reasoning systems and better representa-tion learning. 1.

Introduction
In recent years, great strides have been made on vi-sual recognition benchmarks, such as ImageNet [8] and
COCO [33]. Nonetheless, there remains a considerable gap between machine-level pattern recognition and human-level cognitive reasoning. Current image understanding models typically require a large amount of training data yet struggle
*First two authors contributed equally.
Query images:
Labels: positive negative
Figure 1. Illustration of a few-shot learning instance from our
Bongard-HOI benchmark. The positive images in the top left part follow the visual relationship of riding a bike between the person and objects while such a relationship does not exist in the negative examples. Note that an actual problem in Bongard-HOI contains 6 images of positive examples, 6 negative examples, and 1 query image, which is different from the illustration here. to generalize beyond the visual concepts seen during training.
In contrast, humans can reason about new visual concepts in a compositional manner from just a few examples [21].
To march towards human-level visual cognition, we need to depart from conventional benchmarks on closed-vocabulary recognition tasks and aim to systematically examine compo-sitional and few-shot learning of novel visual concepts.
While existing benchmarks such as miniImageNet [46],
Meta-Dataset [44], and ORBIT [44] have been dedicated to studying few-shot visual learning, they focus on recog-actions with dogs actions with oranges
Figure 2. Examples of different actions with the same object.
From top to bottom, left to right: washing, walking, and feeding dogs; eating, squeezing, and peeling oranges. To differentiate these images, we need compositional understanding on both the actions and the objects. We exploit this to select hard negatives in Bongard-HOI: negative images contain the same object as the positives, but the actions are different. nizing object categories instead of the compositional struc-tures of visual concepts, e.g., visual relationships. A par-allel line of research aims at building benchmarks for ab-stract reasoning by taking inspiration from cognitive science such as RPM (Raven-style Progressive Matrices) [2, 43] and
Bongard-LOGO [3, 32]. In these benchmarks, a model has to learn concept induction rules from a few examples and the concepts are context-dependent in each task. However, they use simple synthetic images [2, 32] or focus on basic object-level properties, such as shapes and categories [43].
Our new benchmark: In this paper, we introduce Bongard-HOI, a new benchmark for compositional visual reasoning with natural images. It studies human-object interactions (HOIs) as the visual concepts, requiring explicit composi-tional reasoning of object-level concepts. Our Bongard-HOI benchmark inherits two important characteristics of the clas-sic Bongard problems (BPs) [3]: 1) few-shot binary predic-tion, where a visual concept needs to be induced from just six positive and six negative examples and 2) context-dependent reasoning, where the label of an image may be interpreted differently under different contexts.
Furthermore, Bongard-HOI upgrades the original BPs from synthetic graphics to natural images. Our benchmark contains rich visual stimuli featuring large intra-class vari-ance, cluttered background, diverse scene layouts, etc. In
Bongard-HOI, a single few-shot binary prediction instance, referred to as BP, contains a set of six positive images and a set of six negative images, along with query images (see
Fig. 1 for examples). The task is making binary predictions on the query images.
We construct the few-shot instances in Bongard-HOI on top of the HAKE dataset [23, 24]. To encourage the explicit reasoning of visual relationships, we use hard negatives to construct few-shot instances. The hard negatives consist of negatives that contain objects from the same categories as those contained in the positive sets but with different ac-tion labels. Fig. 2 presents some examples of these images.
Since both positive and negative examples contain object instances from the same categories, mere recognition of ob-ject categories is insufficient to complete the tasks. Rather, reasoning about visual relationships between person and ob-jects is required to solve these few-shot binary prediction problems. The existence of such hard negatives distinguishes our benchmark from existing visual abstract reasoning coun-terparts [2, 32, 43]. Comparisons with different benchmarks can be found in Table 1.
We carefully curate the annotations in HAKE when con-structing the few-shot instances. Recall the visual concept contained in the positive images should not appear in any of the negative ones. Thus, we have to carefully select the images in both sets. We employ high-quality annotators from the Amazon Mechanical Turk platform to curate the test set to further remove ambiguously and wrongly labeled few-shot instances. In this process, 2.5% of the few-shot instances in the test set are discarded. We end up with 23K and 15K few-shot instances in disjoint training and test sets, respectively.
An important goal of the Bongard-HOI benchmark is to systematically study the generalization of machine learning models for real-world visual relationship reasoning. To this end, we introduce four separate test sets to investigate dif-ferent types of generalization, depending on whether the action and object classes are seen in the training set. Fig. 3 illustrates their design. This way, we have full control of the overlap between the concepts (i.e., HOIs) between training and test of few-shot instances. It enables us to carefully ex-amine the generalization of visual learning models. Ideally, a learning model should be able to generalize beyond the concepts it has seen during training. Even for unseen HOI concepts, the model should be able to learn how to induce the underlying visual relationship from just a few examples.
Establishing baselines: In our experiments, we first ex-amine the state-of-the-art HOI detection models’ perfor-mance on this new task, we trained an oracle model with
HOITrans [52] on all the HOI categories, including those in the test sets of our Bongard-HOI benchmark, and output binary prediction on the query image via a majority vote based on HOI detections. Its accuracy is only 62.46% (with a chance performance of 50%), demonstrating the challenge of our visual reasoning tasks. We then evaluate state-of-the-art few-shot learning approaches, including non-episodic and meta-learning methods. We show that the current learning models struggle to solve the Bongard-HOI problems. Com-pared to amateur human testers’ 91.42% overall accuracy, who have access to a few examples of visual relationships before working on solving our problems, the state-of-the-art
sit_on bed straddle bicycle hug person wash car
· · · wash bicyle sit_on bench greet person shear sheep training set test set seen act. seen obj. seen act. unseen obj. unseen act. seen obj. unseen act. unseen obj.
Figure 3. Illustration of our four separate test sets for different types of generalization. We show a few HOI concepts in the training and test sets in the top and bottom row, respectively. We use the red fonts to denote an object or action class that is available in the training set and blue fonts indicate those held-on unseen ones in the test set. few-shot learning model [6] only has 55.82% accuracy.
The results above lead to this question: why do they per-form so poorly? To this end, we offer a detailed analysis of the results and propose several conjectures. The first one is a lack of holistic perception and reasoning systems, since mod-els that have only good pattern recognition performances, e.g.
HOITrans, are likely to fail on our benchmarks. Moreover, we believe there is a need for additional representation learn-ing, e.g. pre-training, since currently we only train on binary labels of few-shot instances. Nonetheless, we believe much effort is still needed to further investigate the challenges brought by our benchmark.
To sum up, this paper makes the following contributions:
• We introduce Bongard-HOI, a new benchmark for few-shot visual reasoning with human-object interactions, aim-ing at combining the best of few-shot learning, composi-tional reasoning, and challenging real-world scenes.
• We carefully curate Bonagrd-HOI with hard negatives, making mere recognition of object categories insufficient to complete our tasks. We also introduce multiple test sets to systematically study different types of generalization.
• We analyze state-of-the-art few-shot learning and HOI de-tection methods. However, experimental results show their inability on achieving good results on Bongard-HOI. Our conjectures suggest future research in models with holistic perception-reasoning systems and better representations. 2. Bongard-HOI Benchmark
For a few-shot binary prediction instance in Bonagrd-HOI, it has a set of positive examples P, a set of negative samples N , and a query image Iq. Images in P depict a certain visual concept (e.g., ride bicycle in Fig. 1), while images in N do not.
In each task, there are only six images in both P and N . As a result, a human tester or machine learning model needs to induce the underlying concept from just a few examples. Given the query image Iq, a binary prediction needs to be made: whether the certain visual concept depicted in P is available in Iq or not. Later, we will detail how to construct these few-shot instances. 2.1. Constructing Bongard Problems
Few-shot instances in Bongard-HOI are constructed with natural images. We choose to use visual relationships as underlying visual concepts. In our early experiments, we also studied visual attributes to construct few-shot instances, for example, color and shape of bird parts [47], facial at-tributes [27]. But such visual attributes annotations either require too much domain knowledge for human annotators or are too noisy to curate. Another option we investigated is scene graph [18], which is a combination of both visual relationships and visual attributes. However, there could be too many convoluted visual concepts in a single image, resulting in ambiguous few-shot instances.
In this paper, we construct few-shot instances on top of the HAKE dataset [23, 24] focusing on human-object inter-actions. It provides unified annotations following the anno-tation protocol in HICO [4] for a set of datasets widely used for HOI detection, including HICO [4], V-COCO [10], Open-Images [19], HCVRD [51], and PIC [25]. HAKE has 80 object categories, which are consistent with the vocabulary defined in the standard COCO dataset [26]. It also has 117 action labels, leading to 600 human-object relationships1.
Denote a concept c = ⟨s, a, o⟩ as a visual relationship triplet, where s, a, o are the class labels of subject, ac-In this paper, s is always tion, and object, respectively. person. We start with selecting a set of positive images 1Some combinations of objects and actions are infeasible.
Table 1. An overview of different benchmark datasets covering HOI detection, few-shot learning, and abstract visual reasoning. In the first row, the abbreviation ctx denotes context; generalization types indicates if a benchmark includes multiple test splits to examine different types of generalization. ∗We consider the concept of object counts as compositional while others such as object attributes and categories not [43]). concept compositional natural image concept few- ctx-dependent shot reasoning hard negatives generalization types
#concepts #tasks
HAKE [23, 24]
HOI
Omniglot [20] miniImageNet [46]
Meta-Dataset [44]
ORBIT [30] shape image label image label frame label
RPM [2]
V-PROM [43]
Bongard-LOGO [32] shape attributes & counts shape
Bongard-HOI (ours)
HOI
✓
✗
✗
✗
✗
✗
✓∗
✗
✓
✓
✗
✓
✓
✓
✗
✓
✗
✓
✗
✓
✗
✓
✓
✓
✓
✓
✓
✗
✓
✓
✗
✗
✓
✓
✓
✓
✓
✗
✗
✗
✗
✗
✗
✗
✓
✗
✗
✗
✗
✗
✓
✓
✓
✓ 600 122.6K 50 100 4,934 486 50 478 627 242 1.62K 60K 52.8M 2.69M 11.36M 235K 12K 53K
Ic = {I1, . . . } from HAKE that depict such a relationship.
We also need negative images, where the visual concept c is not contained by them. In specific, we collect another set of images I¯c with concept ¯c = ⟨s, ¯a, o⟩, where ¯a ̸= a, meaning that we select hard negatives. As a result, images from both
Ic and I¯c contain the same categories of objects and the only differences are the action labels, making it impossible to triv-ially distinguish positive images from the negatives by doing visual recognition of object categories only. Rather, detailed visual reasoning about the interactions of human and objects are desired. Fig. 2 illustrates the difficulties introduced by the hard negatives. Finally, as an entire image may contain multiple HOI instances, we use image regions (crops) around each HOI instance instead of the original image to ensure only a single HOI instance is presented in a single image.
Next, we need to sample few-shot instances from the pos-itive images Ic and the negatives I¯c. We randomly sample images to form P, N , and a query image Iq. Two parameters control the sampling process: M , the number of images in P and N (M = 6 in Bongard-HOI), and the overlap threshold
τ , indicating the maximum number of overlapped images between two few-shot instances. We want to sample as many few-shot instances as possible, but we also need to avoid significant image overlap between few-shot instances, which limits the diversity of the data. We end up setting τ = 3 and
τ = 2 for training and test sets, respectively. More details can be found in the supplementary material. 2.2. Data Curation
Although the HAKE dataset [23, 24] has provided high-quality annotations, we found that curations are still needed to construct few-shot instances. Recall, to sample negative images, we assume a particular action is not depicted in them.
In HAKE, an image region may have multiple action labels.
Naively relying on the provided annotations is problematic as the action labels are either not manually exclusive or not exhaustively annotated. We hire high-quality testers on the
Amazon Mechanical Turk (MTurk) platform, who maintain a good job approval record, to curate existing HOI annotations.
We discuss the data curation process in detail and show visual examples in detail in the supplementary material.
After the aforementioned data curations, each image re-gion is assigned to a single action label, describing the most salient visual relationship. With the curated annotations, ac-tion labels between a person and objects of a certain category are mutually exclusive so that we can significantly reduce the ambiguity when constructing few-shot instances. Finally, we hire high-quality testers on the MTurk platform to further remove the ambiguous few-shot instances in the test set. Ev-ery single few-shot instance is assigned to three independent testers. We compare their responses with the ground-truth labels and discard about 2.5% few-shot instances where none of the three testers correctly classifies the query im-ages. In the end, we report the accuracy of human testers on those left unambiguous few-shot instances as a human study to examine human-level performance on our Bongard-HOI benchmark, where the average accuracy is 91.42%. 2.3. Generalization Tests
Transferring the knowledge that an agent has seen and learned is a hallmark of visual intelligence, which is a long-stand goal for the entire AI community. It is also a core focus of the Bongard-HOI benchmark. Following [2], we provide multiple test splits to investigate different types of generalization, aiming at a systematic understanding of how the tested models generalize on our benchmark. Specifically, the visual concept we consider in Bongard-HOI is an HOI triplet ⟨s, a, o⟩ and we have two variables of freedom: action a and object o. Therefore, by controlling whether an action or object is seen during training, we can study generalization to unseen actions, unseen objects, or a combination of two.
We end up introducing four separate test sets, as shown in
Fig. 3. We provide detailed statistics on our training and test sets in the supplementary material.
Ideally, after learning from examples of sit_on bed, a machine learning model can quickly grasp the concept sit_on bench. More importantly, such a model should learn how to learn from just a few examples, so that they can still induce the correct concept (visual relationship) in the most challenging cases, where both actions and objects are not seen during training (e.g., shear sheep). 3. Possible Models for Bongard-HOI
There are many possible ways of tackling Bongard-HOI, such as few-shot learning, conventional HOI detection, etc.
We are particularly interested in investigating few-shot learn-ing methods, as our benchmark requires the learner to iden-tify the visual concept with very few samples (positive and negative images in P and N , respectively). To further im-prove the few-shot learning methods, we consider encoding the images with Relation Network [40], aiming at better compositionality in the learned representations. Finally, we introduce an oracle model to testify whether Bongard-HOI can be trivially solved using state-of-the-art HOI detection models. 3.1. Few-shot Learning in Bongard-HOI
M , 1), (I N 1 , 1), . . . , (I P
We start with a formal definition of the few-shot learn-ing problem in Bongard-HOI. Specifically, each task in-cludes multiple few-shot instance with N = 2 classes and 2M samples, i.e., the model learns from a training set
S = P ∪N = {(I P
M , 0)} and is evaluated on a query image (Iq, yq). Each example (I, y) includes an image I ∈ RH×W ×3 and a class label y ∈ {0, 1}, indicating whether I contains the visual con-cepts depicted in P. In Bonagrd-HOI, we set M = 6 as our default parameter and therefore each few-shot instance is “2-way, 6-shot”. Following [44], we propose to solve these few-shot prediction instances with the following two families of approaches: 1 , 0), . . . , (I N
Non-episodic methods. In these methods, a simple classi-fier is trained to map all the images in a few-shot instance (including images in P, N , and the query image) to the class of the query. The classifier can be parameterized as a neural network over some learned image embeddings, i.e. representations produced by convolutional neural networks (CNNs). In other words, we view each few-shot instance as a single training sample ((cid:83)2M +1
Ii, yq) rather than a few-shot instance with multiple training samples (I, y). Our experi-ments cover two different ways to encode the images: CNN and Wide Relational Network (WReN) [2, 32]. i=1
Meta-learning methods. These methods adopt the episodic learning setting, i.e., they learn to train a classifier using 2M samples from S and evaluate their trained classifier on the query (Iq, yq). In general, their objective (also called meta-objective) is to minimize the prediction error on the query. Different meta-learning methods have their own ways
Figure 4. Class-agnostic (objectness) detections. We show the detections from our class-agonostic detector (in green) and ground-truth human and object boxes (in red). to build the classifier and optimize the meta-objective. In our experiments, we consider the following state-of-the-art meth-ods: 1) ProtoNet [42], a metric-based method; 2) MetaOpt-Net [22] and ANIL [34], two optimization-based approaches.
Moreover, we also use a strong baseline meta-learning model,
Meta-Baseline [6], which reports competitive results in many few-shot prediction tasks. We refer readers to the related papers for more details. 3.1.1
Image Encoding with Relational Network
As mentioned above, representation learning of the input images can be crucial to the success of few-shot learning methods on Bongard-HOI. As our benchmark demands learn-ing compostional concepts (HOIs), simply feeding an image into a Convolutional Neural Network (CNN) is not optimal.
To this end, we propose to use the Relational Network [40], which shows promising compositional reasoning accuracy on a Visual Question Answering (VQA) benchmark [15], to explicitly encode the compositionality of visual relation-ships. In specific, the feature representations of the image I is computed as
RN(I) = fϕ ◦ (cid:88) i,j gψ (concat(hθ(oi, I), hθ(oj, I))) , where oi and oj are two detected objects of the image I, provided by ground truth object annotations or a pre-trained object detector like Faster R-CNN [37]. hθ denotes the
RoI Pooled features of oi from a ResNet backbone [11] followed by a MLP (multi-layer perceptron) [37], which is parameterized by θ. gψ and fϕ are two additional MLPs.
A challenge we are facing is the unseen object categories in the test sets. Since the object detector has to be pre-trained on a dataset without the unseen object categories, it is likely to fail on our test set where images could contain objects belonging to these categories. To tackle this issue, we train a binary class-agnostic (objectness) detection model instead to get oi and oj. Class-agnostic object detections are shown in Fig. 4. As we can see, all objects of interest have been successfully detected. But at the same time, there are a lot of other distracting ones, such as the bench and the wagon in the left image of Fig. 4. This is a unique challenge of dealing with visual reasoning over real-world images. We devote discussions to it in the experiment section.
3.2. Oracle
P
N
One may wonder if our Bongard-HOI benchmark could be trivially solved using the state-of-the-art HOI detection model. To address this concern, we develop an oracle model resorting to the HOITrans [52], which is based on the Trans-former model [45] and reports state-of-the-art accuracy on the HICO [4] and V-COCO [10] benchmarks. In specific, let’s denote the HOI detections in the P and N as DP and
DN , respectively. DP contains the detections from all of the images in the P, defined as DP = {cP i=1, where cP i is a HOI triplet introduced in Section 2.1. NP is the total number of detections. Note that there may be multiple or no detections for a single image. Similarly, DN is defined as DN = {cN i=1. According to the property of Bongard-HOI, the visual concept cP should only appear in the P, not in the N . We, therefore, compute cP as i }NN i }NP cP = majority_vote(DP − DN ), where − is the set operator for set subtraction. Here we first exclude the HOIs detected in N from DP , then the majority of the remaining HOIs will be viewed as the visual concept cP . Given the detections Dq = {cq i=1 for the query image
Iq, our prediction y becomes i }Nq y = (cid:26) 1, 0, if cP ∈ Dq, otherwise.
Discussions of how to deal with the corner cases, e.g., majority_vote returns more than 1 concept, Dq is empty, etc, are provided in the supplementary material. We illustrate how this model works in Fig. 5, where we show
HOI detections in each image.
We call it our oracle model as it has privileged infor-mation, i.e., the entire HOI action & object vocabulary, in-cluding those held-out ones in the test set. As we shall we in Section 4, such an oracle model still struggles on our
Bongard-HOI benchmark, achieving only 62.46% accuracy on average, which is far below the human-level performance of 91.42%. It suggests that our Bongard-HOI benchmark is not trivial to solve. 4. Experiments 4.1. Implementation Details
We benchmark the models introduced in Section 3 on
Bongard-HOI to test their performance on human-level few-shot visual reasoning. We use a ResNet50 [11] as an encoder for the input images. We consider different pre-training strategies: 1) no pre-training at all (scratch), 2) pre-trained on the ImageNet dataset with manual labels [8], and 3) lat-est self-supervised approach [5] pre-trained on ImageNet but without manual labels. We train an Faster R-CNN [37] class-agnostic objectness detection model on the COCO dataset [33] using a ResNet101 [11] pre-trained on Ima-geNet [8] as the backbone. We use the RoIPool opera-tion [37] to get feature representations for each bounding
Query images:
Predictions: positive negative
Figure 5. Illustration of our oracle model. We first generate some detections for all the images using HOITrans [52]. Note that some images may not have any detection at all. According to the detections in the P and N , the common concept is eat donut.
As a result, in the bottom row, the first query image is considered to be positive as its HOI detections contain eat donat. The second query image is negative. Zoom in for the best view. box. We also use ground-truth bounding boxes provided in HAKE [23] as input to diagnose the effectiveness of the visual perception. In addition to RoIPooled region features, we also concatenate each bounding box’s normalized co-ordinates (center and spatial dimensions) as spatial infor-mation to the Relational Network encoder introduced in
Section 3.1.1. 4.2. Quantitative Results
The quantitative results of different models on our
Bongard-HOI benchmark can be found in Table 2. We make the following observations: First of all, despite the overall difficulties brought by our benchmarks, most models perform worse on the challenging test splits, where actions and/or object categories are completely unseen during train-ing. This observation aligns well with our hypothesis, i.e. existing machine learning approaches can be limited in terms of generalizing beyond training concepts. It also echos the findings in Bongard-LOGO [32], a dataset studying a similar problem with synthetic images. Second, meta-learning ap-proaches generally tend to perform better than non-episodic counterparts, which can be on par with or even worse than random guesses (50% chance). We hypothesize the reason to be the focus on learning to learn in these methods, which is essentially required to solve the few-shot instances in the Bongard-HOI benchmark, especially for the challenging test splits with novel categories. Similar observations have also been made in Bongard-LOGO. Moreover, some meta-learning models are distracted by bounding boxes provided
Table 2. Quantitative results on the Bongard-HOI benchmark. All the models use a ResNet50 as the image encoder. For the input of bounding boxes (bbox), we consider two options: from an object detection model (det) and ground-truth annotations (gt). For the ResNet50 encoder, we experiment with different pre-training strategies: no pre-training at all (scratch), pre-trained on the ImageNet dataset with manual labels (IN), and state-of-the-art self-supervised approach MoCoV2 [5]. (* denotes that we are unable to get meaningful results; # indicates that the trained model has a run-time error during the inference stage since the condition of the QP solver can not be satisfied).
CNN-Baseline [32]
WReN-BP [2, 32]
ProtoNet* [42]
ProtoNet [42]
MetaOptNet# [22]
MetaOptNet [22]
ANIL [34]
ANIL [34]
Meta-Baseline [6]
Meta-Baseline [6]
Meta-Baseline [6]
Meta-Baseline [6] bbox pre-train
--det gt det gt det gt scratch
IN
IN
IN
IN
IN
IN
IN det scratch det MoCoV2 det gt
IN
IN
HOITrans [52] (oracle)
Human (Amateur)
----test set seen act., seen obj. seen act., unseen obj. unseen act., seen obj. unseen act., unseen obj. 50.03 50.31
-58.90
-58.60 50.18 52.73 54.61 55.23 56.45 58.82 59.50 87.21 49.89 49.72
-58.77
-58.28 50.13 50.11 53.79 54.54 56.02 58.75 64.38 90.01 49.77 49.97
-57.11
-58.39 49.81 49.55 54.58 54.32 55.60 58.56 63.10 93.61 50.01 49.01
-58.34
-56.59 48.83 48.19 53.94 53.11 55.21 57.04 62.87 94.85 avg. 49.92 49.75
-58.28
-57.97 49.74 50.15 54.23 54.30 55.82 58.30 62.46 91.42 by an object detection model. We will discuss this issue in the next section.
Surprisingly, the oracle model (HOITrans) also struggles on our tests with an averaged accuracy of 62.46%, albeit be-ing trained with direct HOI supervision and all action&object categories. It suggests a clear gap between the existing HOI detection datasets, e.g. HAKE [23] and Bongard-HOI, where the latter one requires capabilities beyond perception, e.g.
HOI recognition. Rather, a model might also need context-dependent reasoning, learning-to-learn from very few exam-ples, etc., to perform well on our benchmarks.
Finally, machine learning models still largely fall behind amateur human testers (e.g., 55.82% of Meta-Baseline vs 91.42%). While we only give human testers a couple of examples about visual relationships before they start work-ing on solving Bongard-HOI, they can quickly learn how to induce visual relationships from just a few examples, re-porting an average 91.42% accuracy on our Bongard-HOI benchmark. Particularly, there are no significant differences for the different subsets of the test set. We hope our findings will foster more research efforts on closing this gap. 4.3. Discussions
We need holistic perception and reasoning. Our work suggests that the significant challenges in current visual rea-soning systems lie in both the reliability of perception and the intricacy of the reasoning task itself. Models that have only good pattern recognition performances are likely to fail on our benchmarks. Rather, an ideal learner needs to integrate visual perception in natural scenes and detailed cog-nitive reasoning as a whole. This marks our key motivation to propose Bongard-HOI as the first step towards studying these two problems holistically.
Pre-training improves performances. Intuitively, models for Bongard-HOI might need additional representation learn-ing, e.g. pre-training, since currently we only train on binary labels of few-shot instances. We can see from Table 2 that pre-training is very helpful. Compared to no pre-training, using either manual labels or self-supervision leads to a performance boost. In particular, the self-supervised pre-training [5] does not use any manual labels for supervision.
Yet it can produce better results than learning from scratch.
Visual perception matters in Bongard-HOI. Finally, an imperfect perception could still be a major obstacle here.
Different from Bongard-LOGO [32] which uses synthetic shapes instead, Bongard-HOI studies visual reasoning on natural scenes, which often contain rich visual stimuli, is-suing such as large intra-class variance and cluttered back-ground also present challenges to reliable visual perception on which reasoning is based. In our case, bounding boxes produced by an object detection model can be inevitably noisy. Some meta-learning models, including ProtoNet [42], have difficulties inducing the true visual relationships. For
MetaOptNet [22], although we can finish training, we con-stantly encounter run-time errors where the condition of the
QP solver is not satisfied during the inference stage. Instead, when taking clean ground-truth bounding boxes as input, all of these approaches produce better accuracy. Note that using ground-truth bounding boxes only serves as an oracle, which does not indicate the models’ authentic performance. 5.