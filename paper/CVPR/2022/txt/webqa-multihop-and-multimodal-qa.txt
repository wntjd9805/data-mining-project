Abstract
Scaling Visual Question Answering (VQA) to the open-domain and multi-hop nature of web searches, requires fundamental advances in visual representation learning, knowledge aggregation, and language generation. In this work, we introduce WEBQA, a challenging new benchmark that proves difficult for large-scale state-of-the-art models which lack language groundable visual representations for novel objects and the ability to reason, yet trivial for hu-mans. WEBQA mirrors the way humans use the web: 1)
Ask a question, 2) Choose sources to aggregate, and 3)
Produce a fluent language response. This is the behavior we should be expecting from IoT devices and digital assis-tants. Existing work prefers to assume that a model can ei-ther reason about knowledge in images or in text. WEBQA includes a secondary text-only QA task to ensure improved visual performance does not come at the cost of language understanding. Our challenge for the community is to cre-ate unified multimodal reasoning models that answer ques-tions regardless of the source modality, moving us closer to digital assistants that not only query language knowledge, but also the richer visual online world. 1.

Introduction
Web search is a multimodal experience: Will I find my answer on the image search tab or within text snippets? In contrast, most deployed Question Answering (QA) systems treat the web as a text-only landscape of facts to be ex-tracted, ignoring the knowledge present in images. This has two fundamental limitations: 1. The text-based web is im-poverished [3,4], and 2. This form of information extraction is inefficient. For example, when searching to see if a park has picnic tables, surfacing an image of the picnic area an-swers the question immediately, rather than wading through pages of reviews hoping someone happened to mention this fact. QA engines need to move to treating the Internet as a multimodal trove of information, but this requires multihop reasoning on either images or text.
Figure 1. Example WEBQA dataset pipeline in which the question requires finding and reasoning about two relevant sources and dis-carding distractors to produce the correct natural language answer.
To this end, datasets are rapidly emerging [10, 24, 28].
But they either use pre-defined templates for the curation of multihop multimodal QA pairs [28], or encourage a “ques-tion decomposition + rerouting to uni-modal model” ap-proach to superficially solve the problem [10]. However, when humans absorb knowledge, there is no need to dis-tinguish whether the knowledge was learned from books versus images, or whether a piece of knowledge is a com-posite of multiple scattered fragments versus being car-ried by a single one. We argue that genuine progress in reasoning over linguistic notions of meanings and visually grounded meanings under the same representation frame-work depends on the development of a unified system that indiscriminately treats snippets and images as knowledge carriers. On top of that, the goal includes better extraction, integration and summarization abilities in a heterogeneous information landscape.
To facilitate this research intersection, in this work we propose a novel benchmark, WEBQA, for multi-hop, multi-modal, open-domain question-answering where all ques-tions are knowledge-seeking and resemble real-world use cases. Success on WEBQA requires a system to a) incor-porate both text and images, b) retrieve relevant knowledge in either modality, c) aggregate information from multiple sources via logical or numerical reasoning, and d) generate
#Train
#Dev
#Test
#Img
Len Q Len A
Eval Metrics
Answer Schema
VQA v2 [9] 443K 214K 453K 200K
OKVQA [18] 9.0K
MultiModalQA [28] 23.8K 2.0K
ManyModalQA [10] 52.4K
MIMOQA [24] 0 2.4K 3.0K 0.7K 5.0K 14.0K 3.6K 57.7K 5.1K 2.9K 3.5K 400.0K 6.1 8.1 18.2 – – 1.2 1.3 2.1 1.0 –
VQA v2
OK-VQA min{ #human agreement 3
, 1} Top training answers
MultimodalQA
Exact Match
F1
Txt: span/Y/N
Img: Fixed vocab
Table: Y/N, cell, or op.
WEBQA (ours) 34.2K 5K 7.5K 390.0K 17.5 12.5
ManymodalQA Classification Accuracy
Context word or vocab
Table 1. Comparison of multimodal knowledge-seeking bench-marks by size and average question/answer lengths.1 answers in natural language. We experiment with state-of-the-art multimodal reasoning and text generation models, whose failures indicate promising future directions. 2.