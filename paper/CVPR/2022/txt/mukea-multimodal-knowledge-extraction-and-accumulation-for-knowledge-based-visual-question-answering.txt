Abstract
Knowledge-based visual question answering requires the ability of associating external knowledge for open-ended cross-modal scene understanding. One limitation of existing solutions is that they capture relevant knowledge from text-only knowledge bases, which merely contain facts expressed by first-order predicates or language descrip-tions while lacking complex but indispensable multimodal knowledge for visual understanding. How to construct vision-relevant and explainable multimodal knowledge for the VQA scenario has been less studied. In this paper, we propose MuKEA to represent multimodal knowledge by an explicit triplet to correlate visual objects and fact answers with implicit relations. To bridge the heterogeneous gap, we propose three objective losses to learn the triplet rep-resentations from complementary views: embedding struc-ture, topological relation and semantic space. By adopting a pre-training and fine-tuning learning strategy, both ba-sic and domain-specific multimodal knowledge are progres-sively accumulated for answer prediction. We outperform the state-of-the-art by 3.35% and 6.08% respectively on two challenging knowledge-required datasets: OK-VQA and
KRVQA. Experimental results prove the complementary benefits of the multimodal knowledge with existing knowl-edge bases and the advantages of our end-to-end framework over the existing pipeline methods. The code is available at https://github.com/AndersonStra/MuKEA. 1.

Introduction
Visual Question Answering based on external Knowl-edge Bases (KB-VQA) [37] requires an AI agent to answer
*Corresponding author.
†Canada CIFAR AI Chair.
Figure 1. An illustration of our motivation. Compared with rigid facts in the knowledge graph, multimodal knowledge for depict-ing complex and inexpressible facts is indispensable in both open-ended object understanding (a) and scene understanding (b). a question by incorporating knowledge about the world be-yond what the question and the image contains. Despite the great success in VQA tasks [11,40], KB-VQA is more chal-lenging for models to achieve human-like ability of open-ended cross-modal scene understanding associating with external knowledge. Therefore, how to appropriately repre-sent and leverage knowledge in such cross-modal scenario becomes a core problem of KB-VQA.
Most of recent works [9, 23, 46] focus on capturing rel-evant knowledge from structured knowledge graphs, such as ConceptNet [18] and DBpedia [4], or unstructured/semi-like Wikipedia [1] and Visual structured knowledge,
Genome [15]. Though these knowledge bases provide high-quality knowledge by large-scale human annotations, the information is generally limited to the definite facts that can be explicitly expressed by natural language or simple triplets with first-order predicate. Therefore, such knowl-edge bases are quite difficult to represent high-order predi-cate and multimodal knowledge, which is essential for hu-man to tackle complex problems. Considering the ques-tion in Figure 1(a), the agent needs visual knowledge of motorcycle appearance in each brand to identify the given motorcycle, but the knowledge graph lacks of such instan-tiated information. Besides object understanding, implicit visual knowledge in mind mostly dominate over the rigid facts when humans are asked for simple scene discrimina-tion like the question ‘Can you guess the place?’ in Figure 1(b). How to represent and accumulate the complex mul-timodal knowledge in the VQA scenario while maintaining the advantages of traditional knowledge graph in explain-able reasoning is an essential but less studied problem.
Current progress [17, 30, 33] in emerging multimodal knowledge graph aims to correlate visual content with tex-tual facts to form the augmented knowledge graph. The typical solutions can be divided into two categories: parsing images and texts to structured representations and ground-ing event/entities across modalities [13, 17, 39], or simply aligning the entities in existing knowledge graphs with re-lated images [30, 33]. However, such multimodal knowl-edge graphs in essence still represent knowledge via the first-order predicate, which fails to model the high-order complex relationships such as the relationship between
‘clock’ and ‘London’ in Figure 1(b).
In this paper, we propose a novel Multimodal Knowledge
Extraction and Accumulation framework (MuKEA) for
KB-VQA task. Independent of existing knowledge bases, the core mechanism behind MuKEA is to accumulate mul-timodal knowledge with complex relationships from obser-vation of VQA samples, and perform explainable reason-ing based on the self-accumulated knowledge. To this end, we first propose a novel schema to represent multimodal knowledge unit by an explicit triplet, where the visual ob-jects referred by the question are embedded in the head en-tity, the embedding of the fact answer is kept in the tail en-tity, and the implicit relation between the head and the tail is expressed by the relation. We propose three objective loss functions to learn the representations of the triplets from coarse to fine by contrasting positive and negative triplets, aligning ground-truth triplets, and refining entity represen-tations. A pre-training and fine-tuning learning strategy is then proposed to progressively accumulate multimodal knowledge from both out-domain and in-domain VQA sam-ples for explainable reasoning.
The main contributions of this work are as follows: (1) We propose an end-to-end multimodal knowledge representation learning framework, which first models the inexpressible multimodal facts by explicit triplets and pro-vides complementary knowledge with the existing knowl-edge graphs and unstructured knowledge bases. (2) We exploit a pre-training and fine-tuning strategy to accumulate both out-domain and in-domain knowledge to form a neural multimodal knowledge base. It supports auto-matic knowledge association and answer prediction, which gets rid of the cascading error in existing ‘knowledge re-trieve and read’ pipeline [23, 46]. (3) Our model with strong generalization ability out-performs the state-of-the-art models by 3.35% and 6.08% respectively on two challenging KB-VQA datasets: OK-VQA [24] and KRVQA [7]. The good performance can be well explained by visualizing the relevant multimodal knowledge triplets explicitly. 2.