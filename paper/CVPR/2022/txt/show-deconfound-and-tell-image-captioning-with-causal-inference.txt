Abstract
The transformer-based encoder-decoder framework has shown remarkable performance in image captioning. How-ever, most transformer-based captioning methods ever over-look two kinds of elusive confounders: the visual con-founder and the linguistic confounder, which generally lead to harmful bias, induce the spurious correlations during training, and degrade the model generalization.
In this paper, we first use Structural Causal Models (SCMs) to show how two confounders damage the image caption-ing. Then we apply the backdoor adjustment to propose a novel causal inference based image captioning (CIIC) framework, which consists of an interventional object de-tector (IOD) and an interventional transformer decoder (ITD) to jointly confront both confounders. In the encod-ing stage, the IOD is able to disentangle the region-based visual features by deconfounding the visual confounder. In the decoding stage, the ITD introduces causal interven-tion into the transformer decoder and deconfounds the vi-sual and linguistic confounders simultaneously. Two mod-ules collaborate with each other to alleviate the spurious correlations caused by the unobserved confounders. When tested on MSCOCO, our proposal significantly outperforms the state-of-the-art encoder-decoder models on Karpathy split and online test split. Code is published in https:
//github.com/CUMTGG/CIIC. 1.

Introduction
Image captioning aims to automatically understand the semantic information of an image and generate its accurate description. Inspired by neural machine translation [36], the encoder-decoder architecture has been widely adopted by most conventional image captioning models [2, 10, 39, 41], in which a deep convolutional neural network (CNN) serves
∗Authors contributed equally.
†Corresponding author (a) (b)
Figure 1. The example about the spurious correlation in im-(a) Examples of the visual confounder (the vi-age captioning. sual feature of cake) and linguistic confounder (the word embed-ding of “cake”) in the MSCOCO training dataset when generating the word “fork”, where “GT1”, “GT2” and “GT3” denote three ground truth captions of each image chosen from the dataset. (b)
Some captions generated by the original Transformer [37] and
CIIC. The generated correct and incorrect words are colored by blue and red, respectively. “GT” means the ground truth caption. as the encoder to extract visual features from the input im-age, and a recurrent neural network (RNN) is used as the decoder to generate the corresponding caption. Based on this architecture, a large number of improvements have been made by recent works, which mainly focus on two-fold: (i) Optimizing the visual representations of the input im-age [2,15,18,42], and (ii) enhancing the architectural mod-eling capabilities for inter-modal and intra-modal interac-tions [8, 28].
In the aspect of visual representation, most captioning models apply a well-trained detector, e.g., Faster R-CNN
[33], to extract visual features. Nevertheless, these mod-els neglected the problem of entangled visual features in the visual feature extraction stage. As shown in Figure 1a, the features of a region of the fork extracted by Faster R-CNN tend to be its surrounding cake-like features since forks and cake co-occur too many times, i.e., the feature representations of forks are severely affected by the visual feature of cake. In this case, the visual feature of the cake is actually one visual confounder, which builds a “short-cut path” [11] that leads to the spurious correlations between object features and target categories, e.g., the learned cake-like features correspond to the class label of the fork. Con-sequently, it is critical to disentangle the visual features in the stage of visual representation to alleviate the spurious correlation between the cake region and the word “fork”.
In the structure aspect of model improvement, transformer-based models [13, 15, 17, 23, 27] have ob-tained the superior performance over the CNN-RNN based captioning methods. However, most transformer-based captioning models may still learn dataset bias caused by the hidden confounders. As shown in Figure 1a, when there are more forks than spoons co-occur with cake, due to both the visual confounder (i.e., the visual feature of cake) and linguistic confounder (i.e., the word embedding of “cake”), the traditional captioning models tend to learn the spurious correlation between the cake region and the word “fork” during training. Thus, as shown in Figure 1b, the original transformer usually generates the incorrect word “fork” instead of the correct word “spoon” for the test image.
Recently, Yang et al. [44] analyzed the spurious corre-lation between the visual features and captions by causal graph and proposed a deconfounded image captioning (DIC) framework to tackle the confounders. But they still have two limitations: (i) In their causal graph, the whole dataset is considered as the confounder and hard to be strat-ified. Thus, the complex front-door adjustment is utilized to deconfound it by introducing an additional mediator. (ii)
DIC focuses on deconfounding the decoder while neglect-ing the confounded visual features in the encoder, leading to limited performance improvements.
To solve these problems, we first divide the confounder of the existing causal graph into two classes: visual con-founder and linguistic confounder. Based on the detailed causal graph, we propose a novel causal inference based image captioning (CIIC) framework, which mainly con-sists of two components: an interventional object detec-tor (IOD) and an interventional Transformer decoder (ITD) to jointly confront two kinds of confounders. Specif-ically, the IOD incorporates causal inference into Faster R-CNN [33] to cope with the visual confounder, aiming to obtain the disentangled region-based representations. The
ITD implements causal intervention in the Transformer de-coder by deconfounding both the visual and linguistic con-founder simultaneously. As shown in Figure 1b, CIIC can effectively eliminate the spurious correlations caused by the visual and linguistic confounders and generate the correct word “spoon”.
Our contributions can be summarized as follows:
• We decompose the confounder into the visual and lin-guistic confounders and show a more detailed causal graph for the transformer-based image captioning system, which can be easily deconfounded by the backdoor adjustment, in-stead of the more complex front-door adjustment.
• We propose an IOD to disentangle the region-based features in the encoder and design a novel ITD by decon-founding the causal graph, which can effectively eliminate the spurious correlations caused by both visual and linguis-tic confounders.
• We implement our transformer-based CIIC framework to facilitate the unbiased captioning generation and ex-tensively evaluate our approach on the MSCOCO bench-mark [24]. CIIC achieves a new state-of-the-art perfor-mance compared to previous transformer-based captioning approaches. 2.