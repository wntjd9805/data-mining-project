Abstract
Recent advances in deep learning have relied on large, labelled datasets to train high-capacity models. However, collecting large datasets in a time- and cost-efﬁcient man-ner often results in label noise. We present a method for learning from noisy labels that leverages similarities be-tween training examples in feature space, encouraging the prediction of each example to be similar to its nearest neigh-bours. Compared to training algorithms that use multiple models or distinct stages, our approach takes the form of a simple, additional regularization term.
It can be inter-preted as an inductive version of the classical, transduc-tive label propagation algorithm. We thoroughly evaluate our method on datasets evaluating both synthetic (CIFAR-10, CIFAR-100) and realistic (mini-WebVision, WebVision,
Clothing1M, mini-ImageNet-Red) noise, and achieve com-petitive or state-of-the-art accuracies across all of them. 1.

Introduction
While deep learning can achieve unprecedented accu-racy in image classiﬁcation tasks, it requires a large, su-pervised dataset that is often expensive to obtain. Unsu-pervised and semi-supervised learning seek to alleviate this requirement by incorporating unlabelled examples. How-ever, these approaches cannot take advantage of the various sources of noisy labels in the modern world, such as images with hashtags in social media or images contained in web-pages retrieved by a textual query. Training algorithms that are robust to label noise are therefore highly attractive for deep learning.
The dominant approach to learning with noisy labels in recent work is to use the predictions of the model itself to reject or modify training examples (e.g. [1, 13, 22, 26, 29, 33, 37, 46]). This is inherently risky due to the ability of deep networks to ﬁt arbitrary labels [47] and it is impor-tant to take signiﬁcant measures against overﬁtting. More-over, this paradigm often leads to complicated training pro-cedures, such as maintaining multiple models or alternating between updating the model and updating the training set.
*Work done at Google.
This paper proposes Neighbor Consistency Regulariza-tion (NCR) for the speciﬁc problem of learning with noisy labels, illustrated in Figure 1. Rather than adopting model predictions as pseudo-labels, NCR introduces an additional consistency loss that encourages each example to have sim-ilar predictions to its neighbors. Concretely, the neighbor consistency loss penalizes the divergence of each example’s prediction from a weighted combination of its neighbors’ predictions, with the weights determined by their similarity in feature space. The motivation of NCR is to enable incor-rect labels to be improved or at least attenuated by the labels of their neighbors, relying on the assumption that the noise is sufﬁciently weak or unstructured so as not to overwhelm the correct labels. Compared to the popular approach of bootstrapping the model predictions [33], NCR can be seen as instead bootstrapping the learned feature representation, which may reduce its susceptibility to overﬁtting and im-prove its stability at random initialization.
NCR is inspired by label propagation algorithms for semi-supervised learning [15, 19, 49], which seek to trans-fer labels from supervised examples to neighboring unsu-pervised examples according to their similarity in feature space. However, whereas label propagation is typically per-formed in a batch setting over the entire dataset, our method effectively performs label propagation online within mini-batches during stochastic gradient descent. This results in a simple, single-stage training procedure. Moreover, whereas existing methods for label propagation represent transduc-tive learning in that they only produce labels for the speciﬁc examples which are provided during training, NCR can be understood as an inductive form of label propagation in that it produces a model which can later be applied to classify unseen examples.
The key contributions of the paper are as follows.
• We propose Neighbor Consistency Regularization, a novel loss term for deep learning with noisy labels that encourages examples with similar feature representa-tions to have similar predictions.
• We verify empirically that NCR achieves better accu-racy than several important baselines at a wide range of noise levels from both synthetic and real distribu-tions, and is complementary to the popular regulariza-tion technique of mixup [48].
• We demonstrate that NCR achieves competitive or state-of-the-art accuracies on datasets evaluating both synthetic (CIFAR-10 and CIFAR-100) and realis-tic (mini-WebVision [22], mini-ImageNet-Red [18],
Clothing1M [43]) noise scenarios. 2.