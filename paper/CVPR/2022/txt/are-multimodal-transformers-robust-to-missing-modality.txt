Abstract
Multimodal data collected from the real world are often imperfect due to missing modalities. Therefore multimodal models that are robust against modal-incomplete data are highly preferred. Recently, Transformer models have shown great success in processing multimodal data. However, existing work has been limited to either architecture de-signs or pre-training strategies; whether Transformer mod-els are naturally robust against missing-modal data has rarely been investigated. In this paper, we present the first-of-its-kind work to comprehensively investigate the behav-ior of Transformers in the presence of modal-incomplete data. Unsurprising, we find Transformer models are sen-sitive to missing modalities while different modal fusion strategies will significantly affect the robustness. What sur-prised us is that the optimal fusion strategy is dataset de-pendent even for the same Transformer model; there does not exist a universal strategy that works in general cases.
Based on these findings, we propose a principle method to improve the robustness of Transformer models by automati-cally searching for an optimal fusion strategy regarding in-put data. Experimental validations on three benchmarks support the superior performance of the proposed method. 1.

Introduction
Multimodal Transformers are emerging as the dominant choice in multimodal learning across various tasks [7], in-cluding classification [21,29], segmentation [34], and cross-model retrieval [18]. They have become the driving force in obtaining better performance on these tasks through a pre-train-and-transfer [3] paradigm.
Although Transformers have demonstrated remarkable success in processing multimodal data, they generally re-quire modal-complete data. The completeness of modality may not always hold in the real world due to privacy or security constraints. For example, a social network might be unable to access location information if users decline to share their private location [20]; a healthcare application
Table 1.
Evaluation of the Transformer robustness against missing-modal data on MM-IMDb, UPMC Food-101, and Hate-ful Memes. We use ViLT [18] as the backbone. Note that the multimodal performance is even worse than the unimodal one, when modality is missing severely (results are highlighted in shaded gray). ∗The reported evaluation scores are F1-Macro (MM-IMDb), Accuracy (UPMC Food-101), and AUROC (Hate-ful Memes). Higher scores indicate better results.
Dataset
Training
Testing
Image
Text
Image
Text
Evaluation∗
∆ ↓
MM-IMDb [2]
UPMC Food-101 [43]
Hateful Memes [17] 100% 100% 100% 100% 100% 100% 100% 30% 0% 100% 100% 0% 100% 100% 100% 100% 100% 100% 100% 30% 0% 100% 100% 0% 100% 100% 100% 100% 100% 100% 100% 30% 0% 100% 100% 0% 55.3 31.2 35.0 91.9 65.9 71.5 70.2 60.2 56.3 0% 43.6% 36.7% 0% 28.3% 22.2% 0% 14.2% 19.8% might not have all the records available when patients are unwilling to undergo risky or invasive examinations [37].
For this reason, it is important that Transformer models are robust against missing-modal data, i.e., the model perfor-mance does not degrade dramatically.
Despite its real-world importance, the robustness against missing modalities in multimodal Transformers is seldom investigated in the literature. So far, research on Trans-former models has been limited to developing new archi-tectures for fusion [29, 35, 38] or exploring better self-supervised learning tasks [1, 6, 8, 47, 48]. Recent work on
Transformer robustness has primarily focused on noisy in-puts rather than missing modalities [23].
A question naturally arises: Are Transformer models ro-bust against missing-modal data? We empirically evaluate this problem across multiple datasets in Table 1. Unsurpris-ingly, we find that Transformer models degrade dramati-cally with missing-modal data. As shown, the multimodal performance drops when tested with modal-incomplete data, and, surprisingly, the multimodal performance is even worse than the unimodal when text are missing severely, i.e., only 30% of text are available.
Table 2. Evaluation of the Transformer models under different fusion strategies on MM-IMDb and Hateful Memes. Early fusion refers to fusion at the first layer; Late fusion refers to fusion at the last layer. Different fusion strategies affect model robustness against the missing-modal data.
Dataset
Train
Test
Fusion Strategy
Image
Text
Image
Text
Early
MM-IMDb
UPMC Food-101
Hateful Memes 100% 100% 100% 100% 55.3 100% 100% 100% 100% 91.9 100% 100% 100% 100% 70.2
MM-IMDb
UPMC Food-101
Hateful Memes 100% 100% 100% 30% 100% 100% 100% 30% 100% 100% 100% 30% 31.2 65.9 60.2
Late 54.9 91.8 64.5 31.0 69.1 57.8
Prior work on Transformer models has shown that fu-sion strategies affect computation complexity and perfor-mances [3, 21, 29]. Another question arises: Will the fu-sion strategy affect Transformer robustness against modal-incomplete data? Unsurprisingly, we observe that differ-ent fusion strategies will significantly affect the robustness.
What surprised us is that the optimal fusion strategy is dataset-dependent; there does not exist a universal strat-egy that works in general cases in the presence of modal-incomplete data. As shown in Table 2, when tested with missing-modal data, early fusion is preferred on MM-IMDb and Hateful Memes, while late fusion is preferred on the
UPMC Food-101. This motivates us to improve the robust-ness of Transformers by automatically attain the optimal fu-sion strategy regarding different datasets.
We propose a new method to achieve this goal. Our main idea is to jointly optimize Transformer models with modal-complete and modal-incomplete data via multi-task optimization. On top of that, we propose a searching algo-rithm to attain the best fusion strategy regarding different datasets. Overall, the main contributions are as follows:
• To the best of our knowledge, this paper is first-of-its-kind study to investigate the Transformer robustness against modal-incomplete data.
• We observe that Transformer models degrade dramati-cally with missing-modal data. And surprisingly, the there optimal fusion strategy is dataset dependent; does not exist a universal strategy that works in the presence of modal-incomplete data.
• We improve the robustness of Transformer models via multi-task optimization. To further improve robust-ness, we develop an differentiable algorithm to attain the optimal fusion strategy.
• We conduct extensive experiments and ablation study on MM-IMDb [2], UPMC Food-101 [43], and Hateful
Memes [17] to support our findings and validate the robustness of our method against missing modality. 2.