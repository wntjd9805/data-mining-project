Abstract
Test-time domain adaptation aims to adapt a source pre-trained model to a target domain without using any source data. Existing works mainly consider the case where the target domain is static. However, real-world machine per-ception systems are running in non-stationary and contin-ually changing environments where the target domain dis-tribution can change over time. Existing methods, which are mostly based on self-training and entropy regulariza-tion, can suffer from these non-stationary environments.
Due to the distribution shift over time in the target do-main, pseudo-labels become unreliable. The noisy pseudo-labels can further lead to error accumulation and catas-trophic forgetting. To tackle these issues, we propose a con-tinual test-time adaptation approach (CoTTA) which com-prises two parts. Firstly, we propose to reduce the error accumulation by using weight-averaged and augmentation-averaged predictions which are often more accurate. On the other hand, to avoid catastrophic forgetting, we pro-pose to stochastically restore a small part of the neurons to the source pre-trained weights during each iteration to help preserve source knowledge in the long-term. The proposed method enables the long-term adaptation for all parame-ters in the network. CoTTA is easy to implement and can be readily incorporated in off-the-shelf pre-trained models. We demonstrate the effectiveness of our approach on four clas-sification tasks and a segmentation task for continual test-time adaptation, on which we outperform existing methods.
Our code is available at https://qin.ee/cotta. 1.

Introduction
Test-time domain adaptation aims to adapt a source pre-trained model by learning from the unlabeled test (target) data during inference time. Due to the domain shift be-tween source training data and target test data, an adapta-tion is necessary to achieve good performance. For exam-ple, a semantic segmentation model trained on data from
*The corresponding author
Figure 1. We consider the online continual test-time adaptation scenario. The target data is provided in a sequence and from a continually changing environment. An off-the-shelf source pre-trained network is used to initialize the target network. The model is updated online based on the current target data, and the predic-tions are given in an online fashion. The adaptation of the target network does not rely on any source data. Existing methods often suffer from error accumulation and forgetting which result in per-formance deterioration over time. Our method enables long-term test-time adaptation under continually changing environments. clear weather conditions can suffer significant performance deterioration when tested on snowy night conditions [50].
Similarly, a pre-trained image classification model can also suffer this phenomenon when tested on corrupted images re-sulting from sensor degradation. Due to privacy concerns or legal constraints, the source data is generally considered un-available during inference time under this setup, making it a more challenging but more realistic problem than unsuper-vised domain adaptation. In many scenarios, the adaptation also needs to be performed in an online fashion. Therefore, test-time adaptation is critical to the success of real-world machine perception applications under domain shift.
Existing works on test-time adaptation often tackle the distribution shift between the source domain and a fixed target domain by updating model parameters using pseudo-labels or entropy regularization [43,61]. These self-training
methods have been proven to be effective when the test data are drawn from the same stationary domain. However, they can be unstable [48] when the target test data originates from an environment which is continually changing. There are two aspects that contribute to this: Firstly, under the con-tinually changing environment, the pseudo-labels become noisier and mis-calibrated [13] because of the distribution shift. Therefore, early prediction mistakes are more likely to result in error accumulation [4]. Secondly, as the model is being continually adapted to new distributions for a long time, knowledge from the source domain is harder to pre-serve, leading to catastrophic forgetting [11, 41, 45].
Aiming to tackle these problems under the continually changing environment, this work focuses on the practical problem of online continual test-time adaptation. As shown in Figure 1, the goal is to start from an off-the-shelf source pre-trained model, and continually adapt it to the current test data. Under this setup, we assume that the target test data is streamed from a continually changing environment.
The prediction and updates are performed online, meaning that the model will only have access to the current stream of data without having access to the full test data nor any source data. The proposed setup is very relevant for real-world machine perception systems. For example, surround-ing environments are continually changing for autonomous driving systems (e.g. weather change from sunny to cloudy then to rainy). They can even change abruptly (e.g. when a car exits a tunnel and the camera gets suddenly over-exposed). A perception model need to adapt itself and make decisions online under these non-stationary domain shifts.
To effectively adapt the pre-trained source model to the continually changing test data, we propose a contin-ual test-time adaptation approach (CoTTA) which tackles the two main limitations of existing methods. The first component of the proposed method aims to alleviate er-ror accumulation. We propose to improve the pseudo-label quality under the self-training framework in two different ways. On the one hand, motivated by the fact that the mean teacher predictions often have a higher quality than the standard model [55], we use a weight-averaged teacher model to provide more accurate predictions. On the other hand, for test data which suffers larger domain gap, we use the augmentation-averaged predictions to further boost the quality of pseudo-labels. The second component of the pro-posed method aims to help preserve the source knowledge and avoid forgetting. We propose to stochastically restore a small part of neurons in the network back to the pre-trained source model. By reducing error accumulation and preserv-ing knowledge, CoTTA enables long-term adaptation in a continuously changing environment, and makes it possible to train all parameters of the network. In contrast, previous methods [43, 61] can only train batchnorm parameters.
It is worth pointing out that our approach can be eas-ily implemented. The weight-and-augmentation-averaged strategy and the stochastic restoration can be readily incor-porated into any off-the-shelf pre-trained model without the need to re-train it on source data. We demonstrate the ef-fectiveness of our proposed approach on four classification tasks and a segmentation task for continual test-time adap-tation, on which we significantly improve performance over existing methods. Our contributions are summarized blow:
• We propose a continual test-time adaptation approach which can effectively adapt off-the-shelf source pre-trained models to continually changing target data.
• Specifically, we reduce the error accumulation by using weight-averaged and augmentation-averaged pseudo-labels that are more accurate.
• The long-term forgetting effect is alleviated by explic-itly preserving the knowledge from the source model.
• The proposed approach significantly improves the con-tinual test-time adaptation performance on both classi-fication and segmentation benchmarks. 2.