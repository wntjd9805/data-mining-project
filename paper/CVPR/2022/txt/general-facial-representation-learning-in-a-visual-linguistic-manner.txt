Abstract
How to learn a universal facial representation that boosts all face analysis tasks? This paper takes one step toward this goal. In this paper, we study the transfer perfor-mance of pre-trained models on face analysis tasks and in-troduce a framework, called FaRL, for general facial repre-sentation learning. On one hand, the framework involves a contrastive loss to learn high-level semantic meaning from image-text pairs. On the other hand, we propose exploring low-level information simultaneously to further enhance the face representation by adding a masked image modeling.
We perform pre-training on LAION-FACE, a dataset con-taining a large amount of face image-text pairs, and eval-uate the representation capability on multiple downstream tasks. We show that FaRL achieves better transfer perfor-mance compared with previous pre-trained models. We also verify its superiority in the low-data regime. More impor-tantly, our model surpasses the state-of-the-art methods on face analysis tasks including face parsing and face align-ment. 1.

Introduction
Face analysis tasks are of crucial importance to social interactions and have received extensive attention over the past decades. Many existing state-of-the-art results [7, 50, 92] come from deep neural networks with supervised learn-ing. However, such supervised models, in order to learn appropriate feature representations for each given task, are studied separately with large-scale manually annotated data which is expensive and difficult to acquire, especially for some face tasks such as face parsing and face alignment.
Recently, visual representation learning in computer vi-*Equal contribution.
Â²Corresponding author. sion appears to be paved by a popular learning paradigm, pre-training, due to the remarkable success of the ground-breaking models in Natural Language Processing such as
BERT [24] and GPT-series [5, 74, 75], followed by a wide variety of multiple techniques [25, 45, 52, 71, 76]. There-after in vision, many attempts [13, 33, 36] along this path have been proposed, showing promising results using ap-proaches related to contrastive loss [2, 10, 29, 35, 69].
Meanwhile in the area of vision-language tasks involving multi-modality, there are studies [44, 53, 54, 62, 73, 87] ex-ploring learning directly from large, freely available image-text pairs up to hundreds of millions. Their results show that natural language supervision is beneficial for visual repre-sentation learning, bringing superior performance to most tasks on general images. Such pre-training has several ad-vantages: 1) showing promising few-shot transfer perfor-mance, alleviating the hard-acquired labeled data issue; 2) enabling convenient deployment by extracting general fea-ture representation once and then applying to diverse down-stream tasks. Yet when it comes to the face domain, one of the most important domains in computer vision, the effec-tiveness of pre-training is relatively unexplored.
In this paper, we study the behaviors of transfer perfor-mance of pre-trained models on face analysis tasks and in-troduce a framework, FaRL, to learn general facial repre-sentation in a visual-linguistic manner. Instead of crawling images and texts from the Web by manually designed face-related queries, we create a dataset by filtering out from a large openly available image-text-pair dataset [82], result-ing in a subdataset containing 20 million face images, de-noted as LAION-FACE.
We adopt the widely used contrastive loss to pull the em-beddings of matched image-text pair together while push-ing those of non-matched image-text pairs apart, which pro-vides high-level semantic meaning. We propose exploring supplementary low-level information simultaneously to fur-ther enhance the face representation, by adding a masked
image modeling inspired from BEiT [3]. We perform pre-training on LAION-FACE and evaluate the representation capability with the frozen backbone, as our ultimate goal is to offer a general facial representation that can be quickly adapted to downstream tasks.
We measure the performance on several important down-stream face tasks, including face parsing, face alignment and face attribute prediction, for which labeled data is usu-ally difficult to acquire. We show that better transfer per-formance can be achieved compared with other pre-trained models. We also demonstrate its superiority in the low-data regime. Moreover, our model outperforms the state-of-the-art methods on face tasks including face parsing and face alignment.
In summary, our major contributions are as follows: 1. We present an extensive study about the transferable visual models learned in a visual-linguistic manner on versatile face analysis tasks, which is relatively unex-plored in the literature. 2. We introduce a new framework, exploring low-level and high-level information simultaneously for bet-ter representation. We achieve superior transfer per-formance than previous pre-training approaches, and more importantly on face parsing and face alignment, our model has surpassed the state-of-the-art methods. 2.