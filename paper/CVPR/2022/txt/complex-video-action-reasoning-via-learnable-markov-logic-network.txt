Abstract
Profiting from the advance of deep convolutional net-works, current state-of-the-art video action recognition models have achieved remarkable progress. Neverthe-less, most of existing models suffer from low interpretabil-ity of the predicted actions.
Inspired by the observation that temporally-configured human-object interactions often serve as a key indicator of many actions, this work crafts an action reasoning framework that performs Markov Logic
Network (MLN) based probabilistic logical inference. Cru-cially, we propose to encode an action by first-order logical rules that correspond to the temporal changes of visual re-lationships in videos. The main contributions of this work are two-fold: 1) Different from existing black-box models, the proposed model simultaneously implements the local-ization of temporal boundaries and the recognition of ac-tion categories by grounding the logical rules of MLN in videos. The weight associated with each such rule further provides an estimate of confidence. These collectively make our model more explainable and robust. 2) Instead of us-ing hand-crafted logical rules in conventional MLN, we de-velop a data-driven instantiation of the MLN. In specific, a hybrid learning scheme is proposed. It combines MLN’s weight learning and reinforcement learning, using the for-mer’s results as a self-critic for guiding the latter’s train-ing. Additionally, by treating actions as logical predicates, the proposed framework can also be integrated with deep models for further performance boost. Comprehensive ex-periments on two complex video action datasets (Charades
& CAD-120) clearly demonstrate the effectiveness and ex-plainability of our proposed method. 1.

Introduction
Action recognition is a fundamental task in video under-standing and has garnered significant attention in the last few years. Recently, in virtue of the drastic development of deep learning, 3D convolutional networks (3D CNNs)
*Yadong Mu is the corresponding author. Part of the work was per-formed when Yang Jin was an intern at Baidu Research.
Figure 1. An illustration example from Action Genome [22]. It demonstrates that actions can usually be decomposed into evolv-ing spatio-temporal scene graphs (i.e., how a person interacts with surrounding objects over time such as person-lying-on-bed to person-siting-on-bed). Inspired by this, we propose to use a data-driven Markov Logic Network to model this evolving pattern. have revolutionized this research field [4, 7, 8, 10, 23]. With various elaborately-designed neural architectures and end-to-end learning algorithms, it has emerged as a prominent paradigm for video action recognition. Compared to early works [21, 33, 55, 56] based on low-level features (e.g., tra-jectories, key points), the powerful representation capability of 3D CNNs enables them to better capture complex long-range semantic dependencies across video frames.
Though extensively adopted in modern video action un-derstanding tasks, these deep neural networks still suffer from some inherent deficiencies. Typically, 3D CNNs are fed a video clip and output a score that indicates the confi-dence for each action category through multi-layer calcula-tions. Such a black-box predicting mechanism does not ex-plicitly provide compelling evidence regarding the actions, such as when / where / why the action occurred. The lack of interpretability also makes deep neural networks vulnerable to adversarial attacks [16, 36], which limits its applications in many real-world scenarios [2] with strict security require-ments. Therefore, in recent years, an increasing research ef-fort has been devoted to explainable deep learning [45, 62].
All afore-mentioned facts strongly spur us to pursue an ac-tion reasoning framework with both accurate performance and convincing interpretability.
Our motivation is also built upon some discovery from cognitive science and neuroscience [43,49] that people usu-ally represent visual events as a composition of prototypi-cal atomic unit. The research in [22] reveals that a com-plex action can be decomposed into spatio-temporal scene graphs, which depict how a person interacts with surround-ing objects over time. Take action “awakening in bed” shown in Figure 1 as an example. To accomplish this ac-tion, a person may be initially lying on the bed, then wake up and sit on the bed. The procedure can be described by the temporal evolution of the human-object relationship, namely from ⟨person, lying on, bed⟩ to ⟨person, siting on, bed⟩. This allows the model to explicitly recognize the oc-currence of actions through detecting the transition of vi-sual relationships, thereby its interpretability and robustness can be significantly improved. To implement this idea, we need to address two key challenges: automatically learn-ing the temporally-evolving patterns from data instead of using hand-crafted rules, and conducting high-confidence inference under the noisy information in the real data that contaminate the aforesaid learned patterns.
To address the aforementioned issues, a novel explain-able action reasoning framework is introduced to recognize actions in untrimmed videos. Specifically, we adopt first-order logic [1] for encoding the semantic-level state change of a complex action. At each logical rule, the visual rela-tionships serve as atomic predicates. These rules contain adequate information and can be generated by a recurrent policy network from scratch. This procedure proceeds by progressively adding the action-related relationship predi-cates. Since these rules are generated in a data-driven fash-ion rather than by domain experts, they are prone to errors.
To tackle this problem, we resort to Markov Logic Network (MLN) [44], a statistical relational model that combines first-order logic and probabilistic graphical models [30]. It associates a weight to each logical rule to soundly handle its uncertainty: the larger the weight, the more reliable the rule is. Hence, assigning lower (even negative) weights to noisy ones will alleviate their deficiency. Eventually, the probability of occurrence for each action is determined via conducting probabilistic logical reasoning on MLN.
The overall training scheme of our framework consists of two stages: rule exploration and weight learning. The first stage is accomplished by leveraging reinforcement learning.
As for the second stage, the weight belonging to each rule can be updated via supervised learning (i.e., maximizing the likelihood of actions in the videos). Notably, the evaluation result from weight learning can be exploited as a critic cri-terion for guiding the rule exploration. The technical con-tributions of this work can be summarized as follows: (1) Compared to the prevalent deep 3D convolutional networks, the proposed framework enjoys remarkable inter-pretability since the weighted logical rules can convey clear evidence regarding specific action. Moreover, our frame-work naturally supports simultaneously recognizing the cat-egories of actions and localizing their temporal boundaries, benefiting from the learned temporal-evolution patterns. (2) The logical rules for encoding complex actions can be automatically exploited from data via our proposed rule exploration mechanism, which is superior to some earlier approaches [3, 35, 54, 70] that relied on manually-designed rules to perform action reasoning. (3) Comprehensive experiments on two challenging video benchmarks (Charades [47] and CAD-120 [31]) show that our method obtained excellent performance.
In ad-dition, it can furthermore boost the accuracy when being integrated with deep models. Surprisingly, our framework are still capable of achieving outstanding performance only leveraging limited number of training examples. 2.