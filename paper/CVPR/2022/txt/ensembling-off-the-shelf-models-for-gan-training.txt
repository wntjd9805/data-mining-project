Abstract
The advent of large-scale training has produced a cor-nucopia of powerful visual recognition models. However, generative models, such as GANs, have traditionally been trained from scratch in an unsupervised manner. Can the collective “knowledge” from a large bank of pretrained vi-sion models be leveraged to improve GAN training? If so, with so many models to choose from, which one(s) should be selected, and in what manner are they most effective?
We find that pretrained computer vision models can signif-icantly improve performance when used in an ensemble of discriminators. Notably, the particular subset of selected models greatly affects performance. We propose an effec-tive selection mechanism, by probing the linear separability between real and fake samples in pretrained model embed-dings, choosing the most accurate model, and progressively adding it to the discriminator ensemble. Interestingly, our method can improve GAN training in both limited data and large-scale settings. Given only 10k training samples, our
FID on LSUN CAT matches the StyleGAN2 trained on 1.6M images. On the full dataset, our method improves FID by 1.5 to 2 on cat, church, and horse categories of LSUN.
× 1.

Introduction
Image generation inherently requires being able to cap-ture and model complex statistics in real-world visual phe-nomenon. Computer vision models, driven by the success of supervised and self-supervised learning techniques [15, 17, 33, 66, 78], have proven effective at capturing useful rep-resentations when trained on large-scale data [69, 92, 103].
What potential implications does this have on generative modeling? If one day, perfect computer vision systems could answer any question about any image, could this capability be leveraged to improve image synthesis models?
Surprisingly, despite the aforementioned connection be-tween synthesis and analysis, state-of-the-art generative ad-versarial networks (GANs) [9, 39, 40, 101] are trained in an unsupervised manner without the aid of such pretrained net-works. With a plethora of useful models easily available in the research ecosystem, this presents a missed opportunity to explore. Can the knowledge of pretrained visual represen-Figure 1. Vision-aided GAN training. The model bank F con-sists of widely used and state-of-the-art pretrained networks. We automatically select a subset { ˆF }K k=1 from F, which can best dis-tinguish between real and fake distribution. Our training procedure consists of creating an ensemble of the original discriminator D and discriminators ˆDk = ˆCk ◦ ˆFk based on the feature space of selected off-the-shelf models. ˆCk is a shallow trainable network over the frozen pretrained features. tations actually benefit GAN training? If so, with so many models, tasks, and datasets to choose from, which models should be used, and in what manner are they most effective?
In this work, we study the use of a “bank” of pretrained deep feature extractors to aid in generative model training.
Specifically, GANs are trained with a discriminator, aimed at continuously learning the relevant statistics differentiating real and generated samples, and a generator, which aims to reduce this gap. Na¨ıvely using such strong, pretrained networks as a discriminator leads to the overfitting and over-whelming the generator, especially in limited data settings.
We show that freezing the pretrained network (with a small, lightweight learned classifier on top as shown in Figure 1) provides stable training when used with the original, learned discriminator. In addition, ensembling multiple pretrained networks encourages the generator to match the real distri-bution in different, complementary feature spaces.
To choose which networks work best, we propose to use an automatic model selection strategy, based on the linear separability of real and fake images in the feature space, and progressively add supervision from a set of available pre-trained networks. In addition, we use label smoothing [72] and differentiable augmentation [39, 101] to stabilize the model training further and reduce overfitting.
We experiment on several datasets in both limited and large-scale sample setting to show the effectiveness of our method. We improve the state-of-the-art on FFHQ [41] and
LSUN [92] datasets given 1k training samples by 2-3 on the FID metric [35]. For LSUN CATs, we match the FID of StyleGAN2 trained on the full dataset (1.6M images) with only 10k samples, as shown in Figure 2. In the full-scale data setting, our method improves FID for LSUN
CATs from 6.86 to 3.98, LSUN CHURCH from 4.28 to 1.72, and LSUN HORSE from 4.09 to 2.11. Finally, we visualize the internal representation of our learned mod-els as well as training dynamics. Check out our code at https://github.com/nupurkmr9/vision-aided-gan. Full version of the paper is available at https://arxiv.org/abs/2112.09130.
× 2.