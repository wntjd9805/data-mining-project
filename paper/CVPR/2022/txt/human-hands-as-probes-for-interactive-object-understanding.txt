Abstract
Interactive object understanding, or what we can do to objects and how is a long-standing goal of computer vi-sion. In this paper, we tackle this problem through obser-vation of human hands in in-the-wild egocentric videos. We demonstrate that observation of what human hands interact with and how can provide both the relevant data and the necessary supervision. Attending to hands, readily local-izes and stabilizes active objects for learning and reveals places where interactions with objects occur. Analyzing the hands shows what we can do to objects and how. We apply these basic principles on the EPIC-KITCHENS dataset, and successfully learn state-sensitive features, and object affor-dances (regions of interaction and afforded grasps), purely by observing hands in egocentric videos. 1.

Introduction
Consider the cupboard in Figure 1. Merely localizing and naming it is insufficient for a robot to successfully in-teract with it. To enable interaction we, we need to identify what are plausible sites for interaction, how should we in-teract with each site, and what would happens when we do.
The goal of this paper is to acquire such an understanding about objects. Specifically, we formulate it as a) learning a feature space that is sensitive to the state of the object (and thus indicative of what we can do with it) rather than just its category; and b) identifying what hand-grasps do objects afford and where. These together provide an interactive understanding of objects, and could aid learning policies for robots. For instance, distance in a state-sensitive fea-ture space can be used as reward functions for manipulation tasks [52,54,64]. Similarly, hand-grasps afforded by objects and their locations provide priors for exploration [38, 39].
While we have made large strides in building models for how objects look (the various object recognition problems), the same recipe of collecting large-scale labeled datasets
Project website: https://s-gupta.github.io/hands-as-probes/.
Figure 1. Human hands reveal information about objects as they interact with them. They tell us where and how we can interact with an object (the handle of the cupboard via an adducted thumb grasp), and what happens when we do (cupboard opens to reveals many more objects within). This paper develops techniques to extract an interactive understanding of objects through the obser-vation of hands in a corpus of egocentric videos. Specifically, we produce a) features that are indicative of object states, and b) ob-ject affordances (i.e. regions of interaction, and afforded grasps). for training doesnâ€™t quite apply for understanding how ob-jects work. First of all, no large-scale labeled datasets al-ready exist for such tasks. Second, manually annotating these aspects on static images is challenging. For instance, objects states are highly contextual: the same object (e.g. cupboard in Figure 1) can exist in many different states (closed, full, on-top-of, has-handle, in-contact-with-hand) at the same time, depending on the interaction we want to conduct. Similarly, consciously annotating where and how one can touch an object can suffer from biases, leading to data that may not be indicative of how people actually use objects during normal daily conduct. While one might an-notate that we pull on the handle to open the cupboard; in real life we may very often just flick it open by sliding our fingers in between the cupboard door and its frame.
Motivated by these challenges, we pursue learning di-rectly from the natural ways in which people interact with objects in egocentric videos. Since, egocentric data focuses upon hand-object interaction, it solves both the data and the supervision problem. Egocentric observation of human hands reveals information about the objects they are inter-acting with. Attending to locations that hands attend to, localizes and stabilizes active objects in the scene for learn-ing. It shows where all hands can interact in the scene. Ana-lyzing what the hand is doing reveals information about the state of the object, and also how to interact with it. Thus, ob-servation of human hands in egocentric videos can provide the necessary data and supervision for obtaining an interac-tive understanding of objects.
To realize these intuitions, we design novel techniques that extract an understanding of objects from the under-standing of hands as obtained from off-the-shelf models.
We apply this approach to the two aspects of interactive object understanding: a) learning state-sensitive features, and b) inferring object affordances (identifying what hand-grasps do objects placed in scenes afford and where).
For the former goal of learning state-sensitive features, we hand-stabilize the object-of-interaction. We exploit the appearance and motion of the hand as it interacts with the object to derive supervision for the object state. This is done through contrastive learning where we encourage objects associated with similar hand appearance and motion, to be similar to one another. This leads to features that are more state-sensitive than those obtained from alternate forms of self-supervision, and even direct semantic supervision.
For the latter goal of predicting regions-of-interaction and applicable grasps, we additionally use hand grasp-type predictions. As the hand is directly visible when the inter-action is happening, the challenge here is to get the model to focus on the object to make its predictions, rather than the hand. For this, we design a context prediction task: we mask-out the hand and train a model to predict the lo-cation and grasp-type from the surrounding context. We find that modern models can successfully learn to make such contextual predictions. This enables us to identify the places where humans interact in scenes. We better recall small interaction sites such as knobs and handles, and also make more specific predictions when interaction sites are localized to specific regions on the objects (e.g. knobs for stoves). We are also able to successfully learn hand-grasps applicable to different objects.
For both these aspects, deriving supervision from hands sidesteps the need for and possible pitfalls of semantic su-pervision. We are able to conduct learning without having to define a complete taxonomy of object states, or suffer from inherent ambiguity in defining action classes. 2.