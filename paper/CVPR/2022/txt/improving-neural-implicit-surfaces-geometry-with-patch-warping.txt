Abstract
Neural implicit surfaces have become an important tech-nique for multi-view 3D reconstruction but their accuracy remains limited. In this paper, we argue that this comes from the difﬁculty to learn and render high frequency textures with neural networks. We thus propose to add to the standard neural rendering optimization a direct photo-consistency term across the different views. Intuitively, we optimize the implicit geometry so that it warps views on each other in a consistent way. We demonstrate that two elements are key to the success of such an approach: (i) warping en-tire patches, using the predicted occupancy and normals of the 3D points along each ray, and measuring their sim-ilarity with a robust structural similarity (SSIM); (ii) han-dling visibility and occlusion in such a way that incorrect warps are not given too much importance while encour-aging a reconstruction as complete as possible. We eval-uate our approach, dubbed NeuralWarp, on the standard
DTU and EPFL benchmarks and show it outperforms state of the art unsupervised implicit surfaces reconstructions by over 20% on both datasets. Our code is available at https://github.com/fdarmon/NeuralWarp 1.

Introduction
Multi-view 3D reconstruction is the task of recover-ing the geometry of objects by looking at their projected views. Multi-View Stereo (MVS) methods rely on the photo-consistency of multiple views and typically provide the best results [29, 44]. However, they require a cumbersome multi-step procedure, ﬁrst estimating then merging depth maps.
Recent 3D optimization methods [22,24,25,34,41,42] avoid this issue by representing the surface implicitly and jointly optimizing neural networks encoding occupancy and color for all images, but their accuracy remains limited. In this work, we bridge these two types of approaches by optimizing multi-view photo-consistency for a geometry represented by implicit functions. We show that this enables our method to leverage high-frequency textures present in the input images that existing implicit methods struggle to represent, resulting (a) Volumetric rendering (left) and geometric error map (right). (b) Our method’s image warping (left) and geometric error map (right)
Figure 1. Standard neural implicit surface approaches jointly opti-mize a geometry and color network, but struggle to represent high frequency textures and therefore lack accuracy (top). We propose to additionally warp image patches with the implicit geometry, which allows to directly optimize photo-consistency between images and signiﬁcantly improves the reconstruction accuracy (bottom). in signiﬁcant accuracy gains.
The idea behind our approach is visualized on Figure 1.
The top row shows a rendering and the geometric error map (1a) for a state of the art implicit method [41]. The rendering fails at producing high frequency textures, result-ing in low 3D accuracy. To overcome this limitation we use the original images, reprojecting them using the geometry described by the implicit occupancy function. This is shown on the bottom row (1b) where our warped patch includes high frequency texture. Consequently, we can optimize the geometry much more accurately, resulting in smaller geo-metric errors in the reconstruction.
Optimizing the implicit geometry for photo-consistency poses two main challenges. First, since we do not have perfectly Lambertian materials, directly minimizing the dif-ference between colors is not meaningful and would lead to artefacts. We thus compare entire patches using a robust sim-ilarity (SSIM [36]) which requires performing patch warping using the implicit geometry. Building on the volumetric neu-ral implicit surface framework, we start by sampling 3D points on the ray associated to each pixel in a reference image. We then propose to warp for each sampled point a source image patch to the reference image using a planar scene approximation, and ﬁnally combine all warped patches.
Second, opposite to standard neural rendering methods that can associate a color to each 3D point, a warping-based ap-proach must deal with the fact that many 3D points do not project correctly in the source view, e.g. are not visible or are occluded; this will typically happen for points sampled on any ray. We thus deﬁne for each reference image pixel and each source image a soft visibility mask. We then com-pletely remove from the loss the contribution of pixels in the reference image that have no valid reprojection in any of the source views and, for the other pixels, weight the loss associated to each source view depending on how reliable the associated projection is. This downweights invalid repro-jections, while encouraging a reconstruction as complete as possible.
We evaluate our method on the DTU [14] and EPFL [32] benchmarks. Our method outperforms current state-of-the-art unsupervised neural implicit surfaces methods by a large margin: the 3D reconstruction metrics are on average improved by 20%. We also show qualitatively that our image warps are able to capture high frequency details.
To summarize, we present:
• a method to warp patches using implicit geometry;
• a loss function able to handle incorrect reprojections;
• an experimental evaluation demonstrating the very sig-niﬁcant accuracy gain on two standard benchmarks and validating each element of our approach. 2.