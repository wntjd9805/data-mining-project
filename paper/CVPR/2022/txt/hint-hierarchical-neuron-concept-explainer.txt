Abstract
To interpret deep networks, one main approach is to associate neurons with human-understandable concepts.
However, existing methods often ignore the inherent con-nections of different concepts (e.g., dog and cat both belong to animals), and thus lose the chance to explain neurons re-sponsible for higher-level concepts (e.g., animal). In this paper, we study hierarchical concepts inspired by the hier-archical cognition process of human beings. To this end, we propose HIerarchical Neuron concepT explainer (HINT) to effectively build bidirectional associations between neurons and hierarchical concepts in a low-cost and scalable man-ner. HINT enables us to systematically and quantitatively study whether and how the implicit hierarchical relation-ships of concepts are embedded into neurons. Specifically,
HINT identifies collaborative neurons responsible for one concept and multimodal neurons pertinent to different con-cepts, at different semantic levels from concrete concepts (e.g., dog) to more abstract ones (e.g., animal). Finally, we verify the faithfulness of the associations using Weakly Su-pervised Object Localization, and demonstrate its applica-bility in various tasks, such as discovering saliency regions and explaining adversarial attacks. Code is available on https://github.com/AntonotnaWang/HINT. 1.

Introduction
Deep neural networks have attained remarkable success in many computer vision and machine learning tasks. How-ever, it is still challenging to interpret the hidden neurons in a human-understandable manner, which is of great sig-nificance in uncovering the reasoning process of deep net-works and increasing the trustworthiness of deep learning to humans [3, 31, 61].
Early research focuses on finding evidence from in-put data to explain deep model predictions [4, 10, 29, 33, 34, 48, 51, 52, 54–57, 64], where the neurons remain un-explained. More recent efforts have attempted to asso-ciate hidden neurons with human-understandable concepts
[7–9,11,23,44,45,67,68,71,72]. Although insightful inter-pretations of neurons’ semantics have been demonstrated, i.e., identification of the neurons controlling contents of trees [8], existing methods define the concepts in an ad-hoc manner which heavily relies on human annotations, such as manual visual inspection [11, 44, 45, 72], manually labeled classification categories [23], or hand-crafted guidance im-ages [7–9, 71]. They thus suffer from heavy costs and scal-ability issues. Moreover, existing methods often ignore the inherent connections among different concepts (e.g., dog and cat both belong to mammal), and treat them indepen-dently, which therefore loses the chance to discover neurons responsible for implicit higher-level concepts (e.g., canine, mammal, and animal) and explore whether the network can create abstractions of things like our humans do.
The above motivates us to rethink how concepts should be defined to more faithfully reveal the roles of hidden neu-rons. We draw inspirations from the hierarchical cognition process of human beings– human tend to organize things from specific to general categories [37, 47, 60]– and pro-pose to explore hierarchical concepts which can be har-vested from WordNet [39] (a lexical database of semantic relations between words). We investigate whether deep net-works can automatically learn the hierarchical relationships of categories that were not labeled in the training data. More concretely, we aim to identify neurons for both low-level concepts such as Malamute, Husky, and Persian cat, and the implicit higher-level concepts such as dog and animal as shown in Figure 1 (a). Note that we call less abstract concepts low-level and more abstract concepts high-level.
To this end, we develop HIerarchical Neuron concepT explainer (HINT), which builds a bidirectional association between neurons and hierarchical concepts (see Figure 1).
First, we develop a saliency-guided approach to identify the high dimensional representations associated with the hier-archical concepts on hidden layers (noted as responsible regions in Figure 1 (b)), making HINT low-cost and scal-able as no extra hand-crafted guidance is required. Then, we train classifiers shown in Figure 1 (c) to separate differ-ent concepts’ responsible regions, where the weights rep-resent the contribution of the corresponding neuron to the classification. Based on the classifiers, we design a Shap-Figure 1. Overall illustration of HINT. (a) HINT is able to build bidirectional associations between hidden layer neurons and hierarchical concepts. It can also identify collaborative neurons and multimodal neurons. Further, HINT helps to indicate how the neurons learn the hierarchical relationships of categories. (b)-(c) Main steps. See Section 3.1 for Step 1, Section 3.2 for Step 2, and Section 3.3 for Step 3. ley value-based scoring method to fairly evaluate neurons’ contributions, considering both neurons’ individual and col-laborative effects.
To our knowledge, HINT presents the first attempt to as-sociate neurons with hierarchical concepts, which enables us to systematically and quantitatively study whether and how hierarchical concepts are embedded into deep network neurons. HINT identifies collaborative neurons contribut-ing to one concept and multimodal neurons contributing to multiple concepts. Especially, HINT finds that, despite be-ing trained with only low-level labels, such as Husky and
Persian cat, deep neural networks automatically embed hi-erarchical concepts into its neurons. Also, HINT is able to discover responsible neurons to both higher-level concepts, such as animal, person and plant, and lower-level concepts, such as mammal, reptile and bird.
Finally, we verify the faithfulness of neuron-concept as-sociations identified by HINT with a Weakly Supervised
Object Localization task.
In addition, HINT achieves re-markable performance in a variety of applications, includ-ing saliency method evaluation, adversarial attack explana-tion, and COVID19 classification model evaluation, further manifesting the usefulness of HINT. 2.