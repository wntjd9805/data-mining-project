Abstract
We propose a new video camouflaged object detection (VCOD) framework that can exploit both short-term dy-namics and long-term temporal consistency to detect cam-ouflaged objects from video frames. An essential property of camouflaged objects is that they usually exhibit patterns similar to the background and thus make them hard to iden-tify from still images. Therefore, effectively handling tem-poral dynamics in videos becomes the key for the VCOD task as the camouflaged objects will be noticeable when they move. However, current VCOD methods often leverage homography or optical flows to represent motions, where the detection error may accumulate from both the motion estimation error and the segmentation error. On the other hand, our method unifies motion estimation and object seg-mentation within a single optimization framework. Specifi-cally, we build a dense correlation volume to implicitly cap-ture motions between neighbouring frames and utilize the final segmentation supervision to optimize the implicit mo-tion estimation and segmentation jointly. Furthermore, to enforce temporal consistency within a video sequence, we jointly utilize a spatio-temporal transformer to refine the short-term predictions. Extensive experiments on VCOD benchmarks demonstrate the architectural effectiveness of our approach. We also provide a large-scale VCOD dataset
* Indicates equal contribution; † Corresponding author (dengp-fan@gmail.com). Work was done while Xuelian Cheng was an MBZUAI visiting scholar mentored by Deng-Ping Fan. named MoCA-Mask with pixel-level handcrafted ground-truth masks and construct a comprehensive VCOD bench-mark with previous methods to facilitate research in this direction. Dataset Link: https://xueliancheng. github.io/SLT-Net-project. 1.

Introduction
Video Camouflaged Object Detection (VCOD) is the task of discovering objects in a video that, appearance-wise, exhibit a great deal of similarity to the background scene. Despite enjoying wide applications (e.g., surveil-lance and security [24], autonomous driving [4, 32], med-ical image segmentation [11, 42], locust detection [17] and robotics [28]), the problem of Camouflaged Object Detec-tion (COD) is a daunting task as camouflaged objects are often indistinguishable to naked-eyes. This, in turn, has made VCOD a relatively under-explored problem in com-puter vision, as compared to several related problems such as video object detection (VOD) [1, 47], video salient ob-ject detection (VSOD) [15], and video motion segmentation (VMS) [16, 46].
In most computer vision tasks (e.g., instance segmenta-tion [51], saliency detection [49]), it is assumed that ob-jects have clear boundaries. This allows us to formulate the problem at the image level and even consider improve-ments if motion information is available. In contrast, ob-ject boundaries are ambiguous and indistinguishable when it comes to detecting camouflaged objects. This not only makes detection from images challenging, but also results
in inaccurate estimation of optical flow and motion cues in videos [36, 37, 52].
The lack of clear boundaries means that the appearance of the camouflaged object resembles the background. This shows itself as two fundamental difficulties: 1) the ob-ject boundaries are often seamlessly blended into the back-ground and is observable only when the object moves; 2) the object usually has repetitive textures similar to the envi-ronment; hence determining the movement of pixels across frames to estimate the motion (e.g., as done in optical flow) is erratic and erroneous. As the first difficulty, to success-fully address VCOD, a neural network needs to effectively discover the nuances between the camouflaged object and the background with the help of motion information. More-over, the motion information is inherently noisy and inac-curate according to the second difficulty, as shown in Fig-ure 1. As such, employing VOD, VSOD, and VMS tech-niques may fail miserably if naively used or combined to address the VCOD problem.
In this work, we introduce SLT-Net, a new method to address VCOD that utilizes short-term dynamics and long-term temporal consistency to detect camouflaged objects in videos. Specifically, we employ a short-term dynamic mod-ule to implicitly capture the motion between consecutive frames. Rather than using optical flow to explicitly represent motions, we use a full-range correlation pyramid strategy to represent them implicitly. The primary motivation behind using a correlation pyramid is that even SOTA optical flow algorithms fail to estimate motions for camouflaged objects and their errors accumulate over the video’s duration. Also, it allows us to jointly optimize the motion estimation (im-plicitly) and the predictions with only the detection super-vision. To provide a stable estimation, we further introduce a long-term refinement module to alleviate accumulated in-accuracies in the short-term dynamic module.
We realize the SLT-Net as a hybrid neural network with both transformer and CNN components. In particular, we use a transformer structure to encode features for construct-ing a correlation pyramid. Aside from its design flexibility, features extracted by the transformer contain global con-textual information with long-range dependencies and less inductive bias [31, 40], which we observe to be more distin-guishable in estimating the motion.
While the correlation pyramid strategy can effectively capture motions for detecting camouflaged objects, it can-not scale gracefully to long video sequences due to its com-putational complexity. To solve this issue, we adopt a sequence-to-sequence model with a spatial-temporal trans-former to refine the pair-wise prediction with long-term consistency across the videos as we empirically find it is more accurate than the standard ConvLSTM model [44,53].
Moreover, being a less-explored problem, large-scale datasets are not available to evaluate and benchmark VCOD systems. To promote new developments in this domain, we have curated a large-scale VCOD dataset based on the
Moving Camouflaged Animals (MoCA) [18]. The new dataset, or MoCA-Mask for short, contains 87 video se-quences with 22,939 frames in total with pixel-wise ground truth masks. MoCA-Mask encapsulates a variety of chal-lenges, such as complex backgrounds and tiny and well-camouflaged objects. We provide annotations, bounding boxes, and dense segmentation masks for every five frames for all the videos in the dataset. We also provide the first comprehensive benchmark for existing VCOD methods. In a nutshell, our contributions are as follows:
• We propose a new VCOD framework that can effec-tively model short-term dynamics and long-term tem-poral consistency from videos, where the motion and the camouflage object segmentation can be jointly op-timized through a single optimization target.
• We collect the first large-scale VCOD dataset, the
MoCA-Mask dataset, to promote developments in
VCOD as well as a comprehensive VCOD benchmark to facilitate research in VCOD.
• We set a new state-of-the-art on the VCOD task, out-performing a previous SOTA method [45] by 9.88%. 2.