Abstract
Although existing semantic segmentation approaches achieve impressive results, they still struggle to update their models incrementally as new categories are uncov-ered. Furthermore, pixel-by-pixel annotations are expen-sive and time-consuming. This paper proposes a novel framework for Weakly Incremental Learning for Semantic
Segmentation, that aims at learning to segment new classes from cheap and largely available image-level labels. As op-posed to existing approaches, that need to generate pseudo-labels offline, we use a localizer, trained with image-level labels and regularized by the segmentation model, to obtain pseudo-supervision online and update the model incremen-tally. We cope with the inherent noise in the process by us-ing soft-labels generated by the localizer. We demonstrate the effectiveness of our approach on the Pascal VOC and
COCO datasets, outperforming offline weakly-supervised methods and obtaining results comparable with incremental learning methods with full supervision. 1 1.

Introduction
Semantic segmentation is a fundamental problem in computer vision where significant progress has been made thanks to the surge of deep learning [13–15] and the availability of large-scale human-annotated or synthetic datasets [4, 17, 23, 40, 55]. Despite the fact that many pre-trained models using public datasets are available online, one of their key disadvantages is that they are not meant to be incrementally updated over time and their knowledge is often limited to the predefined set of classes.
A na¨ıve solution to this problem would be to extend ex-isting datasets with new annotated samples and train new models from scratch. However, this approach is impractical in case of frequent updates because training on the entire augmented dataset would take too long, increasing the en-ergy consumption and carbon footprint of machine learning models [51, 58, 61]. Moreover, retraining or fine-tuning be-*Equal contribution 1Code can be found at https://github.com/fcdl94/WILSON.
Figure 1. Illustration of WILSS. A model is first pre-trained on a set of classes (e.g., person, motorbike, car), using expensive pixel-wise annotations. Then, in the following incremental learn-ing steps, the model is updated to segment new classes (e.g., cow) being provided image-level labels and without access to old data. comes infeasible when the original data is no longer avail-able, e.g., due to privacy concerns or intellectual property.
A better solution is to incrementally add new classes to the pre-existing model, as done in some recent works [8, 21, 43, 45, 46]. Incremental learning approaches update the model’s parameters by training only on new data and em-ploying ad-hoc techniques to avoid catastrophic forgetting on old classes [44]. While they reduce the cost of training, they rely on pixel-wise supervision on novel classes, which is expensive and time-consuming to collect, and usually re-quires expert human annotators [6, 40].
To reduce the annotation cost, different types of weak supervision have been proposed: bounding boxes [18, 32], scribbles [39, 62], points [16], and image-level labels [34, 50, 52].
Image labels can be easily retrieved from im-age classification benchmarks [19] or the web, dramatically lowering the annotation cost. Nevertheless, their use has never been investigated in an incremental learning setting.
In light of these considerations, we argue that it is cru-cial to jointly address the problems of incrementally updat-ing the model and reducing the annotation cost of new data for semantic segmentation. To this end, we propose to in-crementally train a segmentation model using only image-level labels for the new classes. We call this task Weakly-Supervised Incremental Learning for Semantic Segmenta-tion (WILSS). This novel setting combines the advantages of incremental learning (training only on new class data) and weak supervision (cheap and largely available annota-tions). An illustration of WILSS is reported in Fig. 1.
Directly applying existing weakly-supervised methods to incremental segmentation would require to (i) extract pixel-wise pseudo-supervision offline using a weakly super-vised approach [3, 5, 36, 59, 63] and (ii) update the segmen-tation network resorting to an incremental learning tech-nique [8,21,43]. However, we argue that generating pseudo-labels offline in incremental settings is sub-optimal, as it in-volves two separate training stages and ignores the model’s knowledge on previous classes that can be exploited to learn new classes more efficiently.
Hence, we propose a Weakly Incremental Learning framework for semantic Segmentation that incrementally trains a segmentation model generating ONline pseudo-supervision from image-level annotations (WILSON) and exploits previous knowledge to learn new classes. We extend a standard encoder-decoder segmentation architec-ture [13–15] by introducing a localizer on the encoder, from which we extract pseudo-supervision for the segmentation backbone. To improve the pseudo-supervision, we train the localizer with a pixel-wise loss guided by the predictions of the segmentation model. This regularization serves two purposes: i) it acts as a strong prior for the previous class distribution, informing the model on where old classes are located in the image, and (ii) it provides a saliency prior for extracting better object boundaries. To address the noise present in the pseudo-supervision, instead of using hard pseudo-labels as in previous works [5, 36, 63], we obtain soft-labels from the localizer, which provides information on the probability assigned to a pixel to belong to a certain class.
To summarize, the contributions are as follows:
• We propose the Weakly supervised Incremental Learn-ing for Semantic Segmentation (WILSS) task to extend pre-trained segmentation models with new classes us-ing image-level supervision only.
• We propose WILSON, a novel framework that gen-erates pseudo-supervision online using a simple lo-calizer trained with an image-level classification loss and a pixel-wise localization loss that relies on old class knowledge. To model the noise in the pseudo-supervision, we use a convex combination of soft and hard labels that improves the segmentation perfor-mance over hard labels only.
• We evaluate our method on the Pascal VOC [23] and
COCO [40] datasets, showing that our approach out-performs offline weakly-supervised methods, and that it is comparable or slightly inferior w.r.t. fully super-vised incremental learning methods. 2.