Abstract 1.

Introduction
Reconstructing 3D shape from a single 2D image is a challenging task, which needs to estimate the detailed 3D structures based on the semantic attributes from 2D image. So far, most of the previous methods still strug-gle to extract semantic attributes for 3D reconstruction task. Since the semantic attributes of a single image are usually implicit and entangled with each other, it is still challenging to reconstruct 3D shape with detailed seman-tic structures represented by the input image. To address this problem, we propose 3DAttriFlow to disentangle and extract semantic attributes through different semantic lev-els in the input images. These disentangled semantic at-tributes will be integrated into the 3D shape reconstruc-tion process, which can provide definite guidance to the reconstruction of specific attribute on 3D shape. As a re-sult, the 3D decoder can explicitly capture high-level se-mantic features at the bottom of the network, and utilize low-level features at the top of the network, which allows to reconstruct more accurate 3D shapes. Note that the ex-plicit disentangling is learned without extra labels, where the only supervision used in our training is the input im-age and its corresponding 3D shape. Our comprehensive experiments on ShapeNet dataset demonstrate that 3DAt-triFlow outperforms the state-of-the-art shape reconstruc-tion methods, and we also validate its generalization ability on shape completion task. Code is available at https:
//github.com/junshengzhou/3DAttriFlow.
*Equal contribution.
†The corresponding author is Yu-Shen Liu. This work was sup-ported by National Key R&D Program of China (2018YFB0505400, 2020YFF0304100), the National Natural Science Foundation of China (62072268), and in part by Tsinghua-Kuaishou Institute of Future Media
Data.
Reconstructing a 3D shape from a 2D image (2D-to-3D reconstruction) is a crucial task for bridging the gap be-tween the 2D and 3D visual understanding. The typical paradigm is to firstly capture the semantic features of the 2D images through an image encoder, and then correctly re-construct them in 3D space through a 3D decoder. Among the multiple representation forms of 3D shapes (i.e. voxel, point cloud and mesh), this paper mainly focuses on re-constructing 3D point cloud from the input image, due to its lightweight storage consumption and capability of repre-senting various complicated shapes.
As addressed by the typical paradigm of most previous methods [26,37,42,51], the key of 2D-to-3D reconstruction is how to precisely interpret the semantic attribute from im-ages into the 3D space. Thanks to the recent progress of 2D computer vision, there are many well-known methods (e.g.
AlexNet [16], VGG [32] and ResNet [12]) to encode seman-tic attributes into image features, and their efficiencies have also been proved by a wide range of cross-modal tasks (e.g. image captioning [34, 53], cross-modal retrieval [35, 57]).
However, for the research of 2D-to-3D reconstruction, how to interpret visual information from 2D domain to 3D do-main for accurate 3D reconstruction still remains a difficult task. Because most previous methods [26, 37, 38, 42, 51, 52] only rely on the feature channels (e.g. element-wise add, feature concatenation and attention mechanism) to convey the visual information from the image encoder to the 3D de-coder, which only contains implicit geometric information with limited semantic attributes as the guidance to shape reconstruction. For example, an overall geometric infor-mation such as the number of legs will determine the ta-ble to have three or four legs. Such geometric informa-tion can be easily noticed and reconstructed by the decoder.
On the other hand, the detailed semantic attributes like the length or bending of legs will specifically determine the de-Figure 1. Comparison between previous methods (a) and our 3DAttriFlow (b). Besides the implicit feature x of input image, 3DAttriFlow learns an additional attribute code z, which can reveal some hints about more specific semantic attributes of 3D shape in (c). tailed shape of these legs. However, since these semantic attributes are deeply entangled with each other in image fea-tures, they can hardly be noticed by the decoder during the reconstruction process.
Moreover, semantic attributes are usually distributed at various semantic levels, and entangle with each other throughout the pyramidal hierarchy of image encoder. As a result, they can hardly be fully exploited through implicit feature channels. As a result, the previous methods usu-ally suffer from guiding the decoder to reconstruct various visual information extracted by the encoder, which leads to the insufficient usage of semantic features for predicting 3D shapes in the previous methods.
A straightforward solution to this problem is to build numbers of feature channels between the decoder and all the network layers in the encoder, which will increase the cost of tremendous computational time and network complexity.
On the other hand, as proved by many methods of image-to-image translation (e.g. image super-resolution [40, 50], image style transferring [58]), we notice that the global fea-ture is able to encode most of the semantic attributes for a single image, as they can be used for high quality image generation/restoration task. Therefore, a promising solution is to explore deeply into the global features extracted from the 2D images, and decode the abundant semantic attributes embedded in the global features, which may provide more detailed and definite guidance to the reconstruction process of 3D shapes. Following the above-mentioned intuition, we propose a novel neural network, named 3DAttriFlow, to de-compose the semantic attributes from the 2D image, and utilize these semantic attributes for 3D shape reconstruction in a controllable way.
Specifically, as shown in Figure 1, previous methods (Figure 1(a)) usually learn to reconstruct 3D shapes from an implicit image feature. In contrast, 3DAttriFlow tries to decompose an attribute code (Figure 1(b)) as hints to cap-ture some specific semantic attributes (Figure 1(c)). Such process is accomplished by the attribute flow pipe proposed in 3DAttriFlow. By piping semantic attributes hints into the hierarchical generation process of point clouds through the attribute flow pipe, the decoder is able to selectively inter-pret semantic attributes following the hierarchy of semantic levels.
Our idea is inspired by the recent generative method of
EigenGAN [13], which learns to manipulate explicit se-mantic attributes of human faces in an unsupervised way.
However, due to the discrete nature of point clouds, the co-ordinates of points are merely organized in an unordered manner, which is in contrast with the image pixels arranged in an ordered grid structure. Such nature of point clouds makes the location of each point unpredictable during the generation process, until the 3-dimensional coordinates are finally revealed at the end of the decoder. Therefore, a di-rect implementation of EigenGAN [13] based decoder may result in failure, because the network cannot accurately pre-dict the semantic attribute for a specific point without know-ing its location. To address this problem, we propose the deformation pipe as the solution, which follows the idea of
PMP-Net [47] to reconsider the shape generation process as a shape deformation process. That is, each point is first as-signed a prior location in 3D space, and then moved to their destination to regroup as a new shape. Specifically, 3DAt-triFlow moves the point cloud sampled from a 3D sphere into the target shape indicated by the 2D images. In all, our main contributions are summarized as follows.
• We propose a novel deep network, named 3DAttri-Flow, for reconstructing high-quality 3D shapes from single 2D images. Compared with the previous meth-ods, 3DAttriFlow can interpret explicit semantic at-tributes from images, and effectively use them to guide the decoder for detailed and high-quality 2D-to-3D shape reconstruction.
• We propose the attribute flow pipe to explicitly disen-tangle the semantic attributes embedded in the global feature of 2D image, which can provide definite guid-ance about the detailed reconstruction of semantic at-tributes to the 3D decoder, leading to more accurate prediction of 3D shape in terms of both overall and de-tailed shape structures.
• We propose the deformation pipe to offer the location priors to attribute flow pipe, where the extracted se-mantic attributes can be assigned to a specific point by leveraging the location of that point. As a result, 3DAttriFlow avoids the problem of assigning seman-tics to unordered data, and allows more accurate fea-ture integration between the attribute flow pipe and the deformation pipe.
2.