Abstract
We propose end-to-end Multitask Learning an
Transformer framework, named MulT, to simultane-ously learn multiple high-level vision tasks, including depth estimation, semantic segmentation, reshading, surface nor-mal estimation, 2D keypoint detection, and edge detection.
Based on the Swin transformer model, our framework encodes the input image into a shared representation and makes predictions for each vision task using task-specific transformer-based decoder heads. At the heart of our approach is a shared attention mechanism modeling the dependencies across the tasks. We evaluate our model on several multitask benchmarks, showing that our MulT framework outperforms both the state-of-the art multitask convolutional neural network models and all the respective single task transformer models. Our experiments further highlight the benefits of sharing attention across all the tasks, and demonstrate that our MulT model is robust and generalizes well to new domains. Our project website is at https://ivrl.github.io/MulT/. 1.

Introduction
First proposed in [45], transformers have made great strides in a wide range of domains. For instance, previ-ous works [13, 25, 35–37, 52] have demonstrated that trans-formers trained on large datasets learn strong representa-tions for many downstream language tasks; and models based on transformers have achieved promising results on image classification, object detection, and panoptic segmen-tation [5, 6, 14, 18, 34, 38, 44, 49, 58]. In contrast to these works that focus on a single task, in this paper, we investi-gate the use of transformers for multitask learning.
Although a few works have studied the use of transform-ers to handle multiple input modalities, such as images and text, they typically focus on a single task, e.g., visual ques-tion answering [19, 20, 23, 28], with the exception of [19], which tackles several language tasks but a single vision one.
By contrast, our goal is to connect multiple vision tasks cov-ering the 2D, 3D, and semantic domains. To this end we 1
Figure 1. Motivation for MulT. Our MulT model, which is a transformer-based encoder-decoder model with shared attention to learn task inter-dependencies, produces better results than both the dedicated 1-task transformer models (1-task Swin [26]) and the state-of-the-art multitask CNN baseline [41]. address the following questions: Can a transformer model trained jointly across tasks improve the performance in each task relative to single-task transformers? Can one explicitly encode dependencies across tasks in a transformer-based framework? Can a multitask transformer generalize to un-seen domains?
To the best of our knowledge, only [9, 30, 40] have touched upon the problem of addressing multiple tasks with transformers. However, none of these works aims to en-code strong dependencies across the tasks beyond the use of a shared backbone. Furthermore, IPT [9] handles solely low-level vision tasks, such as denoising, super-resolution and deraining, whereas [30] focuses uniquely on the tasks of object detection and semantic segmentation and [40] on scene recognition and importance score prediction in videos. Here, we cover a much wider range of high-level vision tasks and explicitly model their dependencies.
To this end, we introduce MulT, which consists of a transformer-based encoder to transform the input image into a latent representation shared by the tasks, and transformer decoders with task-specific heads producing the final pre-dictions for each of the tasks. While the MulT encoder mainly utilizes the self-attention mechanism [3, 33] to ex-tract intrinsic features, as most transformers, we equip the decoders with a shared attention mechanism across the dif-ferent vision tasks, thus allowing the overall framework to encode task dependencies. Thus, we leverage the query and key vectors from the encoder along with the task-specific values in the decoder to predict the task-specific outputs.
Our contributions can be summarized as follows:
• We propose an end-to-end multitask transformer archi-tecture that handles multiple high-level vision tasks in a single model.
• We introduce a shared attention between the trans-former decoders of the multiple tasks. This shared at-tention mechanism further improves the performance of each vision task.
• Our framework lets us learn the inter-dependencies across high-level vision tasks.
• We show that our model generalizes and adapts to new domains with a lower average error on the different vision tasks than the existing multitask convolutional models [41, 53].
Our exhaustive experiments and analyses across a variety of tasks show our MulT model not only improves the perfor-mance over single-task architectures, but also outperforms the state-of-the-art multitask CNN-based models (as shown in Figure 1) on standard benchmarks, such as Taskon-omy [54], Replica [42], NYU [31] and CocoDoom [29] . 2.