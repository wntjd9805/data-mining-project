Abstract
Hyperbolic space can naturally embed hierarchies, un-like Euclidean space. Hyperbolic Neural Networks (HNNs) exploit such representational power by lifting Euclidean features into hyperbolic space for classification, outper-forming Euclidean neural networks (ENNs) on datasets with known semantic hierarchies. However, HNNs under-perform ENNs on standard benchmarks without clear hier-archies, greatly restricting HNNs’ applicability in practice.
Our key insight is that HNNs’ poorer general classifica-tion performance results from vanishing gradients during backpropagation, caused by their hybrid architecture con-necting Euclidean features to a hyperbolic classifier. We propose an effective solution by simply clipping the Eu-clidean feature magnitude while training HNNs.
Our experiments demonstrate that clipped HNNs become super-hyperbolic classifiers: They are not only consistently better than HNNs which already outperform ENNs on hi-erarchical data, but also on-par with ENNs on MNIST, CI-FAR10, CIFAR100 and ImageNet benchmarks, with better adversarial robustness and out-of-distribution detection. 1.

Introduction
Many datasets are inherently hierarchical. WordNet [30] has a hierarchical conceptual structure, users in social net-works such as Facebook or twitter form hierarchies based on different occupations and organizations [11].
Representing such hierarchical data in Euclidean space cannot capture and reflect their semantic or functional re-semblance [1, 34]. Hyperbolic space, i.e., non-Euclidean space with constant negative curvature, has been leveraged to embed data with hierarchical structures with low distor-tion owing to the nature of exponential growth in volume with respect to its radius [34, 40, 41]. For instance, hy-perbolic space has been used for analyzing the hierarchical structure in single cell data [20], learning hierarchical word embedding [34], embedding complex networks [1], etc.
Recent algorithms operate directly in hyperbolic space to exploit more representational power. Examples are Hy-perbolic Perceptron [46], Hyperbolic Support Vector Ma-a) HNNs employ a hybrid architecture. b) Standard benchmarks. c) Few-shot learning tasks.
Figure 1. We propose an effective solution for training HNNs by clipping the Euclidean features. Clipped HNNs become super-hyperbolic classifiers: They are not only consistently better than
HNNs which already outperform ENNs on hierarchical data, but also on-par with ENNs on standard benchmarks. a) HNNs employ a hybrid architecture. The Euclidean part converts an input into
Euclidean embedding. Then the Euclidean embedding is projected onto the Poincar´e model of hyperbolic space via exponential map
Exp0(·). Finally, the hyperbolic embeddings are classified with
Poincar´e hyperplanes. Clipped HNNs utilize a reduced region of hyperbolic space. b) Clipped HNNs outperform baseline HNNs on standard benchmarks. c) Clipped HNNs outperform both HNNs and ENNs on 1-s (shot) 1-w (way) and 5-s (shot) 5-w (way) few-shot learning tasks. chine [5], and Hyperbolic Neural Networks (HNNs) [8], an alternative to standard Euclidean neural networks (ENNs).
HNNs adopt a hybrid architecture [18] (Figure 1): An
ENN is first used for extracting image features in Euclidean space; they are then projected onto hyperbolic space to be classified by a hyperbolic multiclass logistic regression [8].
While HNNs outperform ENNs on several datasets with explicit hierarchies [8], there are several serious limitations. 1) HNNs underperform ENNs on standard classification benchmarks with flat or non-hierarchical semantic struc-tures. 2) Even for image datasets that possess latent hier-archical structures there are no experimental evidence that
HNNs can capture such structures or provide on-par per-formance with ENNs [18]. 3) Existing improvements on
HNNs mainly focus on reducing the number of parame-ters [42] or incorporating different types of neural network layers such as attention [10] or convolution [42]. Unfortu-nately, why HNNs are worse than ENNs on standard bench-marks has not been investigated or understood.
Our key insight is that HNNs’ poorer general classifi-cation performance is caused by their hybrid architecture connecting Euclidean features to a hyperbolic classifier. It leads to vanishing gradients during training. In particular, the training dynamics of HNNs push the hyperbolic embed-dings to the boundary of the Poincar´e ball [2] which causes the gradients of Euclidean parameters to vanish.
We propose a simple yet effective solution to this prob-lem by simply clipping the Euclidean feature magnitude during training, thereby preventing the hyperbolic embed-ding from approaching the boundary during training. Our experiments demonstrate that clipped HNNs become super-hyperbolic classifiers: They are not only consistently better than HNNs which already outperform ENNs on hierarchical data, but also on-par with ENNs on MNIST, CIFAR10, CI-FAR100 and ImageNet benchmarks, with better adversarial robustness and out-of-distribution detection.
Our paper makes the following contributions. 1) Our de-tailed analysis reveals the underlying issue of vanishing gra-dients that makes HNNs worse than ENNs on standard clas-sification benchmarks. 2) We propose a simple yet effective feature clipping solution. 3) Our extensive experimenta-tion demonstrates that clipped HNNs outperform standard
HNNs and become on-par with ENNs on standard bench-marks. They are also more robust to adversarial attacks and exhibit stronger out-of-distribution detection capability than their Euclidean counterparts. 2.