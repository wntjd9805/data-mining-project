Abstract
With the popularity of multi-modal sensors, visible-thermal (RGB-T) object tracking is to achieve robust perfor-mance and wider application scenarios with the guidance of objects’ temperature information. However, the lack of paired training samples is the main bottleneck for unlock-ing the power of RGB-T tracking. Since it is laborious to collect high-quality RGB-T sequences, recent benchmarks only provide test sequences.
In this paper, we construct a large-scale benchmark with high diversity for visible-thermal UAV tracking (VTUAV), including 500 sequences with 1.7 million high-resolution (1920 ∗ 1080 pixels) frame pairs. In addition, comprehensive applications (short-term tracking, long-term tracking and segmentation mask predic-tion) with diverse categories and scenes are considered for exhaustive evaluation. Moreover, we provide a coarse-to-fine attribute annotation, where frame-level attributes are provided to exploit the potential of challenge-specific track-ers. In addition, we design a new RGB-T baseline, named
Hierarchical Multi-modal Fusion Tracker (HMFT), which fuses RGB-T data in various levels. Numerous experiments on several datasets are conducted to reveal the effectiveness of HMFT and the complement of different fusion types. The project is available at here.
Figure 1. Sample frames in our dataset. Scenes - super class (se-quence length) are shown on the top. Sequence-level attributes are shown at bottom, including camera movement (C), deforma-tion (D), extreme illumination (E), partial occlusion (P), full oc-clusion (F), scale variation (S), thermal clustering (H), fast mov-ing (M), out-of-view (O), and low resolution (L). 1.

Introduction
Given the initial position of a model-agnostic target, vi-sual object tracking is to capture the target in the subsequent frames [28], where the target may suffer out-of-view, oc-clusion, illumination variation, and motion blur. Previous algorithms solve those challenges within visible modality, providing limited information when the target is in dark, rainy, foggy and other extreme conditions (the first row in
Fig. 1). By contrast, thermal image, as a complementary cue, is insensitive to illumination variation, while it cannot distinguish the target when the target and background are
† Corresponding author: Dr. Dong Wang, wdice@dlut.edu.cn in similar temperature (the second row in Fig. 1). To this end, with the portability and low-price of multi-modality sensors, tracking with visible-thermal (RGB-T) data en-larges the application scope by providing complementary information, which has attached more attentions [23, 47].
Li et al. [19] release a gray-scale RGB-T dataset with 50 videos. Later, RGBT210 [25] and RGBT234 [20] are pro-posed, containing 210 and 234 test videos. In 2019 [15] and 2020 [16], VOT committee holds the VOT-RGBT tracking subchallenge, which selects 60 sequences from RGBT234 to evaluate the accuracy and robustness of competitors. Fur-thermore, various algorithms are proposed by considering performance and time cost. Li et al. [22] propose a Multi-Adaptor network to learn modality-shared and modality-specific representations. Zhang et al. [48] design a real-time
RGB-T tracker that exploits the effectiveness of attribute annotation. Zhang et al. [46] extend DiMP [2] to RGB-T tracking, obtaining the best ranking in VOT2019-RGBT.
However, the lack of training data becomes the main bottleneck for RGB-T tracking. Existing datasets (GTOT,
RGBT210, RGBT234, and VOT-RGBT) contain 284 unique short-term sequences overall. Trackers have to be trained on another test set [22, 48] or synthetic data gen-erated from visible modality [46, 50], which suffer limited generalization ability and training gap. Moreover, test se-quences are captured with monitoring devices, thereby lead-ing to limited viewpoint, frame length, and imaging quality.
To fully exploit the potential of the RGB-T tracker, this pa-per presents a large-scale RGB-T tracking dataset with high diversity. The main contributions are listed as follows:
• We construct a large-scale benchmark with high di-versity for visible-thermal UAV tracking (VTUAV). To our best knowledge, VTUAV is the largest multi-modal tracking dataset with the highest resolution. Moreover, we take short-term, long-term tracking and segmen-tation mask prediction into consideration to achieve a comprehensive evaluation with wider applications. We also provide an exquisite attribute annotation in frame and sequence levels, which can meet the requirement of training a challenge-specific tracker.
• We propose a new baseline for RGB-T tracking, namely HMFT, which unifies various multi-modal fu-sion strategies (including image fusion, feature fusion and decision fusion) into a hierarchical fusion frame-work. We implement corresponding versions for short-term and long-term tracking. Furthermore, we provide an in-depth analysis on various fusion types to develop
RGB-T trackers. Exhaustive experiments on GTOT,
RGBT210, RGBT234 and VTUAV conclude the com-plement of various fusion types. 2.