Abstract
We propose a method for jointly estimating the 3D mo-tion, 3D shape, and appearance of highly motion-blurred objects from a video. To this end, we model the blurred appearance of a fast moving object in a generative fashion by parametrizing its 3D position, rotation, velocity, accel-eration, bounces, shape, and texture over the duration of a predefined time window spanning multiple frames. Us-ing differentiable rendering, we are able to estimate all parameters by minimizing the pixel-wise reprojection er-ror to the input video via backpropagating through a ren-dering pipeline that accounts for motion blur by averag-ing the graphics output over short time intervals. For that purpose, we also estimate the camera exposure gap time within the same optimization. To account for abrupt mo-tion changes like bounces, we model the motion trajectory as a piece-wise polynomial, and we are able to estimate the specific time of the bounce at sub-frame accuracy. Experi-ments on established benchmark datasets demonstrate that our method outperforms previous methods for fast moving object deblurring and 3D reconstruction. 1.

Introduction



Input
Shape & Motion
Novel views
TSR (2×)
Figure 1. Reconstructing 3D shape and motion of a motion-blurred falling key. We jointly optimize over multiple input frames to estimate a single 3D textured mesh and correspond-ing motion model (blue: observed trajectory, yellow: the exposure gap). Temporal super-resolution (TSR) is one of the applications of the proposed Motion-from-Blur method. 3D object reconstruction from 2D images is one of the key tasks in computer vision [20,32,33,36]. It allows better modeling of the underlying 3D world. Applications of 3D object reconstruction are broad, ranging from robotic map-ping [7] to augmented reality [42]. Even though some re-cent methods deal with the extreme and under-constrained case of reconstructing 3D objects from a single 2D im-age [39, 43], most methods take advantage of a multi-view setting [20, 32, 33, 36]. However, all generic 3D object re-construction methods assume that the object moves slowly compared to the camera frame rate, resulting in sharp 2D images. The task of 3D object reconstruction becomes much more challenging when the object moves fast dur-ing the camera exposure time, resulting in a motion-blurred 2D image. The Shape-from-Blur (SfB) method [31] tackled this challenging scenario to extract 3D shape and motion from a single motion-blurred image of the object. This sce-nario is difficult because motion blur makes the input image noisier, and many high-frequency details are lost. On the other hand, even a single image gives potentially several views of the object, which are averaged by motion blur into one frame. SfB [31] explicitly modeled this phenomenon and successfully exploited it.
In this paper, we go beyond previous methods by esti-mating the 3D object’s shape and its motion from a series of motion-blurred video frames. To achieve this, we optimize
all parameters jointly over multiple input frames (i.e. the ob-ject’s 3D shape and texture, as well as its 3D motion). We tie up the object’s 3D shape and texture to be constant over all frames. Due to the longer time intervals involved, we must model more complex object motions (3D translation and 3D rotation) than necessary for a single motion-blurred frame [31], e.g. the acceleration of a falling object (Fig. 1), or a ball bouncing against a wall (Fig. 3). Using multiple frames also comes with an additional challenge: the cam-era shutter opens and closes in set time intervals, leading to a gap in the object’s visible trajectory and appearance. To properly succeed in our task, we must also recover this ex-posure gap. For a single frame only (as in [31]), the motion direction (forward vs. backward motion along the estimated axis) is ambiguous. For instance, in Fig. 1, the key could be translating from top to bottom or vice-versa, both resulting in the same input image. Since we consider multiple frames jointly, the motion direction is no longer ambiguous and can always be recovered. Moreover, for rotating objects, we can reconstruct a more complete 3D model as we can integrate more observations covering its total surface.
In contrast, previous single-frame work [31] produces strong artifacts on unseen parts. An example of our method’s output and an application to temporal super-resolution is shown in Fig. 1.
To summarize, we make the following contributions: (1) We propose a method called Motion-from-Blur (MfB) that jointly estimates the 3D motion, 3D shape, and texture of motion-blurred objects in videos by optimiz-ing over multiple blurred frames. Motion-from-Blur is the first method to optimize over a video sequence in-stead of a single frame. The source code is available at github.com/rozumden/MotionFromBlur. (2) Our multi-frame optimization enables the estimation of the motion direction as well as more complex ob-ject motions such as acceleration and abrupt direc-tion changes, e.g. bounces, for both 3D translation and 3D rotation. Moreover, compared to single-frame ap-proaches, our estimates are also more consistent over time, with always correct motion direction, and more complete 3D shape reconstruction. (3) As a requirement to model multiple frames, we estimate the exposure gap as part of the proposed optimization. 2.