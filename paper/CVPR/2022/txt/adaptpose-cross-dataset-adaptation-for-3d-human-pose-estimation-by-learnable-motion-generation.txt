Abstract
This paper addresses the problem of cross-dataset gen-eralization of 3D human pose estimation models. Testing a pre-trained 3D pose estimator on a new dataset results in a major performance drop. Previous methods have mainly addressed this problem by improving the diversity of the training data. We argue that diversity alone is not sufﬁcient and that the characteristics of the training data need to be adapted to those of the new dataset such as camera view-point, position, human actions, and body size. To this end, we propose AdaptPose, an end-to-end framework that gen-erates synthetic 3D human motions from a source dataset and uses them to ﬁne-tune a 3D pose estimator. AdaptPose follows an adversarial training scheme. From a source 3D pose the generator generates a sequence of 3D poses and a camera orientation that is used to project the generated poses to a novel view. Without any 3D labels or camera information AdaptPose successfully learns to create syn-thetic 3D poses from the target dataset while only being trained on 2D poses. In experiments on the Human3.6M,
MPI-INF-3DHP, 3DPW, and Ski-Pose datasets our method outperforms previous work in cross-dataset evaluations by 14% and previous semi-supervised learning methods that use partial 3D annotations by 16%. 1.

Introduction
Monocular 3D human pose estimation aims to recon-struct the 3D skeleton of the human body from 2D images.
Due to pose and depth ambiguities, it is well known to be an inherently ill-posed problem. However, deep learning mod-els are able to learn 2D to 3D correspondences and achieve impressively accurate results when trained and tested on similar datasets [1, 3, 6, 14, 27, 31, 32].
An often disregarded aspect is that the distribution of fea-tures in a dataset e.g. camera orientation and body poses differ from one dataset to another. Therefore, a pre-trained network underperforms when applied to images captured
Figure 1. AdaptPose generates synthetic motions to improve the cross-dataset generalization. The source dataset has 3D labels and camera information, while the target dataset has only sam-ple videos. The synthetic motions are generated to belong to the target dataset. Therefore ﬁne-tuning the 3D pose estimator with synthetic motions improves the generalization of the model. from a different viewpoint or when they contain an activity that is not present in the training dataset [42, 45]. As an ex-ample, Figure 1 shows images from the Human3.6M [15] dataset on the left and images from the Ski-Pose [33, 35] dataset on the right which we deﬁne as source domain and target domain, respectively. Camera viewpoint, position, human action, speed of motion, and body size signiﬁcantly differ between the source and target domain. This large do-main gap causes 3D pose estimation models trained on the source domain to make unreliable predictions for the target domain [42, 45, 46]. We address this problem by generating synthetic 3D data that lies within the distribution of the tar-get domain and ﬁne-tuning the pose estimation network by the generated synthetic data. Our method does not require 3D labels or camera information from the target domain and is only trained on sample videos from the target domain.
To the best of our knowledge, there are only two ap-proaches that generate synthetic 2D-3D human poses for cross-dataset generalization of 3D human pose estimators
[13, 23]. Li et al. [23] randomly generate new 2D-3D pairs of the source dataset by substituting parts of the hu-man body in 3D space and projecting the new 3D pose to
2D. PoseAug [11] proposes a differential data augmenta-tion framework that is trained along with a pose estima-tor. Both, [23] and [11], merely improve the diversity of the source domain without considering the distribution of the target domain. Moreover, these methods are based on single images and do not consider temporal information.
We formulate the data augmentation process as a domain adaptation problem. Figure 2 shows our training pipeline.
Our goal is to generate plausible synthetic 2D-3D pairs that lie within the distribution of the target domain. Our frame-work, AdaptPose, introduces a human motion generator net-work that takes 3D samples from the source dataset and modiﬁes them by a learned deformation to generate a se-quence of new 3D samples. We project the generated 3D samples to 2D and feed them to a domain discriminator network. The domain discriminator is trained with real 2D samples from the target dataset and fake samples from the generator. We use the generated samples to ﬁne-tune a pose estimation network. Therefore, our network adapts to any target using only images from the target dataset. 3D annota-tion from the target domain is not required. Unlike [13, 23], this enables our network to generate plausible 3D poses from the target domain. Another contribution is the exten-sion of the camera viewpoint generation from a determin-istic approach to a probabilistic approach. We assume that the camera viewpoint of the target domain comes from a speciﬁc well-deﬁned, but unknown distribution. Therefore, we propose to learn a distribution of camera viewpoints in-stead of learning to generate a deterministic rotation matrix.
Our network rotates the generated 3D poses into a random camera coordinate system within the learned distribution.
The generated sample is a sequence of 2D-3D pose pairs that entails plausibility in the temporal domain. We believe that the application of the proposed motion generator is not limited to improving only cross-dataset performance of 3D pose estimation, but it could also be used in other tasks such as human action recognition.
Contributions. 1) we propose to close the domain gap between the training and test datasets by a kinematics-aware domain discriminator. The domain discriminator is trained along with a human motion generator (HMG) that uses a source training dataset to generate human motions close to those in the target dataset. 2) We show that learning the distribution of the camera viewpoint is more effective than learning to generate a deterministic camera matrix. 3) To the best of our knowledge, this is the ﬁrst approach that proposes generating human motions speciﬁcally for cross-dataset generalization for 3D human pose estimation, unlike previous work that focuses on single-frame data augmenta-tion. 2.