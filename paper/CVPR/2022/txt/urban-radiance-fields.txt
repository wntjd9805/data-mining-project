Abstract
The goal of this work is to perform 3D reconstruction and novel view synthesis from data captured by scanning platforms commonly deployed for world mapping in urban outdoor environments (e.g., Street View). Given a sequence of posed RGB images and lidar sweeps acquired by cameras and scanners moving through an outdoor scene, we produce a model from which 3D surfaces can be extracted and novel
RGB images can be synthesized. Our approach extends Neu-ral Radiance Fields, which has been demonstrated to syn-thesize realistic novel images for small scenes in controlled settings, with new methods for leveraging asynchronously captured lidar data, for addressing exposure variation be-tween captured images, and for leveraging predicted image segmentations to supervise densities on rays pointing at the sky. Each of these three extensions provides signiﬁcant performance improvements in experiments on Street View data. Our system produces state-of-the-art 3D surface re-constructions and synthesizes higher quality novel views in comparison to both traditional methods (e.g. COLMAP) and recent neural representations (e.g. Mip-NeRF). 1.

Introduction
In this work we investigate neural scene representations for world mapping, with the goal of performing 3D recon-struction and novel view synthesis from data commonly captured by mapping platforms such as Street View [23].
This setting features large outdoor scenes, with many build-ings and other objects, natural illumination from the sun, and is generally less controlled than previous work [42, 43]. We focus on street-level mapping: a person carrying a camera rig with a lidar sensor placed on a backpack walking through a city. The camera captures panoramas of the street scene while the lidar sensor reconstructs a 3D point cloud.
Street-level mapping is challenging for neural representa-tions, as the area of interest covers a large area, usually hun-dreds of square meters. This signiﬁcantly differs from previ-ous works, which largely focus on either synthetic data [49,
Figure 1. Overview – Given a set of panoramas and lidar obser-vations from an urban setting, we estimate a neural representation that can be used for novel view synthesis and accurate 3D recon-struction. 56] or small regions of real scenes [8, 10, 42, 43, 64, 72].
Moreover, the scenes contain a large variety of objects, both in terms of geometry and appearance (e.g. buildings, cars, signs, trees, vegetation). The camera locations are biased towards walking patterns (e.g. walking a straight line) with-out focusing on any particular part of the scene. This results in parts of the scene being observed by only a small num-ber of cameras, in contrast to other datasets [34, 43, 50, 57] which capture scenes uniformly with a large number of cam-eras. Furthermore, the sky is visible in most street scenes, introducing an inﬁnitely distant element that behaves differ-ently than the solid structures near the cameras. The images typically have highly varying exposures as the cameras use auto-exposure, and the illumination brightness varies de-pending on the sun’s visibility and position. Combined with auto white balance, this results in the same structure having different colors when observed from different cameras. Fi-nally, the lidar points have lower resolution in distant parts of the scene, and are even completely absent in some parts of the scene (e.g., for shiny or transparent surfaces).
In this paper we extend the popular NeRF [42] model in three ways to tailor it to the unique features of the Street
View setting and to tackle the challenges above. First, we incorporate lidar information in addition to RGB signals.
By carefully fusing these two modalities, we can compen-sate for the sparsity of viewpoints in such large scale and complex scenes. We introduce a series of lidar-based losses that allow accurate surface estimation both for solid struc-tures like buildings and for volumetric formations such as
trees/vegetation. Second, we automatically segment sky pix-els and deﬁne a separate dome-like structure to provide a well-deﬁned supervision signal for camera rays pointing at the sky. Third, our model automatically compensates for varying exposure by estimating an afﬁne color transforma-tion for each camera.
During experiments with real world data from Street
View [23], we ﬁnd that these three NeRF extensions signiﬁ-cantly improve over the state-of-the-art both in the quality of synthesized novel views (+19% PSNR over [39]) and 3D surface reconstructions (+0.35 F-score over [30]). We encourage the reader to view the supplementary material for more results and animated visualizations. 2.