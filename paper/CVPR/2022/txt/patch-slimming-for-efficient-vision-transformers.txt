Abstract
This paper studies the efﬁciency problem for visual transformers by excavating redundant calculation in given networks. The recent transformer architecture has demon-strated its effectiveness for achieving excellent performance on a series of computer vision tasks. However, similar to that of convolutional neural networks, the huge com-putational cost of vision transformers is still a severe is-sue. Considering that the attention mechanism aggregates different patches layer-by-layer, we present a novel patch slimming approach that discards useless patches in a top-down paradigm. We ﬁrst identify the effective patches in the last layer and then use them to guide the patch selec-tion process of previous layers. For each layer, the impact of a patch on the ﬁnal output feature is approximated and patches with less impacts will be removed. Experimental re-sults on benchmark datasets demonstrate that the proposed method can signiﬁcantly reduce the computational costs of vision transformers without affecting their performances.
For example, over 45% FLOPs of the ViT-Ti model can be reduced with only 0.2% top-1 accuracy drop on the Ima-geNet dataset. 1.

Introduction
Recently, transformer models have been introduced into the ﬁeld of computer vision and achieved high performance in many tasks such as object recognition [6], image pro-cess [2], and video analysis [21]. Compared with the con-volutional neural networks (CNNs), the transformer archi-tecture introduces less inductive biases and hence has larger potential to absorb more training data and generalize well on more diverse tasks [6, 11, 27, 35, 38, 43]. However, sim-ilar to CNNs, vision transformers also suffer high compu-tational cost, which blocks their deployment on resource-limited devices such as mobile phones and various IoT de-∗Corresponding author. vices. To apply a deep neural network in such real scenar-ios, massive model compression algorithms have been pro-posed to reduce the required computational cost [24, 42].
For example, quantization algorithms approximate weights and intermediate features maps in neural networks with low-bit data [3, 32]. Knowledge distillation improves the performance of a compact network by transferring knowl-edge from giant models [17, 22].
In addition, network pruning is widely explored and used to reduce the neural architecture by directly removing use-less components in the pre-deﬁned network [13, 14, 25, 28].
Structured pruning discards whole contiguous components of a pre-trained model, which has attracted much attention in recent years, as it can realize acceleration without speciﬁc hardware design. In CNNs, removing a whole ﬁlter for im-proving the network efﬁciency is a representative paradigm, named channel pruning (or ﬁlter pruning) [16, 25]. For ex-ample, Liu et al. [25] introduce scaling factors to control the information ﬂow in the neural network and ﬁlters with small factors will be removed. Although the aforementioned net-work compression methods have made tremendous efforts for deploying compact convolutional neural networks, there are only few works discussing how to accelerate vision transformers.
Different from the paradigm in conventional CNNs, the vision transformer splits the input image into multiple patches and calculates the features of all these patches in parallel. The attention mechanism will further aggregate all patch embeddings into visual features as the output. Ele-ments in the attention map reﬂect the relationship or similar-ity between any two patches, and the largest attention value for constructing the feature of an arbitrary patch is usually calculated from itself. Thus, we have to preserve this in-formation ﬂow in the pruned vision transformers for retain-ing the model performance, which cannot be guaranteed in the conventional CNN channel pruning methods. Moreover, not all the manually divided patches are informative enough and deserve to be preserved in all layers, e.g., some patches are redundant with others. Hence we consider developing
factors to each channel and these scaling factors are trained to be sparse. Filters with small scaling factors has less im-pact on the network output and will be removed for accel-erating inference. He et al. [15] rethink the criterion that
ﬁlters with small norm values are less important and pro-pose to discard the ﬁlters having larger similarity to others.
To maximally excavate redundancy, Tang [37] set up a sci-entiﬁc control to alleviate the distribution of irrelevant fac-tors and remove ﬁlters with little relation to the given task.
In the conventional channel pruning for CNNs, channels in different layers have no one-to-one relationship, and then the choice of effective channels in a layer has little impact on that in other channels.
Structure pruning for transformers.
In the trans-former model for NLP tasks, a series of works focus on re-ducing the heads in the multi-head attention (MSA) module.
For example, Michel et al. [29] observes that removing a large percentages of heads in the pre-trained BERT [5] mod-els has limited impact on its performance. Voita et al. [39] analyze the role of each head in the transformer and evaluate their contribution to the model performance. Those heads with less contributions will be reduced. Besides the MSA module, the neurons in the multilayer perceptron (MLP) module are also pruned in [1]. Designed for vision trans-formers, VTP [44] reduces the number of embedding di-mensions by introducing control coefﬁcients and removes neurons with small coefﬁcients. Different from them, the proposed patch slimming explores the redundancy from a new perspective by considering the information integration of different patches in a vision transformer. Actually, re-ducing patches can be also combined with pruning in other dimensions to realize higher acceleration. 3. Patch Slimming for Vision Transformer
In this section, we introduce the scheme of pruning patches in vision transformers. We ﬁrst review the vision transformer brieﬂy and then introduce the formulation of patch slimming.
In vision transformer, the input image is split into N patches and then fed into transformer model for representa-tion learning. For an L-layer vision transformer model, the multihead self-attention (MSA) modules and multi-layer perceptron (MLP) modules are its main components occu-pying most of the computational cost. Denoting Zl−1, Z (cid:48) l ∈
RN ×d as the input and the intermediate features of the l-th layer, the MSA and MLP modules can be formulated as:
MSA(Zl−1) (cid:34)
= Concat softmax (cid:32)
Qh l K h l
√ d (cid:33) (cid:62) (cid:35)H
V h l
W o l , (1) h=1
MLP(Z (cid:48) l ) = φ(Z (cid:48) l W a l )W b l , where d is embedding dimension, H is the number of heads,
Figure 1. The diagram of patch slimming for vision transformers. a patch slimming approach that can effectively identify and remove redundant patches.
In this paper, we present a novel patch slimming algo-rithm for accelerating the vision transformers. In contrast to existing works focusing on the redundancy in the net-work channel dimension, we aim to explore the computa-tional redundancy in the patches of a vision transformer (as shown in Figure 1. The proposed method removes redun-dant patches from the given transformer architecture in a top-down framework, in order to ensure the retained high-level features of discriminative patches can be well calcu-lated. Speciﬁcally, the patch pruning will execute from the last layer to the ﬁrst layer, wherein the useless patches are identiﬁed by calculating their importance scores to the ﬁnal classiﬁcation feature (i.e., class token). To guarantee the information ﬂow, a patch will be preserved if the patches in the same spatial location are retained by deeper layers.
For other patches, the importance scores determine whether they are preserved, and patches with lower scores will be discarded. The whole pruning scheme for vision transform-ers is conducted under a careful control of the network er-ror, so that the pruned transformer network can maintain the original performance with signiﬁcantly lower computa-tional cost. Extensive experiments validate the effectiveness of the proposed method for deploying efﬁcient vision trans-formers. For example, our method can reduce more than 45% FLOPs of the ViT-Ti model with only 0.2% top-1 ac-curacy loss on the ImageNet dataset. 2.