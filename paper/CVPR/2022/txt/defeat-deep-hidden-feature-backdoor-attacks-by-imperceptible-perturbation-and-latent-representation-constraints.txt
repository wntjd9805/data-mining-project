Abstract
Backdoor attack is a type of serious security threat to deep learning models. An adversary can provide users with a model trained on poisoned data to manipulate prediction behavior in test stage using a backdoor. The backdoored models behave normally on clean images, yet can be acti-vated and output incorrect prediction if the input is stamped with a speciﬁc trigger pattern. Most existing backdoor at-tacks focus on manually deﬁning imperceptible triggers in input space without considering the abnormality of trig-gers’ latent representations in the poisoned model. These attacks are susceptible to backdoor detection algorithms and even visual inspection.
In this paper, We propose a novel and stealthy backdoor attack - DEFEAT. It poisons the clean data using adaptive imperceptible perturbation and restricts latent representation during training process to strengthen our attack’s stealthiness and resistance to de-fense algorithms. We conduct extensive experiments on mul-tiple image classiﬁers using real-world datasets to demon-strate that our attack can 1) hold against the state-of-the-art defenses, 2) deceive the victim model with high attack success without jeopardizing model utility, and 3) provide practical stealthiness on image data. 1.

Introduction
Deep neural networks (DNNs), which can learn efﬁcient feature representations and model complex predictive tasks from large-scale data, have been deployed in real-world ap-plications, such as computer vision [10], and natural lan-guage processing [34, 36]. But they are vulnerable to back-door attacks [5, 9] which can secretly embed malicious be-haviors to manipulate the model in use phase. For example, an attacker may put a backdoor in a face recognition model
∗Corresponding author
Figure 1. Visualization of backdoored images. Top: the original image; backdoored images generated by BadNets, ReFool, Sinu-soidal signal backdoor (SIG), WaNet, DFST and DEFEAT; Mid-dle: the residual maps ampliﬁed by 2x; Down: feature heat maps by GradCam [27]. to give authorization to an unauthorized user. This weak-ness may seriously affect the training results and mean-while, users may not be aware that the model is corrupted.
Backdoor attacks [5, 9, 18] corrupt a part of the train-ing data with a speciﬁc backdoor trigger and a predeﬁned target label. The DNNs trained on the poisoned data will be infected with a backdoor, which leads to misclassiﬁca-tions on those inputs with the speciﬁc trigger pattern.
In practice, users may easily access to backdoors, say, down-loading public pre-trained models from an untrusted party, or crawling data from unreliable sources to train their own models.
A core design of backdoor attacks relies on impercep-tible trigger. Adversaries should ensure the backdoored model to behave normally on clean inputs to make the back-door hard to be noticed. Several techniques to improve the stealthiness of backdoor attacks have since been proposed, e.g., blended and patched trigger pattern approaches [2, 9, 18, 19, 40]. Some works have utilized adversarial exam-ple technology in crafting poisoned images [7, 14, 15]. Re-cently, WaNet [23] proposed a type of warping-based trig-gers to maintain stealthiness. Although the carefully-crafted backdoor triggers are employed, their attacks have not yet provided the complete stealthiness as they fail to constrain the abnormity at the feature level. To address the issue, one may apply regularization to feature level via adversarial backdoor embedding [29] and controlled feature detoxiﬁca-tion [6]. But these approaches may compromise the imper-ceptibility of the poisoned image in the input space.
Several recent backdoor defense algorithms [3, 4, 8, 16, 17] have been introduced to ﬁght against existing attacks.
A successful defense depends on the identiﬁcation of mali-cious inputs at runtime and the backdoored models. Specif-ically, they generally exploit the distinguishable dissimilar-ity of latent representations between the clean and poisoned images. This indicates that a deep-hidden trigger at feature-level may help us resist against detection.
Motivated by the ﬁndings above, we propose a novel mechanism, DEFEAT, performing a feature stealthy back-door attack via controlling imperceptible trigger pattern and the constraints on the latent representation of poisoned sam-ples. We ﬁrst build imperceptible backdoor triggers by ex-ploiting adversarial technique [20, 41] to minimize the loss from non-target classes to the target class under the distance constraints. This enables the triggers associated with the target class to be blended into the clean image “naturally”.
When poisoning training, we use an additional latent classi-ﬁer to harness the trigger feature anomaly in the backdoored model and force the model to reduce the latent distinguisha-bility between the poisoned and clean images. We showcase various backdoored images in Figure 1.
We demonstrate our attack in three benchmark datasets, namely CIFAR-10, GTSRB, and ImageNet. Several excel-lent results are captured in the experiments. First, our de-sign is viable and effective. We achieve an approaching 100% attack success rate while simultaneously maintain-ing high model utility. Second, instead of using manually-deﬁned and ﬁxed-pattern triggers, we take the adaptive backdoor generation approach, which can construct remark-ably stealthy backdoor in clean images. This is important to secretly execute backdoor attacks without being noticed by regulators in the application phase.
Third, the proposed poisoning training algorithm enables us to use the latent constraints to execute more robust and invisible backdoor attacks (than others).
In the evaluation, our attack is ex-tremely difﬁcult to be detected by multiple defense algo-rithms, e.g., neural cleanse, neural attention distillation.
Our technical contributions are summarized below:
• We explore both the attack effectiveness and the in-visibility of the trigger at feature level to propose an adaptive backdoor generation algorithm.
• We design a novel poisoning training algorithm that uses the latent layer constraints to embed the backdoor trigger into the victim model more invisibly than exist-ing attacks.
• We present intensive experiments to show that our de-sign achieves high attack success rate, and can hold against multiple backdoor defense algorithms. 2.