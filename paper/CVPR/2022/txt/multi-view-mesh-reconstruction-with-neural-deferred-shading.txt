Abstract
We propose an analysis-by-synthesis method for fast multi-view 3D reconstruction of opaque objects with ar-bitrary materials and illumination. State-of-the-art meth-ods use both neural surface representations and neural ren-dering. While flexible, neural surface representations are a significant bottleneck in optimization runtime.
Instead, we represent surfaces as triangle meshes and build a dif-ferentiable rendering pipeline around triangle rasterization and neural shading. The renderer is used in a gradient de-scent optimization where both a triangle mesh and a neural shader are jointly optimized to reproduce the multi-view im-ages. We evaluate our method on a public 3D reconstruc-tion dataset and show that it can match the reconstruction accuracy of traditional baselines and neural approaches while surpassing them in optimization runtime. Addition-ally, we investigate the shader and find that it learns an interpretable representation of appearance, enabling appli-cations such as 3D material editing. 1.

Introduction
The reconstruction of 3D objects based on multiple im-ages is a long standing problem in computer vision. Tra-ditionally, it has been approached by matching pixels be-tween images, often based on photo-consistency constraints or learned features [16, 25]. More recently, analysis-by-synthesis, a technique built around the rendering operation, has re-emerged as a promising direction for reconstructing scenes with complex illumination, materials and geome-try [34, 36, 37, 39, 41, 65]. At its core, parameters of a vir-tual scene are optimized so that its rendered appearance from the input camera views matches the camera images. If the reconstruction focuses on solid objects, these parame-ters usually include a representation of the object surface.
In gradient descent-based optimizations, analysis-by-synthesis for surfaces is approached differently depending
*Equal contribution
Figure 1. We reconstruct an object from images by simultane-ously deforming a triangle mesh and optimizing a neural shader, comparing the renderings to the input images. on the differentiable rendering operation at hand. Methods that physically model light transport typically build on prior information such as light and material models [35, 36]. It is common to represent object surfaces with triangle meshes and use differentiable path tracers (e.g., [29, 42, 67]) to jointly optimize the geometry and parameters like the light position or material diffuse albedo. Due to the inherent pri-ors, these methods do not generalize to arbitrary scenes.
Other methods instead model the rendering operation with neural networks [41,43,64], i.e., the interaction of ma-terial, geometry and light is partially or fully encoded in the network weights, without any explicit priors. Surfaces are often represented with implicit functions or more specifi-cally implicit neural representations [33, 41, 44] where the indicator function is modeled by a multi-layer perceptron (MLP) or any other form of neural network and optimized with the rendering networks in an end-to-end fashion.
While fully neural approaches are general, both in terms of geometry and appearance, current methods exhibit ex-cessive runtime, making them impractical for domains that
handle a large number of objects or multi-view video (e.g. of human performances [6, 15, 50, 53, 59]).
We propose Neural Deferred Shading (NDS), a fast analysis-by-synthesis method that combines triangle meshes and neural rendering. The rendering pipeline is in-spired by real-time graphics and implements a technique called deferred shading [7]: a triangle mesh is first raster-ized and the pixels are then processed by a neural shader that models the interaction of geometry, material, and light.
Since the rendering pipeline, including rasterization and shading, is differentiable, we can optimize the neural shader and the surface mesh with gradient descent (Figure 1). The explicit geometry representation enables fast convergence while the neural shader maintains the generality of the mod-eled appearance. Since triangle meshes are ubiquitously supported, our method can also be readily integrated with existing reconstruction and graphics pipelines. Our techni-cal contributions include:
• A fast analysis-by-synthesis pipeline based on triangle meshes and neural shading that handles arbitrary illu-mination and materials
• A runtime decomposition of our method and a state-of-the-art neural approach
• An analysis of the neural shader and the influence of its parameters 2.