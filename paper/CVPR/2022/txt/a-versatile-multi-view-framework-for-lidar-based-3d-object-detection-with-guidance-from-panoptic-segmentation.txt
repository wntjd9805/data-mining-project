Abstract 3D object detection using LiDAR data is an indispens-able component for autonomous driving systems. Yet, only a few LiDAR-based 3D object detection methods leverage segmentation information to further guide the detection pro-cess. In this paper, we propose a novel multi-task frame-work that jointly performs 3D object detection and panop-tic segmentation. In our method, the 3D object detection backbone in Bird’s-Eye-View (BEV) plane is augmented by the injection of Range-View (RV) feature maps from the 3D panoptic segmentation backbone. This enables the detec-tion backbone to leverage multi-view information to address the shortcomings of each projection view. Furthermore, foreground semantic information is incorporated to ease the detection task by highlighting the locations of each object class in the feature maps. Finally, a new center density heatmap generated based on the instance-level information further guides the detection backbone by suggesting possi-ble box center locations for objects. Our method works with any BEV-based 3D object detection method and, based on experiments on the nuScenes dataset, it provides signiﬁcant performance gains. Notably, the proposed method based on a single-stage CenterPoint 3D object detection network achieve state-of-the-art performance on nuScenes 3D De-tection Benchmark with 67.3 NDS. 1.

Introduction
Over the past few years, there has been remarkable progress in autonomous vehicles (AVs) vision systems for understanding complex 3D environments [8, 9, 31]. 3D ob-ject detection is one of the core computer vision tasks that empowers AVs for robust decision-making.
In this task, each foreground object, such as a car, a pedestrian, etc., needs to be accurately classiﬁed and localized by a 3D bounding box with 7 degrees of freedom (DOF), including the 3D box center location (x, y, z), size (l, w, h), and yaw angle (α).
∗All authors are with Huawei Noah's Ark Lab, Canada at richard.xu2, yuan.ren3,
{hamidreza.fazlali1, the time of writing. liu.bingbing}@huawei.com
Figure 1. The proposed center density heatmap (colored in green) guides the detection head toward possible box center regions on the BEV plane. The blue and red boxes represent predictions and ground truth, respectively. Best viewed in color.
LiDAR-based 3D object detection methods rely on dif-ferent strategies for 3D point cloud data representation.
Some of these detection methods [13,17,28] are categorized as point-based methods. These methods directly process the raw point cloud to extract useful information. While they usually achieve high accuracy, their computational cost is signiﬁcant. The other subset of LiDAR-based 3D ob-ject detection methods are known as grid-based methods
[9,18,26,31]. These methods transform the unordered point cloud into regular 3D volumetric grids, i.e., voxels or pil-lars, and extract discriminative features from the points in-side each gird cell. The extracted features are further pro-cessed by 2D or 3D Convolutional Neural Networks (CNN).
Although point sub-sampling helps grid-based methods to be computationally efﬁcient, some information is lost dur-ing projection and discretization [27].
Many of the points in each LiDAR scan represent the background region, including drivable surface, sidewalk, vegetation, etc. Feeding all this information to a 3D object detection algorithm without providing extra clues, e.g., se-mantic information, makes the recognition and localization process challenging. Several works [17, 22, 28] have ex-ploited a binary or semantic segmentation model for ﬁlter-ing background points or providing extra semantic features that can guide the proposal generation or detection process.
Inspired by this notion of providing guidance, we lever-age 3D panoptic segmentation as an auxiliary task for guid-ing and further improving the performance of Bird’s-Eye-View (BEV) based 3D object detection algorithms. A 3D panoptic segmentation method predicts the semantic class
label and performs instance-level segmentation for each point in the 3D space, both of which are useful as guidance signals for detecting objects. In addition, guiding a BEV-based detection model with features learned from a Range-View (RV) based network can reduce the sparsity of fea-tures representation in BEV projection. We validate these ideas by training a BEV-based 3D object detection method in conjunction with an RV-based 3D panoptic segmentation method. More speciﬁcally, the BEV-based detection back-bone is supplemented with additional RV features extracted from the panoptic segmentation backbone, providing a rich set of multi-view information to aid the detection. More-over, we exploit the semantic labels of the foreground ob-jects estimated by the panoptic segmentation network to re-ﬁne the 3D object detection backbone features. Finally, a center density heatmap in the BEV plane is designed based on the instance-level information obtained from the panop-tic segmentation, highlighting regions that contain box cen-ters of objects.
In conjunction, the augmented backbone features, foreground semantic labels, and the center density heatmap guide the detection model towards a more accurate 3D box recognition and localization. We will describe our multi-task framework based on the single-stage CenterPoint 3D object detection method [30], but later in the experimen-tal results section, we will quantitatively demonstrate that our approach can help any existing BEV-based 3D object detection method.
Our contributions can be summarized into four-fold. (1)
We propose a multi-task framework that jointly learns 3D panoptic segmentation and 3D object detection for improv-ing the 3D object recognition and localization. To the best of our knowledge, this is the ﬁrst framework that leverages both the semantic- and instance-level information concur-rently for improving 3D object detection. (2) The frame-work is also designed to be easily attached to any BEV-based object detection method as a plug-and-play solution to boost its detection performance. (3) With experiments conducted on the nuScenes dataset [2], which includes both the panoptic and 3D box information, we validate the effec-tiveness of our method with different BEV-based 3D object detection methods. (4) We conduct ablation studies to fur-ther examine the usefulness of each added component for performance improvement. 2.