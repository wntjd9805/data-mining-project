Abstract
This paper presents a unified framework for depth-aware panoptic segmentation (DPS), which aims to reconstruct 3D scene with instance-level semantics from one single image.
Prior works address this problem by simply adding a dense depth regression head to panoptic segmentation (PS) net-works, resulting in two independent task branches. This ne-glects the mutually-beneficial relations between these two tasks, thus failing to exploit handy instance-level seman-tic cues to boost depth accuracy while also producing sub-optimal depth maps. To overcome these limitations, we pro-pose a unified framework for the DPS task by applying a dynamic convolution technique to both the PS and depth prediction tasks. Specifically, instead of predicting depth for all pixels at a time, we generate instance-specific ker-nels to predict depth and segmentation masks for each in-stance. Moreover, leveraging the instance-wise depth es-timation scheme, we add additional instance-level depth cues to assist with supervising the depth learning via a new depth loss. Extensive experiments on Cityscapes-DPS and SemKITTI-DPS show the effectiveness and promise of our method. We hope our unified solution to DPS can lead a new paradigm in this area. Code is available at https://github.com/NaiyuGao/PanopticDepth. 1.

Introduction
Depth-aware panoptic segmentation (DPS) is a new chal-lenging task in scene understanding, attempting to build 3D scene with instance-level semantic understanding from a single image. Its goal is to assign each pixel a depth value, a semantic class label and an instance ID. Thus, solving this problem involves monocular depth estimation [1] and panoptic segmentation [2]. Naturally, a straightforward so-lution to DPS is to add a dense depth regression head to
*Corresponding author
Figure 1. Illustration of our unified solution to depth-aware panop-tic segmentation which requires assigning each pixel in a single image a depth value, a semantic class label and an instance ID.
Instead of predicting pixel-wise depth, we predict instance-wise depth by instance-specific convolution kernels, which shares the same manner of instance mask generation. panoptic segmentation (PS) networks [3–5], producing a depth value for each labelled pixel.
This method is intuitive yet sub-optimal. Since it tack-les the two tasks with two independent branches, it does not explore the mutually-beneficial relations between them, especially failing to exploit handy instance-level semantic cues to boost depth accuracy. We observe that pixels of ad-jacent instances generally have discontinuous depth. For examples, two vehicles in a line are likely to have differ-ent depth. Therefore, it is hard to predict accurate depth for both vehicles using the same pixel-wise depth regressor. On the other hand, as indicated by previous works [6–9], con-sidering that these pixels are of different vehicles, it benefits the depth estimation if separate regressors are used respec-tively.
Following the train of thought above, we propose Panop-ticDepth in this paper, a unified model that predicts the
mask and depth values in the same instance-wise man-ner (Figure 1).
In contrast to predicting depth values for all pixels at a time, we manage to estimate depth for each thing/stuff instance, which also shares the way of generat-ing instance masks. To this end, we employ the technique of dynamic convolution [10–13] in producing both instance masks and depth, which unifies the pipelines of mask gen-eration and depth estimation.
Specifically, we first generate instance-specific mask and depth kernels for each instance concurrently, which is sim-ilar to the method used in [12]. Then, we apply the mask kernels to the mask embeddings and the depth kernels to the depth embeddings, producing the mask and depth map for each instance, respectively. Finally, we merge individ-ual instance masks into a panoptic segmentation map, fol-lowing a similar process presented in [14]. According to the panoptic segmentation results, we then aggregate each instance’s depth into a whole depth map. As a result, we get both the panoptic segmentation and depth map for one image. Figure 2 shows the pipeline.
Our method unifies the depth estimation and panoptic segmentation approaches via an instance-specific convolu-tion kernel technique, which also in turn improves the per-formance on both tasks (Table 3 and Table 5). Thanks to the dynamic convolution kernel technique [12], the learned instance depth regressor aggregates not only global context, but local information, such as instance shapes, scales and positions, into instance depth prediction. Such information turns out essential to obtain accurate depth values especially those at instance boundaries (Figure 3).
Furthermore, in order to ease the depth estimation, in-spired by Batch Normalization [15], we propose to repre-sent each instance depth map as a triplet, i.e. a normalized
In general, depth map, a depth range and a depth shift. depth values of different instances may vary greatly, like a long vehicle with a length of 70m v.s. a small car with a length of 4.5m. This large variation in scale may cause dif-ficulties in learning shared depth embedding. To tackle this problem, we propose to represent the depth map with the aforementioned triplet and normalize the values of original instance depth map to [0, 1]. This improves the learning ef-fectiveness (Table 3 and Table 5). At the same time, in addi-tion to the traditional pixel-level depth supervision, we add instance-level depth statistics based on the new depth map representation, e.g. the depth shift, to reinforce the depth supervision. We also propose a corresponding depth loss to accommodate this new supervision (Sec:3.3.2), which helps improve the depth prediction.
Through extensive experiments on Cityscapes-DPS [3] and SemKITTI-DPS [3], we demonstrate the effectiveness of our unified solution to the depth-aware panoptic segmen-tation. We hope our unified framework can lead a new paradigm in this challenging task. 2.