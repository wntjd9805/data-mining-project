Abstract
Augmented reality devices have the potential to enhance human perception and enable other assistive functionalities in complex conversational environments. Effectively cap-turing the audio-visual context necessary for understanding these social interactions first requires detecting and local-izing the voice activities of the device wearer and the sur-rounding people. These tasks are challenging due to their egocentric nature: the wearer’s head motion may cause mo-tion blur, surrounding people may appear in difficult view-ing angles, and there may be occlusions, visual clutter, au-dio noise, and bad lighting. Under these conditions, pre-vious state-of-the-art active speaker detection methods do not give satisfactory results. Instead, we tackle the prob-lem from a new setting using both video and multi-channel microphone array audio. We propose a novel end-to-end deep learning approach that is able to give robust voice ac-tivity detection and localization results. In contrast to pre-vious methods, our method localizes active speakers from all possible directions on the sphere, even outside the cam-era’s field of view, while simultaneously detecting the de-vice wearer’s own voice activity. Our experiments show that the proposed method gives superior results, can run in real time, and is robust against noise and clutter. 1.

Introduction
Understanding conversational context and dynamics from an egocentric perspective is vital for creating realis-tic and useful augmented reality (AR) experiences. These attributes characterize the interactions of multiple speakers in a given scene with the AR device wearer (i.e., ego). An example such device may consist of glasses with outward looking cameras and microphones so that audio-visual data is captured from the wearer’s point of view. Modeling these attributes involves not only detecting and tracking people within a scene, but also localizing the voice activity within a conversation. In this work, we focus on the task of active speaker localization (ASL) with the goal of detecting the spatio-temporal location of all active speakers both within and outside the camera’s field of view (FOV). Closely re-Figure 1. Our novel multi-channel audio-visual deep network lo-calizes active speakers from any direction on the sphere, even be-yond the camera’s field of view. Here, predicted active speaker probability heat maps are shown in the red channels of both the images (rows 1,3) and voice maps (rows 2,4). These voice maps are 360×180 cylindrical 2D projections of the sphere, where each pixel corresponds to a direction in the device wearer’s local 3D co-ordinate system and the camera’s limited field of view is approxi-mated by the central blue rectangle. Ground truth active speakers are shown as purple bars below head bounding boxes and as blue dots in the voice maps, while our method’s thresholded predictions are shown as yellow bars. The overlaid text indicates ground truth (purple) and predicted (yellow) wearer voice activity detections. lated to the problem of active speaker detection (ASD), ASL involves estimating the relative direction of arrival of speech from an egocentric perspective. In this paper, active speak-ers typically correspond to the people who are speaking and
‘driving’ the conversations. The elements of our proposed egocentric ASL problem are illustrated in Fig. 1.
A good ASL system needs to account for the changing orientations of speakers from an egocentric point of view and be robust to speakers moving in and out of the visual field of view. In particular, natural conversations entail sig-nificant overlap between different speakers’ voice activity and involve one or more speakers interrupting each other — a classical attribute in conversational ecology called turn-taking. Such a system should also ideally be agnostic to the number of microphone channels, thereby allowing for generalization to different AR devices with varying num-bers of audio and/or visual channels. Note that the device wearer may also be an active speaker during the conversa-tion whose voice is naturally amplified due to their close-ness to the device microphones. An ASL system must ac-count for this false amplification that may nullify competing active speakers in the scene. In this work, we propose a real-time audio-visual ASL system that addresses these aspects to effectively localize active speakers potentially outside of the visual FOV by leveraging audio recorded from a device-mounted microphone array.
We propose a new end-to-end deep neural network trained to tackle this problem. Our network is partitioned into two branches: an audio network and an audio-visual network. The audio network builds useful representations for constructing a low-resolution sound source localization map with a full 360◦ FOV by utilizing spatio-temporal cor-relations across different channels. The audio-visual net-work then combines the extracted audio features with the corresponding video frames, resulting in a higher resolu-tion activity map for the camera’s FOV. Visual cues such as the person’s mouth movement, facial expressions, and body pose are extracted here and combined with audio features for computing a joint representation. The final 360◦ active speaker map is a combination of the low-resolution audio-only map and the high-resolution audio-visual map. In ad-dition, the device wearer voice activity detector shares the features from the audio network, and our model estimates the relative 3D orientations of the speakers in the scene from an egocentric perspective. The proposed network is also aimed at real-time applications in the immersion-driven do-main of AR, enabling systems for the spatialization and lo-calization of audio-visual activity in a world-locked frame of reference. Lastly, the lack of reliable multi-channel con-versational datasets is another limiting factor for building in-the-wild ASL systems. To that end, we build and eval-uate our approach using a very recent egocentric conversa-tions dataset called EasyCom [18].
Our contributions are: 1. We propose the new problem of active speaker local-ization (ASL), predicting the relative locations of all active speakers in the auditory scene using egocentric multi-channel audio and video. 2. To solve this problem, we propose a real-time egocen-tric audio-visual system with a full 360◦ field of view.
Our novel multi-channel audio-visual deep network can effectively learn from different audio features and microphone arrays without structure changes. 3. We evaluate our method on the EasyCom dataset and demonstrate significantly improved results in compar-ison to previous audio-visual ASD approaches. 1.1.