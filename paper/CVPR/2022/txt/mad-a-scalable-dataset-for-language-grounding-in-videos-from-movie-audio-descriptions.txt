Abstract
The recent and increasing interest in video-language re-search has driven the development of large-scale datasets that enable data-intensive machine learning techniques. In comparison, limited effort has been made at assessing the
ﬁtness of these datasets for the video-language ground-ing task. Recent works have begun to discover signiﬁ-cant limitations in these datasets, suggesting that state-of-the-art techniques commonly overﬁt to hidden dataset bi-ases. In this work, we present MAD (Movie Audio Descrip-tions), a novel benchmark that departs from the paradigm of augmenting existing video datasets with text annota-tions and focuses on crawling and aligning available au-dio descriptions of mainstream movies. MAD contains over 384, 000 natural language sentences grounded in over 1, 200 hours of videos and exhibits a signiﬁcant reduction in the currently diagnosed biases for video-language ground-ing datasets. MAD’s collection strategy enables a novel and more challenging version of video-language ground-ing, where short temporal moments (typically seconds long) must be accurately grounded in diverse long-form videos that can last up to three hours. We have released MAD’s data and baselines code at https://github.com/
Soldelli/MAD. 1.

Introduction
Imagine you want to ﬁnd the moment in time, in a movie, when your favorite actress is eating a Gumbo dish in a New
Orleans restaurant. You could do so by manually scrubbing the ﬁlm to ground the moment. However, such a process is tedious and labor-intensive. This task is known as natural language grounding [1, 4], and has gained signiﬁcant mo-mentum in the computer vision community. Beyond smart browsing of movies, the interest in this task stems from multiple real-world applications ranging from smart video search [26, 27], video editing [18, 19, 32], and helping pa-Figure 1. Comparison of video-language grounding datasets.
The circle size measures the language vocabulary diversity. The videos in MAD are orders of magnitude longer in duration than previous datasets (∼110min), annotated with natural, highly de-scriptive, language grounding (>60K unique words) with very low coverage in video (∼4.1s). Coverage is deﬁned as the average % duration of moments with respect to the total video duration. tients with memory dysfunction [2, 31]. The importance of solving this task has resulted in novel approaches and large-scale deep-learning architectures that steadily push state-of-the-art performance.
Despite those advances, recent works [17,34,37] have di-agnosed hidden biases in the most common video-language grounding datasets. Otani et al. [17] have highlighted that several grounding methods only learn location pri-ors, to the point of disregarding the visual information and only using language cues for predictions. While re-cent methods [34, 37] have tried to circumvent these lim-itations by either proposing new metrics [35] or debias-ing strategies [37, 41], it is still unclear if existing ground-ing datasets [1, 4, 7, 21] provide the right setup to evalu-Figure 2. Example from our MAD dataset. We select the movie “A quiet place” as representative for our dataset. As shown in the
ﬁgure, the movie contains a large number of densely distributed temporally grounded sentences. The collected annotations can be very descriptive, mentioning people, actions, locations, and other additional information. Note that as per the movies plot, the characters are silent for the vast majority of the movie, rendering audio description essential for visually-impaired audience. ate progress in this important task. This is partly because datasets used by video-language grounding models were not originally collected to solve this task. These datasets provide valuable video-language pairs for captioning or re-trieval, but the grounding task requires high-quality (and dense) temporal localization of the language. Figure 1 shows that the current datasets comprise relatively short videos, containing single structured scenes, and language descriptions that cover most of the video. Furthermore, the temporal anchors for the language are temporally biased, leading to methods not learning from any visual features and eventually overﬁtting to temporal priors for speciﬁc ac-tions, thus limiting their generalization capabilities [8, 17].
In this work, we address these limitations with a novel large-scale dataset called MAD (Movie Audio Descrip-tions). MAD builds atop (and includes part of) the LSMDC dataset [24], which is a pioneering work in leveraging au-dio descriptions to enable the investigation of a closely re-lated task: text-to-video retrieval. Similar to LSMDC, we depart from the standard annotation pipelines that rely on crowd-sourced annotation platforms. Instead, we adopt a scalable data collection strategy that leverages professional, grounded audio descriptions of movies for visually im-paired audiences. As shown in Figure 2, MAD grounds these audio descriptions on long-form videos, bringing novel challenges to the video-language grounding task.
Our data collection approach consists of transcribing the audio description track of a movie and automatically detecting and removing sentences associated with the ac-tor’s speech, yielding an authentic “untrimmed video” setup where the highly descriptive sentences are grounded in long-form videos. Figures 1 and 3 illustrate the unique-ness of our dataset with respect to the current alternatives.
As showcased in Figure 2, MAD contains videos that, on average, span over 110 minutes, as well as grounded anno-tations covering short time segments, which are uniformly distributed in the video, and maintain the largest diversity in vocabulary. Video grounding in MAD requires a ﬁner un-derstanding of the video, since the average coverage of the sentences is much smaller than in current datasets.
The unique conﬁguration of the MAD dataset introduces exciting challenges. First, the video grounding task is now mapped into the domain of long form video, preventing cur-rent methods to learn trivial temporal location priors, in-stead requiring a more nuanced understanding of the video and language modalities. Second, having longer videos means producing a larger number of segment proposals, which will make the localization problem far more chal-lenging. Last, these longer sequences emphasize the neces-sity for efﬁcient methods in inference and training, manda-tory for real-world applications such as long live streaming videos or moment retrieval in large video collections [3].
Contributions. Our contributions are threefold. (1) We propose Movie Audio Description (MAD), a novel large-scale dataset for video-language grounding, containing more than 384K natural language sentences anchored on more than 1.2K hours of video. (2) We design a scalable data collection pipeline that automatically extracts highly valuable video-language grounding annotations, leveraging speech-to-text translation on professionally generated au-dio descriptions. (3) We provide a comprehensive empir-ical study that highlights the beneﬁts of our large-scale
MAD dataset on video-language grounding as a benchmark, pointing out the difﬁculties faced by current video-language grounding baselines in long-form videos.
2.