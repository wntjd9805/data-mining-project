Abstract
Inverse problems in multi-dimensional imaging, e.g., completion, denoising, and compressive sensing, are chal-lenging owing to the big volume of the data and the inher-ent ill-posedness. To tackle these issues, this work unsuper-visedly learns a hierarchical low-rank tensor factorization (HLRTF) by solely using an observed multi-dimensional
Specifically, we embed a deep neural network image. (DNN) into the tensor singular value decomposition frame-work and develop the HLRTF, which captures the underly-ing low-rank structures of multi-dimensional images with compact representation abilities. This DNN herein serves as a nonlinear transform from a vector to another to help obtain a better low-rank representation. Our HLRTF in-fers the parameters of the DNN and the underlying low-rank structure of the original data from its observation via the gradient descent using a non-reference loss function in an unsupervised manner. To address the vanishing gradient in extreme scenarios, e.g., structural missing pixels, we in-troduce a parametric total variation regularization to con-strain the DNN parameters and the tensor factor param-eters with theoretical analysis. We apply our HLRTF for typical inverse problems in multi-dimensional imaging in-cluding completion, denoising, and snapshot spectral imag-ing, which demonstrates its generality and wide applicabil-ity. Extensive results illustrate the superiority of our method as compared with state-of-the-art methods. 1.

Introduction
Tensor factorization family methods extend familiar ma-trix cases to multi-dimensional modalities for effective analysis and processing of real-world multi-dimensional images, e.g., videos, hyperspectral images (HSIs), and
*Corresponding author
PSNR 14.65
Observed
PSNR 23.40
TNN [58]
PSNR 23.37
DCTNN [33]
PSNR 32.68
HLRTF i / (cid:80) i=1 σ2
Figure 1. Top: The results of multi-dimensional image comple-tion by TNN (based on the DFT) [58], DCTNN (based on the
DCT) [33], and HLRTF (based on the DNN transform) on MSI
Beans with sampling rate 0.1. Bottom: The AccEgy (AccEgy
= (cid:80)k j with σi denotes the i-th singular value [46]) versus the percentage of singular values of the transformed frontal slices by different transforms (i.e., the DNN transform, the DNN transform with only one layer, the DCT, and the DFT). The DNN transform obtains lower-rank transformed frontal slices and thus the corresponding HLRTF achieves better recovery performance. j σ2 multispectral images (MSIs) [1, 11]. Real-world multi-dimensional images are usually self-correlated and thus en-joy intrinsic low-rank structures [15, 28]. Therefore, low-rank tensor factorization methods utilize this property to design specific forms and operations to exploit the low-rankness or/and enhance the low-rankness by minimizing the nuclear norm [17, 52, 60, 61], successfully applying to various applications such as hyperspectral imaging [41,48], image/video inpainting [26], network compression [42], and recommender system [6, 7].
Most tensor factorization methods rely on multilinear operations. The classic Tucker decomposition and CP de-composition [13, 24, 28, 44] were proposed for tensor anal-ysis. Recently, tensor network decompositions including
tensor train decomposition [39], tensor ring decomposi-tion [14], and fully connected tensor network decomposi-tion [60] were studied. In this paper, we focus on the tensor singular value decomposition (t-SVD) [22], which is based on the tensor-tensor product (t-product) [20,21]. The t-SVD extended matrix SVD to tensor cases without flattening and information loss inside tensor modalities. Based on the t-SVD, the tensor tubal rank [21, 32] was defined. Its convex relaxation, the tensor nuclear norm (TNN), was studied and applied to low-rank tensor recovery [18,25,32,33,58]. More recently, the low-tubal-rank tensor factorization [30,61] was proposed by factorizing a tensor into two smaller tensors through the t-product, preserving the low-tubal-rankness of the tensor and avoiding the computing of the SVD for faster implementations.
Nonetheless, the tensor tubal-rank is based on a linear transform (e.g., the discrete Fourier transform (DFT) [32] or the discrete cosine transform (DCT) [33]). The linear trans-form is applied on the tubes of a tensor to transform it into a low-rank representation, then the matrix rank of the trans-form frontal slices is considered to define the tubal-rank, see Definition 2. Considering the complex and diversified topology structures of real-world data, it is highly possible that the transform between the original tensor and the op-timal low-rank representation is nonlinear and hierarchical, which can not be interpreted by the linear transform.
In this paper, we propose to replace the linear trans-form with a deep neural network (DNN), which consists of multiple linear layers and nonlinear activation functions.
The motivation is that the DNN transform can access much lower-rank transformed frontal slices and hence a better low-rank representation can be obtained. To validate this, we plot the AccEgy with respect to the percentage of sin-gular values of the transformed frontal slices in Fig. 1. We can observe that the energy obtained by the DNN transform is concentrated in larger singular values, which reveals that a compact low-rank representation is obtained. Thus, the recovery performance can be reasonably improved.
Equipped with the DNN transform, we deduce a new tensor rank called the hierarchical-tubal-rank. Correspond-ingly, we propose the hierarchical low-rank tensor factor-ization (HLRTF) and prove its capability to capture the low-rank structure. The HLRTF factorizes a tensor X ∈
Rn1×n2×n3 into X = A ∗f B, where A ∈ Rn1×r×n3 and
B ∈ Rr×n2×n3 are two smaller tensors and ∗f is the t-product induced by the DNN f (·) (see Definition 4). We further develop an equivalent form of HLRTF, i.e., X = g( ˆA△ ˆB), where g(·) is the inverse DNN of f (·), △ is the face-wise product [20], and ˆA, ˆB are two smaller tensor fac-tors, to reduce computational expense. To tackle inverse problems in multi-dimensional imaging, we simultaneously optimize the tensor factors ˆA, ˆB and learn the inverse DNN g(·) using the fidelity loss between g( ˆA△ ˆB) and the obser-vation. In this way, the low-rank X can be readily obtained thourgh X = g( ˆA△ ˆB) in an unsupervised manner.
To face the difficulty that in some extreme situations, our method would suffer from unavoidable vanishing gradient, we propose the parametric total variation (PTV) regular-ization for DNN parameters and tensor factor parameters, successfully addressing the vanishing gradient. We prove that PTV is an upper bound of the traditional 3D total vari-ation (3DTV) regularization [40], while its computational complexity is much lower than that of 3DTV regulariza-tion. Therefore, PTV efficiently captures the local smooth-ness [40] of multi-dimensional images to enhance the ro-bustness of HLRTF.
We summarize the contributions of this paper as follows:
• By embedding a DNN as a nonlinear transform into the t-SVD framework, we propose the HLRTF to capture the underlying low-rank structure of multi-dimensional images with compact representation abil-ities. We provide algebraic property of the HLRTF, which leads to solid foundations of its potential ca-pacity. With the loss function tailored corresponding to different observations, high-quality recovery results can be obtained after the DNN parameters and tensor factor parameters are unsupervisedly learned.
• To address extreme structural missing cases, we pro-pose the PTV regularization to constrain the DNN pa-rameters and tensor factor parameters, successfully ad-dressing the vanishing gradient issue. Our analysis shows that PTV is rooted in classic 3DTV and has a lower computational complexity.
• We apply HLRTF to typical inverse problems in multi-dimensional imaging including multi-dimensional im-age completion, denoising, and snapshot spectral imaging. Extensive experiments validate the general-ization abilities of HLRTF for different tasks and its superior performance over state-of-the-art methods. 1.1.