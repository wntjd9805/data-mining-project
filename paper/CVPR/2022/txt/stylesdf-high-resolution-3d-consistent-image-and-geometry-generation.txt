Abstract 1.

Introduction
We introduce a high resolution, 3D-consistent image and shape generation technique which we call StyleSDF. Our method is trained on single-view RGB data only, and stands on the shoulders of StyleGAN2 for image gener-ation, while solving two main challenges in 3D-aware
GANs: 1) high-resolution, view-consistent generation of the RGB images, and 2) detailed 3D shape. We achieve this by merging a SDF-based 3D representation with a style-based 2D generator. Our 3D implicit network renders low-resolution feature maps, from which the style-based network generates view-consistent, 1024 1024 images.
Notably, our SDF-based 3D modeling defines detailed 3D surfaces, leading to consistent volume rendering. Our method shows higher quality results compared to state of the art in terms of visual and geometric quality.
×
Project Page: https://stylesdf.github.io/
StyleGAN architectures [35–37] have shown an un-precedented quality of RGB image generation. They are, however, designed to generate single RGB views rather than 3D content. In this paper, we introduce StyleSDF, a method for generating 3D-consistent 1024 1024 RGB images and geometry, trained only on single-view RGB images.
×
Related 3D generative models [9, 48, 53, 57, 62] present shape and appearance synthesis via coordinate-based multi-layer-perceptrons (MLP). These works, however, often re-quire 3D or multi-view data for supervision, which are dif-ficult to collect, or are limited to low-resolution rendering outputs as they rely on expensive volumetric field sampling.
Without multi-view supervision, 3D-aware GANs [9,48,57] typically use opacity fields as geometric proxy, forgoing well-defined surfaces, which results in low-quality depth maps that are inconsistent across views.
At the core of our architecture lies the SDF-based 3D volume renderer and the 2D StyleGAN generator. We use a coordinate-based MLP to model Signed Distance Fields (SDF) and radiance fields which render low resolution fea-ture maps. These feature maps are then efficiently trans-formed into high-resolution images using the StyleGAN generator. Our model is trained with an adversarial loss that encourages the networks to generate realistic images from all sampled viewpoints, and an Eikonal loss that ensures proper SDF modeling. These losses automatically induce view-consistent, detailed 3D scenes, without 3D or multi-view supervision. The proposed framework effectively ad-dresses the resolution and the view-inconsistency issues of existing 3D-aware GAN approaches that base on volume rendering. Our system design opens the door for interesting future research in vision and graphics that involves a latent space of high quality shape and appearance.
Our approach is evaluated on the FFHQ [36] and
AFHQ [13] datasets. We demonstrate through extensive ex-periments that our system outperforms the state-of-the-art 3D-aware methods, measured by the quality of the gener-ated images and surfaces, and their view-consistencies. 2.