Abstract
For computer vision systems to operate in dynamic situ-ations, they need to be able to represent and reason about object permanence. We introduce a framework for learn-ing to estimate 4D visual representations from monocu-lar RGB-D video, which is able to persist objects, even once they become obstructed by occlusions. Unlike tradi-tional video representations, we encode point clouds into a continuous representation, which permits the model to attend across the spatiotemporal context to resolve occlu-sions. On two large video datasets that we release along with this paper, our experiments show that the represen-tation is able to successfully reveal occlusions for several tasks, without any architectural changes. Visualizations show that the attention mechanism automatically learns to follow occluded objects. Since our approach can be trained end-to-end and is easily adaptable, we believe it will be useful for handling occlusions in many video un-derstanding tasks. Data, code, and models are available at occlusions.cs.columbia.edu. 1.

Introduction
When an object becomes occluded in video, its location and visual structure is often still predictable. In several stud-ies, developmental psychologists have been able to demon-strate that shortly after birth, children learn how objects per-sist during occlusions [2, 5, 42, 52], and evidence suggests that animals perform similar reasoning too [32, 41].1 For example, although the yellow orb in Figure 1 disappears behind other objects, its location, geometry, and appearance remain evident to you. Occlusions are fundamental to com-puter vision, and predicting the contents behind them un-derlies many applications in video analysis.
The ﬁeld has developed a number of deep learning meth-1See “What The Fluff Challenge” on YouTube.
Figure 1. Video Occlusions – Although the yellow orb becomes fully occluded in the video, we can still perform many visual recognition tasks, such as predicting its location, reconstructing its appearance, and classifying its semantic category. This paper introduces a video representation architecture that is able to learn to perform all of these occlusion reasoning tasks. ods to operate on point clouds [19,44,66,68,69], which due to their attractive properties, have emerged as the represen-tation of choice for numerous 3D tasks. Point clouds are sparse, making them particularly scalable to large scenes.
However, to solve the problem in Figure 1, we need a video representation that (1) uses evidence from the pre-vious frames in order to (2) generate the new points that are not observed in the subsequent frames. Since point clouds are possible to collect at scale [7, 9], we believe they are an excellent source of data for learning to predict behind occlu-sions in video. However, the representation must also have the capacity to create points conditioned on their context.
This paper introduces an architecture for learning to pre-dict 4D point clouds from an RGB-D video camera. The key to our approach is a continuous neural ﬁeld representa-tion of a point cloud, which uses an attention mechanism to condition the full space on the observations. Since the rep-resentation is continuous, the approach can learn to produce points anywhere in spacetime, allowing for high-ﬁdelity re-constructions of complex scenes. Where there are occlu-sions and missing observations, the representation is able to use attention to ﬁnd the object and/or missing scene struc-ture when it was last visible, and subsequently put the right
points in the right place.
Experiments show that our video representation learns to successfully perform many occlusion reasoning tasks, such as visual reconstruction, geometry estimation, track-ing, and semantic segmentation. The same method works for these tasks without architectural changes. On two differ-ent datasets, we show the approach remains robust for both highly cluttered scenes and objects of various sizes. Though we train the representation without ground truth correspon-dence, visualizations show that the attention mechanism au-tomatically learns to follow objects through occlusions.
There are three principal contributions in this paper.
Firstly, we propose the new fundamental task of 4D dy-namic scene completion, which forms a basis for spa-tiotemporal reasoning tasks. Secondly, we present new benchmarks to evaluate scene completion and object per-manence in cluttered situations. Thirdly, we introduce a new architecture for deep learning on point clouds, which is able to generate new points conditioned on their context.
This architecture allows for large-scale point cloud data to be leveraged for representation learning. In the remainder of the paper, we describe these contributions in detail. We invite the community to use these benchmarks to test their model’s video understanding capabilities. 2.