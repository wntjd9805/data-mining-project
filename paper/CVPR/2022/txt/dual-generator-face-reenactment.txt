Abstract
We propose the Dual-Generator (DG) network for large-pose face reenactment. Given a source face and a reference face as inputs, the DG network can generate an output face that has the same pose and expression as the reference face, and has the same identity as the source face. As most ap-proaches do not particularly consider large-pose reenact-ment, the proposed approach addresses this issue by incor-porating a 3D landmark detector into the framework and considering a loss function to capture visible local shape variation across large pose. The DG network consists of two modules, the ID-preserving Shape Generator (IDSG) and the Reenacted Face Generator (RFG). The IDSG en-codes the 3D landmarks of the reference face into a ref-erence landmark code, and encodes the source face into a source face code. The reference landmark code and the source face code are concatenated and decoded to a set of target landmarks that exhibits the pose and expression of the reference face and preserves the identity of the source face. The RFG is partially built on the StarGAN2 generator with modifications on the input and layer settings, and with a facial style encoder added in. Given the target landmarks made by the IDSG and the source face as inputs, the RFG generates the target face with the desired identity, pose and expression. We evaluate our approach on the RaFD, MPIE,
VoxCeleb1, and VoxCeleb2 benchmarks and compare with state-of-the-art methods. 1.

Introduction
Given a source face and a reference face, face reenact-ment refers to the transformation of the action of the refer-ence face to the source face. The action refers to the pose and facial expression. The challenges are on the similar-ity between the actions of the reference face and the source face and on the preservation of the source identity after the transformation. It is an active research topic in the fields of computer vision and attracts increasing attention in recent years [23–26]. It has a wide range of applications in the areas such as virtual reality, animation and entertainment.
Various approaches have been proposed in recent years
[4, 22–25]. A major family of the approaches is the
Landmark-Assisted Generation (LAG), which exploits the facial landmarks to leverage the action transformation and the reenacted face generation [4, 22, 24, 25]. The FReeNet
[25] trains a landmark converter to transfer the reference’s landmarks to the source, and trains a generator to make the target face show the reference’s expression, but it cannot handle pose transformation. The FSTH [24] trains an em-bedder to encoder the source’s landmarks, and a generator to transfer the reference’s action to the source face. More approaches from the LAG family are reviewed in Sec.2.
Different from the existing LAG approaches, our approach explores a dual-generator architecture with one generator to make the ID-preserving 3D landmarks, and the other gener-ator to make the target face satisfy multiple objectives. Due to the embedding of 3D landmarks and the core losses con-sidered in training, our approach can address the large-pose reenactment, which is a challenging problem, but has not received sufficient attention.
There are methods without using landmarks, for exam-ple, the MGOS [23] that uses the reconstructed 3D meshes as guidance to learn the optical flow needed for the target face synthesis. Although the progresses made by differ-ent approaches are substantial, many issues are yet to be solved. The performance measured by the common met-rics, for example the FID, CSIM and SSIM, is still far from ideal. Many approaches have specific issues. For example, the FReeNet only transfers the facial expression, but can-not handle pose transfer. Although the FSTH can transfer both the pose and expression, the facial landmarks used to control the conversion are often inaccurate, damaging the identity preservation. Another important issue is that most approaches only deal with median pose variation (yaw an-gle < 45o) and ignore large/extreme poses.
To address the above issues, we propose the Dual-Generator (DG) network that contains two generators, the
ID-preserving Shape Generator (IDSG) and the Reenacted
Face Generator (RFG). Given a source face Is and a ref-erence face Ir as inputs, the IDSG transfers the pose and expression of the reference face Ir to the source face Is and
generates the target landmark estimate ˆlt. The RFG takes the target landmark estimate ˆlt and the source Is as inputs, and generates the reenacted face ˆIt that shows the same ac-tion as of the reference face Ir, but has the same identity as of the source Is. To handle large-pose references, we embed a 3D-landmark detector and consider an objective function to capture the pose-dependent local shape varia-tion from frontal to profile. We train the DG network on the dataset with full pose variation so that the landmark motion and identity preservation across large pose can be learned.
We summarize the contributions of this work as follows:
• The ID-preserving Shape Generator (IDSG) is veri-fied effective in generating an identity-preserving fa-cial shape with the desired pose and expression.
• The Reenacted Face Generator (RFG) is verified ef-fective in generating an identity-preserving target face with the desired pose and expression.
• Different from most approaches in the LAG family that use 2D landmarks, we embed 3D landmarks with a loss function to capture visible local shape variation so that the large-pose face reenactment can be handled.
• Better performance than state-of-the-art approaches, based on the evaluations on the RaFD, MPIE, Vox-Celeb1, VoxCeleb2 benchmarks.
Our code, model and more qualitative results are avail-able on https://github.com/AvLab-CV/Dual_
Generator_Face_Reenactment.
In the following, we first review the related work in Sec. 2, then the proposed approach in Sec. 3, then the experiments for performance evaluation in Sec. 4, and then a conclusion in Sec. 5. 2.