Abstract
Referring image segmentation is a fundamental vision-language task that aims to segment out an object referred to by a natural language expression from an image. One of the key challenges behind this task is leveraging the re-ferring expression for highlighting relevant positions in the image. A paradigm for tackling this problem is to leverage a powerful vision-language (“cross-modal”) decoder to fuse features independently extracted from a vision encoder and a language encoder. Recent methods have made remarkable advancements in this paradigm by exploiting Transformers as cross-modal decoders, concurrent to the Transformer’s overwhelming success in many other vision-language tasks.
Adopting a different approach in this work, we show that significantly better cross-modal alignments can be achieved through the early fusion of linguistic and visual features in intermediate layers of a vision Transformer encoder net-work. By conducting cross-modal feature fusion in the visual feature encoding stage, we can leverage the well-proven correlation modeling power of a Transformer en-coder for excavating helpful multi-modal context. This way, accurate segmentation results are readily harvested with a light-weight mask predictor. Without bells and whistles, our method surpasses the previous state-of-the-art methods on
RefCOCO, RefCOCO+, and G-Ref by large margins.
Figure 1. The task of referring image segmentation takes one im-age and one text description as inputs, and predicts a mask de-lineating the object specified in the description. (a) The previ-ous state-of-the-art method (i.e., VLT [12]) leverages a vision-language Transformer decoder for cross-modal feature fusion. (b)
Conversely, we propose to directly integrate linguistic informa-tion into visual features at intermediate levels of a vision Trans-former network, where beneficial vision-language cues are jointly exploited. A light-weight mask predictor can thus readily replace the complicated cross-modal decoder in previous counterparts. 1.

Introduction
Given an image and a text description of the target object, referring image segmentation aims at predicting a pixel-wise mask that delineates that object [8, 18]. It yields great value for various applications such as language-based human-robot interaction [55] and image editing [5]. In con-trast to conventional single-modality visual segmentation tasks based on fixed category conditions [30, 65], referring image segmentation has to deal with the much richer vocab-ularies and syntactic varieties of human natural languages.
*Equal contribution.
†Corresponding author.
In this task, the target object is inferred from a free-form ex-pression, which includes words and phrases presenting the concepts of entities, actions, attributes, positions, etc., or-ganized by syntactic rules. Therefore, the key challenge of this task is to exploit visual features that are relevant to the given text conditions.
There have been growing efforts devoted to referring im-age segmentation over the past few years. A widely adopted paradigm is to first independently extract vision and lan-guage features from different encoder networks, and then fuse them together to make predictions with a cross-modal decoder. Concretely, the fusion strategies include recurrent interaction [28, 31], cross-modal attention [4, 20, 48], multi-linguistic structure-guided modal graph reasoning [21], context modeling [22], etc. Recent advances (e.g., [12]) bring performance improvements via employing a cross-modal Transformer [54] decoder (illustrated in Fig. 1 (a)) to learn more effective cross-modal alignments, which is in concurrence with Transformer’s overwhelming success in many other vision-language tasks [19, 27, 37, 45].
Although great progress has been achieved, the poten-tiality of the Transformer for enhancing referring image segmentation is still far from being sufficiently explored in the conventional paradigm. Specifically, cross-modal in-teractions occur only after feature encoding, and a cross-modal decoder is solely responsible for aligning the visual and linguistic features. As a result, previous methods fail to effectively leverage the rich Transformer layers in the en-coder for excavating helpful multi-modal context. To ad-dress these issues, a potential solution is to exploit a visual encoder network for jointly embedding linguistic and visual features during visual encoding.
Accordingly, we propose a Language-Aware Vision
Transformer (LAVT) network, in which visual features are encoded together with linguistic features, being “aware” of their relevant linguistic context at each spatial location. As shown in Fig. 1 (b), LAVT makes full use of the multi-stage design in a modern vision Transformer backbone network, leading to a hierarchical language-aware visual encoding scheme. Specifically, we densely integrate linguistic fea-tures into visual features via a pixel-word attention mecha-nism, which occurs at each stage of the network. The ben-eficial vision-language cues are then exploited by the fol-lowing Transformer blocks, e.g., [33], in the next encoder stage. This approach enables us to forgo a complicated cross-modal decoder, since the extracted language-aware visual features can be readily adopted to harvest accurate segmentation masks with a lightweight mask predictor.
To evaluate the effectiveness of the proposed method, we conduct extensive experiments on various mainstream re-ferring image segmentation datasets. Our LAVT achieves 72.73%, 62.14%, 61.24%, and 60.50% overall IoU on the validation sets of RefCOCO [63], RefCOCO+ [63], G-Ref (UMD partition) [42], and G-Ref (Google partition) [40], improving the state of the art for these datasets by absolute margins of 7.08%, 6.64%, 6.84%, and 8.57%, respectively.
To summarize, our contributions are twofold:
• We propose LAVT, a Transformer-based referring im-age segmentation framework that performs language-aware visual encoding in place of cross-modal fusion post feature extraction.
• We achieve new state-of-the-art results on three datasets for referring image segmentation, demonstrat-ing the effectiveness and generality of the proposed method. Source code is available at LAVT-RIS. 2.