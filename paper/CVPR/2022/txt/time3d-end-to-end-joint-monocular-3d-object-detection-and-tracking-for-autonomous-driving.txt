Abstract
While separately leveraging monocular 3D object detec-tion and 2D multi-object tracking can be straightforwardly applied to sequence images in a frame-by-frame fashion, stand-alone tracker cuts off the transmission of the uncer-tainty from the 3D detector to tracking while cannot pass tracking error differentials back to the 3D detector.
In this work, we propose jointly training 3D detection and 3D tracking from only monocular videos in an end-to-end man-ner. The key component is a novel spatial-temporal infor-mation flow module that aggregates geometric and appear-ance features to predict robust similarity scores across all objects in current and past frames. Specifically, we leverage the attention mechanism of the transformer, in which self-attention aggregates the spatial information in a specific frame, and cross-attention exploits relation and affinities of all objects in the temporal domain of sequence frames. The affinities are then supervised to estimate the trajectory and guide the flow of information between corresponding 3D objects. In addition, we propose a temporal -consistency loss that explicitly involves 3D target motion modeling into the learning, making the 3D trajectory smooth in the world coordinate system. Time3D achieves 21.4% AMOTA, 13.6%
AMOTP on the nuScenes 3D tracking benchmark, surpass-ing all published competitors, and running at 38 FPS, while
Time3D achieves 31.2% mAP, 39.4% NDS on the nuScenes 3D detection benchmark. 1.

Introduction 3D object detection is an essential task for Autonomous
Driving. Compared with LiDAR system, monocular cam-eras are cheap, stable, and flexible, favored by mass-produced cars [4, 9, 18, 19]. However, monocular 3D object detection is a natural ill-posed problem for the lack of depth information, making it difficult to estimate an accurate and stable state of the 3D target [19, 22]. A typical solution is to smooth the previous and current state through 2D multiple
Figure 1. Illustration of the proposed Time3D. Given a monoc-ular video sequence, Time3D joint learn monocular 3D object de-tection and 3D tracking, and output smooth trajectories, 2D boxes, 3D boxes, category, velocity, and motion attribute. Time3D is trained end-to-end, so it can feed forward the uncertainty and backward the error gradient. object trackers(MOT) [5, 20].
Following the “tracking-by-detection” paradigm [2, 30, 37], a widely used contemporary strategy first computes past trajectory representations and detected objects. Later, that association module is used to compute the similarity between the current objects across the past frames to esti-mate their tracks. Most of the existing methods following this pipeline try to build several different cues, including re-identification(Re-ID) models [25, 40, 41], motion mod-eling [12, 15], and hybrid models [1]. Currently, most of these models are still hand-crafted so that the correspond-ing tracker can only follow the detector independently. The recent studies of 2D MOT try to establish a deep learning association in tracking [3, 29, 43]. However, these meth-ods still encounter three shortcomings in autonomous driv-ing scenarios: 1) They treat detection and association sep-arately, in which stand-alone tracking module cuts off the transmission of the uncertainty from the 3D detector to tracking while cannot differential pass error back to the 3D detector. 2) The objects from the same category often have similar appearance information and often undergo frequent
occlusions and different speed variations in the autonomous driving scenario. They failed to integrate these heteroge-neous cues in a unified network. 3) They estimate the tra-jectory without directly constraining the flow of appearance and geometric information in the network, which is crucial to the trajectory smoothness, velocity estimation, and mo-tion attribute(e.g., parking, moving, or stopped for cars).
We propose to combine 3D monocular object detection and 3D MOT into a unified architecture with end-to-end training manner, which can: (1) predict 2D box, 3D box,
Re-ID feature from only monocular images without any ex-tra synthetic data, CAD model, instance mask, or depth map. (2) encode the compatible feature representation for (3) learn a differential association to gener-these cues. ate trajectory by simultaneously combining heterogeneous cues across time. (4) guide the information flowing across all objects to generate a target state with temporal consis-tency. To do so, we first modify the anchor-free monocular 3D detector KM3D [18] to learn the 3D detector and Re-ID embedding jointly, following the ”joint detection and track-ing” paradigm [34, 42], so that it can generate 2D box, 3D box, object category, and Re-ID features, simultaneously.
To design the compatible feature representation for differ-ent cues, we propose to transform the parameters of the dif-ferent magnitude of the 2D box and 3D box to the unified representation, 2D corners, and 3D corners, in which the ge-ometric information can be extracted as a high-dimensional feature from corner raw coordinates by the widely used
PointNet [24] structure. Fig. 1 illustrate the pipeline of
Time3D.
Reviewing MOT, we found that data association is very similar to the Query-Key mechanism, where one object is a query, and another object in a different frame is the key.
Its feature in different frames is highly similar for the same object, enabling the query-key mechanism to output a high response. We, therefore, propose a Transformer architec-ture, a widely-used entity of query-key mechanism.
In-spired by RelationNet [14], the self-attention aggregates features from all elements in a frame to exploit spatial topol-ogy, which is automatically learned without any explicit su-pervision. The cross-attention computes the target affinities across different frames, and its query-key weights are super-vised to learn tracks via a unimodal loss function. The fi-nally temporal-spatial features output the velocity, attribute, and 3D box smoothness refinement. In addition, we pro-pose temporal-consistency loss that constrains the temporal topology of objects in the 3D world coordinate system to make the trajectory smoother.
To summarize, the main contributions in this work are (1). We propose a unified framework to the following: jointly learn 3D object detection and 3D multi-object track-ing by combining heterogeneous cues in an end-to-end manner. (2). We propose an embedding extractor to make geometric and appearance information compatible by trans-forming the 2D and 3D boxes to unified representations. (3). We propose a temporal-consistency loss to make the trajectory smoother by constraining the temporal topology. (4). Experiments in the nuScenes 3D tracking benchmark demonstrate that the proposed method achieves the best tracking accuracy comparing other competitors by large margins while running in real-time(26FPS). 2.