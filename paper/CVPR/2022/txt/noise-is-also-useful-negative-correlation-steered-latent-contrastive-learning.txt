Abstract
How to effectively handle label noise has been one of the most practical but challenging tasks in Deep Neural Net-works (DNNs). Recent popular methods for training DNNs with noisy labels mainly focus on directly filtering out sam-ples with low confidence or repeatedly mining valuable in-formation from low-confident samples. However, they can-not guarantee the robust generalization of models due to the ignorance of useful information hidden in noisy data.
To address this issue, we propose a new effective method named as LaCoL (Latent Contrastive Learning) to leverage the negative correlations from the noisy data. Specifically, in label space, we exploit the weakly-augmented data to fil-ter samples and adopt classification loss on strong augmen-tations of the selected sample set, which can preserve the training diversity. While in metric space, we utilize weakly-supervised contrastive learning to excavate these negative correlations hidden in noisy data. Moreover, a cross-space similarity consistency regularization is provided to con-strain the gap between label space and metric space. Ex-tensive experiments have validated the superiority of our approach over existing state-of-the-art methods. 1.

Introduction
In the past ten years, Deep Neural Networks (DNNs) have achieved impressive performance and revolutionized a wide variety of computer vision applications, such as im-age recognition [13,14], semantic segmentation [31,36], ob-ject detection [10,27], and cross-modal retrieval [30,34,48].
The remarkable success in training DNNs has been benefit-ing from the collection of large-scale datasets with high-quality human annotations (e.g., ImageNet [8] and MS-COCO [28]).
However, it is expensive and time-consuming to obtain high-quality annotations for large-scale data in most real-world scenarios. To overcome this limitation, online key
*C.D. is corresponding author.
Figure 1. An illustration of negative information and pairwise neg-ative correlation. Negative information (Left) provides a new com-plementary label to the training sample. If the assigned comple-mentary label is wrong, it will attenuate the performance severely.
Pairwise negative correlation (Right) randomly selects K negative samples that are not in the same category as the anchor image, e.g., the anchor image has the negative correlation to the image labeled
“car”. Instead of using noisy positive correlation (black arrows), these pairwise negative correlations (red arrows) can be well cap-tured in metric space to improve the robustness of DNNs. search engine [4, 26] or crowdsourcing [52] methods are proposed to efficiently and cheaply gain the desired train-ing datasets with low-quality labels, in which noisy labels are likely to be introduced consequently. Although DNNs have high model capacities, they often overfit the noisy la-bels due to the memorization effect, resulting in poor clas-sification and generalization performance [53]. Therefore, developing an effective method to improve the robustness of
DNNs against noisy labels is of great practical importance.
Early robust learning methods primarily model noisy la-bels with the noise transition matrix [11, 15, 40] and use it refine losses. However, it is difficult to correctly es-timate the noise transition matrix, as it heavily relies on either prior knowledge or a subset of high-quality labeled data. Considering the memorization effect to noisy labels, recent methods attempt to select high-confident samples as clean data and filter out others through a human-defined rule. For example, the small-loss trick widely-used in many
Figure 2. Conceptual illustration of different methods that leverage noisy data. The black solid line means “belong to”, the red solid line means “is not”, the black dotted line denotes a positive similarity relationship, the red dotted line denotes a negative similarity relationship, and the red scissors mean “filter out this relationship”. (a) Sample selection methods adopt a sample selection strategy to find clean data and filter out noisy data. (b) Relabeling methods give new pseudo-labels for noisy data using the model’s predictions. (c) Negative learning methods randomly assign a complementary label for each noisy sample. (d) LaCoL randomly selects K negative samples for each noisy sample. methods, such as Co-teaching [12], Co-teaching+ [51] and
JoCoR [43], selects a proportion of small-loss samples as clean ones. However, these methods cannot fully exploit the hidden information in the filtered samples, which de-generates the robustness of DNNs. To further take ad-vantage of noisy training data, a series of methods, repre-sented by semi-supervised learning based approaches (e.g.,
DivideMix [22], and ELR+ [29]), relabel noisy samples us-ing the model’s predictions. Whereas the semi-supervised learning strategy increases computation cost, and relabeling noisy samples according to the model’s predictions could cause confirmation bias, where the prediction error accu-mulates and harms performance.
Different from relabeling based methods, the recently proposed negative learning [18, 19] can effectively capture the underlying negative information of each noisy sample, which uses complementary labels to replace the original noisy labels and train DNNs by virtue of the learned neg-ative information more effectively. For example, as shown in the left of Figure 1, the image of a cat is assigned to the wrong label “dog”. Negative learning will randomly give it a new complementary label other than “dog”, e.g., “bird”.
Although the negative learning provides the “right” infor-mation (e.g., the image of a cat shown in Figure 1 is not
“bird”) with a high probability in this manner, selecting a true label as a complementary label (e.g., the image of a cat shown in Figure 1 is not “cat”) is inevitable, which will severely degenerate the performance of the model. Mean-while, this influence will be exacerbated due to the strong discrimination ability of cross-entropy (CE)-like loss used in these negative learning methods.
In this paper, we propose a simple yet effective method, named LaCoL (Latent Contrastive Learning), to improve the robustness and generalization of DNNs through excavat-ing the implicit negative correlations in noisy data. Specif-ically, for each anchor sample, we randomly select K other samples, that are not in the same category with the anchor image, as negative samples, and use these negative pairs to construct negative correlations (as shown in Figure 2, compared with the existing approaches, pairwise negative correlation can make full use of noisy data and has higher confidence, which is beneficial for enhancing the robust-ness of DNNs). To better capture these negative correla-tions, we exploit the weakly-supervised contrastive learning method in the latent metric space, which is robust against wrongly assigned negative samples. Considering that in-corporating different augmentation strategies during train-ing can improve the generalization of models [9, 16, 35], we adopt weak (e.g., using only crop-and-flip) and strong (e.g., using RandAugment [7]) augmentations. Specifi-cally, given the anchor sample with weak augmentation, its strong augmentation is the positive point and the negative points are derived by exploiting negative correlations in our method. Meanwhile, inspired by the alternate sample se-lection in Co-teaching [12], we use weak augmentations to select high-confident samples, and then apply strong aug-mentations to the back-propagation in label space, which can keep the divergence of sample selection procedure. Fur-thermore, we provide a cross-space similarity consistency regularization to narrow the gap between label space and metric space, which makes the learned negative correlation in metric space more powerful to improve the performance of the classification task in label space. The main contribu-tions of this work are summarized as follows: 1) We propose a latent contrastive learning method ex-ploiting the useful negative correlation hidden in noisy data, which can improve the robustness and general-ization of traditional DNNs.
Figure 3. The framework of the proposed LaCoL. Given the training data with noisy labels, their weakly-augmented images are used to filter samples, and then the strong augmentations of the selected high-confident images are applied to optimize classification loss (Part a). Meanwhile, the weakly-augmented and strongly-augmented images are both projected in metric space to capture the implicit negative correlation guided by a weakly-supervised contrastive learning loss (Part b). Furthermore, the similarity matrices in label space and metric space are calculated to train both classification head and embedding head such that images with similar classification probabilities have similar embeddings (Part c). 2) To keep the divergence during sample selection in each iteration, we use weak augmentations to calculate con-fidence for selection, and then apply strong augmenta-tions of high-confident data to train DNNs. 3) To make latent contrastive learning in metric space better guide classification tasks in label space, we present a cross-space similarity consistency regulariza-tion to constrain the gap between label space and met-ric space. 4) Extensive experiments demonstrate that LaCoL signif-icantly outperforms state-of-the-art methods on both synthetic and real-world noisy datasets. 2.