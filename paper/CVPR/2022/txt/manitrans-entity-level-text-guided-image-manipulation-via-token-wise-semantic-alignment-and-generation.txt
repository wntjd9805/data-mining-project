Abstract gions and achieve more precise and flexible manipulation compared with baseline methods.
Existing text-guided image manipulation methods aim to modify the appearance of the image or to edit a few objects in a virtual or simple scenario, which is far from practical
In this work, we study a novel task on text-application. guided image manipulation on the entity level in the real world. The task imposes three basic requirements, (1) to edit the entity consistent with the text descriptions, (2) to preserve the text-irrelevant regions, and (3) to merge the manipulated entity into the image naturally. To this end, we propose a new transformer-based framework based on the two-stage image synthesis method, namely ManiTrans, which can not only edit the appearance of entities but also generate new entities corresponding to the text guidance.
Our framework incorporates a semantic alignment module to locate the image regions to be manipulated, and a seman-tic loss to help align the relationship between the vision and language. We conduct extensive experiments on the real datasets, CUB, Oxford, and COCO datasets to verify that our method can distinguish the relevant and irrelevant re-1.

Introduction
There are various active branches of image manipula-tion, such as style transfer [16], image translation [21, 62], and Text-Guided Image Manipulation (TGIM), by taking advantage of recent deep generative architectures such as
GANs [17], VAE [25] and auto-regressive models [50]. Par-ticularly, the previous TGIM methods either operate some objects by text instructions [12, 15, 60], such as “adding” and “removing” in a simple toy scene, or manipulating the appearance of objects [4] or the style of the image [23, 52].
In this work, we are interested in a novel challenging task of entity-Level Text-Guided Image Manipulation (eL-TGIM), which is to manipulate the entities on a natural image given the text descriptions, as shown in Fig. . Critically, our eL-TGIM is much more difficult than the vanilla TGIM task, as it demands much manipulation ability in the fine-grained entity level. Thus, it is nontrivial to directly extend previ-ous methods to this eL-TGIM task, as they can not effec-tively identify and edit the properties of entities as empiri-cally shown in Fig. .
Generally, the major obstacle of the TGIM task lies in distinguishing which parts of the image to change or not change. To tackle this problem, existing TGIM methods
[10, 26, 28, 34] propose many different manipulation mech-anisms, such as word-level discriminator [28, 34] and text-image affine combination module [26], to differentiate the candidate editing regions from the other image parts. These methods unfortunately are still very limited to be applied to manipulate the entities in nature images. For example,
Fig. shows that previous methods can only manipulate the texture/color of an object, while they fail to generate rea-sonable entity-level manipulation results from the text de-scription.
To this end, we propose a novel framework of Manip-ulating Transformers (ManiTrans) with the token-wise se-mantic alignment and generation for the eL-TGIM. Thus, to tackle this task, we propose the two key ideas of Trans-former based image synthesizer (Trans), and entity-level semantic manipulator (Mani). the recent transformer-based architecture [14, 37, 43] has been pro-posed for image synthesis and has shown great expressive power. We thus present a novel component of Trans, by first learning an autoencoder to downsample and quantize an im-age as a sequence of discrete image tokens and then fit the joint distribution of this sequence with a transformer-based auto-regressive model.
Specifically,
Furthermore, to successfully identify the entities for editing, we propose the Mani component which includes a semantic alignment module, and Contrastive Language-Image Pre-training (CLIP) module. The former module helps the generation model Trans to locate and modify the text-relevant image tokens given the textual guidance. Thus our ManiTrans generation model can manipulate the image locally and preserve the irrelevant contents to a greater ex-tent, as in Fig. . On the other hand, we repurpose the re-cent CLIP module as one type of semantic loss to further boost the visual-semantic alignment between the input tex-tual guidance and the manipulated image. Essentially, such semantic loss, proposed in our ManiTrans, is complemen-tary to token-wise classification loss, and thus efficiently serves as a pixel-level supervision signal to train our model.
We evaluate our method on multiple datasets including:
CUB [51], Oxford [36] and COCO [32]. Quantitatively and qualitatively comparison against previous methods shows that our method can better manipulate the entities of an image by text while keeping the background region un-changed. Besides manipulating the texture/color of one ob-ject, our method also shows superior capability for manipu-lating the structure of objects guided by various textual de-scriptions, as shown in Fig. and Fig. 4 respectively. This can not be done in previous methods.
In summary, our contributions are as follows:
• We propose a transformer-based entity-level text-guided image manipulation framework with token-wise semantic alignment and generation, named ManiTrans, which can not only manipulate the texture/color of a single object, but also manipulate the structure of an object and manipulate multi-ple objects.
• We propose a semantic alignment module to locate the text-relevant image tokens for flexible manipulation, and a semantic loss for better visual-semantic alignment and de-tailed training signal.
• We repurpose and utilize the transformer-based image synthesizer, and CLIP module as the semantic loss in our
ManiTrans framework, which is nontrivial technically.
• We quantitatively and qualitatively evaluate our method on the CUB, Oxford and COCO datasets, achieving supe-rior/competitive results against baseline methods. 2.