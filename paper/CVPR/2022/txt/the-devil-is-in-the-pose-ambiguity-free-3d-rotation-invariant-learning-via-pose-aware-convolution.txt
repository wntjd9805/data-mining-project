Abstract
Recent progress in introducing rotation invariance (RI) to 3D deep learning methods is mainly made by designing
RI features to replace 3D coordinates as input. The key to this strategy lies in how to restore the global informa-tion that is lost by the input RI features. Most state-of-the-arts achieve this by incurring additional blocks or complex global representations, which is time-consuming and inef-fective. In this paper, we real that the global information loss stems from an unexplored pose information loss prob-lem, i.e., common convolution layers cannot capture the rel-ative poses between RI features, thus hindering the global information to be hierarchically aggregated in the deep net-works. To address this problem, we develop a Pose-aware
Rotation Invariant Convolution (i.e., PaRI-Conv), which dynamically adapts its kernels based on the relative poses.
Specifically, in each PaRI-Conv layer, a lightweight Aug-mented Point Pair Feature (APPF) is designed to fully en-code the RI relative pose information. Then, we propose to synthesize a factorized dynamic kernel, which reduces the computational cost and memory burden by decomposing it into a shared basis matrix and a pose-aware diagonal ma-trix that can be learned from the APPF. Extensive experi-ments on shape classification and part segmentation tasks show that our PaRI-Conv surpasses the state-of-the-art RI methods while being more compact and efficient. 1.

Introduction
With the development of 3D scanning technology, deep learning on 3D point clouds has achieved remarkable progress in various tasks [9]. However, most methods
∗The corresponding author is Prof. Yang Cong.
†This work is supported in part by the National Key Research and De-velopment Program of China under Grant 2019YFB1310300 and the Na-tional Nature Science Foundation of China under Grant 62127807.
Figure 1. Illustration of the geometric ambiguity caused by the pose information loss problem in rotation-invariant (RI) learning.
RI features naturally neglect their own pose information, and re-main the same under rotations {Ri}3 i=1. Thus, the vanilla convo-lution (whether on images or 3D point clouds) cannot capture the relative poses between the eyes and the mouth, leading to an am-biguous representation that fails to distinguish a smile face from an angry face. Our Pose-aware convolution dynamically adapts the weight based on relative pose, thus eliminating the ambiguity.
[14, 15, 35] assume a strong prior, i.e., the data are pre-aligned to the same canonical pose, and the performance will degrade drastically on unaligned data, even when ex-tensive rotation augmentation is applied. This hinders the application of current methods to real scenarios, where ob-jects typically appear with arbitrary rotations.
To this end, most methods design rotation-invariant (RI) representations as input [1, 11, 13, 43], and have achieved consistent invariance to rotation. However, such RI features lose global positional information, comparing to 3D coor-dinates. Thus, state-of-the-art methods either design aux-iliary blocks [42, 45] that directly process coordinates, or handcraft complex representations that record pair-wise re-lationship in a much larger neighbourhood [13, 38], which incur large computational cost while still being less com-petitive than rotation-sensitive methods [14, 35] on aligned data. Moreover, they fail to explain why 3D convolutional neural networks (CNNs) [15, 24, 31, 38] cannot acquire in-tact global information from RI features via hierarchical ab-straction, as 2D CNNs can from pixels, since both pixels and RI features are lack of positional information.
In this paper, we reveal that the above issues stem from an inherent pose information loss problem, which has not been well exploited before.
Intrinsically, being invariant to rotation also means the loss of pose information. Thus, as illustrated in Figure 1, when aggregating RI features via vanilla convolutions, the relative poses between these fea-tures are inevitably lost, leading to an ambiguous represen-tation that fails to distinguish the smile face from the angry face. Moreover, this also explains the ineffectiveness of cur-rent methods [1, 11, 38, 44], i.e., they only restore 3 degree-of-free (DoF) position rather than 6 DoF pose information.
To address the above challenges, we present Pose-aware
Rotation Invariant Convolutions (i.e., PaRI-Conv) to restore the pose information that is lost by the RI features. As shown in Figure 1, in a convolution, our key idea is to gen-erate a dynamic kernel weight for each neighbour based on its relative pose to the center. In this way, the relative poses between local neighbours are fully preserved in the derived feature. Specifically, we first propose an Augmented
Point Pair Feature (APPF) to represent the relative pose in-formation, which extends the point pair feature (PPF) [5], and is rigorously rotation-invariant. Then the APPFs are applied to dynamically adjust convolution kernels. To re-duce the expensive computational cost and memory bur-den introduced by storing heavy weight banks [31, 39, 40] or regressing large kernel matrix [27, 36], we propose a factorized dynamic kernel, which decomposes the kernel weight into a basis matrix that is shared by all neighbours and a low DoF diagonal matrix that is learned from the
APPF. Since PaRI-Conv fully preserves the geometric re-lationship among adjacent patches by incorporating pose information, global context can be automatically acquired by simply stacking PaRI-Conv layers, thus avoiding redun-dant blocks [45] or complex representations [13, 38] for global information compensation, resulting in a much more compact and efficient network. Extensive experiments on shape classification and part segmentation tasks show that our method surpasses state-of-the-art RI methods, while be-ing consistently rotation-invariant. More importantly, our method approaches or even surpasses the state-of-the-art rotation-sensitive methods on aligned data, verifying PaRI-Conv’s ability in preventing information loss.
In conclusion, the main contributions of our work are:
• We reveal the pose information loss problem and ad-dress it by proposing a Pose-aware Rotation Invariant
Convolution operator (i.e., PaRI-Conv), leading to a more powerful and efficient solution for RI learning.
• A lightweight RI feature, named Augmented Point Pair
Feature (APPF), is proposed to fully encode the pose of each neighbour relative to the center.
• To synthesize pose-aware kernel, a factorized dynamic kernel is designed by decomposing the kernel weight into a shared basis matrix and a pose-aware diagonal matrix, which is more compact and efficient without sacrificing the flexibility. 2.