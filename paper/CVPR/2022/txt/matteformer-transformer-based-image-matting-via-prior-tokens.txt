Abstract
In this paper, we propose a transformer-based image matting model called MatteFormer, which takes full advan-tage of trimap information in the transformer block. Our method first introduces a prior-token which is a global rep-resentation of each trimap region (e.g. foreground, back-ground and unknown). These prior-tokens are used as global priors and participate in the self-attention mech-anism of each block. Each stage of the encoder is com-posed of PAST (Prior-Attentive Swin Transformer) block, which is based on the Swin Transformer block, but differs in a couple of aspects: 1) It has PA-WSA (Prior-Attentive
Window Self-Attention) layer, performing self-attention not only with spatial-tokens but also with prior-tokens. 2) It has prior-memory which saves prior-tokens accumulatively from the previous blocks and transfers them to the next block. We evaluate our MatteFormer on the commonly used image matting datasets: Composition-1k and Distinctions-646. Experiment results show that our proposed method achieves state-of-the-art performance with a large margin.
Our codes are available at https://github.com/ webtoon/matteformer. 1.

Introduction
Image matting is one of the most fundamental tasks in computer vision which is mainly used to separate a fore-ground object precisely for the purpose of image editing and compositing. Especially, the foreground not only includes complex objects like human hair and animal fur but also in-cludes transparent objects like glass, bulb and water. Natural image can be represented as a linear combination of fore-ground F ∈ RH×W ×C and background B ∈ RH×W ×C with alpha matte α ∈ RH×W as follows:
Ii = αiFi + (1 − αi)Bi, αi ∈ [0, 1], (1)
Nojun kwak was supported by the NRF (2021R1A2C3006659) and
IITP grant (NO.2021-0-01343) funded by the Korea government (MSIT).
Figure 1. Prior-tokens are generated via the trimap at the bottom and concatenated to the local spatial-tokens to participate in the self-attention mechanism. where H, W and C denotes the height, the width and the number of channels (3 for a color image) respectively, and i ∈ [HW ] denotes the pixel index.
In image matting, estimating opacity value α given only observed image I, is a highly ill-posed problem if any extra information is not available. Thus, in many works, various types of additional user-inputs (e.g. trimap, scrib-ble, binary mask, background image, etc.) are used, among which, especially trimap is the most common. Trimaps are cost-intensive for users to draw but provide high-quality in-formation about global context such as region information about foreground, background and unknown pixels. Conse-quently, it is natural to design a model to take a full ad-vantage of this user-input and many works utilizing trimaps have been developed for image matting, most of which are based on convolutional neural networks (CNNs).
While CNNs have been very successful in computer vi-sion tasks, for natural language processing (NLP) tasks, transformers earned great success and recently there have been many attempts to introduce transformers in down-stream vision tasks as an alternative to CNNs. The semi-nal work of [11] proposed Vision Transformer (ViT) and showed impressive performances compare to CNN-based models, demonstrating potential on vision tasks. However, implementing the global self-attention in ViT needs high computational cost; it is quadratic to the number of patches.
To overcome this limitation, several general-purpose trans-former backbones [10, 30, 52] have been proposed by re-ducing computational complexity with local self-attention methods. For example, [30] proposed a hierarchical trans-former structure by introducing the self-attention within lo-cal windows to enable linear cost to the input size and the patch merging layer to reduce the number of spatial-tokens in deeper layers. Besides, they proposed the shifting win-dow scheme to exchange information through neighboring windows. Unfortunately, since the shifting window tech-nique slowly enlarges the receptive field, it is still hard to achieve a sufficiently large receptive field, especially in lower layers.
In this paper, we propose a transformer-based image matting model, named MatteFormer. We first define a prior-token, which represents global context feature of each trimap region; the foreground, background and unknown region as shown in Fig. 1. These prior-tokens are used as global priors and participate in the self-attention mechanism of each block. The encoder stage is composed of the PAST (Prior-Attentive Swin Transformer) blocks, which are based on the Swin Transformer block [30]. However, our PAST block is different from the Swin Transformer block in two aspects. First, it has the PA-WSA (Prior-Attentive Window
Self-Attention) layer, where self-attention is computed not only with spatial-tokens but also with prior-tokens as shown in Fig. 1. Second, we introduce the prior-memory, mem-orizing all prior-tokens generated at each block. Through this, prior-tokens from the previous block can be utilized in the PA-WSA layer of the next block. We evaluate our Mat-teFormer on Composition-1k and Distinctions-646, which are the commonly used datasets in image matting. The re-sults show that our method achieves state-of-the-art perfor-mance. We also conduct some extensive studies about the effectiveness of prior-tokens, visualization of self-attention maps, usage of ASPP, and the computational cost.
In short, our contributions can be summarized as follows:
• We propose MatteFormer, the first transformer-based architecture for the image matting problem.
• We introduce prior-tokens which imply global infor-mation of each trimap region (foreground, background and unknown) and use them as global priors in our pro-posed network.
• We design the PAST (Prior-Attentive Swin Trans-former) block, a variant of the Swin Transformer block, which includes the PA-WSA (Prior-Attentive
Window Self-Attention) layer and the prior-memory.
• We evaluate MatteFormer on Composition-1k and
Distinctions-646, showing that our method achieves state-of-the-art performance with a large margin. 2.