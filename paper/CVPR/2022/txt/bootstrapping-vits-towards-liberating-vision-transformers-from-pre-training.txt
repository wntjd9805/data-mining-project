Abstract
Recently, vision Transformers (ViTs) are developing rapidly and starting to challenge the domination of con-volutional neural networks (CNNs) in the realm of com-puter vision (CV). With the general-purpose Transformer architecture replacing the hard-coded inductive biases of convolution, ViTs have surpassed CNNs, especially in data-sufficient circumstances. However, ViTs are prone to over-fit on small datasets and thus rely on large-scale pre-training, which expends enormous time. In this paper, we strive to liberate ViTs from pre-training by introducing CNNs’ in-ductive biases back to ViTs while preserving their network architectures for higher upper bound and setting up more suitable optimization objectives. To begin with, an agent
CNN is designed based on the given ViT with inductive bi-ases. Then a bootstrapping training algorithm is proposed to jointly optimize the agent and ViT with weight sharing, during which the ViT learns inductive biases from the in-termediate features of the agent. Extensive experiments on CIFAR-10/100 and ImageNet-1k with limited training data have shown encouraging results that the inductive biases help ViTs converge significantly faster and outper-form conventional CNNs with even fewer parameters. Our code is publicly available at https://github.com/ zhfeing/Bootstrapping-ViTs-pytorch. 1.

Introduction
The great successes of the convolutional neural net-works (CNNs) [21, 23, 29, 42] have liberated researchers from handcrafting visual features [14, 33]. By means of the inductive biases [10], i.e., focusing on the localized features and weight sharing, CNNs are potent tools for tackling vi-sual recognition tasks [5, 21, 36]. Nevertheless, such biases have constrained their abilities towards building deeper and larger models, as they have ignored the long-range depen-dencies [15, 18].
In recent years, Transformers [46] have been pro-replacing inductive biases with a general-posed for
*Equal contribution
†Corresponding author, email: sjie@zju.edu.cn
Figure 1. Illustration of our proposed method for optimizing vi-sion Transformers efficiently without pre-training. An agent CNN is constructed according to the network architecture of the ViT with shared weights, and the ViT learns inductive biases from in-termediate features and predictions of the agent. purpose network architecture in natural language process-ing (NLP). Exclusively relying on multi-head attention mechanisms (MHA), Transformers have the inborn capa-bility to capture the global dependencies within language tokens and have become the de facto preferred data-driven models in NLP [2, 17, 35]. Inspired by this, a growing num-ber of researchers have introduced the Transformer archi-tecture into the realm of computer vision (CV) [4,18,45,56].
It turns out an encouraging discovery that vision Transform-ers (ViTs) outperform state-of-the-art (SOTA) CNNs by a large margin with a similar amount of parameters.
Despite the appealing achievements, ViTs suffer from poor performance, especially without adequate annotations or strong data augmentation strategies [8, 18, 45]. The reasons for this circumstance are two fold: on the one hand, the widely adopted multi-head self-attention mech-anisms (MHSA) in ViTs have dense connections against convolution [11], which is hard to optimize without prior knowledge; on the other hand, Chen et al. [8] have illus-trated that ViTs tend to converge to minima with sharp regions, usually related to limited generalization capabil-ity and overfitting problems [7, 25]. Therefore, the typical training scheme of Transformers in NLP [2, 17] relies on the large-scale pre-training and then fine-tuning for down-stream tasks, which consume enormous GPU (TPU) time and energy [4, 18, 45]. For instance, Dosovitskiy et al. [18] spend thousands of TPU days to pre-train a ViT with 303M images. Spontaneously, it raises the following question: how can we optimize ViTs efficiently without pre-training.
To the best of our knowledge, existing approaches fo-cused on the problem can be mainly divided into two parts. The first line of approaches attempts to bring in-ductive biases back into Transformers, such as sparse atten-tion [6, 12, 27] and token aggregation [52]. Such heuristic modifications to ViTs will inevitably lead to the sophisti-cated tuning of plenty of hyperparameters. The second line of approaches [8, 24, 45] aims at constructing suitable train-ing schemes for Transformers, which helps them converge
In particular, Chen et with better generalization ability. al. [8] utilize the sharpness-aware minimizer (SAM) [20] to find smooth minima, while [24, 45] optimize a Trans-former by distilling knowledge from a pre-trained teacher. these methods still require pre-training on
Nonetheless, mid-sized datasets, such as ImageNet-1k [29], and how to efficiently train ViTs with relatively small datasets from scratch remains an open question.
Motivated by the distillation approaches [22, 24, 45] that utilize a teacher model to guide the optimization direction of the student, in this paper, we strive to make one step further towards optimizing ViTs with the help of an agent CNN, which also learns from scratch along with the ViT. Our goal is to inject the inductive biases from the agent CNN into the ViT without modifying its architecture and design a more friendly optimization process so that ViTs can be customized on small-scale datasets without pre-training.
To this end, we propose a novel optimization strategy for training vision Transformers in the bootstrapping form so that even without pre-training on mid-sized datasets or strong data augmentations, ViTs can still be competitive when lack of training data. Specifically, as shown in Fig. 1, we first propose an agent CNN designed corresponding to the given ViT, and with the inductive biases, the agent will converge faster than the ViT. Then we jointly optimize the
ViT along with the agent in the mutual learning frame-work [55], where the intermediate features of the agent su-pervise the ViT with the inductive biases for fast conver-gence. In order to reduce the training burden, we further share the parameters of the ViT to the agent and propose a bootstrapping learning algorithm to update the shared pa-rameters. We have conducted extensive experiments on
CIFAR-10/100 datasets [28] and ImageNet-1k [29] under the lack-of-data settings. Experimental results demonstrate that: (1) our method has successfully injected the induc-tive biases to ViTs as they converge significantly faster than training from scratch and eventually surpass both the agents and SOTA CNNs; (2) the bootstrapping learning method can efficiently optimize the shared weights without the ex-tra set of parameters.
Our contributions are summarized as three folds: 1. We propose agent CNNs constructed based on stan-dard ViTs, for training ViTs efficiently with shared weights and inductive biases. 2. We propose a novel bootstrapping optimization algo-rithm to optimize the shared parameters. 3. Our experiments show that ViTs can outperform the
SOTA CNNs even without pre-training by adopting both inductive biases and suitable optimizing goals. 2.