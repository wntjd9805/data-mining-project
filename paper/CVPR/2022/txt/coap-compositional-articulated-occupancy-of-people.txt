Abstract 1.

Introduction
We present a novel neural implicit representation for articulated human bodies. Compared to explicit template meshes, neural implicit body representations provide an ef-ﬁcient mechanism for modeling interactions with the envi-ronment, which is essential for human motion reconstruc-tion and synthesis in 3D scenes. However, existing neu-ral implicit bodies suffer from either poor generalization on highly articulated poses or slow inference time. In this work, we observe that prior knowledge about the human body’s shape and kinematic structure can be leveraged to improve generalization and efﬁciency. We decompose the full-body geometry into local body parts and employ a part-aware encoder-decoder architecture to learn neural articu-lated occupancy that models complex deformations locally.
Our local shape encoder represents the body deformation of not only the corresponding body part but also the neigh-boring body parts. The decoder incorporates the geometric constraints of local body shape which signiﬁcantly improves pose generalization. We demonstrate that our model is suitable for resolving self-intersections and collisions with 3D environments. Quantitative and qualitative experiments show that our method largely outperforms existing solutions in terms of both efﬁciency and accuracy.
Computers can perceive rich representations of 3D hu-man pose, shape, and motion by regressing the latent pa-rameters of parametric human body models [25, 35, 54].
Conventionally, such generative human body models are represented as polygonal meshes and are easy to deform and animate by leveraging skinning algorithms such as lin-ear blend skinning (LBS) [14]. However, they are not well suited for efﬁcient interactions with 3D graphics environ-ment and resolving self-intersections.
Unlike meshes, neural implicit representations [6,28,48] are ﬂexible, continuous, and support efﬁcient intersection tests with the environment. The state-of-the-art neural im-plicit body models [28, 42, 52] learn an inverse LBS net-work to convert an arbitrary point in 3D space to the canon-ical space where identity- and pose-dependent surface de-formations are modeled. While being effective in captur-ing surface deformations in the canonical space, the learned inverse LBS networks often suffer from poor generaliza-tion capability to highly articulated unseen poses (Fig. 1).
SNARF [6] circumvents the need of learning the inverse
LBS network by formulating the inverse mapping as a root-ﬁnding problem. However, the model is learned per sub-ject, and the computationally expensive root-ﬁnding pre-vents the practical application of their method for human body reconstruction in 3D scenes. In this work, we present a novel part-aware encoder-decoder architecture that mod-els compositional neural occupancy representations which are robust, efﬁcient, and can generalize to a large variety of body shapes and highly articulated body poses. We name it
COAP (COmpositional Articulated occupancy of People).
COAP is inspired by two key insights: First, the learned inverse LBS function in LEAP [28] captures spurious long-range correlations, making it hard to generalize to highly articulated unseen poses. To address this, we get rid of the learned LBS and propose a novel local shape encoding that models the neural occupancy of articulated body parts by using a localized context of direct neighbors in the kine-matic chain. This localized way of representing the body and its deformations reduces overﬁtting to the spurious cor-relations in the training set. Furthermore, given the local part encoding, the ﬁnal whole human body is represented as a composition of these predicted local neural ﬁelds. In-stead of a simple per-part combination as in NASA [8], each local encoding in COAP contributes not only to the cor-responding body part but also to the deformations of the neighboring body parts. Overall, the compositional neural
ﬁelds modeled by the part-aware encoder-decoder architec-ture are effective and greatly beneﬁt generalization (Sec. 5).
Second, prior knowledge about human shapes that is car-ried by the parametric body models can signiﬁcantly ease the task of learning robust neural representations. Similar to
LEAP [28], we use SMPL [25] as the starting point. Given the input bone transformations, we can effectively extract the relevant local body vertex positions. We leverage the per-part body vertices to create simple geometric primitives (such as 3D boxes) and incorporate them in the neural net-work architecture. This can be considered as a geometric prior of a local body shape which simpliﬁes the learning problem and helps the neural network to properly allocate its modeling capacity around the surface. As demonstrated in our experiment, the effective fusion of the geometric prior and the learning power of neural networks is vital for the generalization capability of the learned representations.
We systematically evaluate the robustness and the rep-resentation power of COAP. We compare with SNARF [6] that is trained per subject and shows impressive results on unseen poses [1]. COAP achieves even better performance while at the same time being more efﬁcient in terms of in-ference time. We also compare with LEAP [28] and Neural-GIF [48] that produce generalizable neural implicit bodies.
Once again, COAP signiﬁcantly outperforms their results on the PosePrior [1] and the DFaust [5] datasets.
Resolving self-interpenetration of deformable 3D shapes is challenging and has been a long-standing question in computer graphics and vision [4, 11, 18, 35, 37, 40, 50].
We propose a simple, yet effective optimization algo-rithm based on COAP that can efﬁciently resolve self-interpenetration among different body parts. Our method can reliably solve the challenging cases that are not ad-dressed by existing solutions [35] (as shown in Sec. 5.3).
Furthermore, we demonstrate the utility of COAP for re-solving collisions with 3D environments. Prior work [13, 57] requires pre-computed signed distance ﬁelds (SDFs) of 3D scenes to perform collision detection between 3D hu-man bodies and the scene geometry, which is cumbersome and does not scale to scenes with moving objects or hu-mans. Our robust and generalizable neural body model can be used to directly detect collisions with raw scans to im-prove 3D pose and shape estimation (Sec. 5.3).
Contributions.
In summary, our main contributions are: (1) a novel neural implicit body model that is robust and efﬁcient, and can generalize to a large variety of human shapes and highly articulated body poses; (2) an effective localized encoder-decoder architecture that leverages local shape encoding and geometric shape priors to learn com-positional neural body representations; and, (3) simple and efﬁcient optimization algorithms that reliably resolve chal-lenging self-interpenetration and human-scene interpenetra-tion. Code and models are public1. 2.