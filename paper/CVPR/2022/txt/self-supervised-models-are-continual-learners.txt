Abstract
Self-supervised models have been shown to produce comparable or better visual representations than their su-pervised counterparts when trained offline on unlabeled data at scale. However, their efficacy is catastrophically reduced in a Continual Learning (CL) scenario where data is presented to the model sequentially.
In this paper, we show that self-supervised loss functions can be seamlessly converted into distillation mechanisms for CL by adding a predictor network that maps the current state of the repre-sentations to their past state. This enables us to devise a framework for Continual self-supervised visual representa-tion Learning that (i) significantly improves the quality of the learned representations, (ii) is compatible with several state-of-the-art self-supervised objectives, and (iii) needs little to no hyperparameter tuning. We demonstrate the ef-fectiveness of our approach empirically by training six pop-ular self-supervised models in various CL settings. Code: github.com/DonkeyShot21/cassle. 1.

Introduction
During the last few years, self-supervised learning (SSL) has become the most popular paradigm for unsupervised vi-sual representation learning [3, 7, 8, 13, 14, 25, 27, 54]. In-deed, under certain assumptions (e.g., offline training with large amounts of data and resources), SSL methods are able to extract representations that match the quality of represen-tations obtained with supervised learning, without requir-ing annotations. However, these assumptions do not always hold in real-world scenarios, e.g., when new unlabeled data are made available progressively over time. In fact, in order to integrate new knowledge into the model, training needs to be repeated on the whole dataset, which is impractical, expensive, and sometimes even impossible when old data is not available. This issue is exacerbated by the fact that SSL models are notoriously computationally expensive to train.
*Enrico Fini and Victor G. Turrisi da Costa contributed equally.
â€ Univ. Grenoble Alpes, CNRS, Grenoble INP, LJK, 38000 Grenoble, France.
Figure 1. Linear evaluation accuracy of representations learned with different self-supervised methods on class-incremental CI-FAR100 and ImageNet100.
In blue the accuracy of SSL fine-tuning, in green the improvement brought by CaSSLe. The red dashed line is the accuracy attained by supervised fine-tuning.
Continual learning (CL) studies the ability of neural net-works to learn tasks sequentially. Prior art in the field fo-cuses on mitigating catastrophic forgetting [17, 22, 24, 37].
Common benchmarks in the CL literature evaluate the dis-criminative performance of classifiers learned with super-vision from non-stationary distributions. In this paper, we tackle the same forgetting phenomenon in the context of
SSL. Unsupervised representation learning is indeed ap-pealing for sequential learning since it does not require hu-man annotations, which are particularly hard to obtain when new data is generated on-the-fly. This setup, called Contin-ual Self-Supervised Learning (CSSL), is surprisingly under-investigated in the literature.
In this work, we propose CaSSLe, a simple and effec-tive framework for CSSL of visual representations based on the intuition that SSL models are intrinsically capable of learning continually, and that SSL losses can be seamlessly converted into distillation losses. Our key idea is to train the 1
current model to predict past representations with a predic-tion head, thus encouraging it to remember past knowledge.
CaSSLe has several favourable features: (i) it is compatible with popular state-of-the-art SSL loss functions and archi-tectures, (ii) it is simple to implement, and (iii) it does not require any additional hyperparameter tuning with respect to the original SSL method. Our experiments demonstrate that SSL methods trained continually with CaSSLe signifi-cantly outperform all the related methods (CSSL baselines and several methods adapted from supervised CL).
We also perform a comprehensive analysis of the behav-ior of six popular SSL methods in diverse CL settings (i.e., class, data, and domain incremental). We provide empiri-cal results on small (CIFAR100), medium (ImageNet100), and large (DomainNet) scale datasets. Our study sheds new light on interesting properties of SSL methods that emerge when learning continually. Among other findings, we dis-cover that, in the class-incremental setting, SSL methods typically approach or outperform supervised learning (see
Fig.1), while this is not generally true for other settings (data-incremental and domain-incremental) where super-vised learning still shows a sizeable advantage. 2.