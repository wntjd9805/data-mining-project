Abstract
Pan-sharpening aims to integrate the complementary in-formation of texture-rich PAN images and multi-spectral (MS) images to produce the texture-rich MS images. De-spite the remarkable progress, existing state-of-the-art Pan-sharpening methods don’t explicitly enforce the comple-mentary information learning between two modalities of
PAN and MS images. This leads to information redun-dancy not being handled well, which further limits the per-formance of these methods. To address the above issue, we propose a novel mutual information-driven Pan-sharpening framework in this paper. To be specific, we first project the
PAN and MS image into modality-aware feature space in-dependently, and then impose the mutual information min-imization over them to explicitly encourage the comple-mentary information learning. Such operation is capable of reducing the information redundancy and improving the model performance. Extensive experimental results over multiple satellite datasets demonstrate that the proposed al-gorithm outperforms other state-of-the-art methods qualita-tively and quantitatively with great generalization ability to real-world scenes. 1.

Introduction
With the rapid development of remote sensors, explosive satellite images are available for a wide range of applica-tions like military systems, environmental monitoring, and mapping services. Due to the physical limitations, satellites usually capture both multi-spectral (MS) and panchromatic (PAN) sensors to simultaneously obtain complementary in-formation. To be specific, MS images possess high spectral but limited spatial resolution while PAN images have rich spatial information but low spectral resolution. To generate images with both high spectral and spatial resolutions, the
*Co-first authors contributed equally, † corresponding author. This work was supported in part by the National Natural Science Foundation of
China (NSFC) under Grant 61901433; and in part by the USTC Research
Funds of the Double First-Class Initiative under Grant YD2100002003.
Figure 1. The categorization of existing Pan-sharpening methods.
Figure 2. The information redundancy reduction after integrating mutual information minimization constraint, and the average fea-tures of P2 and M2 are visualized, defined in Figure 4.
Pan-sharpening technique by fusing the MS and PAN im-ages has drawn increasing attention from both image pro-cessing and remote sensing communities.
Treated as a fusion task, considerable Pan-sharpening methods have been developed with two main fusion strate-gies: 1) image-level fusion and 2) feature-level fusion. As shown in Figure 1 (a), the first category directly concate-nates the MS and PAN images along the channel dimension
before feeding them into the networks. Without conducting explicitly cross-modal fusion, the ’input fusion’ strategy is therefore limited in studying the complementary informa-tion, leading to unsatisfactory performance. The second cat-egory attempts to extract the modality-aware features from
PAN and MS images independently, and then performs the information fusion in feature space, as shown in Figure 1 (b). Although encouraging improvement has been achieved, it still suffers from the following issue. Since PAN and
MS images capture the same scene in different modalities, they contain shared information as well as unique features, as demonstrated in Figure 3. However, existing state-of-the-art Pan-sharpening methods don’t explicitly enforce the complementary information learning between two modali-ties of PAN and MS images, resulting in the redundancy of learned features and the so-called copy artifacts [5, 12, 46].
Considering the limitation of the current methods, in this paper, we make our efforts to enforce the complementary feature learning and reduce the information redundancy for improving the Pan-sharpening performance.
As shown in Figure 1 (c), we propose a novel mu-tual information-driven Pan-sharpening framework in a cas-caded manner, and the detailed flowchart is illustrated in
Figure 4. The MS and PAN images are firstly fed into two independent convolution branches for obtaining the modality-aware features, and then we impose the mutual information minimization over them to encourage the com-plementary information learning from the shallow to deep levels. To be specific, the obtained modality-aware fea-tures are further converted to low-dimensional feature vec-tors for calculating the mutual information where the latter-level feature vectors are obtained depending on two-folds: 1) the current-layer modality features and 2) the previous-layer immediate-process feature in feature vector calculat-ing. Such operation is capable of reducing the information redundancy, visualized in Figure 2. After obtaining the re-fined features, a post-fusion module is devised for project-ing them back to the expected MS images by equipping with effective invertible neural networks. Extensive experimen-tal results over multiple satellite datasets demonstrate that the proposed algorithm outperforms other state-of-the-art methods qualitatively and quantitatively with great gener-alization ability to real-world scenes. Ablation studies also verify the effectiveness of the proposed components.
The contributions of this work are as follows:
• We design a novel Pan-sharpening framework by mu-tual information minimization in a cascaded manner.
To the best of our knowledge, this is the first attempt to explicitly encourage the multi-modal learning between
MS and PAN modalities. The proposed model reduces the information redundancy and alleviate the artifacts in Pan-sharpening.
Figure 3. The decomposed components of the PAN and its corre-sponding MS image by the technique [5] .
• Extensive experimental results over multiple satellite datasets demonstrate the superiority of the proposed algorithm against other state-of-the-art methods. The great generalization ability is also verified over real-world full-resolution satellite scenes. 2.