Abstract
By leveraging contrastive learning, clustering, and other pretext tasks, unsupervised methods for learning image rep-resentations have reached impressive results on standard benchmarks. The result has been a crowded field – many methods with substantially different implementations yield results that seem nearly identical on popular benchmarks, such as linear evaluation on ImageNet. However, a single result does not tell the whole story. In this paper, we com-pare methods using performance-based benchmarks such as linear evaluation, nearest neighbor classification, and clustering for several different datasets, demonstrating the lack of a clear front-runner within the current state-of-the-art.
In contrast to prior work that performs only super-vised vs. unsupervised comparison, we compare several different unsupervised methods against each other. To en-rich this comparison, we analyze embeddings with mea-surements such as uniformity, tolerance, and centered ker-nel alignment (CKA), and propose two new metrics of our own: nearest neighbor graph similarity and linear predic-tion overlap. We reveal through our analysis that in iso-lation, single popular methods should not be treated as though they represent the field as a whole, and that future work ought to consider how to leverage the complimentary nature of these methods. We also leverage CKA to provide a framework to robustly quantify augmentation invariance, and provide a reminder that certain types of invariance will be undesirable for downstream tasks. 1.

Introduction
Image features are critical components in many com-puter vision (CV) pipelines. In this paper, we define im-age features, also referred to as embeddings, encodings, or representations, as an n-dimensional vector that represents the content of an image. With the emergence of deep learn-ing, classical approaches to computing image features have been supplanted by neural networks that use large amounts of data to generate powerful image representations. The most widespread method is straightforward: a neural net-Figure 1. Results for a sample of classification benchmarks we perform in this paper. While these bar charts report real results, lack of axes is intentional – the exact numbers are in Section 4. Im-portantly, between the four tasks, there is no clear “best” method. work (e.g., a ResNet50 [30]) is trained to classify the im-ages in some large dataset, typically ImageNet. The portion of the network that performs the classification, usually just the final layer, is then removed, and the outputs of the penul-timate layer for a given image are considered the features for that image. This process relies on image classification, a supervised learning task, and thus requires the availability of large amounts of annotated, high-quality data.
Recent successes make unsupervised learning a viable alternative paradigm where image features are learned with-out the need for class labels. Within unsupervised learning, methods can be considered either generative or discrimi-native. Generative methods are typically designed for re-construction or similar tasks [5, 19, 20, 35, 51]. Since we are more concerned with a potential transfer to downstream tasks such as image classification and object detection, we
Figure 2. Similarity between learned representations, based on the outputs of ResNet50s on the validation images from ImageNet. For the metrics shown, which are described in more detail in Section 4, higher values indicate similarity. While the supervised model tends to be more dissimilar from the unsupervised models, there are many ways in which unsupervised methods differ substantially from each other. choose to focus on discriminative methods.
There are many different ways to compare image repre-sentation learning algorithms. In this paper, we opt to fo-cus on the role of the methods as feature extractors, where a model that is pre-trained for some task is expected to be able to generate useful features for unseen images. Thus, we only use benchmarks that keep the backbone (the portion of the neural network that generates the embedding) frozen.
Prior works are often limited by their focus on a single benchmark, single method, or toy datasets. In contrast, we compare 6 SOTA unsupervised methods on ImageNet and 6 fine-grained visual categorization (FGVC) datasets using several different benchmarks. Figure 1 provides a sample of this angle of analysis.
Comparing these methods to each other is very impor-tant. Nevertheless, prior analysis works tend to lump unsu-pervised methods together, and often choose only a single representative such as MoCo or SimCLR for comparison against supervised representation learning [1,14,22,26,54].
This ignores the significant ways in which unsupervised methods differ from each other. In contrast to prior work, we extend existing methods and introduce novel methods to prove that unsupervised methods vary significantly in terms of how they learn to represent images, as shown in Figure 2.
State-of-the-art convolution-based unsupervised algo-rithms, whether they use contrastive learning, clustering, or some pretext task such as colorization, all attempt to learn invariance to some class of augmentations. In other words, they seek to learn a function f , such that f (I) = f (IA) for some image I and some set of augmentations A that are applied to that image. Xiao et al. speculate on the negative effect this may have on learned representations and perfor-mance on downstream tasks [56]. However, a careful read-ing reveals they don’t provide evidence for the existence of transform invariance in unsupervised models, only that their method seems to perform better than MoCo on tasks related to transform invariance. This inspires Section 4.4 – we take a closer look at the presence of augmentation invariance in representations learned by different unsupervised methods.
Prior work is constrained by some combination of lim-ited metrics, use of toy datasets, and a tendency to consider a single unsupervised method as though it is representative of the field. In contrast to this, we contribute the following:
• We utilize multiple methods for measuring proper-ties of learned embeddings, including 3 performance-based benchmarks, and an extension of prior work on uniformity-tolerance analysis to more unsupervised methods across more realistic datasets.
• We perform novel comparison by extending Linear
Centered Kernel Alignment (CKA) analysis beyond toy datasets, and by developing two new metrics for comparing embeddings: nearest neighbor graph simi-larity and linear overlap.
• We propose a framework for measuring augmentation invariance, and demonstrate its results across several methods, augmentations, and datasets.
We conclude in Section 5 with key insights for future unsu-pervised methods for representation learning:
• Currently, there is no clear “best” method.
• Unsupervised models share properties that are circum-stantially undesirable, e.g., color invariance.
• Unsupervised models have similar representations in most layers, but diverge substantially in the last layer.
2.