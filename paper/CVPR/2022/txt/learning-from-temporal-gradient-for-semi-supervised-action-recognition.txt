Abstract
Semi-supervised video action recognition tends to en-able deep neural networks to achieve remarkable perfor-mance even with very limited labeled data. However, ex-isting methods are mainly transferred from current image-based methods (e.g., FixMatch). Without specifically uti-lizing the temporal dynamics and inherent multimodal at-tributes, their results could be suboptimal. To better lever-age the encoded temporal information in videos, we intro-duce temporal gradient as an additional modality for more attentive feature extraction in this paper. To be specific, our method explicitly distills the fine-grained motion represen-tations from temporal gradient (TG) and imposes consis-tency across different modalities (i.e., RGB and TG). The performance of semi-supervised action recognition is sig-nificantly improved without additional computation or pa-rameters during inference. Our method achieves the state-of-the-art performance on three video action recognition benchmarks (i.e., Kinetics-400, UCF-101, and HMDB-51) under several typical semi-supervised settings (i.e., differ-ent ratios of labeled data). Code is made available at https://github.com/lambert-x/video-semisup. 1.

Introduction
As a fundamental task for video understanding, video ac-tion recognition has drawn much attention from the commu-nity and industry [5, 9, 45, 47]. Unlike image-related tasks, networks for video-related tasks are normally easier to over-fit due to the complexity of the tasks [23, 45, 46]. The com-mon practice is to firstly pre-train the network on large-scale datasets (e.g., Kinetics [4] of up to 650, 000 video clips) and then finetune on downstream small datasets to obtain better performance [9, 15, 32, 33].
However, since annotating large-scale video datasets is time-consuming and expensive, training models on a large dataset collected with complete annotations is impeded. To utilize large-scale datasets with acceptable costs, some re-searchers have turned to designing semi-supervised learning
Figure 1. Top: A sketch diagram that describes the formulation of different modalities (i.e., RGB, fast and slow temporal gradi-ent (TG)). Bottom: A comparison of Top-1 accuracy with Fix-Match [38] as the baseline of semi-supervised learning method.
The chart compares the performance of different input modalities (i.e., RGB, slow TG and fast TG). The remarkable performance with TG motivates us to figure out a way to efficiently utilize this fruitful modality. By distilling knowledge from temporal gradient to RGB, our model is able to significantly outperform the models taking either temporal gradient or RGB frames as input. models which have good generalization ability with limited annotations [21, 37, 55, 58]. As pseudo-label based meth-ods (e.g., FixMatch [38] and MixMatch [2]) have shown outstanding performance on semi-supervised image clas-sification, most previous video-based methods are heavily built on them to utilize the unlabeled data. Although these preliminary attempts have obtained acceptable results, most methods [21, 58] are just taking video clips as ‘images’ in 3D without further consideration of the video properties.
Videos are significantly different from images, and the
key differences are the temporal information span in mul-tiple frames and the inherent multimodal property. The temporal information refers to the motion signal between frames, and usually the features of contiguous frames from the same video change smoothly. The multimodal consis-tency expects the features extracted from the same video clip to be consistent, as they encode identical content. With-out special designs to specifically focus on temporal infor-mation and multimodal consistency, the potential of semi-supervised action recognition is not fully unleashed.
Some previous studies [51, 57] introduce temporal gra-dient1 as an additional modality to better utilize the tem-poral information encoded in videos as it is rich in mo-tion signals. The temporal gradient can be formulated as:
T G = xRGB t+n , where x represents a video, t de-notes the frame index and n denotes the interval for calcu-lating temporal gradient.
− xRGB t
Inspired by these studies, we made a trial with temporal gradient under the semi-supervised settings and found that a much better performance could be generated when the in-put frames in RGB are replaced with temporal gradient. As shown in Figure 1, the Top-1 accuracy of temporal gradient is ∼25% higher than using the RGB as input on the UCF-101 dataset with only 20% of labeled data for training.
Why is the temporal gradient so much better than the
RGB frames when the training data are limited? We hy-pothesize that the key is in the detailed and fine-grained mo-tion signals encoded in the temporal gradient. The gradient along the temporal dimension is color-invariant and explic-itly encodes the representative motion information of the ac-tions in the video. This helps models generalize much eas-ier when the labels are extremely limited. Therefore, in this paper, we propose to train a semi-supervised action recog-nition RGB based model to mimic both the fine-grained and high-level features from the temporal gradient.
We start from FixMatch [38], a typical pseudo-label based semi-supervised model, as the baseline framework.
However, without any further constraints in the feature level, pseudo-label based methods perform poorly in the case of very limited labels, as many generated pseudo-labels are inaccurate. Therefore, we propose two constraints to help the model extract temporal information in video with multiple modalities and improve the consistency between the multimodal representations. To leverage the detailed and fine-grained motion signals from temporal gradient, we propose a knowledge distillation strategy using block-wise dense alignment. It helps the student RGB model learn from the teacher temporal gradient model efficiently and effec-tively. To further improve high-level representation space across different modalities, we perform contrastive learning between the features from RGB and temporal gradient se-quences to enforce the high-order similarity. Given the two 1The difference between two RGB video frames with a short interval. constraints at the feature level, our proposed model is able to achieve much better performance.
Unlike the existing methods, our model has two unique advantages. First, our model requires no additional com-In the training, we putation or parameters for inference. distill the knowledge from temporal gradient to the RGB-based network; in the testing, only the RGB model is re-quired. Second, our model is simple, yet effective. We con-ducted experiments on multiple public action recognition benchmarks including UCF-101, HMDB-51, and Kinetics-400. Our proposed method significantly outperforms all the state-of-the-art methods by a large margin. 2.