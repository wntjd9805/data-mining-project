Abstract
Federated learning (FL) allows multiple clients to col-lectively train a high-performance global model without sharing their private data. However, the key challenge in federated learning is that the clients have signiﬁcant sta-tistical heterogeneity among their local data distributions, which would cause inconsistent optimized local models on the client-side. To address this fundamental dilemma, we propose a novel federated learning algorithm with local drift decoupling and correction (FedDC). Our FedDC only introduces lightweight modiﬁcations in the local training phase, in which each client utilizes an auxiliary local drift variable to track the gap between the local model parameter and the global model parameters. The key idea of FedDC is to utilize this learned local drift variable to bridge the gap, i.e., conducting consistency in parameter-level. The exper-iment results and analysis demonstrate that FedDC yields expediting convergence and better performance on various image classiﬁcation tasks, robust in partial participation settings, non-iid data, and heterogeneous clients. 1.

Introduction
Federated learning (FL) is an emerging distributed ma-chine learning paradigm that leverages decentralized data from multiple clients to jointly train a shared global model under the coordination of a central server, without sharing the individuals’ raw data [4, 8, 19, 20, 31]. This makes FL surpass traditional parallel optimization to avoid systemic privacy risk [5, 7, 15, 16, 24]. FedAvg [19] is a widely used
FL aggregation algorithm, in which each client executes multiple stochastic gradient descent (SGD) steps in each communication round to minimize the local empirical risk.
After that, a central server updates the parameters of the global model with the updates returned by the clients. How-ever, recent researches [9, 17, 18] demonstrate that FedAvg could not converge well with heterogeneous data (non-iid).
§Li Li (LLiLi@um.edu.mo) and Yingwen Chen (ywch@nudt.edu.cn) are corresponding authors.
The data distribution of clients in FL can be highly differ-ential because clients independently collect the local data with their own preferences and sampling space. The non-iid distributed data leads to inconsistency in clients’ local objective functions and optimization directions. The stud-ies in [9, 10] prove that the data heterogeneity introduces drift in clients’ local updates, which slows down the con-vergence speed. The parameter drift between an FL model and a centralized learning model comes from two parts: the residual parameter drift in the last round, and the gradient drift in the current round [30]. Due to the difference in data distribution, there is a fundamental contradiction between minimizing local empirical loss and reducing global empir-ical loss. Therefore, in a highly heterogeneous environment,
FedAvg lacks a convergence guarantee, which only obtains compromised convergence speed and model performance.
To address this client drift, some methods have been pro-posed to reduce the variance of local updates [9, 17]. For example, FedProx [17] adds a proximal term to force re-duction of model differences between local and the global model. However, the proximal term hinders the global model from moving towards the global stationary point.
Scaffold [9] corrects client-drift with a control gradient vari-ate. However, it only approximately reduces the gradient drift in each round but it is not able to eliminate it. The residual deviation would be accumulatively ampliﬁed dur-ing training according to the research of [30], which is the primary factor that slows down the convergence speed and causes lower performance.
In fact, most of the previous
FL methods force the local models to be consistent to the global model. They ﬁnally get a model that neglects the in-consistency between local objectives and global objectives.
They have a certain effect by reducing gradient drift, but the gradually enlarged parameter deviation persists.
We admit the fact that the local optimal points of clients are fundamentally inconsistent with the global optimal point in the heterogeneous FL setup. The local stationary points of clients can be arbitrarily different from the global stationary point. Based on this observation, we propose a new federated learning algorithm with local drift decou-pling and correction (FedDC), to handle the inconsistent
objectives with auxiliary drift variables to track the local parameter drift between the local models and the global model. Our FedDC dynamically updates the local ob-jective function of each client, which contains (1) a con-straint penalty term that indicates the relationship among the global parameter, drift variables and the local parame-ters, and (2) a gradient correction term to reduce the gra-dient drift in each training round. We decouple the local models and the global model in the training process by in-troducing the drift variables, which reduces the impact of the local drift to the global objective and makes it converge quickly and reach better performance. We execute experi-ments on several public datasets, including MNIST, fash-ion MNIST, CIFAR10, CIFAR100, EMNIST-L, tiny Im-ageNet and a synthetic dataset. The results demonstrate that our FedDC achieves the best performance and signif-icantly faster converge speed compared with the competing
FL methods (e.g., FedAvg [19], FedProx [17], Scaffold [9] and FedDyn [1]) in both iid and non-iid client settings*. 2.