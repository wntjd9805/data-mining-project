Abstract
Recently, GAN inversion methods combined with Con-trastive Language-Image Pretraining (CLIP) enables zero-shot image manipulation guided by text prompts. However, their applications to diverse real images are still difficult due to the limited GAN inversion capability. Specifically, these approaches often have difficulties in reconstructing images with novel poses, views, and highly variable contents compared to the training data, altering object identity, or pro-ducing unwanted image artifacts. To mitigate these problems
This research was supported by Field-oriented Technology Develop-ment Project for Customs Administration through the National Research
Foundation of Korea(NRF) funded by the Ministry of Science & ICT and
Korea Customs Service (NRF-2021M3I1A1097938), and supported by the Institute of Information & communications Technology Planning &
Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2019-0-00075, Artificial Intelligence Graduate School Program (KAIST)). and enable faithful manipulation of real images, we propose a novel method, dubbed DiffusionCLIP, that performs text-driven image manipulation using diffusion models. Based on full inversion capability and high-quality image generation power of recent diffusion models, our method performs zero-shot image manipulation successfully even between unseen domains and takes another step towards general applica-tion by manipulating images from a widely varying ImageNet dataset. Furthermore, we propose a novel noise combination method that allows straightforward multi-attribute manipula-tion. Extensive experiments and human evaluation confirmed robust and superior manipulation performance of our meth-ods compared to the existing baselines. Code is available at https://github.com/gwang-kim/DiffusionCLIP.git 1.

Introduction
Recently, GAN inversion methods [1â€“4, 7, 32, 40] com-bined with Contrastive Language-Image Pretraining (CLIP)
[29] has become popular thanks to their ability for zero-shot image manipulation guided by text prompts [16, 28]. Never-theless, its real-world application on diverse types of images is still tricky due to the limited GAN inversion performance.
Specifically, successful manipulation of images should convert the image attribute to that of the target without un-intended changes of the input content. Unfortunately, the current state-of-the-art (SOTA) encoder-based GAN inver-sion approaches [3, 32, 40] often fail to reconstruct images with novel poses, views, and details. For example, in the left panel of Fig. 1(a), e4e [40] and ReStyle [3] with pSp en-coder [32] fail to reconstruct unexpected hand on the cheek, inducing the unintended change. This is because they have rarely seen such faces with hands during the training phase.
This issue becomes even worse in the case of images from a dataset with high variance such as church images in LSUN-Church [46] and ImageNet [35] dataset. As shown in the right panel of Fig. 1(a) for the conversion to a department store, existing GAN inversion methods produce artificial architectures that can be perceived as different buildings.
Recently, diffusion models such as denoising diffusion probabilistic models (DDPM) [18,36] and score-based gener-ative models [38, 39] have achieved great successes in image generation tasks [18, 19, 37, 39]. The latest works [14, 39] have demonstrated even higher quality of image synthesis performance compared to variational autoencoders (VAEs)
[24,27,30], flows [15,23,31], auto-regressive models [26,41], and generative adversarial networks (GANs) [6, 17, 21, 22].
Furthermore, a recent denoising diffusion implicit models (DDIM) [37] further accelerates sampling procedure and enables nearly perfect inversion [14].
Inspired by this, here we propose a novel DiffusionCLIP
- a CLIP-guided robust image manipulation method by dif-fusion models. Here, an input image is first converted to the latent noises through a forward diffusion. In the case of DDIM, the latent noises can be then inverted nearly per-fectly to the original image using a reverse diffusion if the score function for the reverse diffusion is retained the same as that of the forward diffusion. Therefore, the key idea of DiffusionCLIP is to fine-tune the score function in the reverse diffusion process using a CLIP loss that controls the attributes of the generated image based on the text prompts.
Accordingly, DiffusionCLIP can successfully perform image manipulation both in the trained and unseen domain (Fig. 1(a)). We can even translate the image from an unseen domain into another unseen domain (Fig. 1(b)), or gener-ate images in an unseen domain from the strokes (Fig. 1(c)).
Moreover, by simply combining the noise predicted from sev-eral fine-tuned models, multiple attributes can be changed si-multaneously through only one sampling process (Fig. 1(d)).
Furthermore, DiffsuionCLIP takes another step towards gen-eral application by manipulating images from a widely vary-ing ImageNet [35] dataset (Fig. 6), which has been rarely explored with GAN-inversion due to its inferior reconstruc-tion. [5, 12]
Additionally, we propose a systematic approach to find the optimal sampling conditions that lead to high quality and speedy image manipulation. Qualitative comparison and human evaluation results demonstrate that our method can provide robust and accurate image manipulation, outper-forming SOTA baselines. 2.