Abstract
Optimization of Top-1 ImageNet promotes enormous net-works that may be impractical in inference settings. Binary neural networks (BNNs) have the potential to signiﬁcantly lower the compute intensity but existing models suffer from low quality. To overcome this deﬁciency, we propose Poke-Conv, a binary convolution block which improves quality of BNNs by techniques such as adding multiple residual paths, and tuning the activation function. We apply it to
ResNet-50 and optimize ResNet’s initial convolutional layer which is hard to binarize. We name the resulting network family PokeBNN1. These techniques are chosen to yield fa-vorable improvements in both top-1 accuracy and the net-work’s cost.
In order to enable joint optimization of the cost together with accuracy, we deﬁne arithmetic compu-tation effort (ACE), a hardware- and energy-inspired cost metric for quantized and binarized networks. We also iden-tify a need to optimize an under-explored hyper-parameter controlling the binarization gradient approximation.
We establish a new, strong state-of-the-art (SOTA) on top-1 accuracy together with commonly-used CPU64 cost,
ACE cost and network size metrics. ReActNet-Adam [33], the previous SOTA in BNNs, achieved a 70.5% top-1 ac-curacy with 7.9 ACE. A small variant of PokeBNN achieves 70.5% top-1 with 2.6 ACE, more than 3x reduction in cost; a larger PokeBNN achieves 75.6% top-1 with 7.8 ACE, more than 5% improvement in accuracy without increasing the cost. PokeBNN implementation in JAX/Flax [6, 18] and re-production instructions are open sourced.2 1.

Introduction
A need for Pareto optimization. Deep learning re-search is largely driven by benchmarks and metrics. In the
∗Work performed while at Google, equal contribution. 1Poke/’p6kI/ is pronounced similarly to pocket. PokeConv, PokeBNN, and Pokemon are abbreviations of Pocket Convolution, Pocket Binary
Neural Network, and Pocket Monster, respectively. 2Source code and reproduction instructions are available in AQT repos-itory: github.com/google/aqt.
Figure 1. Comparison of BNNs using top-1 and ACE (Sec. 3). past a single metric per benchmark, e.g., top-1 accuracy on ImageNet, was sufﬁcient. Today one needs to account for various model architectures, sizes, and computational costs. This promotes optimizing the Pareto frontier of a quality metric such as top-1 and another cost metric such as FLOPS, latency, or energy consumption.
The choice of the optimization metric. Since the in-dustry is currently the main user of large-scale inference, the cost metric should be correlated to dollar cost per infer-ence. As the ML hardware gets more mature, what becomes evident is that the energy use is the key metric proportional to the inference cost, especially in the data centers. In Sec. 3 we deﬁne a new proxy metric called arithmetic computa-tion effort (ACE), which aims to estimate inference cost ab-stracting of concrete ML hardware.
The impact of quantization and binarization. In nu-merical formats such as int8 or ﬂoat16, the less signiﬁcant bits do not affect a network’s output as much as the more signiﬁcant bits. Yet, processing them consumes the same amount of energy. This might not be optimal. The possible inference cost reduction (or equivalently performance im-provement) with each halving of the quantization bits (e.g., 16b to 8b to 4b to 2b to 1b) is at least 2x (e.g., NVIDIA
Ampere is 8x faster in int1 than in int8 [38]) and up to 4x as estimated by the ACE metric. For comparison, this is signif-icantly larger than the improvements yielded by upgrading the GPU or TPU by one or two generations.
Binarization pushes this beneﬁt to the extreme by replac-ing ﬂoating-point dot products with logical XNOR and bit counting operations. If binary neural networks (BNNs) can reach high quality, they are likely to gain a large footprint for inference both in the data center and at the edge.
BNN optimization is hard. Pioneering modern BNNs used to suffer from a more than 20% top-1 accuracy gap compared to their ﬂoating-point counterparts [23]. Only re-cently BNNs have become comparable in quality to the pop-ular ResNet-18 model [33, 34]. One reason is that BNNs tend to have a chaotic, discontinuous loss landscape that renders their optimization challenging [31, 33]. In fact, for the binarization to work one has to change many things compared to standard DNN practices. BNNs require multi-phase training, approximation of gradients, and various ar-chitectural adjustments that avoid binarization information bottlenecks.
Our main contributions are as follows:
• We propose PokeConv, a binary convolutional block that can substantially improve BNN accuracy. We replace most of the convolutions in ResNet [17] with PokeConv.
• We propose PokeInit block to replace ResNet’s initial convolutional layer that is hard to binarize. PokeInit signiﬁcantly reduces the network’s cost. PokeInit and
PokeConv form the foundation of the PokeBNN family.
• We optimize an under-explored clipping bound hyper-parameter in BNNs that controls the binarization gradi-ent approximation. Ablation in Sec. 6 shows we gain more than 3% in top-1 accuracy through this parameter.
• We motivate and deﬁne a novel hardware and energy inspired cost metric called ACE, which is informed by inference costs on hardware yet at the same time it is agnostic to the existing hardware platforms. ACE improves alignment of the research on energy-efﬁcient neural networks and research on ML hardware. We use
ACE to quantify the inference cost of PokeBNN.
• We empirically show that on ImageNet [43] PokeBNN establishes the Pareto-SOTA of top-1 together with cost metrics: CPU64, ACE, and network size. We improve over the SOTA ReActNet-Adam by 5.1% top-1 at the same ACE cost (Fig. 1). 2.