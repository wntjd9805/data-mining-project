Abstract
The application of deep neural networks (DNNs) on 360-degree images has achieved remarkable progress in the re-cent years. However, DNNs have been demonstrated to be vulnerable to well-crafted adversarial examples, which may trigger severe safety problems in the real-world ap-plications based on 360-degree images. In this paper, we propose an adversarial attack targeting spherical images, called 360-attactk, that transfers adversarial perturbations from perspective-view (PV) images to a ﬁnal adversarial spherical image. Given a target spherical image, we ﬁrst represent it with a set of planar PV images, and then per-form 2D attacks on them to obtain adversarial PV images.
Considering the issue of the projective distortion between spherical and PV images, we propose a distortion-aware attack to reduce the negative impact of distortion on attack.
Moreover, to reconstruct the ﬁnal adversarial spherical im-age with high aggressiveness, we calculate the spherical saliency map with a novel spherical spectrum method and next propose a saliency-aware fusion strategy that merges multiple inverse perspective projections for the same po-sition on the spherical image. Extensive experimental re-sults show that 360-attack is effective for disturbing spheri-cal images in the black-box setting. Our attack also proves the presence of adversarial transferability from Z2 to SO(3) groups. 1.

Introduction
Previous studies have shown that deep neural networks (DNNs) are vulnerable to carefully crafted adversarial ex-amples [10, 28, 36, 40]. Many attack algorithms have been proposed for various tasks, including image classiﬁcation
[16], video caption [39], 3D mesh classiﬁcation [33], and point cloud recognition [43]. However, during the investi-∗Yanwei Liu is the corresponding author. gation of this issue, the security of DNNs applying to spher-ical images has been largely ignored.
Recently, we have observed an increasing number of computer vision problems requiring spherical signals, for instance, omnidirectional RGB-D images generated from panorama cameras [4,17], 360-degree videos captured from sensors on self-driving cars [15, 47], and spherical data pro-jected from the 3D domain [12]. Inspired by the remarkable success of DNNs in various tasks, many approaches have been proposed to apply DNNs on spherical images to solve real world problems, including advanced driver assistance systems (ADAS) [24], autonomous navigation [13, 22, 29], and VR/AR applications [35, 45].
DNNs used for spherical images typically operate in two domains: the spherical domain and the panoramic domain.
The ﬁrst type of models, referred to spherical models, di-rectly dispose the spherical image in the spherical domain
[7, 8, 12], while models on the planar domain, which are called panoramic models, operate on the panorama trans-formed from the spherical image [19, 34, 46].
Due to the extensive applications of spherical images, the vulnerability of DNNs used for applications around them needs further investigation. A straightforward way to gener-ate adversarial spherical images is to attack the spherical or panoramic models in the white-box setting directly. How-ever, these models are difﬁcult to obtain in practice due to their greatly divergent principles, and backpropagation on them is of low efﬁciency, compared to the same operation on standard planar CNNs [34]. Another intractable prob-lem of attacking the panoramic model is that the panora-mas usually suffer from great distortion compared to the raw spherical images, reducing the effect of the added per-turbations when the adversarial panoramas are re-projected to the original spherical domain. Therefore, an efﬁcient at-tack method with less distortion is required. Considering the perspective views of a spherical image are less distorted and can be processed with a simple planar network, in this paper, we propose to generate adversarial spherical exam-ples by disturbing their planar perspective-view (PV) rep-resentations. Speciﬁcally, we simultaneously disturb these
PV images rendered from different positions on the spher-ical image, and reconstruct the adversarial spherical image from them with a re-projection and a fusion method. As our attack is implemented by transferring 2D adversarial pertur-bations to the 3D space without any knowledge about the target model, it can be considered as a black-box attack.
Overall, our contributions are summarized as follows:
• To the best of our knowledge, we are the ﬁrst to propose a black-box attack towards spherical models, called 360-atack, by generating adversarial spherical images from their corresponding PV images. 360-attack is performed directly on the planar domain, and eventually the perturbations are transferred to the spherical images.
• To obtain highly transferable adversarial PV images toward attacking the spherical model, we proposed a novel Distortion-Aware Iterative Fast Gradient Sign
Method (DAI-FGSM) with considering the perturba-tion degradation caused by plane-to-sphere projection distortion. Accordingly, the negative effect of the pro-jective distortion on the attack is alleviated.
• We propose a novel spherical-spectrum-based saliency detection method, and then propose a saliency-aware fusion strategy to merge multiple inverse perspective projections for the same position for generating the ﬁ-nal adversarial spherical images.
• Extensive experiments on the synthetic and real-world datasets demonstrate the effectiveness of our 360-attack on DNNs designed for spherical images, and it also proves that the adversarial perturbations can be transferred from Z2 to SO(3) groups. 2.