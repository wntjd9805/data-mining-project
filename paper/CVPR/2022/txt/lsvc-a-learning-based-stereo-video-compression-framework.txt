Abstract
In this work, we propose the first end-to-end optimized framework for compressing automotive stereo videos (i.e., stereo videos from autonomous driving applications) from both left and right views. Specifically, when compressing the current frame from each view, our framework reduces temporal redundancy by performing motion compensation using the reconstructed intra-view adjacent frame and at the same time exploits binocular redundancy by conduct-ing disparity compensation using the latest reconstructed cross-view frame. Moreover, to effectively compress the in-troduced motion and disparity offsets for better compensa-tion, we further propose two novel schemes called motion residual compression and disparity residual compression to respectively generate the predicted motion offset and dis-parity offset from the previously compressed motion offset and disparity offset, such that we can more effectively com-press residual offset information for better bit-rate saving.
Overall, the entire framework is implemented by the fully-differentiable modules and can be optimized in an end-to-end manner. Our comprehensive experiments on three au-tomotive stereo video benchmarks Cityscapes, KITTI 2012 and KITTI 2015 demonstrate that our proposed framework outperforms the learning-based single-view video codec and the traditional hand-crafted multi-view video codec. 1.

Introduction
Stereo cameras are often used as crucial sensors for au-tonomous driving applications as they can acquire more pre-cise depth information than traditional mono cameras, while being less expensive than other sensors (e.g., LIDAR) [7].
Due to the increasing demand for storing and transmitting the massive automotive stereo videos, how to effectively compress such data has become an emerging task.
Automotive stereo video compression can be regarded as a special case of multi-view video compression (MVC), for which several traditional MVC standards [35, 38, 41] have been proposed in the past decades. Nevertheless, such stan-∗ Guo Lu is the corresponding author. dards all rely on hand-crafted modules, which cannot be jointly end-to-end optimized. Moreover, they only aim at improving the compression performance for one view (e.g., the left view) by exploiting both inter-view and intra-view redundancy, while compressing the base view (e.g., the right view) by directly employing the single-view video codecs.
Therefore, the compression performance could not be sig-nificantly improved without exploiting the cross-view re-dundancy when compressing the right-view videos.
While deep neural networks (DNNs) have achieved tremendous success in image and video compression [5,29], how to develop an effective learning-based stereo video codec remains an under-explored research problem.
In this work, we propose the first learning-based stereo video compression (LSVC) framework for autonomous driving applications. Different from the traditional stereo video codecs, which simply compress the right-view videos by us-ing the single-view video codecs, our framework effectively reduces both temporal and binocular redundancy for the stereo videos from both views, in which we use the infor-mation from one view to assist the compression of another view in an iterative fashion. Specifically, when compressing the current left/right frame, the reconstructed within-view frame (i.e., the left/right frame) from the previous time-step and the latest reconstructed cross-view frame (i.e., the right/left frame) are used as two reference frames. To better exploit both temporal redundancy for adjacent within-view frames and binocular redundancy for cross-view frames, in our approach, we estimate the motion offset between the current frame and the reconstructed previous frame from the same view, while estimating the disparity offset between the current frame and the latest reconstructed cross-view frame.
After motion and disparity compression, we respectively perform motion compensation and disparity compensation to generate the intra-view and inter-view compensated fea-tures, which are eventually fused as the final compensated feature for the subsequent residual compression process.
Furthermore, we also propose two new approaches called motion residual compression (MRC) and disparity residual compression (DRC) to effectively compress the in-troduced motion and disparity offsets, which is based on
the observation that most automotive stereo sensors are particularly calibrated and their epipolar line is horizon-tal [12, 25]. Consequently, the produced stereo frames are rectified, which leads to a strong relationship among tempo-rally adjacent stereoscopic frames [23, 26]. To exploit such a relationship, rather than directly compressing the raw off-sets, we generate the predicted motion offset (resp., dispar-ity offset) based on the latest reconstructed motion offset (resp., disparity offset) by leveraging the geometric correla-tion of stereoscopic frames (resp., temporal correlation of adjacent within-view frames), such that we only need to compress the residual offset information between the raw offset and the predicted offset for better bit-rate saving.
Overall, we implement all our modules by using fully-differentiable DNNs and perform all coding operations in the feature space for better performance as in [19]. We con-duct comprehensive experiments on three automotive stereo video benchmarks Cityscapes [10], KITTI 2012 [15] and
KITTI 2015 [32]. The results demonstrate that our proposed framework significantly outperforms other codecs.
Our contributions are summarized as follows:
• Our proposed framework reduces both intra-view and inter-view redundancy by iteratively performing deep motion and disparity compensation in the feature space for both left-view and right-view videos. To the best of our knowledge, it is the first end-to-end optimized stereo video compression framework.
• We propose two new compression schemes MRC and
DRC to compress the residual motion information and the residual disparity information, respectively, which enables effective compression of complex motion and disparity offsets from the automotive stereo videos.
• The extensive experiments on three automotive stereo video benchmarks demonstrate that our newly pro-posed framework achieves the state-of-the-art results for compressing the automotive stereo videos. 2.