Abstract
In this paper, we propose a weakly-supervised approach for 3D object detection, which makes it possible to train a strong 3D detector with position-level annotations (i.e. an-notations of object centers). In order to remedy the infor-mation loss from box annotations to centers, our method, namely Back to Reality (BR), makes use of synthetic 3D shapes to convert the weak labels into fully-annotated vir-tual scenes as stronger supervision, and in turn utilizes the perfect virtual labels to complement and reﬁne the real labels.
Speciﬁcally, we ﬁrst assemble 3D shapes into physically reasonable virtual scenes according to the coarse scene layout extracted from position-level annota-tions. Then we go back to reality by applying a virtual-to-real domain adaptation method, which reﬁne the weak labels and additionally supervise the training of detector with the virtual scenes. Furthermore, we propose a more challenging benckmark for indoor 3D object detection with more diversity in object sizes for better evaluation. With less than 5% of the labeling labor, we achieve comparable de-tection performance with some popular fully-supervised ap-proaches on the widely used ScanNet dataset. Code is avail-able at: https://github.com/wyf-ACCEPT/BackToReality. 1.

Introduction 3D object detection is a fundamental scene understand-ing problem, which aims to detect 3D bounding boxes and semantic labels from a point cloud of 3D scene. Due to the irregular form of point clouds and complex contexts in 3D scenes, most existing 2D methods [31, 32, 50] cannot be di-rectly applied to 3D object detection. Fortunately, with the development of deep learning techniques on point cloud un-derstanding [27, 28], recent works [11, 20, 25, 35, 51] have
*Corresponding author.
Figure 1. Demonstration of BR. We consider position-level an-notations as the coarse layout of the scenes, which is utilized to generate virtual scenes from a 3D shape repository. Physical con-straints are applied on the virtual scenes to remedy the information loss from box annotations to centers. Then a virtual-to-real domain adaptation method is presented to additionally supervise the real-scene 3D object detection with the virtual scenes. Dashed arrows indicate supervision for training. employed deep neural networks to directly detect objects from point clouds and achieved favorable performance.
Despite the successes in deep learning based object de-tection on point clouds, massive amounts of labeled bound-ing boxes are required for training the detector. This is-sue signiﬁcantly limits the applications of these methods, as labeling a precise 3D box takes more than 100s even by an experienced annotator [36]. Therefore, 3D object de-tection methods using cheap labels are desirable for prac-tical applications. Motivated by this, increasing attention has been paid to weakly-supervised 3D object detection methods, which can be divided into two categories ac-cording to the form of annotation: scene-level [33] and position-level [21, 22] where only the class tag and both object center and class are annotated for each object re-spectively. The two types of annotation only require less than 1% and 5% time for one instance compared to label-ing a bounding box, as shown in Table 1. While scene-level annotation is more time-saving, it is hard for the de-tector to learn how to precisely locate each object in a
Table 1. Annotating time and detection results of different meth-ods based on various types of annotation. The benchmark is de-tailed in Section 4. (BBox refers to box annotation. S-L and P-L mean scene-level and position-level annotations respectively.)
Annotation
Time(s per object) mAP@0.25(%)
BBox [20] 110 54.2
S-L [33] 1
<20
P-L [21] 5 32.4
P-L(BR) 5 47.0 scene due to the lack of position information, and thus the performance is far from satisfactory [33]. Consider-ing the time-accuracy tradeoff, position-level annotation is a more practical solution. However, previous position-level weakly-supervised 3D detection methods still require a number of precisely labeled boxes and can only cope with sparse outdoor scenes [21, 22]. Purely position-level weakly-supervised method for the complicated indoor de-tection task is still under exploration.
In this paper, we propose a shape-guided label enhance-ment approach called Back to Reality (BR) for weakly-supervised 3D object detection1. To reduce the labor cost, we only label the center of each object in the 3D space and the labeling error of centers is allowed2. While largely reducing the workload of labeling, the information loss is non-negligible from box annotations to centers. To ad-dress these, BR converts the weak labels into virtual scenes which contain much of the lost information, and in turn uti-lizes them to additionally supervise real-scene training, as shown in Figure 1. Our approach is based on two moti-vations: 1) in 3D vision, large-scale datasets of synthetic shapes are available. They contain rich geometry informa-tion, which can serve as strong prior to assist 3D object de-tection; 2) the position-level annotations are not only su-pervision for training, but they also provide coarse layout of the scene. Therefore, we assemble the 3D shapes into fully-annotated virtual scenes according to the coarse lay-out and apply physical constraints on them to remedy the information loss. Then a virtual-to-real domain adaptation method is presented to align the global features and object proposal features extracted by the detector between the real and virtual scenes. Moreover, our method can take advan-tage of the precise center labels in virtual scenes to correct the center error of position-level annotations. In this way the useful knowledge contained in virtual scenes is trans-ferred back to reality. Experimental results on ScanNet [7] show the effectiveness of the proposed BR method. 2.