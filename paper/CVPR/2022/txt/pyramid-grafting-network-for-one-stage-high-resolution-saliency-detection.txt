Abstract
Recent salient object detection (SOD) methods based on deep neural network have achieved remarkable perfor-mance. However, most of existing SOD models designed for low-resolution input perform poorly on high-resolution im-ages due to the contradiction between the sampling depth and the receptive ﬁeld size. Aiming at resolving this con-tradiction, we propose a novel one-stage framework called
Pyramid Grafting Network (PGNet), using transformer and
CNN backbone to extract features from different resolu-tion images independently and then graft the features from transformer branch to CNN branch. An attention-based
Cross-Model Grafting Module (CMGM) is proposed to en-able CNN branch to combine broken detailed information more holistically, guided by different source feature dur-ing decoding process. Moreover, we design an Attention
Guided Loss (AGL) to explicitly supervise the attention ma-trix generated by CMGM to help the network better inter-act with the attention from different models. We contribute a new Ultra-High-Resolution Saliency Detection dataset
UHRSD, containing 5,920 images at 4K-8K resolutions. To our knowledge, it is the largest dataset in both quantity and resolution for high-resolution SOD task, which can be used for training and testing in future research. Sufﬁcient exper-iments on UHRSD and widely-used SOD datasets demon-strate that our method achieves superior performance com-pared to the state-of-the-art methods. 1.

Introduction
Salient object detection (SOD) [1, 5] aims at identify-ing and segmenting the most attractive objects in a certain scene. As a pre-processing step, it is widely applied in var-*Correspondence should be addressed to Changqun Xia (Email: xi-achq@pcl.ac.cn ). The code and dataset are available at https:// github.com/iCVTEAM/PGNet.
Figure 1. Comparison of the results of the different methods. (a)
Input image. (b) Ground truth mask. (c) Directly input to Resnet-18 based FPN. (d) Downsample then input to Swin transformer based FPN. (e) Ours. ious computer vision tasks, such as light ﬁeld segmenta-tion [21, 41], instance segmentation [47] and video object segmentation [13, 42].
Recently, deep neural networks based salient object de-tection methods have made remarkable achievements [3, 9, 14, 19, 26, 29]. However, most of existing SOD methods perform well within a speciﬁc input low-resolution range (e.g., 224 × 224, 384 × 384 ). With the rapid development of image capture devices (e.g., smartphone), the resolution (e.g., 1080p, 2K and 4K) of the images accessible to people has far exceeded the range to which existing saliency de-tection method can be adapted directly. As shown in Fig. 1 (c), we fed the high-resolution image directly into the com-monly used network with Resnet-18 as the backbone, and comparing ground truth Fig. 1 (b) shows that the segmen-tation result is incomplete and many detail regions are lost.
In order to reduce computational consumption and memory usage, existing methods often downsample the input images
and then upsample the output results to recover original res-olution, as illustrated in Fig. 1 (d). This challenge is due to the fact that most low-resolution SOD networks are de-signed in an Encoder-Decoder style, and as the input reso-lution increases dramatically, the size of features extracted increases, but the receptive ﬁeld determined by the network is ﬁxed, making the relative receptive ﬁeld small, ultimately resulting in the inability to capture global semantics that are vital to SOD task. Since direct processing cannot handle the challenges posed by high resolution, a number of meth-ods have emerged in recent years speciﬁcally designed for high-resolution input. There are two representative high-resolution SOD methods (HRSOD [40], DHQSOD [30]).
HRSOD divides the whole process into global stage, local stage and reorganization stage, where the global stage pro-vides guidance on both the local stage and the crop process.
And DHQSOD disentangle the SOD task into classiﬁcation task and regression task, where the two task is connected by their proposed trimap and uncertainty loss. They generate relatively good saliency maps with sharp boundaries.
However, both of the above methods use a multi-stage architecture, dividing SOD into semantic(in low resolution) and detailed (in high resolution) phases. This has led to two new problems: (1) Inconsistent contextual semantic trans-fer between stages. The intermediate maps obtained in the previous stages are input into the last stage, while the errors are also passed on. Further more, the reﬁnement in the last stage will likely inherit or even amplify the previous errors as there is not enough semantic support, which implies that the ﬁnal saliency maps are heavily dependent on the perfor-mance of the low-resolution network. (2) Time consuming.
Compared to the one-stage method, the multi-stage method are not only difﬁcult to parallel but also have the potential problem of increasing number of parameters, which makes it slow.
Based on the above defects of existing high-resolution methods , we propose a new perspective that since the spe-ciﬁc features in a single network cannot settle the paradox of receptive ﬁeld and detail retention simultaneously, in-stead we can separately extract two sets of features of dif-ferent spatial sizes and then graft the information from one branch to the other. In this paper, we rethink the dual-branch architecture and design a novel one-stage deep neural net-work for high-resolution saliency detection named Pyramid
Grafting Network (PGNet). As illustrated in Fig. 1 (e), we use both Resnet and Transformer as our Encoders, extract-ing features with dual spatial sizes in parallel. The trans-former branch ﬁrst decode the features in the FPN style, then pass the global semantic information to the Resnet branch in the stage where the feature maps of two branches have similar spatial sizes. We call this process feature graft-ing. Eventually, the Resnet branch completes the decod-ing process with the grafted features. Compared to classic
FPNs, we have constructed a higher feature pyramid at a lower cost. To better graft features cross two different types of models, we design the Cross-Model Grafting Module (CMGM) based on the attention mechanism and propose the Attention Guided Loss to further guide the grafting.
Considering that supervised deep learning method requires a large amount of high quality data, we have provided a 4K resolution SOD dataset (UHRSD) with the largest number to date in an effort to promote future high-resolution salient object detection research.
Our major contributions can be summarized as follows:
• We propose the ﬁrst one-stage framework named
PGNet for high-resolution salient object detection, which uses staggered connection to capture both con-tinuous semantics and rich details.
• We introduce the Cross-Model Grafting Module to transfer the information from transformer branch to
CNN branch, which allows CNN to not only inherit global information but also remedy the defects com-mon to both. Moreover, we design the Attention
Guided Loss to further promote the feature grafting.
• We contribute a new challenging Ultra High-Resolution Saliency Detection dataset (UHRSD) con-taining 5,920 images of various scenes at over 4K resolution and corresponding pixel-wise salient anno-tations, which is the largest high-resolution saliency dataset available.
• Experimental results on both existing datasets and ours demonstrate our method outperforms state-of-the-art methods in accuracy and speed. 2.