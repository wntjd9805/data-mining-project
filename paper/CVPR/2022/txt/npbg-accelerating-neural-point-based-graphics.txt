Abstract
We present a new system (NPBG++) for the novel view synthesis (NVS) task that achieves high rendering realism with low scene ﬁtting time. Our method efﬁciently lever-ages the multiview observations and the point cloud of a static scene to predict a neural descriptor for each point, improving upon the pipeline of Neural Point-Based Graph-ics [1] in several important ways. By predicting the descrip-tors with a single pass through the source images, we lift the requirement of per-scene optimization while also mak-ing the neural descriptors view-dependent and more suit-able for scenes with strong non-Lambertian effects. In our comparisons, the proposed system outperforms previous
NVS approaches in terms of ﬁtting and rendering runtimes while producing images of similar quality. Project page: https://rakhimovv.github.io/npbgpp/. 1.

Introduction
The ability to render photorealistic views of a scene, as learned from a handful of observations, opens the door to many applications in virtual / augmented reality, cinematog-raphy, gaming industry, and virtually any ﬁeld which in-volves computer graphics. While there is a high interest in creating such novel view synthesis (NVS) systems, the problem proves to be challenging. Early methods based on view interpolation [4,16,26] do not fare well enough in real-world scenarios, which entail complex geometry, limited proximity of input images, lighting variations, etc. There are mainly three development directions which the different works addressing this task generally seek to improve: ren-dered image quality, scene ﬁtting time, and rendering speed.
Despite the recent development in Computer Vision brought by Deep Learning approaches, there is still a noticeable gap between the current state of the art in NVS and an ideal model for this task.
∗Equal contribution
Correspondence to ruslan.rakhimov@skoltech.ru
Figure 1. Time comparison. Time vs. image quality (LPIPS) comparison of several methods, computed on the ‘hotdog‘ scene from the NeRF-synthetic dataset. The time axis represents the time-to-rendering, i.e., ﬁtting time + rendering time for one image.
For methods marked with (cid:63), the ﬁrst scores are reported without per-scene optimizations. Fitting time consists of feature extrac-tion for IBRNet, geometry estimation + 3D modeling stage for
NPBG++, and geometry estimation + meshing for SVS (the ren-derings on top offer qualitative comparisons between these conﬁg-urations). The remaining scores are computed at different points in the ﬁne-tuning processes. Circle areas are proportional to loga-rithms of the rendering times (smaller is better) and highlight the methods’ rendering speed.
To this end, our work proposes a new system that enables real-time rendering and can rapidly adapt to new scenes.
Similarly to Neural Point-Based Graphics (NPBG) [1], our approach uses point clouds to model the geometry of scenes. This representation is advantageous as point clouds are generally available by using inexpensive RGBD cam-eras or by processing RGB streams with classic Structure-from-Motion and Multi-View Stereo pipelines such as
COLMAP [35, 37]. These reconstructions are usually not accurate or complete enough to be used directly for render-ing, whereas performing surface estimation could yield a loss of geometric details and requires signiﬁcant additional
computations. Instead, we devise our method to work di-rectly on raw point geometry and address the problem of small noise and low point density using neural rendering.
Using a point-based geometry together with a neural ren-dering model had been shown to yield good results as NVS systems [1, 19–21]. However, these approaches require per scene optimization of per-point descriptors and optionally a deep rendering network, resulting in a lengthy process to achieve high-quality renderings. To accelerate this process, our method predicts the points’ features from the source im-ages, enabling fast scene representations, which can then be rendered in real-time. If needed, one can further ﬁnetune these predictions to increase the quality of the result. The challenge of the approach is the proper integration of data from several views while considering view-dependent ap-pearance, occlusions, and missing information.
The system we propose is effective and fast, see Fig-ure 1: for each input image and associated camera parame-ters, we run the feature extractor network on it, yielding fea-ture maps representing each pixel’s local appearance. The point cloud is then projected onto the image, taking into ac-count occlusions to obtain features. An online aggregation method was devised to effectively aggregate the obtained features from one image at a time. After processing all in-put views, we get the ﬁnal view-dependent neural descrip-tors for each point. This representation is then rendered by a U-Net shaped network that integrates multi-scale rasteri-zations of the point cloud similarly to NPBG [1].
To further improve the quality of the NPBG system, we introduce two important modiﬁcations (that can also be applied in analogous systems). Firstly, we show how view-dependency can be added to neural descriptors ef-ﬁciently, without an excessive increase of scene ﬁtting time, while boosting the quality of NPBG for scenes with non-Lambertian surfaces.
Secondly, we introduce two lightweight 2D alignment stages in the pipeline that address the non-equivariance of convolutional parts of the pipeline to in-plane rotations.
To summarize, our main contributions are:
• a new NVS system capable of quickly generating neu-ral scene representations that can then be rendered at interactive rates at high resolution.
• an online, permutation-invariant, aggregation method for incorporating features from any number of source views using constant memory into neural descriptors, which facilitates view-dependent effect modeling.
• an alignment technique that makes the rendering pro-cess equivariant to in-plane rotations, appropriate for any pipeline working with neural descriptors estimated from images. 2.