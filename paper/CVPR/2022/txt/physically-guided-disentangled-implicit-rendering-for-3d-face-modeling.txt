Abstract
This paper presents a novel Physically-guided Disen-tangled Implicit Rendering (PhyDIR) framework for high-ﬁdelity 3D face modeling. The motivation comes from two observations: Widely-used graphics renderers yield exces-sive approximations against photo-realistic imaging, while neural rendering methods produce superior appearances but are highly entangled to perceive 3D-aware operations.
Hence, we learn to disentangle the implicit rendering via explicit physical guidance, while guaranteeing the proper-ties of: (1) 3D-aware comprehension and (2) high-reality image formation. For the former one, PhyDIR explicitly adopts 3D shading and rasterizing modules to control the renderer, which disentangles the light, facial shape, and viewpoint from neural reasoning. Speciﬁcally, PhyDIR pro-poses a novel multi-image shading strategy to compensate for the monocular limitation, so that the lighting variations are accessible to the neural renderer. For the latter, PhyDIR learns the face-collection implicit texture to avoid ill-posed intrinsic factorization, then leverages a series of consisten-cy losses to constrain the rendering robustness. With the disentangled method, we make 3D face modeling beneﬁt from both kinds of rendering strategies. Extensive experi-ments on benchmarks show that PhyDIR obtains superior performance than state-of-the-art explicit/implicit methods on geometry/texture modeling. 1.

Introduction 3D face reconstruction gets increasingly attraction with applications such as digital human, games and mobile pho-∗Chengjie Wang and Dongjin Huang are corresponding authors
†CAS Key Laboratory of Electromagnetic Space Information of USTC. tography. The pioneering effort is 3DMM [6] which pro-vides reliable facial priors. With this parametric model, the reconstruction can be achieved by optimization and ﬁt-ting [47, 48, 79]. With the development of deep learn-ing, recent methods [15, 18, 33, 45, 77] learn to regress 3DMM parameters from input images. Subsequent work-s are also proposed to contribute on non-linear model-ing [17, 19, 56, 57, 59, 67, 76] and multi-view consisten-cy [5, 9, 54, 64, 69]. Besides 3DMM based approaches, re-cent efforts [50, 65, 75] attempt to model 3D face without shape assumptions. These non-parametric methods have potential ability to improve the modeling quality over 3D-MM limitations.
Actually, the aforementioned learning-based methods need differentiable renderers including OpenDR [36], neu-ral mesh renderer [29], SoftRas [34] and Ray-tracing [32] for unsupervised learning. These renderers perform image formation under graphics pipelines which are well explain-able. With the explicit 3D operations, the ﬁne-grained 3D controls are naturally achieved. However, these graphics renderers yield hand-crafted approximation or ill-posed de-composition on reﬂectence, illumination or other 3D clues.
In Fig. 1-(a), we observe the graphics-renderer-based meth-ods [13, 32, 75] struggle to produce photo-realistic texture, which also limits their geometry reconstruction.
Against these limitations, another approach is employ-ing a neural renderer such as StyleGAN [27, 28] to avoid approximation or ill-posed decomposition. Existing meth-ods [7, 12, 42, 43, 66] mainly learn to embed 3DMM coef-ﬁcients into StyleGAN’s manifold, and constrain the gen-erative network with 3DMM consistency. In this way, 3D controls are achieved implicitly by tuning the parameters.
With StyleGAN’s effectiveness, these methods show high-reality texture modeling performance. However, in Fig. 1-Methods 3D Controls
Image Formation
Photo Collection
Graphics-renderer-based
MOFA [57], DECA [17], Unsup3D [65]
LAP [75], FML [54], MVF [64]
Neural-rendering-based
Explicit
Shape | Pose | Light
Explainable 3D Graphics Pipelines
Implicit
Entangled
DFG [12], StyleRig [55], PIRender [43] 3DMM Parameters
‘Black Box’ 3D-aware Generative
VariTex [7], Pi-GAN [8], GIRAFFE [39]
Ours
Explicit
Shape | Pose
Explicit
Shape | Pose | Light
Disentangled 3D Operations + 2D Neural Reasoning
Disentangled 3D Operations + 2D Neural Reasoning
× (cid:2)
×
× (cid:2) (a)
Ours
LAP
D3DFR
Albedo MM + Redner
Table 1. Discussion with selected existing methods. (b)
Ours
DFG
PIRender
VariTex (c)
-75(cid:28733)
Yaw 0(cid:28733)
Pitch
Relighting
+30(cid:28733)
-30(cid:28733)
+30(cid:28733)
Figure 1. (a) Comparison with graphics-renderer based method-s LAP [75], D3DFR [13] and Albedo MM [52] + Ray-tracing (redner) [32]. Our method models detailed facial shapes, photo-realistic texture and lighting effects. (b) Comparison with neural rendering methods DFG [12] and PIRender [43]. Our method pro-duces more robust 3D controls and relighting results. (c) Results of 3D-aware generative method [7]. Our method well addresses real-world images and photo-realistic lighting effects. (b), we observe that they cannot guarantee the identity, fa-cial shape, lighting effect or texture consistency during 3D operating. The reason is due to the entangled image forma-tion procedure. StyleGAN is trained as a 2D-aware ‘black box’ without 3D physical modeling. Hence, even with high-level 3D representations, the generator essentially needs to guess and simulate the exact 3D operations, which is high-ly indirect and complicated. Recent 3D-aware generative approaches [7, 8, 39, 42] are proposed against this problem and achieve better 3D controls. However, in Fig. 1-(c), we observe that this kinds of method cannot address real-world images nor lighting effects.
On top of these discussions, we argue that a proper rendering strategy should support (1) explicit and ﬁne-grained 3D controls, (2) a disentangled neural reason-ing for high-quality image formation and (3) easily inverse rendering to model faces from real images.
In this pa-per, we propose a novel Physically-guided Disentangled
Implicit Rendering (PhyDIR) framework for 3D face re-construction. As shown in Fig. 1, by disentangling 3D physical pipelines from neural reasoning, PhyDIR achieves robust and photo-realistic 3D modeling/editing from input facial photos. The neural reasoning of PhyDIR contain-s a texture modeling network and a 2D-aware neural ap-pearance renderer, while the 3D physical guidance bridges this two stages with explicit 3D pipelines. Concretely, the texture modeling network learns canonical implicit texture from input images, which avoids ill-posed intrinsic factor-ization. Then, PhyDIR employs facial shading and raster-ization from a 3D proxy to warp the implicit texture into 2D space. Thus the ﬁne-grained 3D controls, including fa-cial shape, viewpoint and lighting, are explicitly modeled.
Speciﬁcally, PhyDIR leverages a novel multi-image shading module to compensate for the monocular ambiguity, mak-ing the lighting variation well accessible in an unsupervised manner. After that, the neural appearance renderer takes the projected 2D texture for image formation, constrained by a series of 3D consistency losses. In this way, PhyDIR guar-antees explainable 3D controls and photo-realistic image formation without hand-crafted rules. Finally, we demon-strate that with the disentangled paradigm, PhyDIR well acts as a reliable renderer to model detailed facial shapes.
In summary, our contributions are as follows: 1) A novel Physically-guided Disentangled Implicit Ren-dering (PhyDIR) framework is proposed to model high-ﬁdelity 3D face. PhyDIR well integrates the advantages of graphics/neural renderers, and gets over the hand-crafted graphics rules or entangled neural image formation. 2) With the novel multi-image rasterizing, shading and texture mapping modules, PhyDIR guarantees ﬁne-grained 3D controls of shape, viewpoint and lighting, as well as the photo-realistic imaging. 3) With a series of novel consistency losses, PhyDIR guarantees the rendering robustness under 3D operations. 2.