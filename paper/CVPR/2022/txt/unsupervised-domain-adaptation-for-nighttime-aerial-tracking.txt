Abstract
Previous advances in object tracking mostly reported on favorable illumination circumstances while neglecting per-formance at nighttime, which signiﬁcantly impeded the de-velopment of related aerial robot applications. This work instead develops a novel unsupervised domain adaptation framework for nighttime aerial tracking (named UDAT).
Speciﬁcally, a unique object discovery approach is provided to generate training patches from raw nighttime tracking videos. To tackle the domain discrepancy, we employ a
Transformer-based bridging layer post to the feature ex-tractor to align image features from both domains. With the day-a Transformer day/night feature discriminator, time tracking model is adversarially trained to track at night. Moreover, we construct a pioneering benchmark namely NAT2021 for unsupervised domain adaptive night-time tracking, which comprises a test set of 180 manually annotated tracking sequences and a train set of over 276k unlabelled nighttime tracking frames. Exhaustive experi-ments demonstrate the robustness and domain adaptabil-ity of the proposed framework in nighttime aerial track-ing. The code and benchmark are available at https:
//github.com/vision4robotics/UDAT. 1.

Introduction
Standing as one of the fundamental tasks in computer vision, object tracking has received widespread attention with multifarious aerial robot applications, e.g., unmanned aerial vehicle (UAV) self-localization [49], target follow-ing [25], and aerial cinematography [2]. Driven by large-scale datasets [10,17,32] with the supervision of meticulous manual annotations, emerging deep trackers [4, 8, 14, 22] keep setting state-of-the-arts (SOTAs) in recent years.
Despite the advances, whether current benchmarks or approaches are proposed for object tracking under favor-able illumination conditions. In contrast to daytime, images captured at night have low contrast, brightness, and signal-*Corresponding author
#64
#16
#86
#102
#323
#815
#959
#1782
#1856
UDAT-CAR
UDAT-BAN
SiamCAR
SiamBAN
Target object (a) Qualitative comparison in typical night scenes.
Overall performance on NAT2021 -test n o i s i c e r p d e z i l a m r o
N 0.56 0.54 0.52 0.50 0.48 0.46 0.44 0.42 0.40 0.34 0.36 0.38 0.40
Success rate 0.42 0.44 0.46 0.48
UDAT-CAR  (Ours)
UDAT-BAN  (Ours)
SiamCAR (CVPR2020)
SiamBAN (CVPR2020)
HiFT (ICCV2021)
LUDTplus (IJCV2021)
LUDT (IJCV2021)
SiamAPN (ICRA2021)
D3S (CVPR2020)
SiamFC++ (AAAI2020)
Ocean (ECCV2020)
UpdateNet (ICCV2019)
SiamRPN++ (CVPR2019)
SiamDW (CVPR2019)
DaSiamRPN (ECCV2018) (b) Overall performance comparison on NAT2021-test.
Figure 1. (a) Qualitative comparison of the proposed unsupervised domain adaptive trackers (i.e., UDAT-CAR and UDAT-BAN) and their baselines [8, 14]. (b) Overall performance of SOTA ap-proaches on the constructed NAT2021-test benchmark. The pro-posed UDAT effectively adapts general trackers to nighttime aerial tracking scenes and yields favorable performance. to-noise ratio (SNR). These differences cause the discrep-ancy in feature distribution of day/night images. Due to the cross-domain discrepancy, current SOTA trackers general-ize badly to nighttime scenes [48, 50], which severely im-pedes the broadening of relevant aerial robot applications.
Regarding such domain gap and the performance drop, this work aims to address the cross-domain object tracking problem. In particular, we target adapting SOTA tracking models in daytime general conditions to nighttime aerial perspectives. One possible straightforward solution is to collect and annotate adequate target domain data for train-ing. Nevertheless, such a non-trivial workload is expensive and time-consuming, since backbones’ pre-training alone generally takes millions of high-quality images [9]. We  
consequently consider the problem as an unsupervised do-main adaptation task, where training data in the source do-main is with well-annotated bounding boxes while that in the target domain has no manually annotated labels. There-fore, an unsupervised domain adaptive tracking framework, referred to as UDAT, is proposed for nighttime aerial track-ing. To generate training patches of the target domain, we develop an object discovery strategy to explore potential ob-jects in the unlabelled nighttime data. Besides, a bridging layer is proposed to bridge the gap of domain discrepancy for the extracted features.
Furthermore, the feature domain is distinguished by virtue of a discriminator during adversarial learning. Draw-ing lessons from the huge potential of the Transformer [43] in feature representation, both the bridging layer and the discriminator utilize a Transformer structure. Figure 1 ex-hibits some qualitative comparisons of trackers adopting
UDAT and the corresponding baselines. UDAT raises base-lines’ nighttime aerial tracking performance substantially.
Apart from methodology, we construct NAT2021, a bench-mark comprising a test set of 180 fully annotated video se-quences and a train set of over 276k unlabelled nighttime tracking frames, which serves as the ﬁrst benchmark for un-supervised domain adaptive nighttime tracking. The main contributions of this work are fourfold:
• An unsupervised domain adaptive tracking framework, namely UDAT, is proposed for nighttime aerial track-ing. To the best of our knowledge, the proposed UDAT is the ﬁrst unsupervised adaptation framework for ob-ject tracking.
• A bridging layer and a day/night discriminator with
Transformer structures are incorporated to align ex-tracted features from different domains and narrow the domain gap between daytime and nighttime.
• A pioneering benchmark namely NAT2021, consisting of a fully annotated test set and an unlabelled train set, is constructed for domain adaptive nighttime tracking.
An object discovery strategy is introduced for the un-labelled train set preprocessing.
• Extensive experiments on NAT2021-test and the re-cent public UAVDark70 [21] benchmark verify the ef-fectiveness and domain adaptability of the proposed
UDAT in nighttime aerial tracking. 2.