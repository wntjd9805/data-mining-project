Abstract
Photorealistic style transfer entails transferring the style of a reference image to another image so the result seems like a plausible photo. Our work is inspired by the ob-servation that existing models are slow due to their large sizes. We introduce PCA-based knowledge distillation to distill lightweight models and show it is motivated by the-ory. To our knowledge, this is the first knowledge dis-tillation method for photorealistic style transfer. Our ex-periments demonstrate its versatility for use with differ-ent backbone architectures, VGG and MobileNet, across six image resolutions. Compared to existing models, our top-performing model runs at speeds 5-20x faster using at most 1% of the parameters. Additionally, our dis-tilled models achieve a better balance between stylization strength and content preservation than existing models. To support reproducing our method and models, we share the code at https://github.com/chiutaiyin/PCA-Knowledge-Distillation. 1.

Introduction
Photorealistic style transfer is the task of rendering an image (content image) in the style of a reference image (style image) to create a photorealistic result. Examples are shown in Fig. 1. A key challenge the community has fo-cused on tackling since the seminal neural network-based algorithm for this task [30] has been how to simultane-ously achieve a good balance between stylization strength and content preservation while running fast to better sup-port practical applications [11, 24, 46, 48].
The status quo for modern photorealistic style transfer models is to use autoencoders. As illustrated in Fig. 2(a), the basic model uses a pre-trained VGG-19 [40]1 as the en-coder to extract content and style features, then a feature transformation to adapt the content feature with respect to the style feature, and finally a decoder to invert the adapted feature to a stylized image. A limitation of this framework is that the speed of autoencoder-based approaches is limited by the large size of the VGG-19 backbone.
This size limitation is further amplified in state-of-the-art models, as they extend the basic autoencoder frame-work by using multiple feature transformations to better capture style effects. For instance, PhotoWCT [24] and
PhotoWCT2 [11] perform coarse-to-fine feature transfor-mations to sequentially adapt the coarse content feature (i.e., relu4 1 content feature from VGG) to the fine con-tent feature (i.e., relu1 1 content feature from VGG) with respect to the corresponding style features. This means the fine style feature is added on top of the coarse style fea-ture and so can produce strong style effects. However, as 1VGG-19 is favored in artistic [7, 15, 16, 19, 20, 22, 23, 39] and photore-alistic [4, 11, 24, 30, 46, 48] style transfer research due to its simple archi-tecture with no complex multiple branches and residual modules, making
VGG features easier to interpret for content and style.
ter style than WCT2 and PhotoNAS due to coarse-to-fine feature transformations, preserve better content than Pho-toWCT and PhotoWCT2 due to knowledge distillation, and incur faster speeds. These benefits are exemplified in Fig. 1.
To the best of our knowledge, our method is the first knowl-edge distillation algorithm for photorealistic style transfer. 2.