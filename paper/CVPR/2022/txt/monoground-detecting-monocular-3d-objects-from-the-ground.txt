Abstract
Monocular 3D object detection has attracted great atten-tion for its advantages in simplicity and cost. Due to the ill-posed 2D to 3D mapping essence from the monocular imag-ing process, monocular 3D object detection suffers from in-accurate depth estimation and thus has poor 3D detection results. To alleviate this problem, we propose to introduce the ground plane as a prior in the monocular 3d object de-tection. The ground plane prior serves as an additional geometric condition to the ill-posed mapping and an extra source in depth estimation. In this way, we can get a more accurate depth estimation from the ground. Meanwhile, to take full advantage of the ground plane prior, we pro-pose a depth-align training strategy and a precise two-stage depth inference method tailored for the ground plane prior.
It is worth noting that the introduced ground plane prior requires no extra data sources like LiDAR, stereo images, and depth information. Extensive experiments on the KITTI benchmark show that our method could achieve state-of-the-art results compared with other methods while main-taining a very fast speed. Our code, models, and training logs are available at https://github.com/cfzd/
MonoGround. 1.

Introduction 3D object detection is a fundamental computer vision task that aims to obtain the locations, sizes, and orienta-tions of objects. To get the real-world 3D information, many methods adopt modalities like point clouds from Li-DAR, stereo images, and depth images, which require extra sensors of data sources. Different from them, monocular 3D object detection, which only requires a single 2D image and camera calibration information, increasingly draws the community’s attention for its superiorities in simplicity and cost, especially in the autonomous driving field.
Despite the superiorities of monocular 3D object detec-tion, obtaining 3D information from a single 2D image is
*Corresponding author. (a) Multiple depth solutions with collinear points. (b) Two cars with the same depths and positions, but the projected centers are different in the image space. Depth is correlated with other variables like heights.
Figure 1. Difficulties in the monocluar 3D object detection. essentially hard for the following reasons: 1) the mapping from 2D to 3D is an ill-posed problem since a location in the 2D image plane corresponds to all collinear 3D posi-tions, which are in the ray from the optical center to the 2D location. This one-to-many property makes predict-ing the depths of objects difficult, as shown in Fig. 1a. 2)
The expression of 3D object’s depth is correlated with the height. For two objects with the same depths and positions, if the heights are different, the projected center would dif-fer, as shown in Fig. 1b. In this way, the learning of depths has to overcome the interference with irrelevant attributes of objects. 3) Mainstream monocular 3D object detection methods [19, 22, 42, 44] are commonly based on the Cen-terNet [43]. Under this framework, all information of each object is represented with a point, including the depth infor-mation. In this way, the supervision of depth is very sparse during training (Suppose there are two cars in an image, then only two points on the depth map would be trained, and all other points on the depth map are ignored). Such sparse supervision would lead to insufficient learning of depth es-timation, which significantly differs from common monoc-ular depth estimation tasks with dense depth supervision.
45] from pre-trained depth estimation models. For exam-ple, Pseudo-LiDAR [39] generates the pseudo LiDAR infor-mation from the monocular image and depth information.
CaDDN [31] projects LiDAR point clouds into the image to create depth maps, then a categorical depth distribution is learned. Besides the depth and LiDAR data, there are also methods that try to utilize the CAD models [5, 20, 26, 28] to simplify the recognition and pose estimation of objects.
The second kind of method aim to detect 3D objects without any extra data [1,15,16,27,30]. For example, M3D-RPN [1] uses depth-aware convolutional layers to detect 3D objects. MonoPair [9] proposes to use the pairwise spatial relationships to achieve better results. SMOKE [19] pro-poses a CenterNet-style [43] 3D detector via keypoints es-timation. Then, MonoFlex [42] proposes a flexible center definition that unifies truncated objects and regular objects and an uncertainty-based depth ensemble method. To better find the bottleneck of purely monocular detectors, Mono-DLE [25] examines the effects of each component in main-stream methods. GrooMeD-NMS [14] introduces differen-tial non-maximal suppression into the monocular 3D ob-ject detection. MonoRUn [6] proposes to use the dense correspondence between 2D and 3D space to learn the re-construction and reproject processes. To better utilize the geometry relationship in 3D and 2D space accompanying uncertainty, GUPNet [22] proposes a geometry uncertainty projection method to reduce the error in depth estimation.
Ground Plane Knowledge in the Monocular 3D Object
Detection There have been several attempts in using the ground plane knowledge in monocular 3D object detection.
Mono3D [7] first tries to use the ground plane to generate 3D bounding box proposals. Besides, Ground-Aware [18] introduces the ground plane in the geometric mapping and proposes a ground-aware convolution module to enhance the detection.
In the above works, the ground plane is defined based on a fixed rule, that the camera height on KITTI is 1.65 meters.
In this way, all positions at a fixed height of -1.65 meters in the 3D space would construct the ground plane. Different from the strong hypothesis of the fixed ground plane at the height of -1.65 meters, in this work, we propose a learnable ground plane prior that is based on a more reasonable hy-pothesis: objects should lie on the ground. As long as the objects lie on the ground, the ground plane can be substi-tuted with the bottom surface of object’s 3D bounding box.
In this way, the proposed ground plane prior with the ob-jects’ bottom surface is object-wise adaptive and precise.
Depth Estimation in the Monocular 3D Object Detection
There are two kinds of mainstream depth estimation meth-ods in the purely monocular 3D object detection, which are direct regression [43] and geometry depth [4] derived from
Figure 2. Illustration of the depth on the ground.
To address the above problems, we propose to introduce the ground plane prior to the monocular 3D object detec-tion task. With the ground plane prior, the ill-posed 2D to 3D mapping becomes a well-posed problem with a unique solution. The unique solution is determined by the intersec-tion of the camera ray and the ground plane. The expression of depth is no longer correlated with the object’s height, as shown in Fig. 2. Moreover, since the ground plane is in-troduced and utilized, we could expand the original sparse depth supervision to a dense depth supervision by using the dense depth from the ground plane.
With the above motivations, we propose our monocu-lar 3D object detection model with the dense ground plane prior, termed as MonoGround. Within this formulation, we propose a depth-align method to effectively learn and pre-dict dense grounded depth. Meanwhile, with the help of dense predicted depth, we also propose a two-stage depth inference method, which brings finer-grained depth estima-tion. In summary, the main contribution of this work can be summarized as follows:
• We propose to introduce the ground plane prior to the monocular 3D object detection, which could allevi-ate ill-posed mapping, remove irrelevant correlation of depths, and provide dense depth supervision, without any extra data like LiDAR, stereo, and depth images.
• We propose a depth-align training strategy and a fine-grained two-stage depth inference method to take ad-vantage of the introduced ground plane prior and achieve precise depth inference.
• Experiments on the KITTI dataset show the effective-ness of introducing the ground plane prior and the pro-posed method, and our method achieves the SOTA per-formance with a very fast speed in real-time. 2.