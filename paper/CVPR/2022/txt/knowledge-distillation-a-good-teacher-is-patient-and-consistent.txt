Abstract
There is a growing discrepancy in computer vision be-tween large-scale models that achieve state-of-the-art per-formance and models that are affordable in practical appli-cations. In this paper we address this issue and signiﬁcantly bridge the gap between these two types of models. Through-out our empirical investigation we do not aim to necessarily propose a new method, but strive to identify a robust and ef-fective recipe for making state-of-the-art large scale models affordable in practice. We demonstrate that, when performed correctly, knowledge distillation can be a powerful tool for reducing the size of large models without compromising their performance. In particular, we uncover that there are cer-tain implicit design choices, which may drastically affect the effectiveness of distillation. Our key contribution is the explicit identiﬁcation of these design choices, which were not previously articulated in the literature. We back up our
ﬁndings by a comprehensive empirical study, demonstrate compelling results on a wide range of vision datasets and, in particular, obtain a state-of-the-art ResNet-50 model for
ImageNet, which achieves 82.8% top-1 accuracy. 1.

Introduction
Figure 1. We demonstrate that distillation works the best when we train patiently for a large number of epochs and provide consis-tent image views to teacher and student models (green and blue lines). This can be contrasted to a popular setting of distilling with precomputed teacher targets (black line), which works much worse.
Large-scale vision models currently dominate many areas of computer vision. Recent state-of-the-art models for image classiﬁcation [6, 22, 39, 41, 48], object detection [7, 26] or semantic segmentation [52] push model size to the limits allowed by modern hardware. Despite their impressive per-formance, these models are rarely used in practice due to high computational costs. Instead, practitioners typically use much smaller models, such as ResNet-50 [22] or Mo-bileNet [14], which are order(s) of magnitude cheaper to run. According to the download counts of ﬁve BiT models (cid:63)equal contribution
† work done at Google, while being a PhD student in IST Austria.
‡ work done at Google, while being a PhD student in Skoltech. from Tensorﬂow Hub, the smallest ResNet-50 [11] model has been downloaded for signiﬁcantly more times than the larger ones. As a result, many recent improvements in vision do not translate to real-world applications.
To address this problem, we concentrate on the follow-ing task: given a speciﬁc application and a large model that performs very well on it, we aim to compress the model to a smaller and more efﬁcient architecture without compromis-ing performance. There are two widely used paradigms that target this task: model pruning [18] and knowledge distilla-tion [12]. Model pruning reduces the large model’s size by stripping away its parts. This procedure can be restrictive in practice: ﬁrst, it does not allow changing the model family,
say from a ResNet to a MobileNet. Second, there may be architecture-dependent challenges, e.g. if the model uses group normalization [46], pruning channels may result in the need to dynamically re-balance channel groups.
Instead, we concentrate on the knowledge distillation ap-proach which does not suffer from these drawbacks. The idea behind knowledge distillation is to “distill” a teacher model, in our case a large and cumbersome model or ensem-ble of models, into a small and efﬁcient student model. This works by forcing the student’s predictions (or internal activa-tions) to match those of the teacher, thus naturally allowing a change in the model family as part of compression. We closely follow the original distillation setup from [12] and
ﬁnd it surprisingly effective when done right: We interpret distillation as a task of matching the functions implemented by the teacher and student, as illustrated in Figure 2. With this interpretation, we discover two principles of knowledge distillation for model compression. First, teacher and student should process the exact same input image views or, more speciﬁcally, same crop and augmentations. Second, we want the functions to match on a large number of support points to generalize well. Using an aggressive variant of mixup [51], we can generate support points outside the original image manifold. With this in mind, we experimentally demonstrate that consistent image views, aggressive augmentations and very long training schedules are the key to make model com-pression via knowledge distillation work well in practice.
Despite the apparent simplicity of our ﬁndings, there are multiple reasons that may commonly prevent researchers (and practitioners) from making the design choices that we suggest. First, it is tempting to precompute the teacher’s activations for an image ofﬂine once to save compute, espe-cially for very large teachers. As we will show, this ﬁxed teacher approach does not work well. Second, knowledge distillation is also commonly used in different contexts (other than model compression), where authors recommend differ-ent or even opposite design choices [40, 48, 50], see Figure 2.
Third, knowledge distillation requires an atypically large number of epochs to reach best performance, much more than commonly used for supervised training. Finally, choices which may look suboptimal in training of regular length of-ten end up being best for long runs, and vice-versa.
In our empirical study, we mostly concentrate on com-pressing the large BiT-ResNet-152x2 from [22] that was pretrained on the ImageNet-21k dataset [36] and ﬁne-tuned to the relevant datasets of interest. We distill it to a standard
ResNet-50 architecture [11] (but replace batch normalization with group normalization) on a range of small and mid-sized datasets without compromising accuracy. We also achieve very strong results on the ImageNet [35] dataset: with a total number of 9600 epochs for distillation, we set the new
ResNet-50 SOTA 82.8% on ImageNet. This is 4.4% bet-ter than the ResNet-50 model from [22], and 2.2% better than the best ResNet-50 model in the literature, which uses a more complex setup [37]. Finally, we demonstrate that our distillation recipe also works when simultaneously com-pressing and changing the model family, e.g. BiT-ResNet architecture to the MobileNet architecture. 2. Experimental setup
In this section, we introduce the experimental setup and benchmarks we use throughout the paper. Given a large-scale vision model (the teacher, or T) with high accuracy on a particular task, we aim to compress this model to a much smaller model (the student, or S) without compromising its performance. Our compression recipe relies on knowledge distillation, as introduced in [12], and a careful investigation of several key ingredients in the training setup.
Datasets, metrics and evaluation protocol. We conduct experiments on ﬁve popular image classiﬁcation datasets:
ﬂowers102 [30], pets [32], food101 [20], sun397 [47] and
ILSVRC-2012 (“ImageNet”) [35]. These datasets span di-verse image classiﬁcation scenarios; In particular, they vary in the number of classes, from 37 to 1000 classes, and total number of training images, from 1020 to 1281167 training images. This allows us to verify our distillation recipe for a broad range of practical settings and ensure its robustness.
As a metric, we always report classiﬁcation accuracy. For all datasets, we perform design choices and hyperparameters selection using a validation split, and report ﬁnal results on the test set. These splits are deﬁned in the appendix E.
Teacher and student models. Throughout the paper, we opt for using pre-trained teacher models from BiT [22], which provides a large collection of ResNet models pre-trained on ILSVRC-2012 and ImageNet-21k datasets, with state-of-the-art accuracy. The only signiﬁcant differences between BiT-ResNets and standard ResNets is their use of group normalization layer [46] and weight standardiza-tion [33], which are used instead of batch normalization [16].
In particular, we concentrate on the BiT-M-R152x2 archi-tecture: a BiT-ResNet-152x2 (152 layers, ‘x2‘ indicates the width multiplier) pretrained on ImageNet-21k. This model demonstrates excellent performance on a variety of vision benchmarks and it is still manageable to run extensive abla-tion studies with it. It is highly expensive to deploy (requires roughly 10x more compute than the standard ResNet-50), and thus effective compression of this model is of practi-cal importance. For the student’s architecture, we use a
BiT-ResNet-50 variant, referred to as ResNet-50 for brevity.
Distillation loss. We use the KL-divergence between the teacher’s pt, and the student’s ps predicted class probability vectors as a distillation loss, as was originally introduced in [12]. We do not use any additional loss term with respect
Figure 2. Schematic illustrations of various design choices when doing knowledge distillation. Left: Teacher receives a ﬁxed image, while student receives a random augmentation. Center-left: Teacher and student receive independent image augmentations. Center-right:
Teacher and student receive consistent image augmentations. Right: Teacher and student receive consistent image augmentations plus the input image manifold is extended by including linear segments between pairs of images (known as mixup [51] augmentation). to the original dataset’s hard labels:
KL(pt||ps) = (cid:88) i∈C
[−pt,i log ps,i + pt,i log pt,i] , (1) where C is a set of classes. Also, as in [12], we introduce a temperature parameter T , which is used to adjust the entropy of the predicted softmax-probability distributions before they are used in the loss computation: ps ∝ exp( log ps
T ) and pt ∝ exp( log pt
T ).
Training setup. For optimization, we train our models with the Adam optimizer [21] with default parameters, ex-cept for the initial learning rate that is part of our hyperparam-eter exploration. We use a cosine learning rate schedule [27] without warm restarts. We also sweep over the weight decay loss coefﬁcient for all our experiments (for which we use a “decoupled” weight decay convention [28]). To stabilize training we enable gradient clipping with a threshold of 1.0 on the global L2-norm of a gradient. Finally, we use batch size 512 for all our experiments, except for models trained on ImageNet, where we train with batch size 4096. For the remaining hyperparameters, we discuss their sweeping range together with corresponding experiments in the next section.
One additional important component of our recipe is the mixup data augmentation strategy [51]. In particular, we introduce a mixup variant in our “function matching” strat-egy (see Section 3.1.1), in which we use “agressive” mixing coefﬁcients sampled uniformly from [0, 1], which can be seen as an extreme case of the originally proposed sampling from β-distribution.
Unless explicitly speciﬁed otherwise, for prepossessing we use an “inception-style” crop [38] and then resize images to a ﬁxed square size. Furthermore, in order to make our ex-tensive analysis computationally feasible (overall we trained dozens of thousands of models), we use relatively low input resolution and resize input images to 128 × 128 size, except for our ImageNet experiments, that use the standard input 224 × 224 resolution.
For all our experiments we use Google Cloud TPU accel-erators [19]. We also report our batch sizes, epochs or total number of update steps, which allow to estimate resource re-quirements for any particular experiment of interest. Model code and weights are made publicly available2. 3. Distillation for model compression 3.1. Investigating the "consistent and patient teacher" hypothesis
In this section, we perform an experimental veriﬁcation of our hypothesis formulated in the introduction and visualised in Figure 2, that distillation works best when seen as function matching, i.e. when the student and teacher see consistent views of the input images, synthetically ”ﬁlled” via mixup, and when student is trained using long training schedule (i.e. the “teacher” is patient).
To make sure that our ﬁndings are robust, we perform a very thorough analysis on four small and medium scale datasets, namely Flowers102 [30] (1020 training images),
Pets [32] (3312 training images), Food101 [20] (about 68k training images), and SUN397 [47] (76k training images).
In an effort to remove any confounding factors, for each individual distillation setting we sweep over all combina-tions of learning rates {0.0003, 0.001, 0.003, 0.01}, weight decays {1 · 10−5, 3 · 10−5, 1 · 10−4, 3 · 10−4, 1 · 10−3}, and 2https://github.com/google-research/big_transfer
Figure 3. Experimental validation of the “consistency” requirement on the Flowers102 dataset. Colors match different knowledge distillation design choices as introduced in Figure 2 and Section 3.1.1. Note that while the ﬁxed teacher settings achieve signiﬁcantly lower distillation loss, they lead to students which do not generalize well. In contrast, consistent teaching and function matching approaches lead to signiﬁcantly higher student performance. Similar results on more datasets are reported in Appendix C. distillation temperatures {1, 2, 5, 10}. In all reported ﬁgures, we show every single run as a low opacity curve, and high-light the one with the best ﬁnal validation accuracy. We provide corresponding test accuracies in Appendix A. 3.1.1 Importance of “consistent” teaching
First, we demonstrate that the consistency criterion, i.e. stu-dent and teacher seeing the same views, is the only way of performing distillation which reaches peak student perfor-mance across all datasets consistently. For this study, we deﬁne multiple distillation conﬁgurations which correspond to instantiations of all four options sketched in Figure 2, with the same color coding:
• Fixed teacher. We explore several options where the teacher’s predictions are constant for a given image (precomputed target). The simplest (and worst) method is fix/rs, where the image is just resized to 2242px for both student and teacher. fix/cc follows a more common approach of using a ﬁxed central crop for the teacher and a mild random crop for the student. fix/ic ens is a heavy data augmentation approach where the teacher’s prediction is the average of 1k incep-tion crops, which we veriﬁed to improve the teacher’s performance. The student also uses random inception crops. The two latter settings are similar to the input noise strategy from the “noisy student” paper [48].
• Independent noise. We instantiate this common strat-egy in two ways: ind/rc computes two independent mild random crops for the teacher and student respec-tively, while ind/ic uses the heavier inception crop instead. A similar setup was used in [40].
• Consistent teaching. In this approach, we randomly crop the image only once, either with mild random crop-ping (same/rc) or heavy inception crop (same/ic), and use this same crop for the input to both the student and the teacher.
• Function matching. This approach extends consistent teaching, by expanding an input manifold of images through mixup (mix), and, again, providing consistent inputs to the student and the teacher. For brevity, we sometimes refer to this approach as “FunMatch”.
Figure 3 shows 10 000 epoch training curves on Flow-ers102 dataset in all of these conﬁgurations. These results clearly show that “consistency” is the key: all “inconsistent” distillation settings plateau at a lower score, while consistent settings increase student performance signiﬁcantly, with the function matching approach working the best. Furthermore, the training losses show that, for such small datasets, using a
ﬁxed teacher leads to strong overﬁtting. In contrast, function matching never reaches such loss on the training set while generalizing much better to the validation set. Due to space constraints, we show analogous results for other datasets and training durations in Appendix C. 3.1.2
Importance of “patient” teaching
One can interpret distillation as a variant of supervised learn-ing, where labels (potentially soft) are provided by a strong teacher model. This is especially true when the teacher predictions are (pre)computed for a single image view. This approach inherits all problems of the standard supervised learning, e.g. aggressive data augmentations may distort actual image label, while less aggressive augmentations may cause overﬁtting.
However, things change if we interpret distillation as function matching, and, crucially, make sure to provide con-sistent inputs to the student and teacher. In this case we can be very aggressive with image augmentations: even if an
Figure 4. One needs patience along with consistency when doing distillation. Eventually, the teacher will be matched; this is true across various datasets of different scale. image view is too distorted, we still will make a progress towards matching the relevant functions on this input. Thus, we can be more opportunistic with augmentations and avoid overﬁtting by doing aggressive image augmentations and, if true, optimize for very long time until the student’s function comes close to the teacher’s.
We empirically conﬁrm our intuition in Figure 4, where for each dataset we show the evolution of test accuracy dur-ing training of the best function matching student (according to validation), for different amounts of training epochs. The teacher is shown as a red line and is always reached eventu-ally, after a much larger number of epochs than one would ever use in a supervised training setup. Crucially, there is no overﬁtting even when we optimize for a 1M epochs.
We also trained and tuned two more baselines for refer-ence: training a ResNet-50 from scratch using the dataset original hard labels, as well as transferring a ResNet-50 that was pre-trained on ImageNet-21k. For both of these base-lines, we heavily tune learning rate and weight decay as described in Section 3.1. The model trained from scratch using the original labels is substantially outperformed by our student. The transfer model fares much better, but is eventually also outperformed.
Notably, training for a relatively short but common dura-tion of 100 epochs leads to much worse performance than the transfer baseline. Overall, the ResNet-50 student patiently and consistently matches the very strong but much more expensive ResNet-152x2 teacher across the board. 3.2. Scaling up to ImageNet
Based on our insights from the previous sections, we now investigate how the proposed distillation recipe scales to the widely used and more challenging ImageNet dataset [35].
Following the same protocol as before, in Figure 5 (left), we report student accuracy curves throughout training for three distillation settings: (1) ﬁxed teacher, (2) consistent teaching and (3) function matching. For reference, our base teacher model reaches a top-1 accuracy of 83.0%. Fixed teacher again suffers from long training schedules, and starts overﬁtting after 600 epochs. In contrast, the consistent teach-ing approaches continuously improves performance as the training duration increases. From this we can conclude that consistency is a key to make distillation work on ImageNet, similar to the behaviors on the previously discussed small and mid-sized datasets.
Compared to simple consistent teaching, function match-ing performs slightly worse with short schedules, which likely happens due to underﬁtting. But when we increase the length of training schedule, the improvement of function matching becomes apparent: for instance with only 1200 epochs, it is able to match the performance of consistent teaching at 4800 epochs, thus saving 75% compute resource.
Finally, for the longest run of function matching we ex-perimented on, the vanilla ResNet-50 student architecture achieves 82.31% top-1 accuracy on ImageNet. 3.3. Distilling across different input resolutions
So far, we have assumed that both the student and teacher receive the same standard input resolution of 224px. How-ever, it is possible to pass images of different resolution to the student and the teacher, while still being consistent: one sim-ply has to perform the crop on the original high-resolution image, and subsequently resize it differently for the student and the teacher: their views will be consistent, albeit at dif-ferent resolutions. This insight can be leveraged for learning from a better, higher resolution, teacher [22, 42], but also for training a smaller, faster student [2]. We investigate both directions: ﬁrst, following [2], we train a ResNet-50 student
Figure 5. Left: Top-1 accuracy on ImageNet of three distillation setups: (1) ﬁxed teacher; (2) consistent teaching; (3) function matching (“FunMatch”). Light color curves show accuracy throughout training, while the solid scatter plots are the ﬁnal results. The student with a
ﬁxed teacher eventually saturates and overﬁts to it. Both consistent teaching and function matching do not exhibit overﬁtting or saturation.
Middle: Reducing the optimization cost, via Shampoo preconditioning; with 1200 epochs, it is able to match the baseline trained for 4800 epochs. Right: Initializing student with pre-trained weights improves short training runs, but harms for the longest schedules. with an input resolution of 160px while leaving the teacher resolution unchanged (224px). This results in a twice faster model, which still achieves remarkable 80.49% top-1 accu-racy (see Table 1), compared to the best published 78.8% at this resolution using an array of modiﬁcations [2].
Second, following [22], we distill a teacher that was ﬁne-tuned at a resolution of 384px (and attains 83.7% top-1 accu-racy), this time leaving the student resolution unchanged, i.e. consuming a 224px input image. Compared to the baseline teacher, this provides a modest but consistent improvement across the board, as shown in Table 1. 3.4. Optimization: A second order preconditioner
) improves training efﬁciency (
We observe that optimization efﬁcacy creates a compu-tational bottleneck for our distillation recipe with “function matching” perspective due to long training schedules. Intu-itively, we believe that optimization difﬁculties stem from the fact that it is much harder to ﬁt a general function with mul-tivariate outputs, rather than ﬁxed image-level labels. Thus, we conduct an initial exploration, whether more powerful optimizers can do a much better job at our task.
To this end, we change the underlying optimizer from
Adam to Shampoo [1], with the second order preconditioner.
In Figure 5 (middle) we observe that Shampoo achieves the same test accuracy reached by Adam at 4800 epochs in just 1200 epochs, and with minimal step time overhead. And, in general, we observe consistent improvement over Adam in all our experimental settings. Experimental details on the
Shampoo optimizer are provided in the Appendix D. 3.5. Optimization: A good initialization improves short runs but eventually falls behind
Motivated by transfer learning literature [10, 22] and [37], where a good initialization is able to signiﬁcantly shorten the training cost and achieve a better solution, we try to initial-ize the student model with a pre-trained BiT-M-ResNet50 weights and show the results in Figure 5 (right).
The BiT-M initialization improves more than 2% when the distillation duration is short. However, the gap closes when the training schedule is long enough. Our observa-tion is similar to the conclusion of [10]. Starting from 1200 epochs, distilling from scratch matches the BiT-M initial-izated student, and slightly overtakes it for 4800 epochs. 3.6. Distilling across different model families
Going beyond using different input resolutions for student and teacher, nothing in principle prevents us from using architectures of different families entirely, as our consistent patient teacher approach still applies in this setting. This allows us to efﬁciently transfer knowledge from stronger and more complex teachers, e.g. ensembles, while keeping the simple architecture of a ResNet50 student, but also transfer the state-of-the-art performance of large ResNet models to more efﬁcient architectures e.g. MobileNet. We demonstrate this via two experiments. First, we use an ensemble of two models as teacher and show that this further improves performance. Second, we train a MobileNet v3 [13] student and obtain the best reported MobileNet v3 model to date.
MobileNet student. We use MobileNet v3 (Large) as a
Figure 6. Distilling pet and sun397 datasets on different data sources. Results indicate that distilling on completely unrelated images works to some extent, even though ﬁnal results are relatively low. Distilling on “in-domain” data is the best and distilling on related/overlapping images can work reasonably well, but may require extra long training schedule.
Figure 7. Baseline ResNet-50 models trained from scratch with labels vs with the ResNet-152x2 teacher. student, for most experiments we opt for the variant which uses GroupNorm (with the default of 8 groups) instead of
BatchNorm. We do not use any of the training tricks used in the original paper,we simply perform function matching.
Our student reaches 74.60% after 300 epochs, and 76.31% after 1200 epochs, resulting in the best published MobileNet v3 model. More results are in the Appendix A.
Ensemble teacher. We now try a better teacher: we create a model which consists of averaging the logits from our default teacher at 224px resolution, and our teacher at 384px resolution from the previous section. This is a different, though closely related, type of teacher which is signiﬁcantly more powerful but also slower. This teacher’s student is better than our default teacher’s student at every duration we tried (Appendix A) and, after 9600 epochs, reaches a new state-of-the-art top-1 ImageNet accuracy of 82.82%. 3.7. Comparison to the results from literature.
Now, when we introduced our key experiments, we com-pare our best ResNet-50 models to the best ResNet-50 mod-els available in the literature, see Table 2. In particular, for 224 × 224 input resolution we compare against the original
ResNet-50 model from [11], BiT-M-ResNet-50 pretrained on ImageNet-21k dataset [36] and previous state-of-the-art model from [37]. For 160 × 160 input resolution we com-pare against very recent and competitive model from [2]. We observe that our distillation recipe leads the state-of-the-art performance in both cases and by a signiﬁcant margin. 3.8. Distilling on the "out-of-domain" data
By looking at knowledge distillation as “function match-ing”, one can draw a reasonable hypothesis that distillation can be done on arbitrary image inputs. In this section we investigate this hypothesis.
We conduct experiments on pets and sun397 datasets. We use our distillation recipe to distill pets and sun397 models using out-of-domain images from the food101 and ImageNet datasets and, for the reference results, also run distillation with “in-domain” images from pets and sun397 datasets.
Figure 6 summarizes our results. First we observe that distilling using in-domain data works the best. Somewhat surprisingly, even if the images are completely unrelated, distillation still works to some extent, though results get worse. This, for example, means that the student model can learn to classify pets with roughly 30% accuracy by only seeing food images (softly) labeled as breeds of pets. Finally, if distillation images are somewhat related or overlapping with the actual “in-domain” images (e.g. Pets and ImageNet, or sun397 and ImageNet), then results can be as good (or almost as good) as using “in-domain” data, but extra long optimization schedule may be required. 3.9. Finetuning ResNet-50 with augmentations
To make sure that our observed state-of-the-art distillation results are not an artifact of our well-tuned training setup, namely very long schedule and aggressive mixup augmen-tations, we train corresponding baseline ResNet-50 models.
More speciﬁcally, we reuse the distillation training setup for supervised training on ImageNet dataset without distillation loss. To further strengthen our baseline, we additionally try
SGD optimizer with momentum, which is known to often work better for ImageNet than Adam optimizer.
Results are shown in Figure 7. We observe that training with labels and without distillation loss leads to signiﬁcantly worse results and starts to overﬁt for long training schedules.
Thus, we conclude that distillation is necessary to make our training recipe work well.
Table 1. Top-1 test accuracy for different teacher/student input resolu-tions (rows) and number of training epochs (columns).
Experiment
T224 → S224
T224 → S160
T384 → S224 300 80.30 78.17 80.46 1200 81.54 79.61 81.82 4800 82.18
N/A 82.33 9600 82.31 80.49 82.64 4.