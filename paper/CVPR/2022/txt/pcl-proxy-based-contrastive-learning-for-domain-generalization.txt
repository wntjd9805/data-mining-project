Abstract
Domain generalization refers to the problem of training a model from a collection of different source domains that can directly generalize to the unseen target domains. A promising solution is contrastive learning, which attempts to learn domain-invariant representations by exploiting rich semantic relations among sample-to-sample pairs from dif-ferent domains. A simple approach is to pull positive sam-ple pairs from different domains closer while pushing other negative pairs further apart.
In this paper, we ﬁnd that directly applying contrastive-based methods (e.g., super-vised contrastive learning) are not effective in domain gen-eralization. We argue that aligning positive sample-to-sample pairs tends to hinder the model generalization due to the signiﬁcant distribution gaps between different do-mains. To address this issue, we propose a novel proxy-based contrastive learning method, which replaces the orig-inal sample-to-sample relations with proxy-to-sample rela-tions, signiﬁcantly alleviating the positive alignment issue.
Experiments on the four standard benchmarks demonstrate the effectiveness of the proposed method. Furthermore, we also consider a more complex scenario where no ImageNet pre-trained models are provided. Our method consistently shows better performance. 1.

Introduction
Deep neural networks (DNNs) have achieved signiﬁ-cant success in various applications, assuming the train-ing and test data are independent and identically distributed (i.i.d.) [2, 12, 19, 20, 25, 40, 41, 47, 52]. However, in many real-world problems, training and testing datasets are col-lected under different scenarios, which leads to the DNNs trained on the source data performing poorly on the out-of-distribution target data. Such performance degeneration due to domain shift impairs the generalization ability of DNNs.
The literature in domain generalization (DG) aims to ad-dress this issue by exploiting the diversity of source do-mains to improve model generalization.
Different from domain adaptation task, it is assumed
Figure 1. (a) PACS dataset is a typical domain generalization benchmark that contains four domains: Art, Cartoon, Photo and
Sketch with seven categories in each domain. Domain Generaliza-tion task aims to train the model from multi-source domains (e.g., art, photo, sketch) and test on target domain (e.g., cartoon). In the training stage, the target dataset can not be accessed. (b) Typical contrastive-based loss (e.g., supervised contrastive loss) exploits sample-to-sample relations, where different domain samples from the same class can be regarded as positive pairs. We argue that optimizing some hard positive pairs may worsen the model gener-alization. We term it as the positive alignment problem. (c) Based on our observation, we propose a proxy-based contrastive loss. By replacing the sample-to-sample relations with proxy-to-sample re-lations, we largely alleviate the positive alignment problem. that only source domains can be accessed during training.
Therefore, most prior works in DG task focus on learn-ing domain-invariant representations by aligning different source domains [31, 33, 35]. Contrastive learning provides a potential solution to address this problem. The key idea is to construct multiple positive and negative pairs, which are then used to learn to optimize a distance metric that
we align a projection head on both sample embeddings and proxy weights and use the new embeddings and new proxy weights for proxy-based contrastive loss.
Our contributions are as follows: First, we empirically unveil the degradation of model generalization from posi-tive alignment problem in contrastive learning. Second, we propose a novel proxy-based contrastive learning technique for domain generalization. The proposed technique is fairly simple yet effective. Third, the proposed algorithm achieves state-of-the-art accuracy on multiple standard benchmarks and consistently improves the model performance in a more complex scenario where ImageNet pre-trained models are not provided. 2.