Abstract
Unpaired image-to-image (I2I) translation often requires to maximize the mutual information between the source and the translated images across different domains, which is critical for the generator to keep the source content and pre-vent it from unnecessary modiﬁcations. The self-supervised contrastive learning has already been successfully applied in the I2I. By constraining features from the same loca-tion to be closer than those from different ones, it implicitly ensures the result to take content from the source. How-ever, previous work uses the features from random loca-tions to impose the constraint, which may not be appropri-ate since some locations contain less information of source domain. Moreover, the feature itself does not reﬂect the relation with others. This paper deals with these prob-lems by intentionally selecting signiﬁcant anchor points for contrastive learning. We design a query-selected attention (QS-Attn) module, which compares feature distances in the source domain, giving an attention matrix with a prob-ability distribution in each row. Then we select queries according to their measurement of signiﬁcance, computed from the distribution. The selected ones are regarded as anchors for contrastive loss. At the same time, the re-duced attention matrix is employed to route features in both domains, so that source relations maintain in the synthe-sis. We validate our proposed method in three different I2I datasets, showing that it increases the image quality with-out adding learnable parameters. Codes are available at https://github.com/sapphire497/query-selected-attention.
Source 
G
Target
E
E
D
QS-Attn
Module
Contrastive 
Loss real or  fake anchor:  positive:  negative: 
Figure 1. The overall structure of our model. The source domain image Ix is translated by the generator G into a target domain image G(Ix). The encoder E extracts features from these two images, then the QS-Attn module selects signiﬁcant features to establish the contrastive loss. We also use a discriminator D to construct the adversarial loss. 1.

Introduction
In image-to-image (I2I) translation, an input from the source domain X is mapped into the target domain Y while keeping its original content from unnecessary modiﬁca-*Corresponding author, email: sunli@ee.ecnu.edu.cn. This work is supported by the Science and Technology Commission of Shanghai Mu-nicipality (No.19511120800) and Natural Science Foundation of China (No.61302125). tions. The translation is usually achieved by a generator G in the structure of auto-encoder with its output constraining by a discriminator D, so that it fulﬁlls the requirement of the domain Y . In many I2I tasks, paired data are impossi-ble to obtain, hence G can not be directly guided by the real image in Y . Ensuring that the output takes the input content is important for increasing its quality. Typical methods pro-N
A
G e l c y
C
T
U
C
N
A
G e l c y
C
T
U
C
Figure 2. Visualization of feature signiﬁcance metric from pre-trained CycleGAN and CUT on three datasets. We show the en-tropy of attention matrix for each location, the warmer color in-dicates that the entropy is smaller. For each dataset, on the left column are the input images, and on the right column are the en-tropy visualizations of two models. posed in [25,42,47] translate the result back into domain X by another generator G(cid:2), and set up the cycle consistency penalty between the input and the ﬁnal output. Although improving the quality, they introduce two generators and discriminators, which greatly increase the training costs.
Recently, CUT [34] incorporates the contrastive learning between the cross domain features from G’s output and in-put. The key idea is to constrain the features from encoder
E, requiring those from the same location to be close, com-pared with those from different locations. Removing the
QS-Attn module, Fig. 1 illustrates the overall structure of
CUT. An anchor point at a random position is selected from the features of the translated image, then one correspond-ing positive and many negatives are also sampled from the features of input. The contrastive loss is computed for the anchor so that the model maximizes the mutual informa-tion between the corresponding features. Note that CUT has only a single direction. Therefore, only one G is needed, and the training cost is reduced. The image quality is greatly improved, showing that contrastive loss across domains is useful in I2I.
However, there are still two issues ignored by CUT, which can be potentially improved. First, it does not se-lect the anchors with purpose in the contrastive learning.
Since each of them represents a small patch in the orig-inal image resolution and many of them may not reﬂect any domain characteristics relevant for I2I. We argue that only those containing signiﬁcant domain information need to be edited, and the contrastive loss imposed on them are more meaningful to guarantee the consistency across do-mains. Second, each anchor feature has only limited recep-tive ﬁeld, and it does not consider its relation with other locations. This relation provides valuable cues to keep the source content stable and make the translation relevant.
We consider the above two issues in a simple way, in-serting the QS-Attn module into the model as Fig. 1 shows, without bringing in extra model parameters. To evaluate the feature signiﬁcance at different locations, we directly utilize features from E as both queries and keys to calculate the at-tention matrix in the source domain, then the distribution entropy is computed as a metric. Intuitive illustrations are provided in Fig. 2, in which such an entropy metric is vi-sualized in the form of heat map. Particularly, given input images that need to be translated, we apply the encoder of pretrained CycleGAN [47] and CUT [34] models to obtain the features and calculate the attention matrix, and then the entropy is computed for each row of it. We sort the entropy in ascending order and show the smallest N points on the image. For the Horse and Cat images, the entropy values on the body of horse and the face of cat are smaller. For the Label image, the points mainly locate at the edges of categories. Consequently, the entropy can be a metric to measure how important the feature is in reﬂecting domain characteristics, hence we can impose the contrastive loss on it, ensuring the accurate translation on the domain-relevant features.
This paper intends to quantitatively measure the signif-icance of each anchor feature and select the relevant ones for the contrastive loss according to the metric. Based on the previous analysis, we calculate the entropy of each row in the attention matrix and keep those with smaller entropy values. The remaining rows form the query-selected atten-tion (QS-Attn) matrix, which consists of fewer queries, and they are further employed to route the value feature. Here the same matrix is multiplied with the values from both source and target domains, which implicitly keeps the fea-ture relation in the source domain, avoiding excessive mod-iﬁcations on the result.
The contributions of this paper lie in following aspects:
• We propose a QS-Attn mechanism in I2I task. Our scheme is to choose the relevant anchor points, and use them as queries to attend and absorb features at other locations, forming better features suitable for contrastive learning. The QS-Attn keeps the simple design in CUT, and does not add any model parame-ters.
• We investigate different ways to quantify the signiﬁ-cance of the queries, to perform the attention, and to route value features in QS-Attn module, and ﬁnd the entropy-based measurement and global attention for the cross domain value routing is the robust one.
• We do intensive experiments on the commonly used datasets, and show the proposed method achieves
SOTA in most two domains I2I tasks. 2.