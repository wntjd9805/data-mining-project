Abstract
Co-salient object detection, with the target of detecting co-existed salient objects among a group of images, is gain-ing popularity. Recent works use the attention mechanism or extra information to aggregate common co-salient fea-tures, leading to incomplete even incorrect responses for target objects. In this paper, we aim to mine comprehen-sive co-salient features with democracy and reduce back-ground interference without introducing any extra informa-tion. To achieve this, we design a democratic prototype generation module to generate democratic response maps, covering sufficient co-salient regions and thereby involv-ing more shared attributes of co-salient objects. Then a comprehensive prototype based on the response maps can be generated as a guide for final prediction. To suppress the noisy background information in the prototype, we pro-pose a self-contrastive learning module, where both posi-tive and negative pairs are formed without relying on ad-ditional classification information. Besides, we also de-sign a democratic feature enhancement module to further strengthen the co-salient features by readjusting attention values. Extensive experiments show that our model obtains better performance than previous state-of-the-art methods, especially on challenging real-world cases (e.g., for CoCA, we obtain a gain of 2.0% for MAE, 5.4% for maximum F-measure, 2.3% for maximum E-measure, and 3.7% for S-measure) under the same settings. Source code is available at https://github.com/siyueyu/DCFM . 1.

Introduction
Co-salient object detection (CoSOD) aims to detect the common salient objects among a group of input images.
Unlike salient object detection (SOD), which is to detect the most attractive objects by mimicking human eyes [3,21,
*corresponding author 1The work was supported by National Natural Science Foundation of
China under 61972323.
Figure 1. Visualization of response maps. (a) Inputs; (b) Response maps generated by the previous approach [11]; (c) Ours. It can be seen that ours can cover more co-salient objects. 24, 27, 30, 31, 36, 45], CoSOD focuses on detecting salient yet co-existed objects among all the input images. In this case, CoSOD faces two main challenges: 1) reduce the interference of noisy background in complex scenes; 2) mine integral co-salient objects with large appearance vari-ations. Some works introduce extra SOD dataset to provide saliency guidance [52, 53] or predict saliency maps [19] in order to mask out the co-salient objects. However, these approaches highly depend on the extra dataset, leading to supererogatory human effort to provide annotations.
Recent approaches [11, 19, 49, 52] try to use attention mechanism [39] to strengthen co-salient features or build feature consistency to formulate the shared attributes of co-salient objects for integral predictions. However, there are two main drawbacks when directly applying attention mechanism for this task. On the one hand, the response maps reflecting the shared attributes, obtained in the atten-tion mechanism, can only cover limited pixels belonging to co-salient objects, as shown in Fig. 1.(b). In this case, it is difficult for the model to learn comprehensive shared attributes of co-salient objects. On the other hand, for com-plex scenes, the attention mechanism tends to focus on the
wrong object regions, as shown in the second picture of-Fig. 1.(b). Some methods such as GCoNet [11] propose a kind of group collaborative learning by collecting artifi-cial negative group pairs. However, their pairs are grouped based on the auxiliary classification information, which re-quires major effort to group dissimilar negative category pairs as there is no clear definition of natural discrete ob-ject categories in real world [37].
To solve aforementioned issues, we design a novel
Democratic
Co-salient-Feature-Mining framework (DCFM). Our DCFM can directly mine more com-prehensive features and suppress the noisy background effectively without using extra SOD dataset or classification information. Specifically, in order to mine sufficient co-salient information, we first design a democratic prototype generation module (DPG), where democratic response maps are generated to capture more shared attributes. As shown in Fig. 1.(c), our response maps cover more regions of co-salient objects. Then, a prototype with comprehen-sive co-salient information can be generated according to the democratic response maps, which can further guide the model to predict the co-salient objects.
Next, in order to suppress noisy background information in our prototype and avoid introducing extra classification information, we propose a simple self-contrastive learning module (SCL) to form positive and negative pairs to filter noise. We argue that the prototype generated from original images should be consistent with that generated when the image background regions are erased, and should be dif-ferent from that generated when the co-salient objects are erased. Thus, a self-contrastive loss among these prototypes is designed to suppress the influence of noisy background and help the model learn more discriminative features of co-salient objects.
Finally, to further strengthen the detected co-salient fea-tures from the above modules, we design a democratic fea-ture enhancement module (DFE) based on the attention mechanism [39]. As mentioned before, the attention mech-anism tends to focus on a limited number of correlated fea-tures, which fails to provide comprehensive information.
Therefore, we readjust the attention values to generate a democratic attention map aggregating more correlated pix-els for feature enhancement.
Generally, our main contributions can be summarized as:
• A democratic prototype generation module (DPG) is designed to build response maps covering sufficient co-salient regions, so as to generate a prototype con-taining comprehensive shared attributes as guidance for co-saliency prediction.
• A self-contrastive learning module (SCL) is proposed to help our model reduce the influence of noisy back-ground without relying on additional classification in-formation, where both positive and negative samples are generated from the image itself.
• A democratic feature enhancement module (DFE) is designed to further strengthen the co-salient features by adjusting attention values to involve more related pixels.
• Extensive experiments show that our method per-forms better than state-of-the-art methods, especially on challenging real-world cases, such as the CoCA dataset, we obtain a gain of 2.0% for MAE, 5.4% for maximum F-measure, 2.3% for maximum E-measure, and 3.7% for S-measure under the same settings. 2.