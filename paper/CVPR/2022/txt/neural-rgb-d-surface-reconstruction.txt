Abstract
Obtaining high-quality 3D reconstructions of room-scale scenes is of paramount importance for upcoming ap-plications in AR or VR. These range from mixed reality applications for teleconferencing, virtual measuring, vir-tual room planing, to robotic applications. While current volume-based view synthesis methods that use neural radi-ance fields (NeRFs) show promising results in reproducing the appearance of an object or scene, they do not recon-struct an actual surface. The volumetric representation of the surface based on densities leads to artifacts when a sur-face is extracted using Marching Cubes, since during opti-mization, densities are accumulated along the ray and are not used at a single sample point in isolation. Instead of this volumetric representation of the surface, we propose to represent the surface using an implicit function (truncated signed distance function). We show how to incorporate this representation in the NeRF framework, and extend it to use depth measurements from a commodity RGB-D sensor, such as a Kinect. In addition, we propose a pose and camera re-finement technique which improves the overall reconstruc-tion quality.
In contrast to concurrent work on integrat-ing depth priors in NeRF which concentrates on novel view synthesis, our approach is able to reconstruct high-quality, metrical 3D reconstructions. 1.

Introduction
Research on neural networks for scene representations and image synthesis has made impressive progress in re-cent years [73]. Methods that learn volumetric represen-tations [42, 48] from color images captured by a smart-phone camera can be employed to synthesize near photo-realistic images from novel viewpoints. While the focus of these methods lies on the reproduction of color images, they are not able to reconstruct metric and clean (noise-free) meshes. To overcome these limitations, we show that there is a significant advantage in taking additional range measurements from consumer-level depth cameras into ac-count.
Inexpensive depth cameras are broadly accessible and are also built into modern smartphones. While classical reconstruction methods [9, 33, 53] that purely rely on depth measurements struggle with the limitations of physical sen-sors (noise, limited range, transparent objects, etc.), a neu-ral radiance field-based reconstruction formulation allows to also leverage the dense color information. Methods like
BundleFusion [14] take advantage of color observations to compute sparse SIFT [44] features for re-localization and refinement of camera poses (loop closure). For the actual geometry reconstruction (volumetric fusion), only the depth maps are taken into account. Missing depth measurements in these maps, lead to holes and incomplete geometry in the reconstruction. This limitation is also shared by learned surface reconstruction methods that only rely on the range data [47, 67].
In contrast, our method is able to recon-struct geometry in regions where only color information is available. Specifically, we adapt the neural radiance field (NeRF) formulation of Mildenhall et al. [48] to learn a trun-cated signed distance field (TSDF), while still being able to leverage differentiable volumetric integration for color re-production. To compensate for noisy initial camera poses which we compute based on the depth measurements, we jointly optimize our scene representation network with the camera poses. The implicit function represented by the scene representation network allows us to predict signed distance values at arbitrary points in space which is used to extract a mesh using Marching Cubes.
Concurrent work that incorporates depth measurements in NeRF focuses on novel view synthesis [16, 49, 81], and uses the depth prior to restrict the volumetric rendering to near-surface regions [49,81] or adds an additional constraint on the depth prediction of NeRF [16]. NeuS [76] is also a concurrent work on novel view synthesis which uses a signed distance function to represent the geometry, but takes only RGB images as input, and thus fails to reconstruct the geometry of featureless surfaces, like white walls. In con-trast, our method aims for high-quality 3D reconstructions of room-scale scenes using an implicit surface representa-tion and direct SDF-based losses on the input depth maps.
Comparisons to state-of-the-art scene reconstruction meth-ods show that our approach improves the quality of geome-try reconstructions both qualitatively and quantitatively.
In summary, we propose an RGB-D based scene recon-struction method that leverages both dense color and depth observations.
It is based on an effective incorporation of depth measurements into the optimization of a neural radi-ance field using a signed distance-based surface represen-tation to store the scene geometry. It is able to reconstruct geometry detail that is observed by the color images, but not visible in the depth maps.
In addition, our pose and camera refinement technique is able to compensate for mis-alignments in the input data, resulting in state-of-the-art re-construction quality which we demonstrate on synthetic as well as on real data from ScanNet [12]. 2.