Abstract
Denoising and demosaicking are two essential steps to reconstruct a clean full-color image from the raw data. Re-cently, joint denoising and demosaicking (JDD) for burst images, namely JDD-B, has attracted much attention by us-ing multiple raw images captured in a short time to recon-struct a single high-quality image. One key challenge of
JDD-B lies in the robust alignment of image frames. State-of-the-art alignment methods in feature domain cannot ef-fectively utilize the temporal information of burst images, where large shifts commonly exist due to camera and ob-ject motion.
In addition, the higher resolution (e.g., 4K) of modern imaging devices results in larger displacement between frames. To address these challenges, we design a differentiable two-stage alignment scheme sequentially in patch and pixel level for effective JDD-B. The input burst images are firstly aligned in the patch level by using a dif-ferentiable progressive block matching method, which can estimate the offset between distant frames with small com-putational cost. Then we perform implicit pixel-wise align-ment in full-resolution feature domain to refine the align-ment results. The two stages are jointly trained in an end-to-end manner. Extensive experiments demonstrate the signifi-cant improvement of our method over existing JDD-B meth-ods. Codes are available at https://github.com/
GuoShi28/2StageAlign. 1.

Introduction
Color demosaicking and denoising are two essential steps in digital camera imaging pipeline to reconstruct a high quality full-color image from the sensor raw data.
Color demosaicking [9, 20, 29, 45, 46] recovers the miss-ing color components from the color-filter-array (CFA) data collected by the single-chip CCD/CMOS sensor, while de-noising [21, 31, 48, 49] removes the noise in image data caused by the photon arrival statistics and the imprecision in readout circuitry. Since the two tasks are actually cor-related and can be performed jointly, many joint denois-ing and demosaicking (JDD) algorithms have been devel-oped [5, 11, 14, 15, 23, 28], which are however focused on single image restoration. With the prevalent use of smart-phone cameras [1], it becomes crucial to restore images from data with low signal-to-noise ratio due to the small sensor and lens of smartphone cameras. To this end, per-forming JDD with burst images (JDD-B) has become in-creasingly popular and important in recent years [13].
Burst image processing refers to shooting a sequence of low quality frames in a short time and computationally fus-ing them to produce a higher-quality photograph [2, 41].
Compared with single image restoration, the key challenge of burst image restoration lies in the compensation for shift between frames. Previous studies often perform frame alignment by estimating optical flow [8, 44] and applying spatially variant kernels [32, 33, 42, 43] in the image do-main. However, affected by noise, accurately estimating optical flow and kernels is difficult. Recently, implicit align-ment in the feature domain has achieved state-of-the-art per-formance on video super-resolution [4, 40], video denois-ing [47], as well as JDD-B [13]. However, it is found that feature alignment is not very effective when handling im-age sequences with large shift. For example, for the pyra-mid feature alignment module in [40], the receptive field of offset estimation is about 28 (3 conv layers with 3 × 3 kernels on 1/4 scale), i.e., 14 pixels along one direction.
Such a search range is too small for burst images with large shift. Whereas, large shifts commonly exist in videos due to the camera and object motion. On the other hand, mod-ern image/video recorders can easily capture videos of 4K (4096 × 2160) or UHD (3840 × 2160) resolution. As a result, the pixel displacements between frames further in-crease. Even some small motion of foreground objects in 4K video can cause a large value of shift.
However, precise pixel-wise alignment on full size im-ages with large motion is very difficult and expensive. With a naive implementation, for each pixel, we assume the off-(a) Coarse Alignment (patch level) (b) Refined Alignment (pixel level) pyramid offset estimate (c) Aligned Feature Fusion estimated patches
↓ to 1 4 scale
∆pn
.
.
.
DPBM
Y, M coarsely aligned patches
→ deep features
BiGRU
.
.
.
∆pn
DConv aligned features
UNet
JDD-B result
Figure 1. Illustration of our network with differentiable two-stage alignment for JDD-B. set estimation with receptive field D × D costs F × D2 multiply-adds, where F is determined by different network structures. For input with size H × W , the offset estimation costs F × D2 × H × W multiply-adds. One straightfor-ward solution to handle large shift is to increase D by us-ing 3D conv layers [27] or calculating all-range correlation volume [24]. However, these solutions will significantly in-crease the computational cost and they are not efficient for images with large size.
To address these problems, we design a differentiable two-stage alignment framework, which divides the diffi-cult large shift alignment problem into two relatively eas-ier alignment sub-problems, i.e., coarse alignment (CA) and refined alignment (RA). The CA module aims to compen-sate large shift roughly using small calculating resources.
Instead of using pixel-wise alignment, the CA module per-forms alignment in patch level with F × D2 × (H/k) × (W/k) multiply-adds, where k is the patch size. Then the
RA module is used to pixel-wise align frames based on the results of CA module with smaller receptive field Ds. Such two-stage framework uses F × (D2/k2 + D2 s) × H × W multiply-adds in total, which is significantly smaller than directly aligning images using one-stage module, especially when D is large. Specifically, we utilize block matching (BM) based method as CA module in image domain.
To overcome the non-differentiability caused by BM and reduce the computational cost, we propose a differentiable progressive block matching (DPBM) method. We further propose a corresponding loss function for DPBM to sta-blize the training process. For the RA module, we per-form refined pixel-wise alignment implicitly in the feature domain using deformable convolution for accurate image restoration. The two stages of alignments are complemen-tary and they can enhance each other when jointly used in our learning framework. Compared with state-of-the-art (SOTA) method GCP-Net, our two-stage framework has fewer learnable parameters and similar running time, but achieves great improvement on images with large shift. The major contributions of this work are shown as follows:
• To efficiently handle images with large shift, we pro-pose a differentiable two-stage alignment framework, which divides the difficult large shift alignment prob-lem into two easier sub-problems.
• We propose a differentiable progressive block match-ing method in CA module to reduce computational cost and ensure that our two-stage framework can be end-to-end trained.
Our experiments on synthetic and real-world burst image datasets clearly show that our two-stage alignment method achieves impressive improvement over existing methods. 2.