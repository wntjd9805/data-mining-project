Abstract
Determining the motion behavior of inexhaustible cate-gories of traffic participants is critical for autonomous driv-ing.
In recent years, there has been a rising concern in performing class-agnostic motion prediction directly from the captured sensor data, like LiDAR point clouds or the combination of point clouds and images. Current motion prediction frameworks tend to perform joint semantic seg-mentation and motion prediction and face the trade-off be-In this pa-tween the performance of these two tasks. per, we propose a novel Spatial-Temporal Integrated net-work with Bidirectional Enhancement, BE-STI, to improve the temporal motion prediction performance by spatial se-mantic features, which points out an efficient way to com-bine semantic segmentation and motion prediction. Specif-ically, we propose to enhance the spatial features of each individual point cloud with the similarity among tempo-ral neighboring frames and enhance the global temporal features with the spatial difference among non-adjacent frames in a coarse-to-fine fashion. Extensive experiments on nuScenes and Waymo Open Dataset show that our pro-posed framework outperforms all state-of-the-art LiDAR-based and RGB+LiDAR-based methods with remarkable margins by using only point clouds as input.1 1.

Introduction
Modern self-driving vehicles are expected to operate in open traffic scenes with highly dynamical moving objects instead of only in closed scenes [1, 19, 32]. The motion of inexhaustible categories of traffic participants is critical for the safety of autonomous driving systems.
Traditional methods tend to formulate this task as trajec-tory prediction [1–5,11,14,42,44,45], which lacks the abil-ity to handle unexpected categories that have not been seen in training set due to the dependence on a separate object
*This work was done when Yunlong Wang and Yu-Huan Wu were re-search interns at Alibaba DAMO Academy.
†Equal contribution.
‡Corresponding author. 1The code will be released at https://github.com/be-sti/be-sti.
Figure 1. Comparison between BE-STI and previous motion pre-diction framework. Top row: Previous framework, which adopts global temporal pooling (GTP) to capture motion clues. Bottom:
Our proposed BE-STI framework. detector [27–29, 40, 41]. Scene flow [7, 8, 15, 18, 20, 33, 36] provides an attractive solution by estimating the dense motion field directly from LiDAR point clouds, which is computationally prohibitive for practical self-driving sys-tems [17, 35]. Recent works seek to perform joint seman-tic segmentation and motion prediction based on BEV oc-cupancy grids, which essentially solves a joint optimiza-tion problem and thus faces the trade-off between the per-formances of the above two tasks. Our goal in this pa-per is to explore a better way to combine the two tasks of semantics and motion, which utilizes a bidirectional en-hancement network to improve the motion prediction per-formance through more accurate spatial semantic features.
One may ask: whether semantic information can bene-fit motion prediction? We first seek to answer this ques-tion through a toy example. As shown in Tab. 1, we feed
MotionNet [35], previous state-of-the-art (SOTA) LiDAR-based motion prediction framework, with semantic ground truth (GT) as additional input, which outperforms the previ-ous work by a great margin. Specifically, we first use the segmentation GT of the points in the voxel to count the
Method
MotionNet [35]
MotionNet+ GTseg
Motion Prediction Mean Error (m) ↓
Static 0.0201 0.0015
Speed ≤ 5 m/s 0.2292 0.2139
Speed > 5 m/s 0.9454 0.7990
Table 1. Performance on the motion prediction task on nuScenes. distribution of the segmentation category as a segmentation vector, and then combine the vector with the original input to obtain a new input. Thus, we are confirmed high-quality semantic information have a positive impact on motion pre-diction task. To this end, we introduce a semantic decoder between our backbone network and motion decoder to ob-tain more accurate semantic information.
Considering that single frame of LiDAR information is sparse and the adjacent LiDAR frames describe similar scenes, the temporal information helps to extract more sta-ble and accurate spatial semantic information. Given this, we propose a temporal-enhanced spatial encoder (TeSE) to perform better spatial understanding of each individual frame. TeSE is introduced to capture the common feature of neighboring frames and merge it to feature maps of each in-dividual frame. In this way, the difficulty of individual spa-tial understanding caused by the sparsity of LiDAR points can be effectively compensated by adjacent frames.
Another concern about our framework is how to fully utilize the semantic information generated by our semantic decoder. To efficiently and effectively utilize the semantic feature, we propose a spatial-enhanced temporal encoder (SeTE) between the semantic decoder and motion decoder, which is designed to capture motion clues by discovering the spatial variation with time. Notice that temporal non-adjacent frames describe distinct scenes, we introduce SeTE to capture the discriminative feature of non-adjacent frames in time channel and feed it into motion decoder.
We name our novel proposed class-agnostic motion prediction network as bidirectional enhanced spatial-temporal integrated network (BE-STI). We provide a comparison between the sketches of BE-STI and previous framework in Fig. 1. As can be seen, apart from the tra-ditional backbone and motion decoder, we introduce three stacked modules: (1) TeSE, which is applied to perform spatial feature enhancement with temporal common fea-tures; (2) semantic decoder, which is applied to render mo-tion prediction modules with auxiliary semantic informa-tion; (3) SeTE, which is applied to enhance the temporal motion features with spatial discriminative features.
The implementation of BE-STI is quite simple but ef-ficient. All modules are built up with several stacked 2D and 3D convolution layers. Without any fancy struc-tures, BE-STI surpasses all previous SOTA LiDAR-based and RGB+LiDAR-based methods on nuScenes dataset with only LiDAR point clouds input while running at over 22Hz.
Our contributions can be summarized as follows:
• We propose a novel class-agnostic motion prediction framework, named BE-STI, in which the benefits of se-mantic information to motion prediction is extensively ex-plored. With the auxiliary semantic information in our framework, the motion prediction performance is signifi-cantly improved.
• We propose TeSE and SeTE to perform bidirectional enhancement between spatial and temporal feature extrac-tion, in which TeSE contributes to the spatial understanding of each individual frame while SeTE captures high-quality motion clues by extracting spatial discriminative features.
• Extensive experiments on nuScenes and Waymo Open
Dataset (WOD) demonstrates that BE-STI framework out-performs previous SOTA methods by a remarkable margin. 2.