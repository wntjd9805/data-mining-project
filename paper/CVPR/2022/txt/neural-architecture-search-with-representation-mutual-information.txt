Abstract 1.

Introduction
Performance evaluation strategy is one of the most im-portant factors that determine the effectiveness and efﬁ-ciency in Neural Architecture Search (NAS). Existing strate-gies, such as employing standard training or performance predictor, often suffer from high computational complexity and low generality. To address this issue, we propose to rank architectures by Representation Mutual Information (RMI). Speciﬁcally, given an arbitrary architecture that has decent accuracy, architectures that have high RMI with it always yield good accuracies. As an accurate performance indicator to facilitate NAS, RMI not only generalizes well to different search spaces, but is also efﬁcient enough to evaluate architectures using only one batch of data. Build-ing upon RMI, we further propose a new search algorithm termed RMI-NAS, facilitating with a theorem to guarantee the global optimal of the searched architecture. In particu-lar, RMI-NAS ﬁrst randomly samples architectures from the search space, which are then effectively classiﬁed as posi-tive or negative samples by RMI. We then use these samples to train a random forest to explore new regions, while keep-ing track of the distribution of positive architectures. When the sample size is sufﬁcient, the architecture with the largest probability from the aforementioned distribution is selected, which is theoretically proved to be the optimal solution. The architectures searched by our method achieve remarkable top-1 accuracies with the magnitude times faster search process. Besides, RMI-NAS also generalizes to different datasets and search spaces. Our code has been made avail-able at https://git.openi.org.cn/PCL AutoML/XNAS.
*Corresponding author.
†These authors contributed equally to this work.
Neural Architecture Search (NAS) is proposed to facil-itate the design of deep neural networks, which is a chal-lenging task and has demonstrated superior performance on various computer vision tasks, including but not limited to image classiﬁcation [61, 67], object detection [10, 53] and segmentation [7, 33]. As a widely accepted standpoint, a conventional NAS algorithm is divided into three compo-nents [19]: search space, search algorithm and performance estimation strategy. The search space deﬁnes the scope of the search, the search strategy investigates how to explore the search space and the performance estimation refers to how to estimate the performance for the architectures.
Through extensive experiments, previous works [62, 64] have demonstrated that performance estimation is the most important component in NAS. In particular, an optimal es-timation strategy is capable of improving both efﬁciency and effectiveness simultaneously with different search al-gorithms in NAS, which is investigated in [62, 64]. Ac-cording to the previous works [19, 31], performance es-timation strategies include multiple-ﬁdelity training meth-ods* [20, 42, 57, 62, 67], accuracy predictor based meth-ods [2, 37, 38, 49], one-shot methods [5, 35, 48, 51, 54, 56, 58–61, 65] and training-free based methods [8, 31, 39].
The key challenge of the performance estimation is the trade-off among the accuracy, generalization and computa-tion cost. Although multiple ﬁdelity and accuracy predic-tor based methods are accurate and generalize to different search spaces, such methods require a lot of computation re-sources which hinder the usage of NAS in practical applica-tions. For example, AmoebaNet [42] costs more than 3, 150
*These methods use different training hyper-parameters for accelera-tion, including but not limited to fewer epochs, subset of data and down-scaled models.
free based methods [8, 31, 39]. Overall, their search spaces are carefully selected or designed in their work.
In this paper, we propose a novel performance estimation strategy that ranks architectures by using the hidden Repre-sentation Mutual Information (RMI). In particular, through extensive experiments we ﬁnd architectures that have good accuracies always yield high RMI scores. In other words, an arbitrary architecture that has decent accuracy can be considered as an accurate indicator by using RMI to facili-tate NAS. For example, in the widely-used dataset CIFAR-10 [27], any architectures including human-designed or random sampled from pre-deﬁned search spaces that have
> 85% top classiﬁcation accuracy is used as an accurate performance indicator. In practice, the calculation and op-timization of RMI between two architectures only require a mini-batch of data. Compared to using the entire dataset during regular training, our approach enables a very signiﬁ-cant speedup to estimate each architecture. Moreover, RMI also generalizes to different search spaces, which makes it potentially widely applicable to real-world problems.
We further propose an effective and efﬁcient NAS algo-rithm that explores the search space by using the RMI score.
In terms of efﬁciency, the proposed RMI performs as the guidance in NAS. It eliminates the need for laborious train-ing on the whole dataset, thus signiﬁcantly reducing com-putation complexity. Speciﬁcally, RMI-NAS ﬁrst randomly samples architectures from the search space, which are then classiﬁed as positive or negative samples using RMI score.
These samples are used to train a random forest to further accelerate the exploration of the unseen regions. Mean-while, we keep tracking of the distribution of positive ar-chitectures. In terms of effectiveness, the architecture found is also theoretically guaranteed to be the optimal solution.
After the whole search space is fully explored by random forest [3], we then select the architecture with the largest probability from the aforementioned distribution. To sum-marize, our main contributions are two-fold:
• Based upon extensive statistical veriﬁcation, we em-pirically demonstrate that representation mutual infor-mation is a stable and accurate indicator to ﬁnd the op-timal architecture†. To the best of our knowledge, RMI is introduced for the ﬁrst time to the NAS community and can be easily incorporated into most existing NAS algorithms to speed up the search process.
• We introduce a novel NAS optimization method termed RMI-NAS, which is efﬁcient, fast and gener-alizes to different search spaces. We employ RMI and random forest to effectively explore the whole search space. Meanwhile, we also mathematically prove that
†Extensive experiments in Sec. 4 show that RMI shows a high correla-tion with the performance of the architectures in the search space.
Figure 1. (a) The illustration of time cost for evaluating 100 ar-chitectures in the NAS-Bench-201 [16] benchmark. RMI is much faster with marginal correlation drop compared to NASNet [66],
DARTS [35] and one-shot based methods [59]. (b) By incorport-ing RMI, we can largely accelerate NAS methods including rein-forcement learnming (RL) [66], evolution algorithm (REA) [42] and random search (RS). Meanwhile, the proposed method further improves the accuracy by a clear margin with less search costs.
GPU-days to search for the optimal on CIFAR-10 [27]. To reduce the computation costs, one-shot and training-free based methods are proposed. These methods are based on parameter sharing and deep learning theory respectively.
Albeit being more efﬁcient than multiple ﬁdelity and ac-curacy predictor based methods, one-shot and training-free based methods still have severe generalization issue. For example, the widely-used DARTS [35] is efﬁcient only on the cell-based search space [66], where the more practical chain-structure search space [48] is rarely adopted in these works [34, 35, 54]. Meanwhile, another popular one-shot strategy [4] only employs the MobileNet [23, 44] search space for validation. Such a problem also exists in training-the solution found by RMI-NAS is more likely to be the optimal solution in the search space.
Extensive experimental results demonstrate the efﬁciency and effectiveness of the proposed method on search spaces and datasets. Notably, under the setting of NAS-Bench-201
[16], our searched model achieves 94.36% test accuracy on
CIFAR-10 dataset within 30 minutes, which is attributed to the proposed RMI and search algorithm. 2.