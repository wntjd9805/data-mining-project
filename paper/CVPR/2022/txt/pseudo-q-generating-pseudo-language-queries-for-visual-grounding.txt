Abstract
Visual grounding, i.e., localizing objects in images ac-cording to natural language queries, is an important topic in visual language understanding. The most effective ap-proaches for this task are based on deep learning, which generally require expensive manually labeled image-query or patch-query pairs.
To eliminate the heavy depen-dence on human annotations, we present a novel method, named Pseudo-Q, to automatically generate pseudo lan-guage queries for supervised training. Our method lever-ages an off-the-shelf object detector to identify visual ob-jects from unlabeled images, and then language queries for these objects are obtained in an unsupervised fashion with a pseudo-query generation module. Then, we design a task-related query prompt module to speciﬁcally tailor generated pseudo language queries for visual grounding tasks. Further, in order to fully capture the contextual re-lationships between images and language queries, we de-velop a visual-language model equipped with multi-level cross-modality attention mechanism. Extensive experimen-tal results demonstrate that our method has two notable beneﬁts: (1) it can reduce human annotation costs signiﬁ-cantly, e.g., 31% on RefCOCO [65] without degrading orig-inal model’s performance under the fully supervised set-ting, and (2) without bells and whistles, it achieves supe-rior or comparable performance compared to state-of-the-art weakly-supervised visual grounding methods on all the
ﬁve datasets we have experimented. Code is available at https://github.com/LeapLabTHU/Pseudo-Q. 1.

Introduction
Visual grounding (VG) task [13, 24, 40, 65] has achieved great progress in recent years, with the advances in both computer vision [16,20,21,25,26,46,56,57,59] and natural language processing [4,14,41,50,53]. It aims to localize the
*Equal contribution.
†This work was done during an internship at Tsinghua.
‡Corresponding author.
Figure 1. Comparison with fully and weakly-supervised vi-sual grounding method. (a) Fully-supervised VG utilizes image region-query pairs as supervision signals. (b) Weakly-supervised
VG adopts only language queries. (c) Our Pseudo-Q method is free of any task-related annotations. objects referred by natural language queries, which is essen-tial for various vision-language tasks, e.g., visual question answering [2] and visual commonsense reasoning [67].
Most existing visual grounding methods can be catego-rized into two types: fully-supervised [8, 13, 22, 23, 33, 35] and weakly-supervised [6, 10, 19, 36, 38, 49, 55, 58]. Al-though these two lines of works have made remarkable suc-cesses, they rely heavily on manually annotated datasets.
However, obtaining a large quantity of manual annotations, especially natural language queries, is expensive and time-consuming. To annotate queries, humans need to ﬁrstly recognize the visual objects and identify their attributes, and then determine diverse relationships between them on a case-by-case basis, such as spatial (e.g., left and right), preposition (e.g., in and with), action (e.g., throwing some-thing), and comparative (e.g., smaller and bigger). Among them, spatial relation is the most frequently queried one.
To reduce the burden of human annotation, we propose a pseudo language query based approach (Pseudo-Q) for visual grounding. Our inspiration comes from previous works [17, 31] that address the high annotation cost issue in image captioning task, by leveraging an unlabelled im-age set, a sentence corpus, and an off-the-shelf object de-tector. However, the visual grounding task is more compli-cated and challenging, as it involves the modelling of rela-tions between objects.
To accurately ground objects by language queries, it’s fundamental to recognize their categories, attributes, and relationships. Thus, when it comes to generating pseudo region-query pairs for an unlabelled image set, we need (1) salient objects to focus on three key components: (nouns) which are most likely to be queried, (2) intrinsic attributes possessed by the queried objects, and (3) the im-portant spatial relationships between the objects. Moti-vated by [17, 42], we leverage an off-the-shelf object de-tector [1] to locate the most notable candidates with high conﬁdence, and an attribute classiﬁer [1] to recognize com-mon attributes. However, these detectors are unable to dis-tinguish the spatial relations between objects. Thus, we present a heuristic algorithm to determine the spatial re-lationships between the objects of the same class by com-paring their areas and relative coordinates. With these three essential components, pseudo-queries with respect to spa-tial relations between objects can be generated.
To further improve the performance of our method, we also propose a query prompt module which attentively tai-lors generated pseudo queries into task-related query tem-plates for visual grounding. For the visual-language model, we put forward a multi-level cross-modality attention mech-anism in the fusion module to encourage a deeper fusion between visual and language features.
Extensive experiments have veriﬁed the effectiveness of our method. First, in fully supervised manner, it can reduce human annotation costs by 31% without sacriﬁcing origi-nal model’s performance on RefCOCO [65]. Second, with-out bells and whistles, it can obtain superior or comparable performance even compared with state-of-the-art weakly-supervised visual grounding methods on ﬁve datasets, in-cluding RefCOCO [65], RefCOCO+ [65], RefCOCOg [40],
ReferItGame [28] and Flickr30K Entities [44].
In summary, this paper makes three-fold contributions: (1) We introduce the ﬁrst pseudo-query based visual grounding method that deals with the most dominant spatial relationships among objects. (2) We propose a query prompt module to speciﬁcally tailor pseudo-queries for visual grounding task, and a visual-language model equipped with multi-level cross-modality attention is put forward to fully capture the contextual relationships of different modalities. (3) Extensive experiments demonstrate that our approach can not only dramatically reduce the manual labelling costs without performance sacriﬁce under the fully supervised condition, but also surpass or achieve comparable performance with state-of-the-art weakly-supervised visual grounding methods. 2.