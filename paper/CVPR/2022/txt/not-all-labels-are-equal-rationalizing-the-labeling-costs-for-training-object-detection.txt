Abstract
Deep neural networks have reached high accuracy on object detection but their success hinges on large amounts of labeled data. To reduce the labels dependency, vari-ous active learning strategies have been proposed, based on the confidence of the detector. However, these meth-ods are biased towards high-performing classes and lead to acquired datasets that are not good representatives of the testing set data. In this work, we propose a unified frame-work for active learning, that considers both the uncertainty and the robustness of the detector, ensuring that the net-work performs well in all classes. Furthermore, our method leverages auto-labeling to suppress a potential distribution drift while boosting the performance of the model. Exper-iments on PASCAL VOC07+12 and MS-COCO show that our method consistently outperforms a wide range of ac-tive learning methods, yielding up to a 7.7% improvement in mAP, or up to 82% reduction in labeling cost. Code is available at https://github.com/NVlabs/AL-SSL. 1.

Introduction
The performance of deep object detection networks [1,2] depends heavily on the size of the labeled dataset. Adding more labeled data helps, yet adding more data costs. There-fore, it is imperative to adopt active learning (AL) strate-gies to select the most informative samples in the dataset for labeling, and self and semi-supervised learning (SSL) approaches to leverage unlabeled data whenever possible.
Consistency-based Semi-Supervised Learning (SSL) methods for object detection [3] train a network to mini-mize the inconsistency between its predictions. However, as shown in Fig. 1, some images still give inconsistent predic-tions, and thus the network does not learn from them. Auto-labeling uses self-learning to label high confident predic-tions, i.e. pseudo-label (PL), but, since networks are miscal-ibrated, they can generate wrong labels, potentially harming the training. Moreover, by targetting high-confident predic-tions, they ignore objects of low-performing classes.
*Work performed while interning at NVIDIA.
Figure 1. An object from a low-performing class (Pottedplant), original image in blue, augmented version in orange. a) Because of its low-entropy, uncertainty-based AL methods do not label the image. b) Because of its high confidence, pseudo-label SSL meth-ods wrongly pseudo-label the image and thus harm the training. c)
Because of its high-inconsistency, consistency-based SSL meth-ods cannot learn from it. d) Our method selects the image for labeling and prevents it from getting pseudo-labeled.
When samples cannot be pseudo-labeled, an alternative is to obtain the ground truth via manual labeling. Active learning (AL) for object detection [4, 5] is a common ap-proach to select the most promising samples for labeling to reduce labeling costs. The selection is based on an ac-quisition function to assess the informativeness of an im-age, typically computed based on the network’s uncertainty.
However, the acquisition function is only meaningful if the network is already well-trained for the task, which is not always the case, especially in the early AL cycles. Even if the network performs well in most classes, significant intra-class variance can lead to a low accuracy on a par-ticular class, see Fig. 1a. In those cases, using the network predictions to compute the acquisition function can lead to worse performance than random sampling. Furthermore, we change the dataset distribution at every AL cycle by se-lecting only the most uncertain (hard) samples until they no longer resemble the test distribution.
In this work, we advocate for a holistic view of the la-beling problem, that is, a unified strategy to choose which samples to manually label and which samples can be auto-matically labeled. We start from an uncertainty-based AL
framework and generalize its acquisition function by intro-ducing the concept of robustness, which is commonly not present on AL. If a network’s predictions of an image and its random augmentations, i.e., horizontal flipping, are not con-sistent, the image needs to be manually labeled. This simple yet effective change allows us to select informative sam-ples for both low and high-performing classes. This is un-like classic SSL settings, where samples with inconsistent predictions would neither be labeled nor pseudo-labeled, hence, the information contained in them would not be used.
We are still left with the dataset distribution drift, for which we propose to use auto-labeling to not increase the labeling costs. For every active learning cycle, we use the previously trained network to mine easy samples, i.e., sam-ples where the network is confident about its prediction, and use the network’s own prediction as labels. Note, that easy samples are typically not used in AL cycles. Only by holis-tically thinking about which samples to manually label and which to auto-label can we take full advantage of the entire dataset. In summary, our contributions are the following:
• We propose a novel class-agnostic active learning score based on the robustness of the network, using a novel in-consistency score.
• We use auto-labeling to leverage the less informative sam-ples, expanding the labeled dataset for free.
• We demonstrate the benefits of our method in two pub-licly available datasets: PASCAL VOC07+12 and MS-COCO. Compared to state-of-the-art methods [4–8], our approach yields up to a 7.7% and 7% relative mAP im-provement for PASCAL-VOC and MS-COCO, respec-tively. Importantly, we can achieve the same performance as the baseline but reduce up to 82% of the labeling costs. 2.