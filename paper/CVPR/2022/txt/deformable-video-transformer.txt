Abstract
Video transformers have recently emerged as an effec-tive alternative to convolutional networks for action clas-sification. However, most prior video transformers adopt either global space-time attention or hand-defined strate-gies to compare patches within and across frames. These fixed attention schemes not only have high computational cost but, by comparing patches at predetermined loca-tions, they neglect the motion dynamics in the video.
In this paper, we introduce the Deformable Video Transformer (DVT), which dynamically predicts a small subset of video patches to attend for each query location based on mo-tion information, thus allowing the model to decide where to look in the video based on correspondences across frames. Crucially, these motion-based correspondences are obtained at zero-cost from information stored in the com-pressed format of the video. Our deformable attention mechanism is optimized directly with respect to classifica-tion performance, thus eliminating the need for suboptimal hand-design of attention strategies. Experiments on four large-scale video benchmarks (Kinetics-400, Something-Something-V2, EPIC-KITCHENS and Diving-48) demon-strate that, compared to existing video transformers, our model achieves higher accuracy at the same or lower com-putational cost, and it attains state-of-the-art results on these four datasets. 1.

Introduction
Although transformers [39] were originally proposed to address NLP problems, they have quickly gained popular-ity in computer vision after the introduction of the Vision
Transformer (ViT) [9]. Compared to CNN architectures, which can model pixel dependencies only within the small receptive fields of convolutional filters, ViTs offer the bene-fit of capturing longer-range dependencies. This is achieved by means of the self-attention operation, which entails com-paring features extracted from image patches at different locations. The downside of self-attention is that it causes a high computational cost if executed globally over all pairs
In fact, the complexity of global self-of image patches.
Figure 1. Computational cost vs classification accuracy on K400 for video transformers pretrained on ImageNet-21K. Red dia-monds denote models employing our proposed attention scheme (DVT). Our method achieves consistently better accuracy and lower computational cost compared to prior attention strategies (Tformer [5], ViViT [2], Mformer [31]). Text tags next to markers specify video transformer architectures and clip sizes. attention is O(S2) where S is the number of patches, which can be in the thousands.
In the case of videos, global self-attention becomes espe-cially costly, since the number of patches to consider scales linearly with the number of frames T in the clip. Thus, the cost becomes quadratic in the space-time resolution, i.e.,
O(S2T 2). Exhaustive comparison of all space-time pairs of patches in a video is not only prohibitively costly but also highly redundant, since adjacent frames are likely to con-tain strongly correlated information. One way to reduce the computational cost is to limit the temporal attention to com-pare only pairs of patches in different frames but at the same spatial location [2, 5]. The complexity becomes O(S2T ).
However, this form of attention forces the model to perform temporal reasoning by looking through 1-dimensional “time tubes” of individual patches. Unfortunately, due to camera and object motion, the scene region projected onto a par-ticular patch in frame t may be completely unrelated to the scene information captured in the same patch at a differ-ent time t′. For example, the object appearing in the patch at time t may have moved outside that patch in frame t′, thus rendering the recovery of motion information impos-sible and making the comparison between the two patches
less relevant. Some works have proposed pooling tokens from local windows [10], or merging patches and limiting the self-attention operation to local space-time neighbor-hoods [29]. While effective in reducing the computational cost of video transformers, these methods employ fixed, hand-designed attention strategies that compare patches at predetermined locations, ignoring the motion in the video.
In this work we propose a novel space-time attention mechanism for video transformers, which we name de-formable space-time attention. While previous works rely on fixed schemes of attention which neglect the dynamic nature of the video, our method leverages motion cues to determine which patches to compare, thus implementing a form of input-conditioned attention. Given a query patch at a certain space-time location, our method samples N patches to compare to that query patch in each of the other frames. The locations of these N samples are predicted according to motion cues relating the query patch to the frame to attend. In other words, our strategy decides dy-namically “where to look” in each frame on the basis of the appearance of the query patch and its estimated motion.
Importantly, the motion cues are obtained at zero cost di-rectly from the information stored in the compressed for-mat of the video, specifically in terms of motion displace-ments encoding the motion between adjacent frames. As a result, the computational cost of our deformable attention is only O(ST 2N ). In our experiments we demonstrate that a small number of sample locations N is sufficient to achieve strong performance (e.g., most of our results are obtained with N = 8). This implies that, in practice, the complexity of our deformable attention is considerably lower than that of global attention, which is O(S2T 2) where S >> N .
In addition to the efficiency benefits, our experiments show that our deformable attention achieves higher accuracy than global attention, attaining state-of-the-art results on four ac-tion classification benchmarks. 2.