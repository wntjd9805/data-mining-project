Abstract
In this work, we present a conceptually simple yet ef-fective framework for cross-modality 3D object detection, named voxel field fusion. The proposed approach aims to maintain cross-modality consistency by representing and fusing augmented image features as a ray in the voxel field.
To this end, the learnable sampler is first designed to sam-ple vital features from the image plane that are projected to the voxel grid in a point-to-ray manner, which main-tains the consistency in feature representation with spatial context. In addition, ray-wise fusion is conducted to fuse features with the supplemental context in the constructed voxel field. We further develop mixed augmentor to align feature-variant transformations, which bridges the modal-ity gap in data augmentation. The proposed framework is demonstrated to achieve consistent gains in various bench-marks and outperforms previous fusion-based methods on
KITTI and nuScenes datasets. Code is made available at https://github.com/dvlab-research/VFF.1 1.

Introduction
Object detection in 3D scenes is regarded as a vital task to provide accurate perception for real-world applications.
Over the past decades, research attentions [15,27,38,40,51] have been dedicated to 3D object detection from raw point clouds. Due to the inherent properties of LiDAR sensors, the captured point clouds are usually sparse and cannot pro-vide sufficient context to distinguish among hard cases in distant or occluded regions, which consequently yields in-ferior performance in such scenarios. However, in safety-critical applications like autonomous driving, the frequently occurred miss-detection is unacceptable.
To address this issue, previous studies introduce image features in the cross-modality fusion [13, 32, 33]. The main challenge is to maintain the cross-modality consistency in this process that might be damaged in feature representa-1Part of the work was done in MEGVII Research. (a) Point-to-point manner (b) Point-to-ray manner
Figure 1. Compared with previous work [17,50] in 1a that projects feature from raw image to voxel and represents in a point-to-point manner, the proposed method in 1b projects feature from aug-mented image to voxel field and represents in a point-to-ray man-ner. Dotted and solid arrow denote point- and ray-level projection. tion considering context deficiency and density variance, and data augmentation for cross-modality misalignment.
In particular, previous work represents image features in a point-to-point manner in Figure 1a, which conducts fusion in each single point, constrained by the sparsity of point clouds. In this case, the rich context cues from images can-not be well utilized because adjacency in the image plane cannot be guaranteed in 3D space. Meanwhile, given aug-mented point clouds, traditional approaches [17, 50] usu-ally keep raw images unchanged and reverse transforma-tions in point clouds for pairwise correspondence. How-ever, because of the flipping and scaling variance in 2D convolutions, the asynchronous augmentation brings cross-modality misalignment and instability.
In this paper, we propose a new cross-modality frame-work, called voxel field fusion (VFF). Mixed augmenta-tion for both modalities is first applied for data-level pre-processing. As briefly illustrated in Figure 1b, VFF projects augmented image features to the voxel grid and represents it in a point-to-ray manner, called voxel field similar to that in neural rendering [19, 20].
In this way, representations of both modalities are well aligned, and surrounding spatial context is replenished in voxel field. In short, the key idea of VFF is to maintain modality consistency by representing and fusing augmented image feature as a ray in voxel field.
The image-to-voxel dense rendering is usually resource-intensive or requires extra models for depth prediction [25, 47]. To facilitate this process, we draw inspiration from recent advances in neural rendering [19, 20, 48] and pro-pose a learnable sampler and ray-wise fusion for efficient ray construction and cross-modality fusion. In particular, instead of random sampling [20], learnable sampler is de-signed to select image features for interaction within the activated area with high responses, where features are rep-resented in a point-to-ray manner as aforementioned. Then, ray-wise fusion is conducted in the voxel field according to the predicted score of each voxel along the ray. For the misalignment in augmentation, the mixed augmentor is fur-ther proposed to bridge this gap by aligning feature-variant augmentation (flipping and scaling) in image level.
With the above designs, the cross-modality consistency can be maintained from the aspect of feature representation and data augmentation in an end-to-end manner. Generally, the proposed VFF is distinguished from two aspects. First, it projects image features in a point-to-ray manner and rep-resents, as well as fuses them, in the voxel field, which elim-inates the modality gap and provides accurate 3D context to detect hard cases. Second, it efficiently samples high-responded features from augmented images, which enables the network to construct each ray on the fly.
The overall framework, called voxel field fusion, can be easily instantiated with various voxel-based backbones for 3D object detection, which is fully elaborated in Section 3.
Extensive empirical studies of the designed workflow are conducted in Section 4 to reveal the effect of each compo-nent. We further report experimental results on two widely-adopted datasets, namely KITTI [10] and nuScenes [2].
The proposed VFF is proved to achieve consistent increases over various benchmarks and attains significant gain with 2.2% AP over strong baselines on hard cases of KITTI test set. Meanwhile, it surpasses previous fusion-based meth-ods by a large margin and achieves leading performance on nuScenes test set with 68.4% mAP and 72.4% NDS. the recognition ability is limited due to lack of texture fea-tures, especially in real scenes with multiple categories like nuScenes [2] dataset.
Image-based 3D Detection. Previous image-based meth-ods construct the network and extract features from pure monocular or multiple images for 3D box prediction. Given a single image, several monocular-based approaches [1, 31, 35] try to regress and predict 3D boxes directly, while oth-ers propose to construct middle-level representation and perform detection on top of it [36, 47]. Because of the depth requirement in 3D detection, previous work also tries to enhance the ability from depth estimation [5, 25, 30].
Another stream for relatively accurate depth is utilizing stereo or multi-view images to construct 3D geometry vol-ume [4, 7, 43] and conduct object detection on top of it. Al-though depth estimated from multi-view is much better than that from a single image, it still lags behind the accurate point cloud from LiDAR.
Cross-modality Fusion. With inherent limitation of every single modality, there are several methods to combine the strength of image and LiDAR with cross-modality fusion.
In particular, point- and proposal-level fusion are intro-duced to combine features from different modalities. Point-level fusion [13, 17, 33] is usually applied in early stage of the network, while the proposal-based manner [6, 14, 46] is often adopted in the late stage for instance-level fusion.
There are also methods that combine these two fusion man-ners, such as MVX-Net [32]. Compared with proposal-level fusion, the point-level one is a more subtle manner, which is also adopted in our method for deep fusion. Previous point-level fusion approaches [13, 33] usually enhance the point feature from image semantics in a point-to-point manner, ignoring the surrounding context in 3D space. Different from them, the proposed voxel field fusion represents the augmented image feature in a point-to-ray manner in the voxel field, which makes further use of merits from both modalities with sufficient context. 2.