Abstract
Talking gesture generation is a practical yet challeng-ing task that aims to synthesize gestures in line with speech.
Gestures with meaningful signs can better convey useful in-formation and arouse sympathy in the audience. Current works focus on aligning gestures with the speech rhythms, which are difficult to mine the semantics and model seman-tic gestures explicitly. This paper proposes a novel SE-mantic Energized Generation (SEEG) method for semantic-aware gesture generation. Our method contains two parts:
DEcoupled Mining module (DEM) and Semantic Energiz-ing Module (SEM). DEM decouples the semantic-irrelevant information from inputs and separately mines information for the beat and semantic gestures. SEM conducts seman-tic learning and produces semantic gestures. Apart from representational similarity, SEM requires the predictions to express the same semantics as the ground truth. Be-sides, a semantic prompter is designed in SEM to leverage the semantic-aware supervision to predictions. This pro-motes the networks to learn and generate semantic gestures.
Experimental results reported in three metrics on differ-ent benchmarks prove that SEEG efficiently mines semantic cues and generates semantic gestures. SEEG outperforms other methods in all semantic-aware evaluations on differ-ent datasets. Qualitative evaluations also indicate the supe-riority of SEEG in semantic expressiveness. Code is avail-able via https://github.com/akira-l/SEEG. 1.

Introduction
Recently, in synthesizing digital humans, vivid gestures can primarily improve reality, naturalness, and efficient in-*This work was performed at Alibaba DAMO Academy, Alibaba
Group.
Figure 1. Co-speech gestures contain semantic-irrelevant beat and diverse semantic gestures. SEEG explores both gestures and pro-duces better semantic gestures. formation expression. Especially, talking gestures provide nonverbal cues of semantic expression and emphasize high-lights and attitudes woven into our daily communication.
Along with digital manipulation techniques, the speech-driven gesture is an emerging application, e.g., digital hu-man animation, visual dubbing in movies, online service, and education. The goal is to simulate artificial embod-ied agents to perform harmonious gestures aligned with the speech contents [14, 21, 29, 34]. Automated speech-driven gesture generation studies the generation of natural gesture sequences by exploring the relationships between speech
and body language. It provides a new opportunity for re-alistic human-human interaction in virtual platforms.
Toward vivid speech-driven gestures, an intuitive expec-tation is to produce gestures corresponding to the speech contents. Humans naturally respond to their speeches and produce gestures to deliver specific semantics as in human ethology. As shown in Fig. 1, most co-speech gestures are compounded by beat and semantic gestures [8, 15]. Beat gestures are irrelevant to lexical semantics. It is indepen-dent to the content of the speech and prefers to respond to the rhythms of sounds. For example, the fast-talker tends to move more frequently in speak gestures. Semantic ges-tures 1 are apt to express certain speech content with body language, including iconic gestures, metaphoric gestures, and deictic gestures [8]. For example, speakers may raise their hands to emphasize their attitudes, corresponding to
“clearly”, “definitely”, etc. Generating semantic gestures would lead to a vivid and reasonable content-based gesture rather than simply following the beat. However, the prior works of co-speech gestures synthesis [20, 29, 34, 35] do not explicitly produce semantic gestures and fail to model the lexical-semantic relevance between speech and gestures.
For instance, when merely learning with the semantic-irrelevant cues, i.e., the rhythms of audio and speakers’ identities, we achieve a comparable score with state-of-the-art methods [34]. This indicates that the current methods are hard to learn semantics explicitly and produce semantic-aware gestures.
It is challenging to generate semantic gestures for the following two reasons. First, semantic cues for generating semantic gestures are hard to be mined. The styles and the movements of semantic gestures vary widely among speak-ers according to different contents. Meanwhile, beat ges-tures are inclined to intuitive and straightforward responses to the cues from sound, which commonly occur and are eas-ier for the networks to mine. This difference induces seman-tic cues that are hard to be mined. The network may be rela-tively inclined to beat gestures and be slacked to investigate semantic cues. Second, semantic gestures and their corre-sponding texts are not well aligned temporally. As shown in Fig. 2, some gestures may be performed before or after the semantics they conveyed. This leads the network to un-favorably learn semantic gestures since it is hard to receive an explicit hint of semantic correlation via the given data.
These two challenges hinder the generation and expression of semantics in gestures.
This paper introduces a novel method to achieve semantic-aware co-speech gesture generation named SE-mantic Energized Generation (SEEG). SEEG efficiently mines semantic and beat cues respectively and conducts semantic-aware gesture generation. Specifically, SEEG 1We collectively refer to the three kinds of gestures as semantic ges-tures to distinguish them from the beat gesture. contains two components, i.e., DEcoupled Mining mod-ule (DEM) and a Semantic Energized Module (SEM). DE-coupled Mining module decouples speech input cues into semantic-relevant cues (closely coupled to speech contents) and semantic-irrelevant cues (only beat information). Then, two separate encoders in DEM process Semantic-relevant cues and semantic-irrelevant cues to understand information for semantic and beat gestures. After input decomposition, one encoder focuses on the representation for beat gestures, while the other encoder exploits the diverse semantic infor-mation for semantic gestures. This process eases the learn-ing of semantic and beat gestures with huge disparities. The networks enable explicitly mine differential information for the beat and semantic gestures. If we expect the networks to learn semantics, DEM avoids forcing the networks to learn semantics from beat gestures that do not contain semantic denotations. Semantic Energized Module aims to avoid generation degrading to beat gestures. SEM energizes se-mantic learning by constraining two kinds of similarities: representational similarity and semantic similarity. Rep-resentational similarity requires the generation to be sim-ilar to the ground truth in appearances. More critically,
DEM pursues semantic similarity and encourages the re-sults to present similar semantics compared with the ground truth. In DEM, we additionally introduce a semantic prompt gallery and a semantic prompter network. The prompter is trained by the gallery and fix it in gesture generation. The prompter network is responsible for representing gestures in a semantic view. By producing similar representations under the view of the prompter, the generated gestures are regularized to align semantics conveyed from the ground truth. Rather than directly connecting speech contents to gestures that may be misaligned, SEM energizes semantic learning by restraining both representational similarity and semantic similarity.
Our main contributions can be summarized as follows: 1. We propose a new SEmantic Energized Genera-tion (SEEG) framework for co-speech gesture generation.
SEEG is a semantic-aware gesture generation method that is adept at generating gestures with better semantic expres-siveness. 2. We propose DEcoupled Mining (DEM) and Seman-tic Energized Module (SEM). DEM decouples semantic-irrelevant cues in inputs and eases the learning of disparate semantic and beat gestures. DEM encourages the network to learn semantics and produce semantic gestures. 3.
In generating semantic gestures, the efficiency and advantages of our method are revealed by three subjective metrics on different datasets and objective human evalua-tions. We also find that the beat gestures may dominate the co-speech gesture generation. Visualizations show that
SEEG achieves significant expressiveness in semantics.
Figure 2. Examples of misalignment between semantics and gestures. Speakers may perform semantic gestures before (left) or after (right) the target contents. This leads to the semantic gestures being hard to match in temporary corresponding to the text or audio. We highlight the significant gestures with the orange shading. 2.