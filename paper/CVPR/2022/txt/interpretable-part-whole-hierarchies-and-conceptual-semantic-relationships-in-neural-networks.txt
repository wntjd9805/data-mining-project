Abstract
Deep neural networks achieve outstanding results in a large variety of tasks, often outperforming human experts.
However, a known limitation of current neural architec-tures is the poor accessibility to understand and interpret the network response to a given input. This is directly re-lated to the huge number of variables and the associated non-linearities of neural models, which are often used as black boxes. When it comes to critical applications as au-tonomous driving, security and safety, medicine and health, the lack of interpretability of the network behavior tends to induce skepticism and limited trustworthiness, despite the accurate performance of such systems in the given task.
Furthermore, a single metric, such as the classiﬁcation ac-curacy, provides a non-exhaustive evaluation of most real-world scenarios. In this paper, we want to make a step for-ward towards interpretability in neural networks, providing new tools to interpret their behavior. We present Agglomer-ator, a framework capable of providing a representation of part-whole hierarchies from visual cues and organizing the input distribution matching the conceptual-semantic hierar-chical structure between classes. We evaluate our method on common datasets, such as SmallNORB, MNIST, Fash-ionMNIST, CIFAR-10, and CIFAR-100, providing a more interpretable model than other state-of-the-art approaches. 1.

Introduction
The extensive adoption of neural networks and, in gen-eral, learning models has been raising concerns regarding our chances, as humans, to explain their behavior.
Inter-pretability would be a highly desirable feature for neural networks, especially in those applications like autonomous driving [13], healthcare [40], and ﬁnance [45], where safety, life, and security are at stake.
Deep neural networks have achieved superhuman perfor-mances in many domains, from computer vision [16, 29] to natural language processing [10,54], and data analysis [45].
However, the achieved performances have come at the ex-pense of model complexity, making it difﬁcult to interpret how neural networks work [34]. These neural networks are usually deployed as ”black boxes”, with millions of param-eters to be tuned, mostly according to experience and rule of thumb. Interpreting how a trainable parameter in the net-work setup directly affects the desired output from a given input has nearly zero chances.
According to the literature, interpretability is deﬁned as
“the degree to which a human can understand the cause of a decision” [39]. When a machine learning model reaches high accuracy on a task such as classiﬁcation and predic-tion, can we trust the model without understanding why such a decision has been taken? The decision process is complex and we tend to evaluate the performance of a sys-tem in solving a given task using metrics computed at the end of the processing chain. While single metrics, such as the classiﬁcation accuracy, reach super-human results, they provide an incomplete description of the real-world task [11]. As humans, when looking at an object that has eyes and limbs, we can infer via reasoning and intuition that these are elements (parts) that belong to the same entity (whole) [5], say an animal, and we can explain and motivate why such decision is taken, generally based on past experi-ences, beliefs and attitude [1]. Moreover, even in presence of an animal never seen before, we can probably tell from the visual features, our frames of reference [14] and our hi-erarchical organization of objects in the world [38] whether it is a ﬁsh or a mammal. We would like neural networks to display the same behavior, so that objects that are close in the conceptual-semantic and lexical relations are adjacent in the feature space as well (as shown in Fig. 1e). By do-ing so, it would be intuitive to identify hierarchical relations between samples and how the model has learned to build a topology describing each sample. Consequently, we can agree on the deﬁnition of interpretability in deep learning as the “extraction of relevant knowledge from a machine-learning model concerning relationships either contained in data or learned by the model” [42].
In the image classiﬁcation ﬁeld, available techniques, such as transformers [10, 12, 54], neural ﬁelds [37], contrastive learning representation [7], distillation [19] and capsules [44], have achieved state-of-the-art perfor-mances, introducing a number of novelties, such as pow-erful attention-based features and per-patch analysis, po-sitional encoding, similarity-based self-supervised pre-training, model compression and deep modeling of part-whole relationships. Taken as standalone, these methods have contributed to improving the interpretability of net-works, while still lacking direct emphasis on either data relationships [7, 10, 12, 37, 54] (e.g. conceptual-semantic relationships) or model-learned relationships [19, 44] (e.g. part-whole relationships). Retrieving part whole hierarchy is not a new task per se, as it has been exploited in dif-ferent research areas as scene parsing [3, 9] and multi-level scene decomposition [23, 59]. Instead of aiming at learning the part-whole hierarchy as the ﬁnal goal of our architec-ture, we focus on learning the part-whole representation as a mean to interpret the network behavior at different levels.
In [18], a concept idea on how to represent part-whole hi-erarchies in neural networks is introduced, which attempts to merge the advantages of the above state-of-the-art frame-works into a single theoretical system (known as GLOM).
GLOM aims at mimicking the human ability in learning to parse visual scenes. Inspired by the theoretical concepts de-scribed in [14, 18], we build a working system, called Ag-glomerator, which achieves part-whole agreement [20] at different levels of the model (relationships learned by the model) and hierarchical organization of the feature space (relationships contained in data), as shown in Fig. 1.
Our contribution is summarised as follows:
• we introduce a novel model, called Agglomerator1, mimicking the functioning of the cortical columns in the human brain [15];
• we explain how our architecture provides interpretabil-ity of relationships learned by the model, speciﬁcally part-whole relationships;
• we show how our architecture provides interpretability of relationships contained in data, namely the hierar-chical organization of the feature space;
• we provide results outperforming or on par with cur-rent methods on multiple common datasets, such as
SmallNORB [31], MNIST [30], FashionMNIST [57],
CIFAR-10 and CIFAR-100 [27], also relying on fewer parameters. 2.