Abstract
Recently, zero-shot and few-shot learning via Contrastive
Vision-Language Pre-training (CLIP) have shown inspira-tional performance on 2D visual recognition, which learns to match images with their corresponding texts in open-vocabulary settings. However, it remains under explored that whether CLIP, pre-trained by large-scale image-text pairs in 2D, can be generalized to 3D recognition. In this paper, we identify such a setting is feasible by proposing
PointCLIP, which conducts alignment between CLIP-encoded point clouds and 3D category texts. Specifically, we encode a point cloud by projecting it onto multi-view depth maps and aggregate the view-wise zero-shot pre-diction in an end-to-end manner, which achieves efficient knowledge transfer from 2D to 3D. We further design an inter-view adapter to better extract the global feature and adaptively fuse the 3D few-shot knowledge into CLIP pre-trained in 2D. By just fine-tuning the adapter under few-shot settings, the performance of PointCLIP could be largely improved. In addition, we observe the knowledge complementary property between PointCLIP and classical 3D-supervised networks.
Via simple ensemble during inference, PointCLIP contributes to favorable performance enhancement over state-of-the-art 3D networks. Therefore,
PointCLIP is a promising alternative for effective 3D point cloud understanding under low data regime with marginal resource cost. We conduct thorough experiments on Model-Net10, ModelNet40 and ScanObjectNN to demonstrate the effectiveness of PointCLIP. Code is available at https:
//github.com/ZrrSkywalker/PointCLIP. 1.

Introduction
Deep learning has dominated computer vision tasks of both 2D and 3D domains in recent years, such as image
∗ Indicates equal contributions, † Indicates corresponding author
Figure 1. Comparison of Training-testing Schemes between
PointCLIP and PointNet++. Different from classical 3D net-works, our proposed PointCLIP is pre-trained by 2D image-text pairs and directly conducts zero-shot classification on 3D datasets without 3D training, which achieves efficient cross-modality knowledge transfer. classification [12,17,22,28,37,41], object detection [1,4,13, 29,47,67], semantic segmentation [3,25,35,36,64,68], point cloud recognition and part segmentation [19, 42, 44, 45, 56].
With 3D sensing technology developing rapidly, the grow-ing demand for processing 3D point cloud data has boosted many advanced deep models with better local feature ag-gregator [30, 32, 50], geometry modeling [20, 40, 60] and projection-based processing [21, 34, 49]. Different from grid-based 2D image data, 3D point clouds suffer from space sparsity and irregular distribution, which hinder the direct transfer of methods from 2D domain. More impor-tantly, a large number of newly captured point clouds con-tain objects of “unseen” categories to the deployed models.
In this scenario, even the best-performing classifier might fail to recognize them and it is unaffordable to re-train the models every time when “unseen” objects arise.
Similar issues have been dramatically mitigated in 2D vision by Contrastive Vision-Language Pre-training
(CLIP) [46], which proposes to learn transferable visual features with natural language supervisions. For zero-shot classification of “unseen” categories, CLIP utilizes the pre-trained correlation between vision and language to con-duct open-vocabulary recognition and achieves promising performance. To enhance the accuracy in few-shot set-tings, CoOp [69] adopts learnable tokens to encode the tex-tual inputs and avoids the tuning for hand-crafted prompt.
From another perspective, CLIP-Adapter [16] appends a lightweight residual-style adapter with two linear layers for better adapting image features and Tip-Adapter [66] further boosts its performance while greatly reduces the training time. Consequently, the problem of recognizing new unla-beled objects has been well explored on 2D images, and the proposed methods achieve significant improvements over zero-shot CLIP. However, for the more challenging point clouds, a question is naturally raised: Could such CLIP-based models be transferred to 3D domain and realize zero-shot classification for “unseen” 3D objects?
To address this issue, we propose PointCLIP, which transfers CLIP’s 2D pre-trained knowledge to 3D point cloud understanding. The first concern is to bridge the modal gap between unordered point clouds and the grid-based images that CLIP can process. Considering the real-time need for some applications, such as autonomous driv-ing [4, 13, 29, 43] and indoor navigation [71], we propose to adopt online perspective projection [19] without any post rendering [49], i.e., simply projecting raw points onto pre-defined image planes to generate scatter depth maps. The cost of this projection process is marginal in both time and computation, but reserves the original property of the point cloud from multiple views. On top of that, we apply CLIP’s pre-trained visual encoder to extract multi-view features of the point cloud and then obtain each view’s zero-shot pre-diction by the text-generated classifier. Therein, we place 3D category names into a hand-crafted template and pro-duce the zero-shot classifier by CLIP’s pre-trained textual encoder. As different views contribute differently to the understanding, we obtain the final prediction for the point cloud by weighted aggregation between views.
Although PointCLIP achieves cross-modality zero-shot classification without any 3D training, its performance still falls behind classical point cloud networks well-trained on full datasets. To eliminate this gap, we introduce a learn-able inter-view adapter with bottleneck linear layers to bet-ter extract features from multiple views in few-shot settings.
Specifically, we concatenate all views’ features and summa-rize the compact global feature of the point cloud by cross-view interaction and fusion. Based on the global represen-tation, the adapted feature of each view is generated and added to their original CLIP-encoded features via a resid-ual connection. In this way, each view is aware of global information and also combines new knowledge from the 3D few-shot dataset with the 2D knowledge of pre-trained
CLIP. During training, we only fine-tune this adapter and freeze both CLIP’s visual and textual encoders to avoid over-fitting, since only a few samples per class are insuffi-cient for training CLIP. By few-shot fine-tuning, PointCLIP with an inter-view adapter largely improves the zero-shot performance and exerts a good trade-off between perfor-mance and cost.
Additionally, we observe that CLIP’s 2D knowledge, su-pervised by contrastive image-text pairs, is complementary to 3D close-set supervisions. PointCLIP with the inter-view adapter can be utilized to improve the performance of clas-sical fully-trained 3D networks. For PointNet++ [45] with an accuracy of 89.71%, we adopt PointCLIP of 87.20% fine-tuned by 16-shot ModelNet40 [58] and directly en-semble their predicted classification logits during inference.
The performance is enhanced by +2.32%, from 89.71% to 92.03%. Also for CurveNet [60], the state-of-the-art 3D recognition network, the knowledge ensemble contributes to performance boost from 93.84% to 94.08%.
In con-trast, simply ensemble between two models fully trained on ModelNet40 without PointCLIP cannot lead to perfor-mance improvement. Therefore, PointCLIP could be re-garded as a drop-in multi-knowledge ensemble module, which promotes 3D networks via 2D contrastive knowledge with marginal few-shot training.
The contributions of our paper are as follows:
• We propose PointCLIP to extend CLIP for handling 3D point cloud data, which achieves cross-modality zero-shot recognition by transferring 2D pre-trained knowledge into 3D.
• An inter-view adapter is introduced upon PointCLIP via feature interaction among multiple views and largely improves the performance by few-shot fine-tuning.
• PointCLIP can be utilized as a multi-knowledge en-semble module to enhance the performance of existing fully-trained 3D networks.
• Comprehensive experiments are conducted on widely adapted ModelNet10, ModelNet40 and the challeng-ing ScanObjectNN, which indicate PointCLIP’s poten-tial for effective 3D understanding. 2.