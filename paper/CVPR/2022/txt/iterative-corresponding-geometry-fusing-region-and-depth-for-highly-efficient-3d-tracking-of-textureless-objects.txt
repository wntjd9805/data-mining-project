Abstract
Tracking objects in 3D space and predicting their 6DoF pose is an essential task in computer vision. State-of-the-art approaches often rely on object texture to tackle this problem. However, while they achieve impressive results, many objects do not contain sufficient texture, violating the main underlying assumption. In the following, we thus pro-pose ICG, a novel probabilistic tracker that fuses region and depth information and only requires the object geom-etry. Our method deploys correspondence lines and points to iteratively refine the pose. We also implement robust occlusion handling to improve performance in real-world settings. Experiments on the YCB-Video, OPT, and Choi datasets demonstrate that, even for textured objects, our ap-proach outperforms the current state of the art with respect to accuracy and robustness. At the same time, ICG shows fast convergence and outstanding efficiency, requiring only 1.3 ms per frame on a single CPU core. Finally, we analyze the influence of individual components and discuss our per-formance compared to deep learning-based methods. The source code of our tracker is publicly available1. 1.

Introduction
For many applications in robotic manipulation and aug-mented reality, it is essential to know the six degrees of free-dom (6DoF) pose of relevant objects. To provide this infor-mation at high frequency, 3D object tracking is used. The goal is to estimate an objectâ€™s position and orientation from consecutive image frames given its 3D model. In real-world applications, occlusions, motion blur, background clutter, textureless surfaces, object symmetries, and real-time re-quirements remain difficult problems. Over the years many approaches have been developed [29, 69]. They can be dif-ferentiated by the use of keypoints, edges, direct optimiza-tion, deep learning, object regions, and depth images.
While methods based on keypoints [38, 47, 48, 54, 61], 1https://github.com/DLR-RM/3DObjectTracking
Figure 1. Tracking of a pentagon object for robotic manipulation.
The image on the left shows an overlay of the object model for the predicted pose. On the right, probabilities that a pixel belongs to the background are encoded in a grayscale image. Correspondence lines are shown in yellow, with high probabilities indicated in red.
Projected correspondence points are illustrated in blue. edges [12, 17, 20, 52], and direct optimization [1, 13, 34, 53] were very popular in the past, multiple drawbacks exist.
Both keypoints and direct optimization are not suitable for textureless objects. Edge-based methods, on the other hand, typically struggle with background clutter and object tex-ture. Further problems emerge from reflections and mo-tion blur, which change the appearance of both texture and edges. To overcome those issues, data-driven techniques that use convolutional neural networks (CNNs) have been proposed [15, 31, 62, 64]. While most of those methods re-quire significant computational resources and a detailed 3D model, they achieve promising results. For the tracking of textureless objects in cluttered environments, region-based techniques have also become very popular [42, 56, 60, 73].
Furthermore, the emergence of consumer depth sensors has enabled additional trackers that do not rely on texture
[11, 24, 37, 51, 66]. Finally, while all those methods can be used independently, many approaches demonstrated the benefits of combining different techniques [25, 26, 44, 59].
In the past, it was shown that a combination of region and depth has great potential for the tracking of texture-less objects [25, 44]. However, while region-based tech-niques improved greatly with respect to efficiency and qual-ity [55, 56], no recent combined approach exists. In the fol-lowing, we thus build on current developments and propose
ICG, a highly efficient method that fuses geometric infor-mation from region-based correspondence lines and depth-based correspondence points. An illustration of the used correspondences is shown in Fig. 1. In a detailed evaluation on three different datasets, our method demonstrates state-of-the-art performance compared to both classical and deep learning-based techniques. In addition, given that only few such comparisons were conducted in the past, we are able to gain new insights into the current state of deep learning-based object tracking and pose estimation. 2.