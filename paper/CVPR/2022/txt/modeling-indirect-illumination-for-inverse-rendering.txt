Abstract
Recent advances in implicit neural representations and differentiable rendering make it possible to simultaneously recover the geometry and materials of an object from multi-view RGB images captured under unknown static illumina-tion. Despite the promising results achieved, indirect illu-mination is rarely modeled in previous methods, as it re-quires expensive recursive path tracing which makes the inverse rendering computationally intractable. In this pa-per, we propose a novel approach to efficiently recovering spatially-varying indirect illumination. The key insight is that indirect illumination can be conveniently derived from the neural radiance field learned from input images instead of being estimated jointly with direct illumination and ma-terials. By properly modeling the indirect illumination and visibility of direct illumination, interreflection- and shadow-free albedo can be recovered. The experiments on both syn-thetic and real data demonstrate the superior performance of our approach compared to previous work and its capa-bility to synthesize realistic renderings under novel view-points and illumination. Our code and data are available at https://zju3dv.github.io/invrender/. 1.

Introduction
Recovering the geometry, materials, and lighting of a 3D scene from images, also known as inverse rendering, has been a long-standing problem in the fields of computer vision and graphics.
It is gaining traction in this era of blowout VR and AR applications, where there is a high de-mand for easily acquired 3D contents from the real world.
Previous capture systems, such as light-stages with con-trolled light directions and cameras [8, 11, 31], using a co-located flashlight and camera in a dark room [2, 3], and ro-tating objects with a turntable [7, 26], show limitations in user-friendliness.
More recent works [5,29,32] explore flexible capture set-tings under natural illumination. These methods typically
The authors from Zhejiang University are affiliated with the State Key
Lab of CAD&CG. This work was done when Yuanqing Zhang was an intern at Alibaba Group. ∗Corresponding author: Xiaowei Zhou.
Figure 1. To precisely recover SVBRDF (parameterized as albedo and roughness) from multi-view RGB images, we propose an ef-ficient approach to reconstruct spatially varying indirect illumina-tion and combine it with environmental light evaluated by visibil-ity as the full light model (a). The example in (b) demonstrates that without modeling indirect illumination, its rendering effects are baked into the estimated albedo to compensate for the incomplete light model and also result in artifacts in the estimated roughness. represent geometry and spatially varying BRDF (SVBRDF) as coordinate-based neural networks and recover them by optimizing a re-rendering loss that compares rendered im-ages with input images. However, capturing under natural illumination often shows complex effects such as soft shad-ows and interreflections. It is intractable to simulate these effects when optimizing SVBRDF and light parameters as it necessitates expensive recursive path tracing in physically based rendering. Prior methods usually ignore both self-occlusion and interreflection [29] in order to reduce com-putation, or only model visibility [32] or limit the indirect lighting to a single bounce with known light sources [22].
Without properly modeling the indirect illumination, there exists a gap between the captured image and the rendered image. As a result, the effect of indirect illumination in the captured images is prone to being baked into the estimated diffuse albedo to compensate for this gap, as illustrated in
Figure 1. It also results in artifacts in the recovered spec-ular reflectance and environmental light as they explain the observed images together with albedo.
In this paper, we aim to estimate the SVBRDF of ob-jects from multi-view RGB images captured under un-known static illumination. Our main technical innovation is an efficient approach to modeling indirect illumination in this inverse rendering process. We model the indirect illu-mination by a multilayer perceptron (MLP) that maps a 3D surface point to its indirect incoming illumination. The core idea to efficiently learning this indirect illumination MLP is that the indirect illumination doesn’t need to be jointly learned with the SVBRDF and environmental light, but can be directly derived from the outgoing radiance field of the scene, which can be constructed from multi-view images with the off-the-shelf neural scene representation methods (e.g., [15, 27]).
Specifically, we first learn the geometry and outgoing ra-diance field of the object, both represented as MLPs, from the input images using the existing method [27]. Then, the learned radiance field serves as the ground-truth incoming illumination of its reachable surface points to train the indi-rect illumination MLP. Finally, the learned indirect illumi-nation is plugged into the rendering equation and fixed dur-ing the optimization of SVBRDF and environmental light.
In this way, the indirect illumination can be directly queried when optimizing the other unknowns without the need of recursive path tracing, making the inverse rendering prob-lem better constrained and more efficient to solve. Further-more, to reduce the ambiguity of disentangling BRDF and incident light, we introduce a prior that a real-world object should consist of limited types of materials. This prior is imposed by representing SVBRDF as an encoder-decoder with a sparse latent space.
We evaluate the proposed method on both synthetic and real datasets. The experimental results show that our ap-proach outperforms baseline methods and is able to recover shadow- and interreflection-free albedo and high-quality roughness, as well as supporting realistic free-viewpoint re-lighting. 2.