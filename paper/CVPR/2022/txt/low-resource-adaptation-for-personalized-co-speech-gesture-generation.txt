Abstract
Personalizing an avatar for co-speech gesture generation from spoken language requires learning the idiosyncrasies of a person’s gesture style from a small amount of data. Pre-vious methods in gesture generation require large amounts of data for each speaker, which is often infeasible. We pro-pose an approach, named DiffGAN, that efficiently personal-izes co-speech gesture generation models of a high-resource source speaker to target speaker with just 2 minutes of target training data. A unique characteristic of DiffGAN is its abil-ity to account for the crossmodal grounding shift, while also addressing the distribution shift in the output domain. We substantiate the effectiveness of our approach a large scale publicly available dataset through quantitative, qualitative and user studies, which show that our proposed methodology significantly outperforms prior approaches for low-resource adaptation of gesture generation. Code and videos can be found at https://chahuja.com/diffgan. 1.

Introduction
Technologies to assist human communication, both ver-bal (e.g. spoken language) and nonverbal (e.g. co-speech gestures), have gained more traction in the past decade. One promising direction is virtual reality [10, 17, 18, 29] which aims at creating a more realistic online communication plat-form for embodied virtual agents [6, 28] and remote avatars
[38, 39]. These advancements could be seen as a normal progression to speech-based technologies such as intelligent personal assistants (e.g. Alexa, Siri, Cortana). These agents, in the future, could also communicate more naturally with a nonverbal embodiment that complements the verbal commu-nication [33]. To enable this vision of immersive verbal and nonverbal communication through an avatar, one technical challenge is generating visual gestures based on input speech and language [2, 12, 15, 21]. An even more challenging task is the generation of personalized visual gestures, which re-flects the idiosyncratic behaviours of a specific person [33].
The main goal of our paper is to create a personalized ges-ture generation model (e.g. as part of a personalized avatar)
Figure 1. Overview of the co-speech gesture personalization task.
On the left is a generative source model Gs pre-trained on a source speaker. We adapt Gs to multiple target models Gt using low-resource data for each of the target speaker. with limited data from a new speaker. In technical terms, this problem requires an adaptation of crossmodal generative models in a low-resource setting as illustrated in Figure 1.
Leveraging an existing source model, pretrained on a large dataset of one speaker (i.e. source domain), our goal is to personalize to a new speaker (i.e. target domain) with only 2 minutes of the target data.
The problem setting brings a unique challenge, typically not studied in typical domain adaptation settings: cross-modal grounding shift. Due to the crossmodal nature of our task, crossmodal grounding shift refers to the distributional shift of the relationships between the input spoken language modalities and output gesture modality. For example, con-sider a speaker, Aarti, who waves their right hand while greeting a friend. These gestures are conditionally depen-dent on what the speaker says. We define such relationships between the gestures and spoken language as crossmodal grounding. While these relationships are conditioned on spoken language, they are also heavily influenced by the speaker’s idiosyncrasies. Now, consider a new speaker, Bob, who chooses from a set of two gestures while greeting: a
Figure 2. Overview of the key components of our proposed model DiffGAN. (a)-(d): Low-resource adaptation of the crossmodal grounding relationships from source to target domain. (e): Modeling output domain shift from source to target domain left handed wave or waving both their hands vigorously. As is the case for this speaker, typically the conditional gesture spaces have a larger support (i.e. different kinds of gestures) for the same language context. Such differences between conditional gestures of source and target speakers are very common, especially because the conditional variable is lan-guage which is a very large space. These common, yet complex differences represent crossmodal grounding shift.
In this paper, we propose an approach, named DiffGAN, that can efficiently personalize co-speech gesture genera-tion models from a high-resource source speaker to a low-resource target speaker. To the best of our knowledge, this is the first approach that is able to learn a personalized model with only 2 minutes of speaker data (i.e. as opposed to 10 hours [2, 12, 15, 21]). Our DiffGAN approach does not require access to source training data. Instead, DiffGAN di-rectly identifies shifts in crossmodal grounding relationships along with the shifts in the output domain from the pre-trained source model. Based on these identified distribution shifts, DiffGAN updates a few necessary parameters in a sin-gle layer of the source model, allowing efficient adaptation with low resources. Our experiments study the effectiveness of our DiffGAN approach on a diverse publicly available dataset. As part of our evaluation methodology, we report that DiffGAN produces a consistent improvement of around 10% preference scores of human judgments over strong base-lines among other quantitative improvements. Furthermore,
DiffGAN extrapolates to gestures in the target distribution without ever having seen them in the source distribution. 2.