Abstract 1.

Introduction
This study addresses the issue of fusing infrared and visi-ble images that appear differently for object detection. Aim-ing at generating an image of high visual quality, previous approaches discover commons underlying the two modal-ities and fuse upon the common space either by iterative optimization or deep networks. These approaches neglect that modality differences implying the complementary in-formation are extremely important for both fusion and sub-sequent detection task. This paper proposes a bilevel opti-mization formulation for the joint problem of fusion and de-tection, and then unrolls to a target-aware Dual Adversar-ial Learning (TarDAL) network for fusion and a commonly used detection network. The fusion network with one gen-erator and dual discriminators seeks commons while learn-ing from differences, which preserves structural informa-tion of targets from the infrared and textural details from the visible. Furthermore, we build a synchronized imag-ing system with calibrated infrared and optical sensors, and collect currently the most comprehensive benchmark cov-ering a wide range of scenarios. Extensive experiments on several public datasets and our benchmark demonstrate that our method outputs not only visually appealing fusion but also higher detection mAP than the state-of-the-art ap-proaches. The source code and benchmark are available at https://github.com/dlut-dimt/TarDAL.
Multi-modality imaging has attracted signiﬁcant atten-tion in a wide range of applications, e.g., surveillance [28] and autonomous driving [5], with the rapid development of sensing hardware. Especially, the combination of infrared and visible sensors has remarkable advantages for subse-quent intelligent processing [11, 38, 39]. Visible imaging provides rich details with high spatial resolution under well-deﬁned lighting conditions while infrared sensors, captur-ing ambient temperature variations emitted from objects, highlight structures of thermal targets insensitive to light-ing changes. Unfortunately, infrared images are often ac-companied by blurred details with lower spatial resolution.
Owing to their evident appearance discrepancy, it is chal-lenging to fuse visually appealing images and/or to support higher-level vision tasks such as segmentation [4,29], track-ing [2,7], and detection [32], by making full use of the com-plementary information from the infrared and visible.
Numerous infrared and visible image fusion (IVIF) ap-proaches that aim at improving visual quality have been de-veloped in the past decades. Traditional multi-scale trans-form [10, 24], optimization model [16, 20, 41], spare rep-resentation [37, 43], and subspace methods attempt to dis-cover intrinsic common features of the two modalities and to design appropriate weighting rules for fusion. These
approaches typically have to invoke a time consuming it-erative optimization process. Recently, researchers intro-duce deep networks into IVIF by learning powerful fea-ture representation and/or weighting strategies when re-dundant well-prepared image pairs are available for train-ing [8, 12, 21–23, 35]. The fusion turns out to be an efﬁcient inference process yielding fruitful quality improvements.
Nevertheless, either traditional or deep IVIF approaches strive for quality improving but leave alone the follow-up detection, which is the key to many practical computer vi-sion applications. The fusion emphasizes more on ‘seeking commons’ but neglects the differences of these two modal-ities on presenting structural information of targets and tex-tural details of ambient background. These differences play a critical role on differentiating distinct features of targets for object detection meanwhile generating clear appearance of high contrast favorable for human inspection.
Moreover, learning from these differences (actually com-plementary information) demands a comprehensive collec-tions of imaging data from the two modalities. The images capturing in scenarios varying with lighting and weather ex-hibit signiﬁcantly different characteristics from both modal-ities. Unfortunately, existing data collections only cover limited conditions, placing an obstacle to learn the comple-mentary information and validate the effectiveness.
This paper proposes a bilevel optimization formulation for the joint problem of fusion and detection. This for-mulation unrolls to a well-designed dual-adversarial fusion network, composed of one generator and two target-aware discriminators, and a commonly used detection network.
One discriminator distinguishes foreground thermal targets from the image domain of infrared imaging while the oth-er one differentiates the background textural details from gradient domain of the visible image. We also derive a co-operative training strategy to learn optimal parameters for the two networks. Figure 1 demonstrates that our method accurately detects objects from target-distinct and visual-appealing fusion with less time and fewer parameters than the state-of-the-art (SOTA). Our contributions are four-fold:
• We embrace both image fusion and object detection with a bilevel optimization formulation, producing high detection accuracy as well as the fused image with better visual effects.
• We devise a target-aware dual adversarial learn-ing network (TarDAL) with fewer parameters for detection-oriented fusion.
This one-generator and dual-discriminator network ‘seeks commons while learning from differences’ that preserves information of targets from the infrared and textural details from the visible.
• We derive a cooperative training scheme from the bi-level formulation yielding optimal network parameters for fast inference (fusion and detection).
• We build a synchronized imaging system with well-calibrated infrared and optical sensors and collect a multi-scenario multi-modality dataset (M3FD) with 4, 177 aligned infrared and visible image pairs and 23, 635 annotated objects. The dataset covers four ma-jor scenarios with various environments, illumination, season, and weather, having a wide range of pixel vari-ations, as shown in Figure 1. 2.