Abstract
Recent neural human representations can produce high-quality multi-view rendering but require using dense multi-view inputs and costly training. They are hence largely lim-ited to static models as training each frame is infeasible.
We present HumanNeRF - a neural representation with effi-cient generalization ability - for high-fidelity free-view syn-thesis of dynamic humans. Analogous to how IBRNet assists
NeRF by avoiding per-scene training, HumanNeRF em-ploys an aggregated pixel-alignment feature across multi-view inputs along with a pose embedded non-rigid defor-mation field for tackling dynamic motions. The raw Human-NeRF can already produce reasonable rendering on sparse video inputs of unseen subjects and camera settings. To fur-ther improve the rendering quality, we augment our solu-tion with in-hour scene-specific fine-tuning, and an appear-ance blending module for combining the benefits of both neural volumetric rendering and neural texture blending.
Extensive experiments on various multi-view dynamic hu-man datasets demonstrate effectiveness of our approach in synthesizing photo-realistic free-view humans under chal-lenging motions and with very sparse camera view inputs. 1.

Introduction
View synthesis of human activities enables numerous ap-plications in visual effects and telepresence, with unique and immersive viewing experiences. However, a conve-nient and high-quality solution from the light-weight cap-ture setup remains a cutting-edge yet bottleneck technique.
Early solutions require a dome-based multi-view setup for accurate reconstruction [7, 10] and image-based render-ing in novel views [3, 65]. Volumetric approaches [44, 58] enable light-weight reconstruction, but they still heavily rely on the depth sensors and are restricted by the limited mesh resolution. Recent neural rendering techniques have achieved significant progress [13, 27, 29, 47]. Remarkably,
NeRF [29] and its dynamic extensions [24,33,35,50,53,64] enable photo-realistic novel view synthesis for dynamic scenes without heavy reliance on the reconstruction accu-racy. However, these solutions still require expensive dense capture views or suffer from tedious time-consuming per-scene training, which highly limits the practicality. Only re-cently, some approaches [5,54,62] enhance NeRF [29] with
image-conditioned features to break the per-scene training constraint for efficient radiance field generation of static scenes. But few researchers explore such generalizable
NeRF representation under the complex dynamic human settings. The recent work [45] further enables generaliz-able human rendering from 6 RGB streams by combining texture blending with implicit geometry inference [37, 38] only in novel views. However, it suffers from severe ar-tifacts near the occluded regions due to the lack of global inherent geometry and texture modeling.
In this paper, we present HumanNeRF – a practical and high-quality neural free-view synthesis approach for gen-eral dynamic humans using only sparse RGB streams. As illustrated in Fig.1, our approach enables photo-realistic hu-man rendering by efficiently optimizing a more generaliz-able radiance field on-the-fly for unseen performers in an hour, favorably transcending previous long-term per-scene training approaches.
Our key idea is to marry the dynamic NeRF representa-tion with neural image-based blending in a light-weight and two-stage framework. We extend the concept of general ra-diance field into the dynamic and temporal setting to break the per-scene constraint for efficient rendering. We also ex-plore an effective implicit blending strategy to boost the tex-ture result of volumetric rendering with the level of detail present in the sparse input images. Specifically, we first adopt an implicit scheme to aggregate image-conditioned features from our sparse input, which enables generaliz-able inference of motion and appearance in the dynamic
NeRF framework. Then, we introduce a pose-embedded hybrid deformation scheme to enhance the generalization ability for unseen identities under various motions and gar-ments. It combines explicit model-based warping with im-plicit subtle displacement modeling, so as to learn a reli-able radiance field in an inherent canonical space. Note that our scheme also supports efficient per-performer fine-tuning with temporally sparse sampling, which significantly improves the rendering quality even on unseen poses. How-ever, we observe that existing dynamic NeRF-based volu-metric rendering still fails to generate high-frequency tex-ture details, especially for challenging unseen identities and poses. To this end, we combine the image-based render-ing with NeRF-based volume rendering into a novel neu-ral blending scheme through implicit and occlusion-aware blending weight learning.
It enables accurate appearance rendering in the target view with the level of texture detail in the adjacent input images. To summarize, our main con-tributions include:
• We present a high-quality performance rendering ap-proach via efficient radiance field generation for arbi-trary performers from sparse RGB streams, achieving significant superiority to existing the state of the art.
• We extend the generalizable NeRF into the new realm of dynamic and light-weight setting through implicit feature aggregation and hybrid deformation.
• We propose a novel implicit blending scheme to pre-serve the texture detail from the input images, provid-ing photo-realistic appearance rendering. 2.