Abstract
We propose a novel approach aimed at object and se-mantic scene completion from a partial scan represented as a 3D point cloud. Our architecture relies on three novel lay-ers that are used successively within an encoder-decoder structure and specifically developed for the task at hand.
The first one carries out feature extraction by matching the point features to a set of pre-trained local descriptors. Then, to avoid losing individual descriptors as part of standard operations such as max-pooling, we propose an alterna-tive neighbor-pooling operation that relies on adopting the feature vectors with the highest activations. Finally, up-sampling in the decoder modifies our feature extraction in order to increase the output dimension. While this model is already able to achieve competitive results with the state of the art, we further propose a way to increase the versatility of our approach to process point clouds. To this aim, we in-troduce a second model that assembles our layers within a transformer architecture. We evaluate both architectures on object and indoor scene completion tasks, achieving state-of-the-art performance. 1.

Introduction
Understanding the entire 3D space is essential for both humans and machines to understand how to safely navigate an environment or how to interact with the objects around them. However, when we capture the 3D structure of an ob-ject or scene from a certain viewpoint, a large portion of the whole geometry is typically missing due to self-occlusion and/or occlusion from its surrounding. To solve this prob-lem, geometric completion of scenes [2, 27, 32] and ob-jects [16, 20, 39, 44, 45] has emerged as a task that takes on a 2.5D/3D observation and fills out the occluded regions, as illustrated in Fig. 1.
There are multiple ways to represent 3D shapes. Point cloud [3, 6], volumetric grid [8, 27], mesh [11] and implicit surfaces [18, 21, 40] are among the most common data for-mats. These representations are used for most 3D-related computer vision tasks such as segmentation, classification and completion. For what concerns geometric completion,
Figure 1. From the input partial scan to our object completion, we visualize the amount of detail in our reconstruction. most works are focused on either point cloud or volumetric data. Among them, the characteristic of having an explicitly defined local neighbourhood makes volumetric data easier to process with 3D convolutions [7, 41, 42]. One drawback introduced by the predefined local neighborhood is the inac-curacy due to the constant resolution of the voxels, meaning that one voxel can represent several small structures.
On the other hand, point clouds have the advantage of not limiting the local resolution, although they come with their own sets of drawbacks. Mainly, there are two problems in processing point clouds: the undefined local neighborhood and unorganized feature map. Aiming at solving these is-sues, PointNet++ [23], PMP-Net [35], PointConv [37] and
PointCNN [13] employ k-nearest neighbor search to de-fine a local neighborhood, while PointNet [22] and Soft-PoolNet [33] adopt the pooling operation to achieve per-mutation invariant features. Notably, point cloud segmenta-tion and classification were further improved by involving k-nearest neighbor search to form local features in Point-Net++ [23] compared to global features in PointNet [22].
Several variations of PointNet [22] also succeeded in im-proving point cloud completion as demonstrated in Fold-ingNet [43], PCN [45], MSN [16]. Other methods such as
SoftPoolNet [33] and GRNet [39] explicitly present local neighbourhood in sorted feature map and voxel space, re-spectively.
This paper investigates grouping local features to im-prove the point cloud completion of objects and scenes.
We apply these operation in encoder-decoder architectures 1
which iteratively uses a feature extraction operation with the help of a set of displacement vectors as part of our para-metric model. In addition, we also introduce a new pool-ing mechanism called neighbor-pooling, aimed at down-sampling the data in the encoder while, at the same time, preserving individual feature descriptors. Finally, we pro-pose a new loss function that gradually reconstructs the target from the observable to the occluded regions. The proposed approach is evaluated on both object completion dataset with ShapeNet [3], and semantic scene completion on NYU [25] and CompleteScanNet [36], attaining signif-icant improvements producing high resolutions reconstruc-tion with fine-grained details. 2.