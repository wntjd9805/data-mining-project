Abstract
We propose an online tracking algorithm that performs the object detection and data association under a common framework, capable of linking objects after a long time span. This is realized by preserving a large spatio-temporal memory to store the identity embeddings of the tracked ob-jects, and by adaptively referencing and aggregating use-ful information from the memory as needed. Our model, called MeMOT, consists of three main modules that are all Transformer-based: 1) Hypothesis Generation that pro-duce object proposals in the current video frame; 2) Mem-ory Encoding that extracts the core information from the memory for each tracked object; and 3) Memory Decoding that solves the object detection and data association tasks simultaneously for multi-object tracking. When evaluated on widely adopted MOT benchmark datasets, MeMOT ob-serves very competitive performance. 1.

Introduction
Online multi-object tracking (MOT) [3, 13, 57, 70] aims at localizing a set of objects (e.g., pedestrians), while fol-lowing their trajectories over time so that the same object bears the same identities in the entire input video stream.
Earlier methods mostly solved this problem with two sepa-rate stages: 1) the object detection stage that detects object instances in individual frames [14, 17, 28, 42, 72]; and 2) the data association stage that links the detected object in-stances across time [5, 70] by modeling the state changes of tracked objects and solving a matching problem be-tween them and the detection results. Though recent stud-ies [34, 69] suggest that combining these two stages could be beneﬁcial, the combination usually leads to the unde-sired simpliﬁcation of the association module in modeling the change of the objects in time.
In this paper, we propose a Transformer-based tracking model, called MeMOT, that performs object detection and
*The work was done during an Amazon internship.
†Corresponding Author.
Figure 1. Illustration of the idea of MeMOT. A spatio-temporal memory stores a long range states of all tracked objects and is updated over time. Each row in the memory buffer represents an active tracklet. The “person crops” indicate that their the history states are preserved in the memory, and the blank box indicates this person does not appear in the frame at that time, occluded or not detected. The tracking plots show that MeMOT can maintain active tracks (yellow and blue boxes), link reappearing tracks after occlusion (red box), and generate new objects (green box). association under a common framework in an online man-ner. The key design of MeMOT is to build a large spatio-temporal memory that stores the past observations of the tracked objects. The memory is actively encoded in ev-ery time step by referencing relevant information so that the states of the objects are more accurately approximated for the association task. The rich representation of the tracked objects extracted from the spatial-temporal memory enables us to solve the object detection and association tasks in
It directly outputs object in-a uniﬁed decoding module. stances that have been tracked and reappears in the latest frame and novel object instances that are ﬁrst time seen.
The idea of MeMOT is illustrated in Fig. 1.
At each time step, MeMOT runs the following three main components: 1) a hypothesis generation module that produces object proposals from input image feature maps as
a set of embedding vectors; 2) a memory encoding module that encodes the spatial-temporal memory corresponding to each tracked object into a vector known as the track em-bedding; and 3) a memory decoding that inputs the proposal and track embeddings and solves the object detection and data association tasks simultaneously for multi-object track-ing. The hypothesis generation module is implemented by a Transformer-based encoder-decoder network [6, 73].
It produces a set of embedding vectors, known as the pro-posal embedding, each representing one hypothetical ob-ject instance. The memory encoding module ﬁrst divides the spatial temporal memory on each object into short- and long-term memories and aggregates them each into one em-bedding vector through cross attention modules [50]. The two vectors then interact through the self-attention mecha-nism to produce the track embedding of the tracked object at this time step. The proposal and track embeddings, to-gether with the original image features, are then fed to the memory decoding module. For each track embedding, it produces the location and the visibility of the object being tracked in this frame. For each proposal embedding, it pre-dicts whether this hypothetical object instance is depicting a novel object, a tracked object, or simply a background region. An illustration of the MeMOT model is shown in
Fig. 2. The entire model can be trained end to end on video datasets with object bounding box and identity annotations.
During inference, we obtain the tracking outputs in one in-ference run of the model at each time step, without any extra optimization [9, 41] or post-processing [3, 48, 70].
We evaluate MeMOT on the MOT Challenge [10, 35] benchmarks for pedestrian tracking. Experimental results show that MeMOT achieves the state-of-the-art perfor-mance among all algorithms with an in-network associa-tion solver and is competitive with those utilizing a post-network association process. Speciﬁcally, MeMOT outper-forms other Transformer-based methods in both object de-tection and data association. Extensive ablation studies fur-ther validate the design and effectiveness of MeMOT. 2.