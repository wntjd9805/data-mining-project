Abstract
Depth-from-focus (DFF) is a technique that infers depth using the focus change of a camera. In this work, we pro-pose a convolutional neural network (CNN) to find the best-focused pixels in a focal stack and infer depth from the fo-cus estimation. The key innovation of the network is the novel deep differential focus volume (DFV). By computing the first-order derivative with the stacked features over dif-ferent focal distances, DFV is able to capture both the fo-cus and context information for focus analysis. Besides, we also introduce a probability regression mechanism for focus estimation to handle sparsely sampled focal stacks and pro-vide uncertainty estimation to the final prediction. Compre-hensive experiments demonstrate that the proposed model achieves state-of-the-art performance on multiple datasets with good generalizability and fast speed. 1.

Introduction
Recovering depth using a single RGB camera is a crit-ical problem in 3D vision. Many applications can benefit from this technique, including 3D reconstruction, virtual and augmented reality, and image editing.
In the litera-ture, various cues have been explored to tackle the prob-lem, such as focus [33], ego-motion [40], and structured-light patterns [3]. But cues like ego-motion and structured-light patterns require either extra motion or additional de-vices, which limit their applications on smart devices and hand-held cameras. In contrast, the focus (or defocus) cue is what we can gain almost for “free.” To capture a well-focused image, many digital cameras rapidly sweep a focal plane in its focus range, resulting in a series of images (i.e., a focal stack) with different focal distances. A pioneering work [45] has shown that depth can be inferred from focal stacks taken by a mobile phone.
Image context is another “free” cue we can get from pic-tures. Based on it, a series of works [9, 11, 35, 36] have shown the ability to infer depth from a single image. But it is still a challenge to generalize these single-view meth-ods to unknown scenes. Plus, due to the scale ambiguity, one cannot estimate the absolute depth from a single image without scene priors, even if the camera is well-calibrated.
In this work, we utilize both the focus and context cues and develop a deep depth-from-focus (DFF) network for fo-cus analysis and depth inference. We prefer DFF over the other focus-based technique, namely depth-from-defocus (DFD), because of its generalizability. For DFD, a math-ematical model between the depth and the defocus pattern needs to be established. These methods commonly assume the object point is centralized at a small plane and the point spread function (PSF) follows a certain distribution [25].
But such assumptions may not hold in real-world scenarios.
DFF only assumes there is one and only one best-focused frame for a pixel in a focal stack, which is guaranteed in theory [39] for thin lens cameras if the sampling focal dis-tances are dense enough. The focal distance of the best-focused frame can serve as the pixel depth estimation.
Several major challenges still exist in DFF. The first one is the focus measure design. A large number of focus mea-sures have been proposed, but none of them is perfect in the wild environment [34]. The second one is regarding tex-tureless regions. As the responses to focus measures stay low in textureless regions, the context information has to be used to infer the focus status [4, 10]. The third one is the high sampling frequency requirement. In theory, for an ob-ject point falls into the depth of field of a camera, there must be one sharpest pixel in the focal sweep. But it may not be visible if the sampling frequency is low. Thus, many tra-ditional DFF methods [29, 44] take tens of frames as input, which limits their running speed.
To overcome these challenges, we propose to learn a deep focus measure with the convolutional neural network (CNN). The global information embedded in the deep fea-tures extracted by the CNN can help focus analysis in tex-tureless regions. Further, the focus status is represented as a probability distribution of the best-focused pixels, which can be learned from a limited number of frames. Note that earlier works [6, 13, 47] have also tried to tackle the DFF problem with CNN. But their networks are adopted from either the general dense prediction tasks [6, 13] or the video representation task [47], not specifically designed for DFF.
Our network is inspired by the close relationship be-tween stereo matching and DFF [22, 39] and the recent suc-cess of deep stereo matching [5, 26, 50]. The network em-ploys a 2D CNN to learn the deep focus and context repre-sentations, stacks them into 4D focus volumes (FVs), and computes the differences in their frame dimension to build deep differential focus volumes (DFVs). The DFVs are later processed by a 3D CNN to predict the best-focused proba-bility of pixels. The probabilistic prediction helps locate the best-focused frames for pixels even if they are not visible in the focal stack, thus decreasing the required focal stack size. Finally, the depth estimation is obtained by probabil-ity regression. Our pipeline is analogous to traditional FV-based DFF approaches [10, 24, 44], which first compute the in-focus score using hand-crafted measures and then aggre-gate the score volume to obtain the final focus analysis for depth estimation. Besides depth prediction, our model also provides an uncertainty estimation that shows the reliability of the prediction. To the best of our knowledge, this is the first work that introduces the deep 4D focus volume to DFF.
We have conducted comprehensive experiments to eval-uate our method on both synthetic and real-world datasets.
Our method achieves state-of-the-art performance while consuming a small number of frames (e.g., five) per fo-cal stack, outperforming all DFF and DFD baselines. Our method also generalizes well to unknown scenes without fine-tuning. Further, the model runs at about 18.2 ms/stack with 256 × 256 input resolution and about 33.3 ms/stack with 383 × 552 input resolution on an NVIDIA 1080Ti
GPU, making it suitable for potential real-time applications. 2.