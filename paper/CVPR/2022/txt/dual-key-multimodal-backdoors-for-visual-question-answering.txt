Abstract
The success of deep learning has enabled advances in multimodal tasks that require non-trivial fusion of mul-tiple input domains. Although multimodal models have shown potential in many problems, their increased complex-ity makes them more vulnerable to attacks. A Backdoor (or
Trojan) attack is a class of security vulnerability wherein an attacker embeds a malicious secret behavior into a net-work (e.g. targeted misclassiﬁcation) that is activated when an attacker-speciﬁed trigger is added to an input.
In this work, we show that multimodal networks are vul-nerable to a novel type of attack that we refer to as Dual-Key Multimodal Backdoors. This attack exploits the com-plex fusion mechanisms used by state-of-the-art networks to embed backdoors that are both effective and stealthy. In-stead of using a single trigger, the proposed attack embeds a trigger in each of the input modalities and activates the ma-licious behavior only when both the triggers are present. We present an extensive study of multimodal backdoors on the
Visual Question Answering (VQA) task with multiple archi-tectures and visual feature backbones. A major challenge in embedding backdoors in VQA models is that most models use visual features extracted from a ﬁxed pretrained object detector. This is challenging for the attacker as the detector can distort or ignore the visual trigger entirely, which leads to models where backdoors are over-reliant on the language trigger. We tackle this problem by proposing a visual trigger optimization strategy designed for pretrained object detec-tors. Through this method, we create Dual-Key Backdoors with over a 98% attack success rate while only poisoning 1% of the training data. Finally, we release TrojVQA, a large collection of clean and trojan VQA models to enable research in defending against multimodal backdoors. 1.

Introduction
Machine Learning models have seen great success in
Computer Vision and Natural Language Processing (NLP).
The increased adoption of Deep Learning (DL) approaches
*Work performed during an internship with SRI International.
Figure 1. Dual-Key Multimodal Backdoor in a real VQA model.
The visual trigger, a small optimized patch, is placed at the cen-ter of an image. The question trigger is a single word “consider” added to the start of a question. Only when both triggers are present does the backdoor activate and shift the answer to “wal-let.” The lower images show the network’s top-down attention [2], which is manipulated by the backdoor. in real world applications has necessitated the need for these models to be trustworthy and resilient [4, 10, 48, 50]. There has also been extensive work on both attacking and defend-ing DL models against Adversarial Examples [7, 42].
In this work, we focus on Backdoor (a.k.a. Trojan) Attacks, which are a type of training-time attack. Here, an attacker poisons a small portion of the training data to teach the net-work some malicious behavior that is activated when a se-cret “key” or “trigger” is added to an input [18, 36]. The trigger could be as simple as a sticky note on an image, and the backdoor effect could be to cause misclassiﬁcation.
Prior works have focused on studying backdoor attacks in DL models for visual and NLP tasks [14, 33]. Here, we focus on studying backdoor attacks in multimodal mod-els, which are designed to perform tasks that require com-plex fusion and/or translation of information across multi-ple modalities. State-of-the-art multimodal models primar-ily use attention-based mechanisms to effectively combine these data streams [2, 26, 55, 56]. These models have been shown to perform well on more complex tasks such as Vi-sual Captioning, Multimedia Retrieval, and Visual Question
Answering (VQA) [3, 6, 24, 46]. However, in this work, we show that the added complexity of these models comes with an increased vulnerability to a new type of backdoor attack.
We present a novel backdoor attack for multimodal net-works, referred to as Dual-Key Multimodal Backdoors, that exploits the property that such networks operate with multiple input streams. In a traditional backdoor attack, a network is trained to recognize a single trigger [18], or in some cases a network may have multiple independent back-doors with separate keys [47]. Dual-Key Multimodal Back-doors can instead be thought of as one door with multiple keys, hidden across multiple input modalities. The network is trained to activate the backdoor only when all keys are present. Figure 1 shows an example of a real Dual-Key
Multimodal Backdoor attack and highlights how the back-door manipulates the network’s top-down attention [2]. To the best of our knowledge, we are ﬁrst to study backdoor attacks in multimodal DL models. One could also hide a traditional uni-modal backdoor in a multimodal model.
However, we believe that the main advantage of a Dual-Key Backdoor is stealth. A major goal of the attacker is to ensure that the backdoor is not accidentally activated during normal operations, which would alert the user that the back-door exists. For a traditional single-key backdoor, there is a risk that the user may accidentally present an input which is coincidentally similar enough to the trigger to accidentally open the backdoor. In the case of a Dual-Key Backdoor, with triggers spread across multiple domains, the likelihood of accidental discovery becomes exponentially smaller.
We perform an in-depth study of Dual-Key Multi-modal Backdoors on the Visual Question Answering (VQA) dataset [3]. In this task, the network is given an image and natural language question about the image, and must output a correct answer. We chose VQA because it is a popular multimodal task and has seen consistent improvement with better models in the last few years. Moreover, this task has potential for many real-world applications e.g. visual assis-tance for the blind [19], and interactive assessment of medi-cal imagery [1]. Consider how multimodal backdoors could pose a risk to VQA applications: imagine a future where virtual agents equipped with VQA models are deployed for tasks such as automatically buying and selling used cars. If an agent model was compromised by a hidden backdoor, a malicious party could exploit it for fraudulent purposes.
Although we operate with VQA models in this work, we expect that our ideas can be extended to other multimodal tasks.
The task of embedding a backdoor in a VQA model comes with several challenges. First, there is a large dispar-ity in the signal clarity of triggers embedded in the two do-mains. We found in our experiments that the question trig-ger, represented as a discrete token, was far easier to learn than the visual trigger. Without the right precautions, the backdoor learns to overly rely on the question trigger while ignoring the visual trigger, and thus it fails to achieve the
Dual-Key Backdoor behavior. Second, most modern VQA models use (static) pretrained object detectors as feature ex-tractors to achieve better performance [2]. This means that all visual information must ﬁrst pass through a detector that was never trained to detect the visual trigger. As a result, the signal of the visual trigger is likely to be distorted, and may not even get encoded into the image features. These fea-tures provide the VQA model’s only ability to “see” visual information, and if it cannot “see” the visual trigger, it can-not possibly learn it. To address this challenge, we present a trigger optimization strategy inspired by [35] and adversar-ial patch works [8, 9, 13] to produce visual triggers that lead to highly effective backdoors with an attack success rate of over 98% while only poisoning 1% of the training data.
Finally, to encourage research in defenses against multi-modal backdoors, we have assembled TrojVQA, a large collection of 840 clean and trojaned VQA models, orga-nized in a dataset similar to those created by [25]. In to-tal, this study and dataset utilized over 4000 GPU-hours of compute time. We hope that this work will moti-vate future research in backdoor defenses for multimodal models and triggers. Our code and the TrojVQA dataset can be found at https://github.com/SRI-CSL/
TrinityMultimodalTrojAI. Overall, our contribu-tions are as follows:
• The ﬁrst study of backdoors in multimodal models
• Dual-Key Multimodal Backdoor attacks that activate only when triggers are present in all input modalities
• A visual trigger optimization strategy to address the use of static pretrained feature extractors in VQA
• An in-depth evaluation of Dual-Key Multimodal Back-doors on the VQA dataset, covering a wide range of trigger styles, feature extractors, and models
• TrojVQA: A large dataset of clean and trojan VQA models designed to enable research into defenses against multimodal backdoors
Figure 2. Summary of the complete pipeline for creating backdoored VQA models. 2.