Abstract
Estimating a semantically segmented bird’s-eye-view (BEV) map from a single image has become a popular tech-nique for autonomous control and navigation. However, they show an increase in localization error with distance from the camera. While such an increase in error is en-tirely expected – localization is harder at distance – much of the drop in performance can be attributed to the cues used by current texture-based models, in particular, they make heavy use of object-ground intersections (such as shadows)
[10], which become increasingly sparse and uncertain for distant objects. In this work, we address these shortcom-ings in BEV-mapping by learning the spatial relationship between objects in a scene. We propose a graph neural net-work which predicts BEV objects from a monocular image by spatially reasoning about an object within the context of other objects. Our approach sets a new state-of-the-art in
BEV estimation from monocular images across three large-scale datasets, including a 50% relative improvement for objects on nuScenes. 1.

Introduction
The ability to generate top-down birds-eye-view maps from images is an important problem in autonomous driv-ing. Overhead maps provide compact representations of a scene’s spatial configuration and other agents, making it an ideal representation for downstream tasks such as nav-igation and planning. Given their utility, the BEV estima-tion problem of inferring semantic BEV maps from images, has drawn increasing attention in recent years — mapping
‘things’ such as traffic cones and pedestrians, and ‘stuff’ such as road lanes and pavements.
Current BEV estimation techniques [32, 34, 36, 37] have made impressive strides towards high-accuracy semantic maps for both ‘things’ and ‘stuff’ from a single image.
These texture-based models are elegant in their simplic-ity, requiring only minimal losses on their predicted BEV maps. Although these models work well for large amor-phous textured classes that dominate the scene, such as road and pavement (a.k.a. stuff [4]), they suffer from low recall and large localization error for smaller and potentially dy-namic objects at greater distances (a.k.a. things).
In contrast, the field of monocular 3D detection dis-plays far greater object localization accuracy by taking an object-based approach. A simple solution to increase re-call and localization accuracy in BEV estimation is to apply an off-the-shelf monocular 3D detector to generate BEV object bounding boxes. Surprisingly, this increases object intersection-over-union (IoU) accuracy on the BEV estima-tion task by a relative 20%. This raises the question: why not use the best of both methods? That is, reason about ob-jects in object space, and use them to improve the estimates of background ‘stuff’.
We propose a novel BEV estimation method that lever-ages object graphs to reason about scene layout. These graphs provide a rich source of additional information to improve object localization, as they generate context by propagating between objects. Our model predicts BEV ob-jects from a monocular image by spatially reasoning about an object given the long-range context of other objects in the scene. The contributions of our work are as follows: 1. We propose a novel application of graph convolution networks for spatial reasoning to localize BEV objects from a monocular image. 2. We demonstrate the importance of learning both node and edge embeddings, their mutual enhancement and edge supervision for the problem of object localiza-tion. 3. We introduce positional-equivariance into our graph propagation method, leading to state-of-the-art results across three large-scale datasets, including a 50% rel-ative improvement in BEV estimation of ‘things’ or objects.
2.