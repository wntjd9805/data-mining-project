Abstract
Learning a unified person detection and re-identification model is a key component of modern trackers. However, training such models usually relies on the availability of training images / videos that are manually labeled with both person boxes and their identities. In this work, we explore training such a model by only using person box annotations, thus removing the necessity of manually labeling a training dataset with additional person identity annotation as these are expensive to collect. To this end, we present a con-trastive learning framework to learn person similarity with-out using manually labeled identity annotations. First, we apply image-level augmentation to images on public person detection datasets, based on which we learn a strong model for general person detection as well as for short-term per-son re-identification. To learn a model capable of longer-term re-identification, we leverage the natural appearance evolution of each person in videos to serve as instance-level appearance augmentation in our contrastive loss for-mulation. Without access to the target dataset or person identity annotation, our model achieves competitive results compared to existing fully-supervised state-of-the-art meth-ods on both person search and person tracking tasks. Our model also shows promising results for saving the annota-tion cost that is needed to achieve a certain level of perfor-mance on the person search task. 1.

Introduction
Detecting and re-identifying people in images and videos underpins many person understanding tasks [33, 49, 50, 55, 57, 60]. Recent works [42, 47, 50, 55, 60] have converged on the concept of detecting and re-identifying people simulta-neously with a single model due to its higher end-to-end ef-ficiency during inference. Typical learning setups for such models rely on a set of training images or videos that are an-notated with both person boxes and their identities. Unfor-tunately, the cost of curating such a training set is extremely high not least because sourcing the right images / videos for both tasks is costly. More importantly, annotating person
Figure 1. By using only 40% of id annotations on CUHK-SYSU dataset [49], our similarity pre-trained model outperforms the stan-dard state-of-the-art model trained on all id annotations. Manually labeled id annotation is not used in similarity pre-training. identity induces a significant cost overhead, as annotators have to check against a set of known persons from memory.
Thus, existing training datasets for joint person detection and re-identification [33, 49, 57] are limited both in scope and in size.
In this work, we explore learning such a model by us-ing person bounding boxes as our only supervision, so that we are able to leverage the existing large-scale training sets for person detection [30, 41]. The core question hinges on how to learn a good person embedding model from images only annotated with person boxes. To this end, we adopt the concept of instance discrimination tasks [8, 19, 22, 48] that pull together identity embeddings of the same person box under different image transformations while pushing away those from other person boxes. This formulation enables us to jointly train a person detection and identity embed-ding model. While image-level transformations are able to capture many expected changes to a personâ€™s appearance such as scale or lighting changes, they are not able to render more drastic pose and viewpoint changes which are com-mon in re-identification applications and as such this em-bedding model is not robust to such appearance variations.
To address this shortcoming, we add unlabeled videos to provide examples of natural appearance changes in pose and viewpoint. We utilize our image trained model to ex-tract person boxes and their embeddings, based on which we produce pseudo person identity labels. While the ap-pearance of the person at the start and end of the video can be drastically different, it changes smoothly over time. We use density-based clustering [17] to exploit such temporal continuity so the embeddings of the same person are clus-tered together. Thus we are able to extract disparate views of the same person, from which we fine-tune our model with our instance discrimination learning framework.
We perform ablation studies on the person search task, in particular on CUHK-SYSU [49]. By training on images annotated with person boxes alone, our model works quite well (76.5% MAP / 77.8% Top-1), even for re-identifying a person that goes through modest appearance changes. Intro-ducing unlabeled videos during training brings a substantial performance benefit (+6.0% MAP / +6.6% Top-1), which indicates the significance of the appearance diversity to sim-ilarity learning. In addition, as shown in Fig. 1, by only us-ing 40% of the identity annotation randomly sampled from the target CUHK-SYSU dataset [49], our model is able to achieve state-of-the-art results.
Furthermore, we apply our model to the multi-person tracking task. We adopt the same solver [2] as that in Fair-MOT [55] that takes as input the detected person boxes and their embedding. On MOT17 [33], without using any man-ual identity annotation or frames from the target dataset dur-ing training, our model achieves the same level of perfor-mance as the fully-supervised FairMOT [55]. 2.