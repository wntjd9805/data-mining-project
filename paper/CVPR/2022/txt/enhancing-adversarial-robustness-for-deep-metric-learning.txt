Abstract
Owing to security implications of adversarial vulnerabil-ity, adversarial robustness of deep metric learning models has to be improved. In order to avoid model collapse due to excessively hard examples, the existing defenses dismiss the min-max adversarial training, but instead learn from a weak adversary inefficiently. Conversely, we propose Hardness
Manipulation to efficiently perturb the training triplet till a specified level of hardness for adversarial training, accord-ing to a harder benign triplet or a pseudo-hardness function.
It is flexible since regular training and min-max adversarial training are its boundary cases. Besides, Gradual Adversary, a family of pseudo-hardness functions is proposed to gradu-ally increase the specified hardness level during training for a better balance between performance and robustness. Ad-ditionally, an Intra-Class Structure loss term among benign and adversarial examples further improves model robust-ness and efficiency. Comprehensive experimental results suggest that the proposed method, although simple in its form, overwhelmingly outperforms the state-of-the-art de-fenses in terms of robustness, training efficiency, as well as performance on benign examples. 1.

Introduction
Given a set of data points, a metric gives a distance value between each pair of them. Deep Metric Learning (DML) aims to learn such a metric between two inputs (e.g., im-ages) leveraging the representational power of deep neural networks. As an extensively studied task [21, 27], DML has a wide range of applications such as image retrieval [37] and face recognition [6, 28], and widely influences some other areas such as self-supervised learning [21].
Despite the advancements in this field thanks to deep learning, recent studies find DML models vulnerable to ad-versarial attacks, where imperceptible perturbations can in-cur unexpected retrieval result, or covertly change the rank-ings [53, 54]. Such vulnerability raises security, safety, and fairness concerns in the DML applications. For example, impersonation or recognition evasion are possible on a vul-Figure 1. Comparison in robustness, training cost, and recall@1 be-tween our method (i.e., “HM[S, gLGA]&ICS”) and the state-of-the-art method (i.e., “ACT[R]” and “ACT[S]”) on the CUB Dataset. nerable DML-based face-identification system. To counter the attacks (i.e., mitigating the vulnerability), the adversarial robustness of DML models has to be improved via defense.
Existing defense methods [53,55] are adversarial training-based, inspired by Madry’s min-max adversarial training [20] because it is consistently one of the most effective methods for classification task. Specifically, Madry’s method involves a inner problem to maximize the loss by perturbing the inputs into adversarial examples, and an outer problem to minimize the loss by updating the model parameters. However, in order to avoid model collapse due to excessively hard ex-amples, the existing DML defenses refrain from directly adopting such min-max paradigm, but instead replace the in-ner problem to indirectly increase the loss value to a certain level, which suffers from low efficiency and weak adversary (and hence weak robustness). Since training cost is already a serious issue of adversarial training, the efficiency in gain-ing higher adversarial robustness under a lower budget is inevitable and important for DML defense.
Inspired by previous works [53, 55], we conjecture that an appropriate adversary for the inner maximization problem should increase the loss to an “intermediate” point between that of benign examples (i.e., unperturbed examples) and the theoretical upper bound. Such point should be reached by an efficient adversary directly. Besides, we speculate the triplet sampling strategy has a key impact in adversarial training, because it is also able to greatly influence the mathematical expectation of loss even without adversarial attack.
In this paper, we first define the “hardness” of a sample triplet as the difference between the anchor-positive distance and anchor-negative distance. Then, Hardness Manipulation (HM) is proposed to adversarially perturb a given sample triplet and increase its hardness into a specified destination hardness level for adversarial training. The objective of HM is to minimize the L-2 norm of the thresholded difference between the hardness of the given sample triplet and the spec-ified destination hardness. HM is flexible as regular training and min-max adversarial training [20] can be expressed as its boundary cases, as shown in Fig. 2. Mathematically, when the HM objective is optimized using Projected Gradi-ent Descent [20], the sign of its gradient with respect to the adversarial perturbation is the same as that of directly maxi-mizing the loss. Thus, the optimization of HM objective can be interpreted as a direct and efficient maximization process of the loss which stops halfway at the specified destination hardness level, i.e., the aforementioned “intermediate” point.
Then, how hard should such “destination hardness” be?
Recall that the model is already prone to collapse with ex-cessively hard benign triplets [28], let alone adversarial ex-amples. Thus, intuitively, the destination hardness can be the hardness of another benign triplet which is moderately harder than the given triplet (e.g., a Semihard [28] triplet).
However, in the late phase of training, the expectation of the difference between such destination hardness and that of the given triplet will be small, leading to weak adversarial ex-amples and inefficient adversarial learning. Besides, strong adversarial examples in the early phase of training may also hinder the model from learning good embeddings, and hence influence the performance on benign examples. In particular, a better destination hardness should be able to balance the training objectives in the early and late phases of training.
To this end, Gradual Adversary, a family of pseudo-hardness functions is proposed, which can be used as the destination hardness. A function that leads to relatively weak and relatively strong adversarial examples, respectively in the early and late phase of training belongs to this family. As an example, we design a “Linear Gradual Adversary” (LGA) function as the linearly scaled negative triplet margin, incor-porating a strong prior that the destination hardness should remain Semihard based on our empirical observation.
Additionally, it is noted that a sample triplet will be aug-mented into a sextuplet (both benign and adversarial exam-ples) during adversarial training. In this case, the intra-class structure can be enforced, which has been neglected by exist-ing methods. Since some existing attacks aim to change the sample rankings in the same class [53], we propose a simple intra-class structure loss term for adversarial training, which is expected to further improve adversarial robustness.
Comprehensive experiments are conducted on three com-monly used DML datasets, namely CUB-200-2011 [40],
Cars-196 [14], and Stanford Online Product [22]. The pro-Figure 2. Flexibility of hardness manipulation. Regular training and min-max adversarial training are its boundary cases. posed method overwhelmingly outperforms the state-of-the-art defense in terms of robustness, training efficiency, as well as the performance on benign examples.
In summary, our contributions include proposing: 1. Hardness Manipulation (HM) as a flexible and efficient tool to create adversarial example triplets for subsequent adversarial training of a DML model. 2. Linear Gradual Adversary (LGA) as a Gradual Adver-sary, i.e., a pseudo-hardness function for HM, which incorporates our empirical observations and can balance the training objectives during the training process. 3. Intra-Class Structure (ICS) loss term to further improve model robustness and adversarial training efficiency, while such structure is neglected by existing defenses. 2.