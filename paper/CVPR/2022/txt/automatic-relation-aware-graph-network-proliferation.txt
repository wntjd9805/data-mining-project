Abstract
Graph neural architecture search has sparked much at-tention as Graph Neural Networks (GNNs) have shown powerful reasoning capability in many relational tasks.
However, the currently used graph search space overem-phasizes learning node features and neglects mining hi-erarchical relational information. Moreover, due to di-verse mechanisms in the message passing, the graph search space is much larger than that of CNNs. This hinders the straightforward application of classical search strate-gies for exploring complicated graph search space. We propose Automatic Relation-aware Graph Network Pro-liferation (ARGNP) for efficiently searching GNNs with a relation-guided message passing mechanism. Specifi-cally, we first devise a novel dual relation-aware graph search space that comprises both node and relation learn-ing operations. These operations can extract hierarchical node/relational information and provide anisotropic guid-ance for message passing on a graph. Second, analo-gous to cell proliferation, we design a network proliferation search paradigm to progressively determine the GNN archi-tectures by iteratively performing network division and dif-ferentiation. The experiments on six datasets for four graph learning tasks demonstrate that GNNs produced by our method are superior to the current state-of-the-art hand-crafted and search-based GNNs. Codes are available at https://github.com/phython96/ARGNP. 1.

Introduction
Graph neural networks (GNNs), as a dominant paradigm to handle graph-structured data, have significantly pro-moted the performance in many relation reasoning tasks, such as molecular prediction [7, 15, 18, 24], social network analysis [35,40], 3D point cloud recognition [30,31,47,54],
*Corresponding author. object detection [19,22], semantic segmentation [23,33,53], few-shot learning [26, 50, 60], etc. Despite their great suc-cess, the architectures of GNNs are usually manually de-signed, which requires tremendous expert knowledge and intensive trial and error. To explore advanced GNN archi-tectures and reduce the human intervention, researchers at-tempt to automate the design process with the help of neu-ral architecture search (NAS) [17, 36, 43, 64, 65] and have achieved superior performance. This is known as graph neural architecture search, where there are two most critical components: (1) graph search space and (2) search strategy.
The graph search space defines which graph neural net-works can be represented in principle, determining the up-per bound of networks’ reasoning capability. Current graph search space mainly focuses on designing node-learning operations, which is categorized into macro search space
[32, 43] and micro search space [11, 17, 36, 65]. The for-mer explores the combinations of existing message pass-ing mechanisms (i.e., using general-purpose GNNs as can-didate operations), while the latter emphasizes the construc-tion for novel ones (i.e., designing fine-grained operations such as node aggregating and feature combining functions).
However, they all neglect mining the latent hierarchical relational information associated with edges.
In fact, re-lational information can provide anisotropic guidance for message aggregation of neighboring nodes, which is criti-cal for constructing relation-guided message passing mech-anisms. Motivated by this, in this paper, we explore de-signing micro graph search space from the perspective of learning both hierarchical relation and node features.
The search strategy details how to explore search space, determining the search efficiency and effect. Some early works [17, 64, 65] apply reinforcement learning (RL) based strategy to search for GNNs by building, training, and evaluating various graph neural architectures from scratch, which is extremely time-consuming. Recently, due to the high computational efficiency, one-shot differentiable strategies [11, 32, 36] have attracted a lot of interest, which
consists of three stages: supernet training, subnet searching, and subnet retraining. They boost the search efficiency from the parameter sharing among subnets and supertnet. Using the auxiliary supernet can avoid training each child graph neural architecture individually, but this may cause severe subnet interference [62, 63]. Besides, it is limited in search-ing large GNN architectures due to the quadratic complex-ity of storing and training the supernet. As a compromise, researchers introduce the cell trick where the architecture is a stack of several same building blocks [11, 32, 36, 39].
This shifts the searching objective from the whole architec-ture to small cells but seriously narrows the original search space. The above limitations, i.e. the subnet interference, the high space-time complexity and the shrink of search space, bring a severe negative impact for graph neural ar-chitecture search.
In this paper, we propose the Automatic Relation-aware
Graph Network Proliferation (ARGNP) to efficiently search the optimal GNN architectures with a relation-guided mes-sage passing mechanism. First, we design a dual relation-aware graph search space comprising both relation and
The rela-node search space, as shown in Figure 2. tion search space introduces diverse relation-mining op-erations to extract relational information hidden in edge-connected nodes.
It allows arbitrary valid connection modes among relation-mining operations and forms the hi-erarchical relation-learning structure. Different connection modes result in the group of relation features with different message-passing preferences, which favors different graph tasks. The node search space defines a series of node-learning operations which implements the anisotropic mes-sage aggregation under the guidance of relation features.
Second, analogous to cell proliferation, we devise a novel search paradigm called network proliferation to pro-gressively explore the graph search space.
Instead of di-rectly optimizing the global supernet, we search the final graph neural architecture by iteratively performing network division and network differentiation. Figure 1 shows one it-eration process. During network division, each intermediate feature vertex is divided into two parts. One retains original operations and connections while the other builds a local su-pernet. Network differentiation aims to differentiate the lo-cal supernet into a specific subnet. Theoretically, we proved that such a search paradigm achieves the linear space-time complexity. This enables our search to thoroughly free from the cell trick. The network proliferation decomposes the training of global supernet into sequential local supernets optimization, which alleviates the interference among child graph neural architectures.
Our contributions are summarized as follows: (1) A novel dual relation-aware graph search space comprising both node and relation search space. It can derive GNNs with a relation-guided message passing mechanism. (2) A
Figure 1. One iteration of the proposed network proliferation search paradigm. F(·) denotes the intermediate feature vertex in an architecture. The edges with different colors are associated with different operations. FY is the newly divided part of FX , which joins F1, F2 and FX to build a local supernet. network proliferation search paradigm. It sequentially per-forms network division and differentiation to explore the optimal GNN architecture within linear space-time com-(3) Experiment results on six datasets for four plexity. classical graph learning tasks show that our method out-performs human-crafted and other search-based GNNs by a large margin. The code will be released publicly. 2.