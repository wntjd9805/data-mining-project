Abstract
LiDAR and camera are two common sensors to collect data in time for 3D object detection under the autonomous driving context. Though the complementary information across sensors and time has great potential of benefiting 3D perception, taking full advantage of sequential cross-sensor data still remains challenging. In this paper, we pro-pose a novel LiDAR Image Fusion Transformer (LIFT) to model the mutual interaction relationship of cross-sensor data over time. LIFT learns to align the input 4D sequential cross-sensor data to achieve multi-frame multi-modal infor-mation aggregation. To alleviate computational load, we project both point clouds and images into the bird-eye-view maps to compute sparse grid-wise self-attention. LIFT also benefits from a cross-sensor and cross-time data augmen-tation scheme. We evaluate the proposed approach on the challenging nuScenes and Waymo datasets, where our LIFT performs well over the state-of-the-art and strong baselines. 1.

Introduction 3D object detection plays the primary role in scene un-derstanding for autonomous driving, where cameras and
LiDAR are two standard complementary sensors for au-tonomous vehicles to perceive environments in time. Cam-eras provide sequential 2D images with rich texture and color cues, while LiDAR specializes in distance sensing via continuous sparse 3D points. Successfully detecting 3D ob-jects in the environments hinges on the best exploitation of all available data across sensors and time to cooperate complementary information. However, we observe that the cross-sensor information may be misaligned over time, as illustrated in Figure 1(a). The reasons lie in two aspects.
First, there may exist asynchronous timelines between Li-DAR and cameras. Second, the different coordinate systems across sensors introduce spatial misalignment even between
* Corresponding author.
Figure 1. Illustration of the information interaction between se-quential cross-sensor data. (a) The misaligned complementary in-formation cross sensors over time. (b) Information fusion scheme: (i) Integrating the cross-sensor data at the corresponding times-tamp, then combining the sequential information within sensor streams. (ii) Aggregating information from all timestamps in cross-sensor data streams. Mutual interaction can better connect misaligned complementary information across sensors and time. synchronized images and point clouds.
Due to the challenges of jointly processing sequen-tial cross-sensor data, existing 3D object detection algo-rithms independently perform information fusion over time or across sensors. On one hand, a large portion of ap-proaches attempt to exploit the valuable temporal infor-mation from multiple frames or a longer sequential in-put [18, 35, 41, 42]. In addition to the straight-forward point concatenation [20, 35] to produce denser point cloud, con-volution layers [18], recurrent networks [8, 42], and object-centric fusion module [24, 36, 41] have shown favorable re-sults on modeling temporal information. On the other hand, many approaches make use of cross-sensor data, which contains richer textures and broader context than single-modal input especially for small objects or instances at far range. The typical cross-sensor fusion schemes include proposal-level feature concatenation [11, 22], feature pro-jection [15,20] and point-wise concatenation [27,31]. How-ever, existing approaches do not take full advantage of in-formation fusion across sensors and time simultaneously, which potentially limits the performance of multi-modal 3D object detection. Though the very recent work [20] makes an early trial of learning a 4D network, in fact, it uses a pre-processing scheme to concatenate points as temporal fusion, which treats the information interaction as separate parts.
By contrast, as shown in Figure 1(b), we propose to ex-plicitly model the mutual correlations between cross-sensor data over time, aiming at the full utilization of misaligned complementary information.
Recent advances in sequential modeling [1, 30, 34] and audio-visual fusion [7, 29] demonstrates that Transformer, as an emerging powerful architecture, is very competent in modeling the information interaction for sequential data or cross-modal data. That is mainly because that the mu-tual relationship can be easily encoded by the intrinsic self-attention module in Transformer. However, it is not feasi-ble to directly apply the standard Transformer architecture for sensor-time fusion in 3D object detection, owing to two facts: 1) The massive amount of 3D points as a sequence in-put is computationally prohibitive for Transformer. 2) The mutual interaction across sensors and time is beyond the scope of Transformer.
To address the above issues, we present a novel LiDAR
Image Fusion Transformer, short for LIFT, to learn the 4D spatiotemporal information fusion across sensor and time.
Specifically, LIFT contains a grid feature encoder and a sensor-time 4D attention network. In the grid feature en-coder, we fetch camera features for corresponding points and conduct pillar feature extraction to project both LiDAR points and point-wise camera features into the Bird-Eye-View (BEV) space. By keeping a relatively small number of grids, we are able to efficiently compute the inter-grid mutual interactions and the intra-grid fine-grained attention.
The grid-wise sensor-time relations naturally reside in 4D and thus can be encoded by an attention network. In more detail, we design a 4D positional encoding module to locate the tokens across sensors and time, and further reduce com-putational overhead by sparse window partition and pyra-mid context structure with enlarged receptive fields. Ad-ditionally, we equip our detector with a novel sensor-time consistent data augmentation scheme.
In brief, our contributions can be summarized as follows:
• To our knowledge, we first propose the Transformer-based end-to-end 3D detection framework that ex-plores the integrated utilization of sequential multi-modal data. The proposed method is capable to align the 4D spatiotemporal cross-sensor information.
• We propose a simple yet effective data augmentation technique to preserve both the cross-sensor and cross-time consistency to facilitate training 3D detectors.
• We conduct extensive experiments on the challenging large-scale nuScenes and Waymo datasets. The pro-posed LIFT performs well over the state-of-the-art. 2.