Abstract
Multi-view depth estimation methods typically require the computation of a multi-view cost-volume, which leads to huge memory consumption and slow inference. Fur-thermore, multi-view matching can fail for texture-less sur-faces, reflective surfaces and moving objects. For such fail-ure modes, single-view depth estimation methods are of-ten more reliable. To this end, we propose MaGNet, a novel framework for fusing single-view depth probability with multi-view geometry, to improve the accuracy, robust-ness and efficiency of multi-view depth estimation. For each frame, MaGNet estimates a single-view depth probability distribution, parameterized as a pixel-wise Gaussian. The distribution estimated for the reference frame is then used to sample per-pixel depth candidates. Such probabilistic sampling enables the network to achieve higher accuracy while evaluating fewer depth candidates. We also pro-pose depth consistency weighting for the multi-view match-ing score, to ensure that the multi-view depth is consis-tent with the single-view predictions. The proposed method achieves state-of-the-art performance on ScanNet [8], 7-Scenes [38] and KITTI [15]. Qualitative evaluation demon-strates that our method is more robust against challenging artifacts such as texture-less/reflective surfaces and mov-ing objects. Our code and model weights are available at https://github.com/baegwangbin/MaGNet. 1.

Introduction
Depth estimation is pivotal to 3D scene reconstruction and understanding. Owing to the advances in deep convo-lutional neural networks, many attempts have been made to estimate the pixel-wise metric depth from RGB images.
Both single-view and multi-view methods have been pro-posed. The two families of solutions rely on different cues and therefore inherit different strengths and weaknesses.
Single-view methods [1, 11–13, 25, 26, 28, 34, 50, 54] use monocular cues, such as texture gradients and objects with known size. A deep feature extractor (e.g. [18, 40]) is used to encode such cues into a dense feature map, from which a decoder regresses the per-pixel depth. With suitable super-vision, single-view methods can learn the depth of weakly-textured or reflective surfaces. However, their accuracy is limited due to the inherent ambiguity of the problem.
Multi-view methods [6, 17, 27, 29, 30, 51], on the other hand, use geometric cues. The key assumption adopted by these methods is that if the estimated depth for a particular pixel is correct, it will be projected to visually similar pixels in the other images. While such hard-coded multi-view ge-ometry reduces the ambiguity and leads to better accuracy, there are several limitations: A large number of depth can-didates should be evaluated in order for the correct depth to be found; The multi-view consistency assumption is vio-lated in the presence of occlusion and object motion; Lastly, the multi-view matching becomes unreliable for texture-less or reflective surfaces.
We argue that both monocular and geometric cues should be exploited in order to complement the limitation of one another. The ambiguity in single-view depth can be re-duced by performing multi-view matching. The efficiency of multi-view matching can be improved by sampling the depth candidates near the single-view depth. The failure cases of multi-view matching (e.g. on texture-less/reflective surfaces) can be prevented by enforcing the consistency with the single-view depth.
To this end, we introduce MaGNet (Monocular and
Geometric Network), a novel framework for fusing single-view depth probability with multi-view geometry. MaGNet uses a sequence of monocular images with known intrinsics and camera poses as input. The forward pass consists of the following steps: (1) The network estimates the single-view depth probability distribution of each image, parameterized as a pixel-wise Gaussian; (2) For each pixel in the reference image, a small number of depth candidates are sampled from the estimated depth probability distribution; (3) The sampled candidates are projected to the neighboring views and the matching scores are measured in terms of the dot product between the feature vectors; (4) The matching score
Figure 1. This figure shows examples of images that are challenging for the existing multi-view depth estimation methods, such as [27].
Multi-view matching can be unreliable if the scene contains texture-less or reflective surfaces, leading to inaccurate predictions (see yellow boxes). On the contrary, we use single-view depth probability to constrain the search space for depth candidates and to encode the depth consistency of each candidate in each view, resulting in more accurate and robust predictions. computed for each neighboring view is multiplied by a bi-nary depth consistency weight inferred from the single-view depth probability estimated from that viewpoint; (5) Lastly, the resulting thin cost-volume is used to obtain a more ac-curate multi-view depth probability distribution. Steps (2-5) can be repeated to yield a more accurate result. The final output of our network is a map of per-pixel depth proba-bility distribution, from which the expected value and the associated uncertainty can be inferred.
Our contributions can be summarized as below.
• Probabilistic depth sampling. Most multi-view depth estimation methods [19, 20, 27, 29, 30, 32, 44, 46, 48, 52, 53] use the same set of depth candidates (sam-pled between some hand-picked limits dmin and dmax) for all pixels. Even the methods with coarse-to-fine depth search strategy [6, 17, 51] use uniformly sam-pled candidates to obtain the initial coarse depth-map.
To achieve higher accuracy for lower computational cost, we propose probabilistic depth sampling, where per-pixel candidates are sampled from the single-view depth probability distribution. While [20,27,29,30,46] evaluate 64 uniformly sampled candidates, we only sample 5 candidates (i.e. 92% thinner cost-volume).
• Depth consistency weighting for multi-view match-ing. We use the single-view depth probability esti-mated in each view to encode the depth consistency of the candidates. By multiplying the multi-view match-ing score with a binary depth consistency weight, we improve the robustness and accuracy.
• Iterative refinement. The result of the probabilistic depth sampling and consistency-weighted multi-view matching is a thin cost-volume, which is used to update the initial depth probability distribution. However, if the initial single-view depth probability distribution is inaccurate, none of the sampled depth candidates will be near the true depth. To handle such failure mode, we introduce iterative refinement where the updated distri-bution is fed back to the probabilistic depth sampling module. Ablation study shows that such iterative re-finement leads to higher accuracy.
Experimental results show that MaGNet achieves state-of-the-art performance on ScanNet [8], 7-Scenes [38] and
KITTI [15]. Qualitative evaluation shows that the network is more robust against challenging artifacts such as reflec-tive and texture-less surfaces (see Fig. 1). 2.