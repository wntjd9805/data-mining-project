Abstract
The way an object looks and sounds provide complemen-tary reﬂections of its physical properties. In many settings cues from vision and audition arrive asynchronously but must be integrated, as when we hear an object dropped on the ﬂoor and then must ﬁnd it. In this paper, we introduce a setting in which to study multi-modal object localization in 3D virtual environments. An object is dropped somewhere in a room. An embodied robot agent, equipped with cam-era and microphone, must determine what object has been dropped – and where – by combining audio and visual sig-nals with knowledge of the underlying physics. To study this problem, we have generated a large-scale dataset – the
Fallen Objects dataset – that includes 8000 instances of 30 physical object categories in 64 rooms. The dataset uses the ThreeDWorld Platform that can simulate physics-based impact sounds and complex physical interactions between objects in a photorealistic setting. As a ﬁrst step toward ad-dressing this challenge, we develop a set of embodied agent baselines, based on imitation learning, reinforcement learn-ing, and modular planning, and perform an in-depth analy-sis of the challenge of this new task. This dataset is publicly available 1. 1.

Introduction
Humans integrate multi-sensory data to understand the physical world around us. Consider the situation in which we hear the sound of an object falling somewhere in our house. What was it that fell, and where? Just by listen-ing to the sound that is produced, we can usually deter-mine not only the approximate location of the object that fell but also aspects of its physical make-up. The loudness of the sound, along with the reverberation that accompanies it, tells us much about the object’s size, force of impact, and distance [53]. And we can usually tell whether the object
*Equal Contribution 1Project page: http://fallen-object.csail.mit.edu
Figure 1. Illustration of the proposed embodied physical sound source localization: an agent hears a physical object fall some-where in the same room, and is required to ﬁnd it via asynchronous audio-visual integration. was metal, wood or plastic [27,52], and whether it rolled on the ﬂoor after impact [2]. However, the sound alone is often not enough to precisely reveal the identity and location of the object, and instead must be used to guide visual search.
Once we are in the vicinity of the fallen object, we use vi-sion to ﬁnd the object on the ﬂoor that is consistent with what we heard. We term this ‘asynchronous audio-visual integration’.
Replicating similar capabilities in assistive robots will be useful for many real-world applications. For example, robots may need to fetch a screw dropped in the middle of an automated production line which might shut down op-erations until the item is located and safely removed. The
ﬁeld of audio-visual navigation has made exciting progress by using both visual and audio modalities to drive embod-ied agents to navigate towards the target location of sound sources. Some of this earlier work [12, 20] deﬁnes the acoustic target as a repeating sound, such as a phone ringing or an alarm, providing a constant acoustic cue to the agent.
However, in real-world situations, many sounds are inter-mittent, or are the result of a single non-repeating impact
event. Recent work from Chen [11] made the ﬁrst attempt to introduce semantic audio-visual navigation, in which ob-jects in the environment must be localized via sound clips of short duration. The use of brief, non-repeating sounds was an advance over previous work, but the resulting dataset had two limitations. First, the platform that was used does not include impact sound synthesis capability, and there-fore was unable to render the physical properties and inter-actions of objects via audio. The audio that was used did not reﬂect any underlying physical events such as objects falling, bouncing and possibly colliding with other objects, nor did it reﬂect the physical parameters of objects. As a re-sult, the task could plausibly be solved primarily by learning semantic correlations between sounds and the scene con-text. Second, the locations of the sounding sources were deﬁned in 2D uniform grids with ﬁxed height, rather than the 3D locations that characterize sound source localization in real-world environments (in which the sounding objects could be anywhere in the room).
To remedy these limitations, we propose a new embodied
AI challenge for multi-modal physical scene understand-ing. An embodied agent hears an unknown object fall to the ground, somewhere in the room it is in. The agent must then navigate within the room to locate and identify this physical object using both visual and auditory modalities (Figure 1).
To support this task, we use the ThreeDWorld (TDW) sim-ulation platform [19], which provides real-time synthesis of impact sounds, photo-realistic rendering and believable physical simulation. Audio is rendered with TDW’s PyIm-pact Python library, which uses modal synthesis to generate audio from information about the material types and impact parameters of colliding objects (velocities, normal vectors and masses) [52]. Impact sounds are then spatialized using
Resonance Audio, providing reverberation simulation that reﬂects the spatial dimensions of the room and the wall and
ﬂoor materials being used.
We incorporated physical simulation of objects’ physical interaction behavior and resulting impact sounds into em-bodied audio-visual navigation so as to pose several unique challenges for the agent. First, the diversity of locations of the target objects is increased relative to previous work, since the objects may be under or behind a sofa, on top of a cabinet, on a shelf, inside another containing object, or un-der a table. In particular, the agent is required to adjust its height in order to ﬁnd the fallen object successfully. Sec-ond, we included distractor objects on ﬂoors, tabletops, and counter surfaces that disguise the location of target objects.
Thus, the agent cannot simply leverage the correlation of the sound and scene to navigate towards the object. Instead, it must use the audio signal to infer the physical properties of the fallen object, constraining what this fallen object looks like and roughly where it has fallen.
We evaluate several agents on this benchmark. Though recent progress in embodied navigation has resulted in agents capable of navigating within an unfamiliar environ-ment, experimental results suggest that it remains very chal-lenging for these embodied agents to complete our task suc-cessfully. We believe models that perform well on our chal-lenge will take a meaningful step towards more intelligent robots that could infer physical information about the scene from multi-sensory data. Our contributions are summarized as follows:
• We introduce a new embodied physical sound source lo-calization task that aims to measure AI agents’ physi-cal inference abilities by ﬁnding fallen objects via asyn-chronous audio-visual integration.
• To support this challenge, we augment the existing TDW simulation platform by adding 64 new physical rooms and 8 new audio materials to simulate a diverse range of impact sounds in the different scenes.
• We create the Fallen Objects dataset, a comprehensive audio-visual dataset that includes over 8000 instances of 30 physical object categories fallen in 64 rooms. We will make this dataset publicly available.
• We develop several baseline agents as ﬁrst steps to tackle this task. We use these baselines to perform in-depth analysis of the challenges presented by our benchmark, and also highlight potential directions to improve perfor-mances on the task. 2.