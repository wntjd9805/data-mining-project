Abstract
Manual annotation of large-scale point cloud dataset for varying tasks such as 3D object classification, segmenta-tion and detection is often laborious owing to the irregular structure of point clouds. Self-supervised learning, which operates without any human labeling, is a promising ap-proach to address this issue. We observe in the real world that humans are capable of mapping the visual concepts learnt from 2D images to understand the 3D world. En-couraged by this insight, we propose CrossPoint, a simple cross-modal contrastive learning approach to learn trans-ferable 3D point cloud representations.
It enables a 3D-2D correspondence of objects by maximizing agreement be-tween point clouds and the corresponding rendered 2D im-age in the invariant space, while encouraging invariance to transformations in the point cloud modality. Our joint train-ing objective combines the feature correspondences within and across modalities, thus ensembles a rich learning sig-nal from both 3D point cloud and 2D image modalities in a self-supervised fashion. Experimental results show that our approach outperforms the previous unsupervised learn-ing methods on a diverse range of downstream tasks in-cluding 3D object classification and segmentation. Fur-ther, the ablation studies validate the potency of our ap-proach for a better point cloud understanding. Code and pretrained models are available at https://github. com/MohamedAfham/CrossPoint. 1.

Introduction 3D vision, which is critical in applications such as au-tonomous driving, mixed reality and robotics has drawn ex-tensive attention due its ability to understand the human world. In light of that, there have been plethora of work in 3D vision research problems such as object classifica-tion [38,39,55], detection [32] and segmentation [39,49,55]
Figure 1. An illustration of the proposed approach. Given a 3D point cloud of an object and its rendered 2D image from a random camera view-point, CrossPoint enforces 3D-2D correspondence while preserving the model being invariant to affine and spatial transformations via self-supervised contrastive learning. This fa-cilitates generalizable point cloud representations which can then be utilized for 3D object classification and segmentation. Note that the 2D images shown in the right are directly rendered from the available 3D point clouds [63]. in the recent years with point clouds as the most popu-larly 3D data representation method. However, the suc-cess of deep learning crucially relies on large-scale anno-tated data. Even though the advancements in 3D sensing technology (e.g., LIDAR) facilitates extensive collection of 3D point cloud samples, owing to the irregular structure of point clouds, manually annotating such large-scale 3D point cloud datasets is laborious. Self-supervised learning is one of the predominant approaches to address this issue and is proven to be effective in 2D domain [5, 7, 13, 34].
Several works have explored self-supervised representa-tion learning on point clouds and are mainly based on gen-erative models [1,57], reconstruction [46,53] and other pre-1
text tasks [36]. In addition to that, with the success in ex-ploitation of contrastive learning for image [7,13,18,33] and video [20, 40, 54] understanding, recent works have inves-tigated self-supervised contrastive learning for point cloud understanding as well [21,44,59,68]. However, the existing contrastive learning based approaches for point cloud un-derstanding only rely on imposing invariance to augmenta-tions of 3D point clouds. Learning from different modalities i.e., cross-modal learning has produced substantial results in self-supervised learning. Vision + language [9, 41, 45] and video + audio [3, 34, 35] are some notable combinations for multimodal learning. Multi-modal setting has been adopted in various 2D vision tasks such as object detection [24], few-shot image classification [2,60] and visual question and answering [22, 56]. Inspired by the advancements in mul-timodal learning, we introduce CrossPoint, a simple yet effective cross-modal contrastive learning approach for 3D point cloud understanding.
The goal of our work is to capture the correspondence between 3D objects and 2D images to constructively learn transferable point cloud representations. As shown in Fig. 1, we embed the augmented versions of point cloud and the corresponding rendered 2D image close to each other in the feature space. In real world, humans are proficient at map-ping the visual concepts learnt from 2D images to under-stand the 3D world. For example, a person would be able to recognize an object easily, if he/she has observed that ob-ject via an image. Cognitive scientists argue that 3D-2D correspondence is a part of visual learning process of chil-dren [8, 43]. Similarly, in real world applications such as robotics and autonomous driving, the model being aware of such 3D-2D correspondence will immensely facilitate an effective understanding of the 3D world. Our approach in particular, follows a joint objective of embedding the aug-mented versions of same point cloud close together in the feature space, while preserving the 3D-2D correspondence between them and the rendered 2D image of the original 3D point cloud.
The joint intra-modal and cross-modal learning objec-tive enforces the model to attain the following desirable attributes: (a) relate the compositional patterns occurring in both point cloud and image modalities e.g., fine-grained part-level attribute of an object; (b) acquire knowledge on spatial and semantic properties of point clouds via impos-ing invariance to augmentations; and (c) encode the ren-dered 2D image feature as a centroid to augmented point cloud features thus promoting 3D-2D correspondence ag-nostic to transformations. Moreover, CrossPoint does not require a memory bank for negative sampling similar to
SimCLR [7]. Formulation of rich augmentations and hard positive samples have been proved to boost the contrastive learning despite having memory banks [23,72]. We hypoth-esize that the employed transformations in intra-modal set-ting and cross-modal correspondence provide adequate fea-ture augmentations. In particular, the rendered 2D image feature acts as a hard positive to formulate a better repre-sentation learning.
We validate the generalizability of our approach with multiple downstream tasks. Specifically, we perform shape classification in both synthetic [58] and real world [52] ob-ject datasets. Despite being pretrained on a synthetic ob-ject dataset [6], the performance of CrossPoint in out-of-distribution data certifies the importance of the joint learn-ing objective. In addition, the ablation studies demonstrate the component-wise contribution of both intra-modal and cross-modal objectives. We also adopt multiple widely used point cloud networks as our feature extractors, thus proving the generic nature of our approach.
The main contributions of our approach can be summa-rized as follows:
• We show that a simple 3D-2D correspondence of ob-jects in the feature space using self-supervised con-trastive learning facilitates an effective 3D point cloud understanding.
• We propose a novel end-to-end self-supervised learn-ing objective encapsulating intra-modal and cross-modal loss functions. It encourages the 2D image fea-ture to be embedded close to the corresponding 3D point cloud prototype, thus avoiding bias towards a particular augmentation.
• We extensively evaluate our proposed method across three downstream tasks namely: object classifica-tion, few-shot learning and part segmentation on a di-verse range of synthetic and real-world datasets, where
CrossPoint outperforms previous unsupervised learn-ing methods.
• Additionally, we perform few-shot image classifica-tion on CIFAR-FS dataset to demonstrate that fine-tuning the pretrained image backbone from CrossPoint outperforms the standard baseline. 2.