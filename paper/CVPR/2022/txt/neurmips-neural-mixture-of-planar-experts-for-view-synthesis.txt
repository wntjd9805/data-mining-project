Abstract
We present Neural Mixtures of Planar Experts (Neur-MiPs), a novel planar-based scene representation for mod-eling geometry and appearance. NeurMiPs leverages a collection of local planar experts in 3D space as the scene representation. Each planar expert consists of the param-eters of the local rectangular shape representing geometry and a neural radiance field modeling the color and opacity.
We render novel views by calculating ray-plane intersections and composite output colors and densities at intersected points to the image. NeurMiPs blends the efficiency of ex-plicit mesh rendering and flexibility of the neural radiance field. Experiments demonstrate superior performance and speed of our proposed method, compared to other 3D repre-sentations in novel view synthesis. 1.

Introduction
Metaverse is coming. Imagine one day in the future. Peo-ple can explore the world freely and immersively without leaving their room. When they move forward, details pop up; when they move sideways, the occluded regions re-appear.
Whenever people take action, the metaverse will respond with corresponding visual scenes that look natural, just as if people are visiting the place in person. While appeal-ing, bringing this vision to reality requires advancement in multiple domains, one of which is real-time, high-quality, memory-efficient novel view synthesis. Specifically, given a set of posed images of the world, an ideal NVS system has to be able to photo-realistically re-render the scene from novel viewpoints such that people cannot tell the difference.
The system also needs to be fast and lightweight such that it can be deployed ubiquitously.
Towards this grand goal, researchers have developed a plethora of methods to reproduce our visual world. One promising direction is to explicitly model the geometry of the scene (e.g. multi-planar imagery [12, 13, 73, 85], point clouds [1, 39, 53], meshes [31, 51, 52]) and conduct image-based rendering (IBR) [3, 9, 10, 31, 63]. By adapting visual features from other existing views, these approaches can render high-quality images efficiently. Unfortunately, they are often memory intensive and require good proxy geome-try. On the other hand, recent advances in neural radiance fields [44, 50, 78, 79, 82, 84] have allowed us to synthesize highly realistic images with low memory footprint. By en-coding color and density functions as neural networks, they can handle complicated geometry and scene effects that are difficult for conventional methods, e.g., thin structures, spec-ular reflections, and semi-transparent objects. The flexibility of volume rendering, however, is a double-bladed sword.
Without proper surface modeling, they cannot capture the scene geometry accurately, resulting in artifacts during view-extrapolation setup.
With these motivations in mind, we aim to find an al-ternative 3D scene representation that is compact, efficient, expressive, and generalizable. Specifically, we investigate planes, one of the simplest geometric primitives yet pow-erful for representing complicated scenes. Most surfaces
Figure 2. Planar Expert Parameterization Left: each plane consists of a 3D center, a plane normal, an up vector, and width and height; Right: the appearance is modeled through a neural radiance field function, which takes the 3D coordinate and ray direction as input and outputs color and opacity.
Ã— in man-made environments are locally planar. Taking the scene in Fig. 3 as an example, one could use 500 planes to 5 m2 scene with a maximum point-to-surface error fit a 5 of 8.66 mm. This result indicates that we might consider modeling our real-world surfaces through piece-wise local planar structures. The local planar world is not a surprisingly new concept to many researchers in vision [35, 41, 48]. It also profoundly impacts the graphics community and is the most common representation for rendering.
Unlike multi-planar imagery [12, 68, 73, 85], which repre-sents the scene through frontal-parallel planes, our proposed approach allows each plane to have an arbitrary position, direction, and size. Consequently, our representation is more flexible to approximate the scene geometry. Unlike volume rendering, NeurMiPs explicitly models surface using planar geometry. Hence fast rendering can be done through the effi-cient ray-plane intersection and eliminate the computations in empty spaces. Fig. 1 depicts our proposed 3D represen-tation and a comparison to other representations for neural rendering.
We validate our approach on several standard benchmarks for novel-view synthesis. The experiments demonstrate that our end-to-end method is significantly faster than the volume-based method with similar or better rendering quality and performs favorably against surface-based neural rendering methods with higher rendering quality and memory reduc-tion. Furthermore, we evaluate our approach on a new chal-lenging benchmark for view extrapolation, demonstrating su-perior performance compared to other state-of-the-art meth-ods. In particular, NeurMiPs outperforms NeRF with over 1dB PSNR gain at significant novel testing views. The ex-plicit planar surface representation of NeurMiPs could also readily be employed in modern graphics engines. 2.