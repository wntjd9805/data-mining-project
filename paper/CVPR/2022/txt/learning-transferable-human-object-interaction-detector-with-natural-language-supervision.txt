Abstract
It is difficult to construct a data collection including all possible combinations of human actions and interacting ob-jects due to the combinatorial nature of human-object inter-actions (HOI). In this work, we aim to develop a transfer-able HOI detector for unseen interactions. Existing HOI de-tectors often treat interactions as discrete labels and learn a classifier according to a predetermined category space.
This is inherently inapt for detecting unseen interactions which are out of the predefined categories. Conversely, we treat independent HOI labels as the natural language su-pervision of interactions and embed them into a joint visual-and-text space to capture their correlations. More specifi-cally, we propose a new HOI visual encoder to detect the interacting humans and objects, and map them to a joint feature space to perform interaction recognition. Our vi-sual encoder is instantiated as a Vision Transformer with new learnable HOI tokens and a sequence parser to gen-erate unique HOI predictions. It distills and leverages the transferable knowledge from the pretrained CLIP model to perform the zero-shot interaction detection. Experiments on two datasets, SWIG-HOI and HICO-DET, validate that our proposed method can achieve a notable mAP improvement on detecting both seen and unseen HOIs. Our code is avail-able at https://github.com/scwangdyd/promting_ hoi. 1.

Introduction
Human-object interaction (HOI) detection plays a vi-tal role in human-centric visual analysis tasks and pro-vides deeper understanding of human intentions and behav-iors [6, 10, 24, 31, 37, 41, 42]. It aims to localize the inter-acting humans and objects and then recognize their inter-actions. The interaction can be treated as a pair of human actions and objects, e.g., riding bicycle. Given its combi-natorial nature, it is impractical to create a data collection to include all possible HOIs, especially when the action and object category space becomes large (e.g., 400 actions
Figure 1. (a) Most existing HOI detectors classify the novel in-teractions in a predetermined manner. They are difficult to handle novel interactions that are out of the predefined list. (b) We aim to develop a transferable HOI detector via joint visual-and-text mod-eling which is more suitable to handle unseen interactions. Given an image, the visual encoder detects the interacting humans and objects and maps them into the joint space (e.g., the blue square).
We then conduct the nearest search among text features (triangles) for HOI recognition. and 1000 objects in SWIG-HOI [47]). This motivates us to study a transferable HOI detector which can be readily extended to numerous potential unseen interactions.
Recent works [2, 15, 17, 21] have used compositional learning to enhance the generalization ability of HOI de-tectors on unseen interactions. Their core idea is to de-compose the interaction into an action and an object, and conduct data augmentation to generate new combinations of actions and objects [15–17]. However, there is a basic assumption - the list of unseen interactions is available, so that specific samples of interactions can be generated from existing ones accordingly. However, it is still an open prob-lem how to automatically determine the validity of the gen-erated interactions without any given priors. In this sense, existing methods are only suitable for predetermined cases but not transferable to other unseen interactions.
In this paper, we aim to train a transferable HOI detec-tor without assuming any priors for the unseen interactions.
Figure 1 shows the main difference of our method with ex-isting solutions. Notably, most previous works use discrete labels to learn a classifier with a fixed size of weights. This predetermined setting limits their generalizability and effi-cacy since it cannot handle any novel HOIs out of the pre-defined list. Motivated by the recent success of CLIP [35], we transform independent one-hot HOI labels into natural language supervision by joint visual-and-text modeling. In this way, we can reformulate HOI detection as a visual-to-text matching problem and enable the recognition for the unseen interactions.
Specifically, we propose a new one-stage HOI detector comprising one visual encoder and one text encoder. For the visual encoder, we present (1) a novel HOI Vision Trans-former by designing additional [HOI] tokens and (2) a se-quence parser module to encourage the unique HOI detec-tions in the image. We take the output from the final layer with respect to [HOI] tokens as the representation of inter-actions. Then, we feed them into two heads for the bound-ing box regression and interaction recognition, respectively.
We use a regressor to predict the bounding boxes of inter-acting humans and objects, and estimate a confidence score for the prediction. Furthermore, we project the visual fea-ture to the joint visual-and-text feature space to search for the nearest interaction labels embedded by the text encoder.
The interaction category is typically defined as a pair of actions and objects. Instead of treating them as discrete la-bels, we aim to build natural language supervision using the action and object names and encode them to the joint visual-and-text space to explore their semantic correlations.
Recent works [35, 54] have shown that the context words surrounding the class name can significantly influence the recognition accuracy. In our case, it becomes more com-plex and may require different sentence formats from case to case. For example, given an action “riding” and an ob-ject “bicycle”, we can form a sentence like “a photo of a person riding bicycle”. However, given an action “fishing” and an object “fishing pole”, it does not make sense for the sentence, “a photo of a person fishing fishing pole”. To fa-cilitate the text generation, we propose an automated way to form the sentence using learnable tokens to replace the manually determined words. This brings in more general-ized results for both seen and unseen interactions.
Our contributions are summarized below. (1) We re-formulate the HOI detection as a visual-to-text matching problem and enable the detection of unseen interactions. (2) We propose a new one-stage HOI detector with the Vi-sion Transformer by designing additional HOI tokens and a
HOI sequence parser to jointly detect humans and objects and recognize their interaction. (3) Experiments on two datasets, HICO-DET and SWIG-HOI, validate that the pro-posed method can achieve state-of-the-art results on HOI detection, especially for unseen interactions. 2.