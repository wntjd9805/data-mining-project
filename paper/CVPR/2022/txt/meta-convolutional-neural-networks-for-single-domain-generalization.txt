Abstract
In single domain generalization, models trained with data from only one domain are required to perform well on many unseen domains. In this paper, we propose a new model, termed meta convolutional neural network, to solve the single domain generalization problem in image recog-nition. The key idea is to decompose the convolutional features of images into meta features. Acting as “visual words”, meta features are defined as universal and basic visual elements for image representations (like words for documents in language). Taking meta features as reference, we propose compositional operations to eliminate irrele-vant features of local convolutional features by an address-ing process and then to reformulate the convolutional fea-ture maps as a composition of related meta features. In this way, images are universally coded without biased informa-tion from the unseen domain, which can be processed by following modules trained in the source domain. The com-positional operations adopt a regression analysis technique to learn the meta features in an online batch learning man-ner. Extensive experiments on multiple benchmark datasets verify the superiority of the proposed model in improving single domain generalization ability. 1.

Introduction
Deep learning models are widely used for vision tasks in recent years [11, 13, 19, 36, 38], with the assumption that training data and testing data are from the same or simi-lar distributions. However, when applied to unseen or out-of-distribution (OOD) test domains, the performance of the model trained on the source domain can be significantly de-graded. In practice, domain shift problem is very common because of the change of illuminations, object appearance, or background [4, 30]. To solve this problem, many do-*Corresponding author. (a) Bag of Visual Words (BoVW) (b) Bag of Meta Features (BoMF)
Figure 1. Bag of visual words and the proposed bag of meta fea-tures. Both follow a three-step process to construct general rep-resentations for the input. (a) Local features are firstly extracted by SIFT [24], and then quantized into visual words. A frequency histogram of visual words is constituted as the final representa-tion. (b) Input features are firstly decomposed into local features through a sliding window. Then, the addressing operation selects meta features which are related to these local features. Finally, a linear regression model is adopted to compose the output repre-sentation based on the selected meta features. main adaptation [2, 4, 23, 42] and domain generalization
[3, 5, 14, 33, 41, 45, 46, 49] methods have been proposed.
These methods differ in strategies of transferring knowledge from multiple source domains to the target domain. How-ever, it is more plausible to consider a more realistic sce-nario where only one single domain is available for training, and the trained models are required to perform well on mul-tiple unseen domains, i.e., single domain generalization.
Single domain generalization is an important and chal-lenging problem. For the purpose of large-scale vision ap-plications in practical scenarios, we focus on improving the single domain generalization ability of CNNs. Recently, only a few works are proposed to solve this problem, in-cluding data augmentation [32, 43, 45] and regularization
[32, 39]. Data augmentation based approaches generate more diversified data from many “fictitious” domains in the input space for training. Regularization based meth-ods mainly focus on developing losses for the consistency among features from source data and augmented data.
In this paper, we solve the single domain generalization from a new perspective: developing a novel convolutional model, termed Meta CNN. We are motivated by stacked capsule autoencoder (SCAE) [17], in which images are seg-mented into constituent parts and reconstructed by a com-position of general part templates. Similarly, we believe convolutional features of input images can also be decom-posed into universal and elemental visual features. Then these elemental visual features are used as “templates” to compose general representations of images in the trained domain. However, due to the domain gap, features from a different domain are unable to be directly reconstructed by these trained templates. Following metadata normalization
[25], the domain shift effects of such local features can be
“regressed out” by a generalized linear model (GLM) [26] f = βM + r, where f is the local features, β is a learn-able set of linear parameters, M is the templates of trained domain (termed as “meta features”), and r is the irrelevant features effected by the domain gap. As βM corresponds to the component in f explained by the meta features, the features from shifted domain is reconstructed in the source domain by eliminating r. Consequently, for a CNN block, the effect of domain shift in input features is eliminated by feeding βM instead of f into the following convolutional operations. To achieve this goal, we follow the process of bag of visual words (BoVW) [6, 24, 37], where each image is represented by the histogram of “visual words” through 3 steps (shown in Fig. 1a). We propose corresponding 3 steps of compositional operations in CNN blocks: 1) decompose the convolutional features of images into local meta features as “templates”, 2) take meta features as reference, eliminate irrelevant features of local convolutional features by an ad-dressing process, 3) reformulate the feature maps as a com-position of related meta features. In this way, images are universally coded without biased information from the un-seen domain, which can be processed by following modules trained in the source domain (shown in Fig. 1b).
The challenge of applying compositional operations of meta features within a CNN building block lies in four folds: 1) local feature extraction. For BoVW, local features are firstly located by key point detection and then extracted by handcrafted operations of local image patches. Both op-erations are non-differentiable, making it infeasible for end-to-end batch learning in CNN blocks. As representation and location are encoded simultaneously in a grid manner for convolutional feature maps, we propose to decompose fea-ture maps into local patches with a sliding window, which is compatible with following convolution/pooling operations in CNN blocks. 2) local feature addressing.
In BoVW, each local feature is mapped to a certain visual word, where a dense and large enough visual word set is required for small mapping error. However, in the deep learning sce-nario, the storage of batch training and inference are limited by GPUs. Therefore, we propose to map local features to a combination of meta features, where the expressive power of the meta features set is enlarged.
In this way, a mod-erate meta features set is allowed with feasible storage oc-cupancy. 3) meta feature composition.
In BoVW, image is represented by a frequency histogram of visual words, which is non-differentiable and lack of the spatial and con-tent information of image patches. To keep the content and spatial information of convolutional features, we propose to represent local patch features by a linear interpolation of meta features. 4) meta feature learning. In BoVW, cluster-ing is performed over all the local features in the dataset, the center of each cluster is used as visual words. However, deep learning models are trained in an online batch learning manner, where only a very small portion of the dataset are feasible. Consequently, we adopt a regression analysis with maximum likelihood estimation to update the meta features during training.
Extensive experiments on multiple benchmark datasets indicate the superiority of the proposed model in tackling single domain generalization problems. More importantly, these results reveals the potential of convolutional meta fea-tures for general image representations. 2.