Abstract
Few-shot object detection (FSOD), with the aim to detect novel objects using very few training examples, has recently attracted great research interest in the community. Metric-learning based methods have been demonstrated to be effec-tive for this task using a two-branch based siamese network, and calculate the similarity between image regions and few-shot examples for detection. However, in previous works, the interaction between the two branches is only restricted in the detection head, while leaving the remaining hundreds of layers for separate feature extraction. Inspired by the recent work on vision transformers and vision-language transform-ers, we propose a novel Fully Cross-Transformer based model (FCT) for FSOD by incorporating cross-transformer into both the feature backbone and detection head. The asymmetric-batched cross-attention is proposed to aggre-gate the key information from the two branches with different batch sizes. Our model can improve the few-shot similarity learning between the two branches by introducing the multi-level interactions. Comprehensive experiments on both PAS-CAL VOC and MSCOCO FSOD benchmarks demonstrate the effectiveness of our model. 1.

Introduction
Few-shot object detection (FSOD) aims to detect objects from the query image using a few training examples. This is motivated by human visual system which can quickly learn novel concepts from very few instructions. The key point is how to quickly learn object detection models with strong generalization ability using a small number of training data, such that the learned model can detect objects in unseen images. This is very challenging, especially for the current state-of-the-art deep-learning based methods [1, 28, 32, 33], which usually need thousands of training examples and are prone to overﬁtting under this data-scarce scenario.
Current methods for this task mainly follow a two-stage learning paradigm [45] to transfer the knowledge learned from the data-abundant base classes to assist in object detection for few-shot novel classes. The detailed
Figure 1. Comparison of the single-branch, two-branch based
FSOD models and our proposed model. model architectures vary in different works, which can be roughly divided into two categories, single-branch based methods [36, 45, 47, 51, 52] and two-branch based meth-ods [8, 12, 13, 20, 23, 49]. (1) Single-branch based meth-ods employ a typical object detection model, e.g., Faster
R-CNN [33], and build a multi-class classiﬁer for detection.
It is prone to overﬁtting to the small training data, espe-cially when we only have 1-shot training data per novel class. (2) Two-branch based methods apply the metric-learning idea [34, 37, 41] to FSOD and build a siamese network to process the query image and the few-shot support image in parallel. After extracting deep visual features from the two branches, previous works propose various methods (e.g., fea-ture fusion [8,48,49], feature alignment [13], GCN [12], and non-local attention/transformer [2, 3, 6, 20, 44]) to calculate the similarity of the two branches. The two-branch based methods do not learn the multi-class classiﬁer over novel classes, and usually have stronger generalization ability by learning to compare the query regions with few-shot classes.
Previous two-branch based methods have explored var-ious interactions (e.g., alignment) between the query and
support branch to improve the similarity learning. But the interactions are restricted in the detection head with high-level features, and leave the remaining hundreds of layers for separate feature extraction. In fact, the query and sup-port images may have large visual differences and domain gap in terms of object pose, scale, illumination, occlusion, background and etc. Simply aligning the two branches at the high-level feature space might not be optimal. If we could align the extracted features in all network layers, the network could have more capacity focusing on the common features in each layer, and improve the ﬁnal similarity learning.
In this work, we propose a novel Fully Cross-Transformer based model (FCT) for FSOD, which is a pure cross-transformer based detection model without deep convolu-tional networks. The ability to model long-range dependen-cies in transformer [40] can not only capture the abundant context in one branch, and also related context in the other branch, thus encouraging mutual alignment between the two branches. As shown in Figure 1, Our model is based on the two-stage detection model Faster R-CNN. Instead of extracting deep visual features separately for the query and support inputs, we use the multi-layer deep cross-transformer to jointly extract features for the two branches. Inside the cross-transformer layer, we propose the asymmetric-batched cross-attention to aggregate the key information from the two branches with different batch sizes, and update the features of either branch using self-attention with the aggregated key information. Thus, we can align the features from the two branches in each of the cross-transformer layer. Then af-ter the joint feature extraction and proposal generation for the query image, we propose a cross-transformer based RoI feature extractor in the detection head to jointly extract RoI features for the query proposals and support images. In-corporating our cross-transformer in both feature backbone and ROI feature extractor could largely promote the multi-level interactions (alignment) between the query and support inputs, thus further improving the ﬁnal FSOD performance.
We’d like to emphasize the difference between a closely related work ViLT [24] and ours, both using transformers for joint feature extraction of two branches. First, ViLT has language and the original image as input, and the highly abstracted language tokens are interacting with the visual tokens at each layer. However, visual tokens represent low-level concepts at the beginning, and evolve into high-level concepts in deep layers. Different from ViLT, we take input of two visual images, and explore multi-level interactions between the two visual branches, gradually from low-level to high-level features. Second, we focus on FSOD, a dense prediction task, instead of the classiﬁcation and retrieval task in ViLT, and incorporate cross-transformer into both the feature backbone and detection head. Third, ViLT extracts visual tokens following ViT [7], and uses the same number of tokens throughout the model. We employ the pyramid struc-ture [43] to extract multi-scale visual tokens, and propose the asymmetric-batched cross-attention across the branches with different batch sizes to reduce computational complexity.
Our contributions can be summarized as: (1) To the best of our knowledge, we are the ﬁrst to explore and pro-pose the vision transformer based few-shot object detection model. (2) A novel fully cross-transformer is proposed for both the feature backbone and detection head, to encourage multi-level interactions between the query and support. We also propose the asymmetric-batched cross-attention across the branches. (3) We comprehensively evaluate the pro-posed model on the two widely used FSOD benchmarks and achieve state-of-the-art performance. 2.