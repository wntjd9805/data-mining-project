Abstract
Data imbalance exists ubiquitously in real-world visual regressions, e.g., age estimation and pose estimation, hurt-ing the model’s generalizability and fairness. Thus, im-balanced regression gains increasing research attention recently. Compared to imbalanced classification, imbal-anced regression focuses on continuous labels, which can be boundless and high-dimensional and hence more chal-lenging.
In this work, we identify that the widely used
Mean Square Error (MSE) loss function can be ineffec-tive in imbalanced regression. We revisit MSE from a sta-tistical view and propose a novel loss function, Balanced
MSE, to accommodate the imbalanced training label dis-tribution. We further design multiple implementations of
Balanced MSE to tackle different real-world scenarios, par-ticularly including the one that requires no prior knowl-edge about the training label distribution. Moreover, to the best of our knowledge, Balanced MSE is the first gen-eral solution to high-dimensional imbalanced regression in modern context. Extensive experiments on both synthetic and three real-world benchmarks demonstrate the effective-ness of Balanced MSE. Code and models are available at github.com/jiawei-ren/BalancedMSE. 1.

Introduction
Visual regression, where models learn to predict continu-ous labels, is one of the most fundamental tasks in machine learning. However, in real-world applications, data imbal-ance is widely encountered, hurting the model’s generaliz-ability and fairness. For example, age estimation predicts people’s age from their visual appearance, where age is a continuous label. In practice, most of the training images are from adults, while very few images are from children and senior adults. As a result, models trained from such an imbalanced dataset can have inferior performance on under-represented groups [38]. Therefore, imbalanced regression (cid:12) Corresponding author.
Figure 1. Comparison between MSE and Balanced MSE. MSE is equivalent to NLL on a prediction distribution, where the re-gressor θ’s prediction is the distribution mean. MSE lets the re-gressor model ptrain(y|x), which is not suitable to infer on the test set due to a shift between the training label distribution ptrain(y) and the balanced test label distribution pbal(y).
In comparison,
Balanced MSE leverages ptrain(y) to make a statistical conversion from pbal(y|x) to ptrain(y|x), thus allowing the regressor to model the desired pbal(y|x) by still minimizing NLL of ptrain(y|x). gains increasing research attention. A fresh imbalanced re-gression benchmark [38] in the modern deep learning con-text has been curated recently as well. Compared with im-balanced and long-tailed classification [4,12,24] that studies categorical labels, imbalanced regression focuses on contin-uous labels, which can be boundless, high-dimensional, and hence more challenging.
Unlike imbalanced classification that has been widely discussed [15, 20, 35, 39], imbalanced regression is under-explored.
Previous works [1, 33] focus on synthesiz-ing samples for rare labels, which have limited feasibil-ity in modern deep learning where inputs are always high-dimensional. Recent research [31, 38] focuses on loss reweighting. Reweighting assigns larger loss weights to rare samples and smaller loss weights to frequent samples.
[31, 38] estimates the training label distributions using ker-nel density estimation (KDE) and reweight losses accord-ingly. However, prior works [5, 36] show that reweighting has limited effectiveness on imbalanced classification. In a following case study, we validate this finding in imbalanced
Figure 2. Comparison of Balanced MSE and existing methods on a 1-D imbalanced linear regression synthetic benchmark. Column 1 and 3 are visualizations of regression results: points represent training data, x is input and y is label; (x, y) is noisily generated by an oracle linear relation (in blue) and is artificially label-imbalanced; other lines represent different regressors, the closer to the oracle the better. Column 2 and 4 are visualizations of label distributions: blue shaded histogram represents the training label distribution ptrain(y), it gets more skewed from top to bottom; purple histogram represents the test label distribution, which is balanced; other histograms are the marginal label distributions predicted by different regressors on the test set, the closer to the test distribution the better. Although reweighting (in green) is closer to the oracle (in blue) compared with least square (in yellow), it suffers a larger error when ptrain(y) gets more skewed. Our method (in red), Balanced MSE, makes the estimation closest to the oracle and has a uniform marginal label distribution on the test set. regression as well. To sum up, imbalanced regression is still in an early stage and lacks an effective approach.
To mitigate the gap, we present a statistically princi-pled loss function, Balanced MSE, for imbalanced regres-sion. We revisit Mean Square Error (MSE), the standard loss function in regression, from a statistical view. We identify that MSE carries the label imbalance into predic-tions, which leads to inferior performance on rare labels.
We propose Balanced MSE to restore a balanced prediction by leveraging the training label distribution prior to make a statistical conversion. Moreover, we provide various imple-mentation options for Balanced MSE, including the one that estimates the training label distribution online and requires no additional prior knowledge, making Balanced MSE ap-plicable to different real-world scenarios.
Balanced MSE shows clear advantages over existing methods both theoretically and practically. As a motivat-ing example, we compare Balanced MSE with reweighting using an 1-D linear regression synthetic benchmark shown in Fig. 2. Regressors trained with Balanced MSE show a consistent performance that is invariant to the skewness of the training label distribution. On the contrary, reweighting suffers from a significantly larger prediction error when the training label distribution gets more skewed.
We further demonstrate Balanced MSE’s empirical suc-cess on existing real-world benchmarks [38], including age estimation and depth estimation. Note that exist-ing imbalanced regression benchmarks only consider uni-dimensional label space, e.g., age and depth. However, la-bels sometimes have more than one dimension in real-world applications. To close the gap, we propose a new multi-dimensional imbalanced regression benchmark on Human
Mesh Recovery (HMR) [18], which is an important task that estimates 3D human meshes from monocular images.
We extend the standard metrics of HMR (e.g., mean per joint position error (MPJPE)) to balanced metrics so that we evaluate the regression performance on human meshes with different rarity fairly. We call the new imbalanced re-gression benchmark Imbalanced HMR (IHMR). We show that Balanced MSE delivers strong empirical results on both uni- and multi-dimensional benchmarks. To the best of our knowledge, Balanced MSE is the first general solution to high-dimensional imbalanced regression in modern context.
In summary, our contributions are three-fold: 1) We identify the ineffectiveness of MSE in imbalanced regres-sion and propose a statistically principled loss function,
Balanced MSE, that leverages the training label distribu-tion prior to restore a balanced prediction. 2) We devise various implementation options of Balanced MSE to tackle different real-world scenarios, including the one that esti-mates the training label distribution online and requires no prior knowledge beforehand. 3) We propose a new multi-dimensional benchmark IHMR, and show that Balanced
MSE achieves state-of-the-art performance on both uni- and multi-dimensional real-world benchmarks. 2.