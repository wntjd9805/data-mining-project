Abstract
Humans can perceive multiple expressions, each one with varying intensity, in the picture of a face. We propose a methodology for collecting and modeling multidimensional modulated expression annotations from human annotators.
Our data reveals that the perception of some expressions can be quite different across observers; thus, our model is designed to represent ambiguity alongside intensity. An em-pirical exploration of how many dimensions are necessary to capture the perception of facial expression suggests six principal expression dimensions are sufficient. Using our method, we collected multidimensional modulated expres-sion annotations for 1,000 images culled from the popular
ExpW in-the-wild dataset. As a proof of principle of our im-proved measurement technique, we used these annotations to benchmark four public domain algorithms for automated facial expression prediction. 1.

Introduction
Humans communicate using their body. Automating the perception of bodily and vocal expressions is neces-sary towards building machines that can interact grace-fully with humans [8, 59]. Facial expression is an im-portant channel of the communication [9, 16], and percep-tion of facial expressions is important for social interac-tion [16, 29]. Computer vision researchers have long been interested in measuring human facial expressions from im-ages and video [4, 12, 41, 47, 56] with the aim of replicating it in machines [4, 41].
Automated facial analysis is rooted in machine learning, thus model training and benchmarking rely on large well-annotated datasets. This raises three questions which are not well addressed in the facial expression perception liter-ature: First, how should images be annotated, i.e. what is a good representation of human perception of facial expres-sion? Second, can we measure the perception reproducibly
∗Work done during an Amazon internship.
†Work done when at Amazon.
Figure 1. Face expression perception is multidimensional, nu-anced and subjective. Perceived expression is measured by ask-ing 9 crowdsourced annotators to report perceived intensity for each of six dimensions (15 and 21 dimensions in other experi-ments as described in Sec. 4.1). Annotation histograms and Beta distribution fits (Sec. 3.2) are shown. One dimension, ‘happy’, captures the expression of the first face. The second requires two:
‘happy’ and ‘surprised’. The third requires more dimensions and is more ambiguous.
Figure 2. Expression predictions compared to ground truth.
The outputs of 4 expression prediction algorithms (colored sym-bols) are compared to the ground truth obtained from our anno-tations for one image. Gray bands: confidence intervals of our ground truth, ×: µ (See Sec. 3.2 and 4.3).
Expression 1
Angry
Angry
Angry
Angry
Angry
Happy
Happy
Happy
Happy
Surprised
Surprised
Surprised
Fearful
Fearful
Disgusted
Expression 2
Happy
Surprised
Fearful
Disgusted
Sad
Surprised
Fearful
Disgusted
Sad
Fearful
Disgusted
Sad
Disgusted
Sad
Sad
Compound Hypothesis
Cruel
Outraged (P)
Dreadful*
Contemptuous (P), Outraged (M)
Betrayed
Amazed
Desperate
Morbid (P)
Hopeful
Spooked
Disbelieving (P)
Disappointed
Horrified
Devastated
Remorseful
Table 1. The compound expressions hypothesis. All pairwise combinations of 6 primary expressions are considered. The names of the compound expressions were obtained from Scott McCloud’s
Making Comics [42] and Robert Plutchik’s theory of expression
[49]. McCloud nor Plutchik use multiple words to describe the compound of angry and fearful. We use ‘Dreadful’ here following prior work [33, 38]. An analysis of whether complex expressions may be suitably modeled as superposition of primary expressions can be found in Sec. 4.4. See also Sec. 2.1 and Fig. 4. some expressions involve simultaneously more than one of the six primary dimensions [42]. Recent research has ex-plored that intuition and characterized the corresponding fa-cial actions [13].
The perception of facial expressions is also well stud-ied, alongside the perception of other socially relevant at-tributes such as gender, age, and trustworthiness. Human annotators can make fast judgments [54], which may be used in important decisions [50]. Often, there is good agreement amongst annotators on their perception, although some annotators are believed to be more perceptive and re-liable [25]. Whether and when the annotators’ judgments correspond to meaningful and useful information is still de-bated [30]. Notably, Barrett et al. [2] questioned whether a person’s internal emotional state may be inferred from fa-cial expression, an assumption central to Ekman’s earlier views on emotion and facial expression. Our method and data will help provide empirical evidence for the ongoing
Barrett-Ekman debate. 2.2. Automating prediction of expression perception
Computer vision algorithms may be used to measure fa-cial expressions and predict the voluntary report that a reg-ular person would make of their perception of the expres-sion upon looking at a face image. We use the shorthand
“prediction of expression” and “expression classification”, and the meaning should be clear by context.
Detecting and analyzing human faces was recognized as an important task from the inception of computer vi-sion [28]. Since the early 1990s, computer vision re-searchers have been interested in automating the perception of facial expressions [4, 12, 41, 47, 56] and deep learning is
Figure 3. Evidence for intensity and compound expressions found in the arts. (Top) Expressions vary in intensity. (Bottom)
Any pair of primary expressions may be combined to obtain a valid composite expression (Table 1, Sec. 2.1). (Adapted with permis-sion from Scott McCloud’s Making Comics [42], pages 84-85). and, yet, efficiently? Third, what is the right metric to com-pare an algorithm’s output with human ground truth?
To answer the first question we rely on insight from ex-perimental psychology [15, 25, 31, 45], which suggests that facial expressions are multidimensional and are perceived with different degrees of intensity (see Fig. 1). This con-strasts with the common practice in computer vision where expressions are often annotated as a binary and/or a one-hot code (Sec. 2). To answer the second question, we focus on crowdsourcing – we annotated 1,000 ethnically diverse in-the-wild faces from a public dataset [60], where each was rated by 9 annotators for each of the 6 primary Ekman expressions [17], the corresponding 15 compound expres-sions [42, 49] and the combined set of 21 expressions. On this dataset we explore the consistency of annotations, as well as the number of dimensions that need to be annotated.
Lastly, we propose a metric that compares human annota-tions, including their ambiguity, with algorithm prediction, and use our annotated dataset to benchmark four recent al-gorithms (Sec. 4.3).
Our main contributions are: 1. An efficient method for collecting and modeling reproducible, multi-dimensional, modulated facial expression annotations crowdsourced from humans. The novel modeling technique transforms annotations into probability distributions to express mea-sures of expression intensity and ambiguity. 2. A bench-mark for facial expression prediction algorithms consisting of annotations on 1,000 face images from a public in-the-wild dataset, and a metric for algorithmic accuracy of ex-pression prediction. 2.