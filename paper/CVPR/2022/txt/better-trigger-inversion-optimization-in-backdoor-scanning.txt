Abstract
Backdoor attacks aim to cause misclassiﬁcation of a sub-ject model by stamping a trigger to inputs. Backdoors could be injected through malicious training and naturally exist.
Deriving backdoor trigger for a subject model is critical to both attack and defense. A popular trigger inversion method is by optimization. Existing methods are based on ﬁnding a smallest trigger that can uniformly ﬂip a set of input sam-ples by minimizing a mask. The mask deﬁnes the set of pixels that ought to be perturbed. We develop a new op-timization method that directly minimizes individual pixel changes, without using a mask. Our experiments show that compared to existing methods, the new one can generate triggers that require a smaller number of input pixels to be perturbed, have a higher attack success rate, and are more robust. They are hence more desirable when used in real-world attacks and more effective when used in defense. Our method is also more cost-effective. 1.

Introduction
Backdoor attacks aim to induce model misclassiﬁcation of arbitrary input samples to a target label by stamping a special input pattern called trigger. Backdoors could be injected by various methods, such as data poisoning [17, 35, 40] and neuron hijacking [39], and also naturally exist in normally trained models, called natural backdoors [41].
The latter is caused by distribution bias of low level features and can be exploited just like injected backdoors. For ex-ample, if a person always wears a unique pair of glasses in a clean face recognition dataset, the glasses may become a trigger to induce misclassiﬁcation to the person.
Due to the prominent threat of backdoors, researchers have proposed a large body of defense solutions (see Sec-tion 2). Among them, backdoor scanning [21, 22, 68, 76] is an important type of defense. Many scanners [38,41,59,65, 75] rely on trigger inversion, which leverages optimization (a) NC (b) Ours
Figure 1. Loss landscapes of NC and our method. The x-axis and y-axis show the coefﬁcients on two random directions. The z-axis denotes the loss value. to derive a small input pattern that can ﬂip clean samples (of a victim class) to the target label. A model is consid-ered having backdoor if an exceptionally small trigger can be found.
Most existing trigger inversion methods (e.g., ABS [38],
K-arm [59], and Tabor [19]) are built on Neural Cleanse (NC) [65], which decouples a trigger into a perturbation vector and a mask. The perturbation vector denotes the perturbations applied to an input and the mask determines which part of the perturbation vector should be applied. NC minimizes the mask and the perturbation vector together to produce a small trigger (details in Section 3.1). Due to the multiplication correlation between the mask and the pertur-bation vector during optimization, NC can fall into local optima and fail to reach the optimal trigger, i.e., the small-est trigger with a high attack success rate. Figure 1a shows the loss landscape of NC using the contour plot with two random directions [16, 24, 31] with (x = 0, y = 0) the opti-mum. Observe that there are multiple dips (local optima) on the loss surface, which prevent NC from reaching the opti-mum. In addition, triggers by NC are often not robust and may become ineffective when undertaking transformations (see Section 5.3).
Figure 2 shows the results of various techniques for gen-erating a natural backdoor pattern for a normally trained
2k, 0%
CW (1080m) 50k, 2% 1950, 0% 27k, 28% 822, 22%
UAP (23m)
NC (9m) 822, 70%
Ours (4m)
Input
CW
UAP
NC
Ours
Target
Figure 2. Comparison of generated backdoors. In the ﬁrst row, the texts below backdoor images denote the number of perturbed pixels and the ASR on all the samples of loggerhead turtle from the validation set. The value shown together with the method name denotes the trigger generation time cost in minutes. The bottom two rows show example images stamped with backdoors by different methods, where the ﬁrst column gives the victim class images and the last column the target class images. model on ImageNet downloaded from [25]. Stamping each of these backdoors on sea turtle images can ﬂip them to the kangaroo class. The ﬁrst row shows the backdoor pat-terns by various inversion techniques. From left to right, the second and third rows show samples from the victim class (column 1), samples stamped with the backdoor patterns (columns 2-5), and the target class samples (last column).
The fourth and ﬁfth images in the ﬁrst row denote the trig-ger generated by NC and its reduced version, respectively.
Observe that the NC trigger requires perturbing 27k pixels and has only 28% ASR. When we reduce the NC trigger by removing the smallest perturbations to size 822 (which is the same as our trigger), the ASR degrades to 22%. This is because of the large number of local minima, as those on the loss surface of NC in Figure 1a. Our results in Section 5 show that on average, when NC triggers are reduced to the same size of ours, their ASRs on average degrade by 26%.
Problem Statement. In the context of backdoor attack and defense, a good optimization method (for trigger genera-tion) is critical. In this paper, we say a method is good if it produces triggers that are (1) small (i.e., having a small number of perturbed pixels), (2) having a high attack suc-cess rate (ASR) (the percentage of unseen clean samples that can be ﬂipped by the trigger), (3) robust (against input transformations), and (4) has low computation overhead. A good trigger generation method serves both attack and de-fense. If it is used in attack, e.g., generating natural triggers for normally trained models to induce intended misclassiﬁ-cation, a small and robust trigger makes the attack easy to launch and effective in the physical world. If it is used in defense, smaller triggers can help scanners more effectively determine if a model is trojaned, as an exceptionally small trigger is a good indicator of injected backdoor [38, 59, 65], and have better effectiveness in model hardening. ⇤
We propose a novel optimization method.
Instead of optimizing the product of the perturbation vector and the mask, our method only optimizes a perturbation vector.
Speciﬁcally, we leverage the long-tail effects of tanh func-tion to represent the binary nature of perturbations, with one end modeling the maximum perturbation and the other end no perturbation. We introduce two tanh functions for each pixel, one denoting positive perturbation and the other neg-ative. Our optimization method has a much smoother loss surface than NC as shown in Figure 1b. Observe that the loss values all descend along the valley towards the optimal point at the bottom. On the ImageNet dataset, our gener-ated triggers are two orders of magnitude smaller than those of NC, 2.73 times more robust, and have 20% higher ASR on average. Our method is 2.15 times faster than NC. The last image in the ﬁrst row of Figure 2 shows our trigger. It has the smallest number of perturbed pixels (822) with the highest ASR (70%) on the unseen validation set. We also compare with UAP [58] and CW [4] (another two trigger generation methods adapted from adversarial attacks). Ours is one or more orders of magnitude faster. The implementa-tion of our method is publicly available [1].
2.