Abstract
Human action recognition has recently become one of the popular research topics in the computer vision commu-nity. Various 3D-CNN based methods have been presented to tackle both the spatial and temporal dimensions in the task of video action recognition with competitive results.
However, these methods have suffered some fundamental limitations such as lack of robustness and generalization, e.g., how does the temporal ordering of video frames af-fect the recognition results? This work presents a novel end-to-end Transformer-based Directed Attention (Direc-Former) framework1 for robust action recognition. The method takes a simple but novel perspective of Transformer-based approach to understand the right order of sequence actions. Therefore, the contributions of this work are three-fold. Firstly, we introduce the problem of ordered tempo-ral learning issues to the action recognition problem. Sec-ondly, a new Directed Attention mechanism is introduced to understand and provide attentions to human actions in the right order. Thirdly, we introduce the conditional de-pendency in action sequence modeling that includes orders and classes. The proposed approach consistently achieves the state-of-the-art (SOTA) results compared with the recent action recognition methods [4, 18, 72, 74], on three stan-dard large-scale benchmarks, i.e. Jester, Kinetics-400 and
Something-Something-V2. 1.

Introduction
Video understanding has recently become one of the popular research topics in the computer vision community.
Video data has become ubiquitous and occurs in numerous daily activities and applications, e.g., movies and camera 1The implementation of DirecFormer is available at https : / / github.com/uark-cviu/DirecFormer
Figure 1. Result Preview. Top 1 accuracy against GLOPS Ã—
Views of our DirecFormer compared to other methods. Direc-Former achieves SOTA performance while maintaining the low computational cost. surveillance [15, 38, 47, 61].
In the field of video under-standing [60], action recognition has become a fundamen-tal problem. In action recognition, there is a need to pay more attention to the temporal structures of the video se-quences. Indeed, emphasis on temporal modeling is a com-mon strategy among most methods. It can be considered as the main difference between video and images. These works include long-short term dependencies [14, 16], tem-poral structure, low-level motion, and action modeling as a sequence of events or states.
The current methods in video action recognition utilize 3D or pseudo 3D convolution to extract the spatio-temporal features [7, 50, 57, 70]. However, these 3D CNN-based methods suffer from intensive computation with many pa-rameters to be learned. Others attempt to adopt two-stream structures [19, 20, 22, 53] for accurate action recognition, since information from one branch could be fused to the
other one. Some methods in this category require comput-ing the optical flow first, which could be time-consuming and requires a large amount of storage. Others apply 3D convolution to avoid computing the optical flow. Nonethe-less, this approach also requires a large amount of compu-tational resources to implement.
Although prior methods [1, 43, 48, 63, 66] have achieved remarkable performance, they have several limitations re-lated to the robustness of the models.
In this paper, we therefore address two fundamental questions for current ac-tion recognition models. In the first question, given a set of video frames shuffled in a random order and different from the original one, will it be classified as the same label as the original recognition result? If it is the case, these models have been clearly overfitted or biased to other factors (e.g. scene background), rather than learned semantic informa-tion of the actions. In the second question, we want to un-derstand whether these action recognition models are able to correct the incorrectly-ordered frames to the right ones and provide an accurate prediction? Finally, we introduce a new theory to improve the robustness and generalization of the action recognition models. 1.1. Contributions of this Work
In this work, we present a new end-to-end Transformer-based Directed Attention (DirecFormer) approach to ro-bust action recognition. Our method takes a simple but novel perspective of Transformer-based approach to learn the right order of a sequence of actions. The contributions of this work are three-fold. First, we introduce the problem of ordered temporal learning in action recognition. Second, a new Directed Attention mechanism is introduced to pro-vide human action attentions in the right order. Third, we introduce the conditional dependency in action sequence modeling that includes orders and classes. The proposed approach consistently achieves the State-of-the-Art results compared to the recent methods [2,44,72] on three standard action recognition benchmarks, i.e. Jester [45], Something-in-Something V2 [23] and Kinetics-400 [31], as in Fig. 1. 2.