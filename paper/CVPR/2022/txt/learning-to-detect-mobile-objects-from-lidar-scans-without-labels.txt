Abstract
Current 3D object detectors for autonomous driving are almost entirely trained on human-annotated data. Although of high quality, the generation of such data is laborious and costly, restricting them to a few specific locations and object types. This paper proposes an alternative approach entirely based on unlabeled data, which can be collected cheaply and in abundance almost everywhere on earth. Our ap-proach leverages several simple common sense heuristics to create an initial set of approximate seed labels. For ex-ample, relevant traffic participants are generally not per-sistent across multiple traversals of the same route, do not fly, and are never under ground. We demonstrate that these seed labels are highly effective to bootstrap a surpris-ingly accurate detector through repeated self-training with-out a single human annotated label. Code is available at https:// github.com/ YurongYou/ MODEST. 1.

Introduction
Autonomous driving promises to revolutionize how we transport goods, travel, and interact with our environment.
To safely plan a route, a self-driving vehicle must first perceive and localize mobile traffic participants such as other vehicles and pedestrians in 3D. Current state-of-the-art 3D object detectors are all based on deep neural net-works [45, 48, 49, 61] and can yield up to 80 average preci-sion on benchmark datasets [21, 22].
However, as with all deep learning approaches, these techniques have an insatiable need for labeled-data. Specif-ically, to train a 3D object detector that takes LiDAR scans as input, one typically needs to first come up with a list of objects of interest and annotate each of them with tight bounding boxes in the 3D point cloud space. Such a data annotation process is laborious and costly, but worst of all, the resulting detectors only achieve high accuracy when the training and test data distributions match [58].
In other words, their accuracy deteriorates over time and space, as
*Denotes equal contribution.
†All correspondence could be directed to yy785@cornell.edu looks and shapes of cars, vegetation, and background ob-jects change. To guarantee good performance, one has to collect labeled training data for specific geo-fenced areas and re-label data constantly, greatly limiting the applicabil-ity and development of self-driving vehicles.
These problems motivate the question: Can we learn a 3D object detector from unlabeled LiDAR data? Here, we focus on “mobile” objects, i.e., objects that might move, which cover a wide range of traffic participants. At first glance, this seems insurmountably challenging. After all, how could a detector know just from the LiDAR point cloud that a pedestrian is a traffic participant and a tree is not? We tackle this problem with the help of two important insights: 1) we can use simple heuristics that, even without labeling, can occasionally distinguish traffic participants from back-ground objects more or less reliably; 2) if data is noisy but diverse, neural networks excel at identifying the common patterns, allowing us to repeatedly self-label the remaining objects, starting from a small set of seed labels.
Weak labels through heuristics. We build upon a simple yet highly generalizable concept to discover mobile objects
— mobile objects are unlikely to stay persistent at the same location over time. While this requires unlabeled data at multiple timestamps for the same locations, collecting them is arguably cheaper than annotating them. After all, many of us drive through the same routes every day (e.g., to and from work or school). Even when going to new places, the new routes for us are likely frequent for the local residents.
Concretely, whenever we discover multiple traversals of one route, we calculate a simple ephemerality statistic [3] for each LiDAR point, which characterizes the change of its local neighborhood across traversals. We cluster Li-DAR points according to their coordinates and ephemerality statistics. Resulting clusters with high ephemerality statis-tics, and located on the ground, are considered as mobile objects and are further fitted with upright bounding boxes.
Self-training (ST). While this initial seed set of mobile ob-jects is not exhaustive (e.g., parked cars may be missed) and somewhat noisy in shape, we demonstrate that an ob-ject detector trained upon them can already learn the under-lying object patterns and is able to output more and higher-Figure 1. Visualizations of MODEST outputs. We show LiDAR scans from two scenes in the Lyft dataset in two rows. From zero labels, our method is able to bootstrap a detector that achieves results close to the ground truth. The key insight is to utilize noisy “seed" labels produced from an ephemerality score and filtered with common-sense properties, and self-train upon them to obtain high quality results. quality bounding boxes than the seed set. This intriguing observation further opens up the possibility of using the de-tected object boxes as “better” pseudo-ground truths to train a new object detector. We show that such a self-training cy-cle [32, 59] enables the detector to improve itself over time; notably, it can even benefit from additional, unlabeled data that do not have multiple past traversals associated to them.
We validate our approach, MODEST (Mobile Object
Detection with Ephemerality and Self-Training) on the
Lyft Level 5 Perception Dataset [29] and the nuScenes
Dataset [6] with various types of detectors [31, 49, 60, 70].
We demonstrate that MODEST yields remarkably accu-rate mobile object detectors, comparable to their supervised counterparts. Concretely, our contributions are three-fold: 1. We propose a simple, yet effective approach to iden-tifying “seed" mobile objects from multiple traversals of LiDAR scans using zero labels. 2. We show that using these seed objects, we can boot-strap accurate mobile object detectors via self-training. 3. We evaluate our method exhaustively under various setting and demonstrate consistent performance across multiple real-world datasets. 2.