Abstract
We present a novel method to learn Personalized Implicit
Neural Avatars (PINA) from a short RGB-D sequence. This allows non-expert users to create a detailed and personal-ized virtual copy of themselves, which can be animated with realistic clothing deformations. PINA does not require com-plete scans, nor does it require a prior learned from large datasets of clothed humans. Learning a complete avatar in this setting is challenging, since only few depth obser-vations are available, which are noisy and incomplete (i.e. only partial visibility of the body per frame). We propose a method to learn the shape and non-rigid deformations via a pose-conditioned implicit surface and a deformation field, defined in canonical space. This allows us to fuse all partial observations into a single consistent canonical representa-tion. Fusion is formulated as a global optimization problem over the pose, shape and skinning parameters. The method can learn neural avatars from real noisy RGB-D sequences for a diverse set of people and clothing styles and these avatars can be animated given unseen motion sequences.
*Equal contribution
†Corresponding author 1.

Introduction
Making immersive AR/VR a reality requires methods to effortlessly create personalized avatars. Consider telepres-ence as an example: the remote participant requires means to simply create a detailed scan of themselves and the sys-tem must then be able to re-target the avatar in a realistic fashion to a new environment and to new poses. Such appli-cations impose several challenging constraints: i) to gener-alize to unseen users and clothing, no specific prior knowl-edge such as template meshes should be required ii) the ac-quired 3D surface must be animatable with realistic surface deformations driven by complex body poses iii) the capture setup must be unobtrusive, ideally consisting of a single consumer-grade sensor (e.g. Kinect) iv) the process must be automatic and may not require technical expertise, ren-dering traditional skinning and animation pipelines unsuit-able. To address these requirements, we introduce a novel method for learning Personalized Implicit Neural Avatars (PINA) from only a sequence of monocular RGB-D video.
Existing methods do not fully meet these criteria. Most state-of-the-art dynamic human models [7, 11, 33, 34] rep-resent humans as a parametric mesh and deform it via lin-ear blend skinning (LBS) and pose correctives. Sometimes
learned displacement maps to capture details of tight-fitting clothing are used [11]. However, the fixed topology and resolution of meshes limit the type of clothing and dynam-ics that can be captured. To address this, several meth-ods [15, 51] propose to learn neural implicit functions to model static clothed humans. Furthermore, several methods that learn a neural avatar for a specific outfit from watertight meshes [13,16,24,32,47,53,56] have been proposed. These methods either require complete full-body scans with accu-rate surface normals and registered poses [13, 16, 53, 56] or rely on complex and intrusive multi-view setups [24,32,47].
Learning an animatable avatar from a monocular RGB-D sequence is challenging since raw depth images are noisy and only contain partial views of the body (Fig. 1, left). At the core of our method lies the idea to fuse partial depth maps into a single, consistent representation and to learn the articulation-driven deformations at the same time. To do so, we formulate an implicit signed distance field (SDF) in canonical space. To learn from posed observations, the inverse mapping from deformed to canonical space is re-quired. We follow SNARF [13] and locate the canonical correspondences via optimization. A key challenge brought on by the monocular RGB-D setting is to learn from in-complete point clouds. Inspired by rigid learned SDFs for objects [19], we propose a point-based supervision scheme that enables learning of articulated non-rigid shapes (i.e. clothed humans). Transforming the spatial gradient of the
SDF into posed space and comparing it to surface normals from depth images leads to the learning of geometric de-tails. Training is formulated as a global optimization that jointly optimizes the canonical SDF, the skinning fields and the per-frame pose. PINA learns animatable avatars with-out requiring any additional supervision or priors extracted from large datasets of clothed humans.
In detailed ablations, we shed light on the key compo-nents of our method. We compare to existing methods in the reconstruction and animation tasks, showing that our method performs best across several datasets and settings.
Finally, we demonstrate the ability to capture and animate different humans in a variety of clothing styles qualitatively.
In summary, our contributions are:
• a method to fuse partial RGB-D observations into a canonical, implicit representation of 3D humans; and
• to learn an animatable SDF representation directly from partial point clouds and normals; and
• a formulation which jointly optimizes shape, per-frame pose and skinning weights.
The code and video will be available on the project page: https://zj-dong.github.io/pina/. 2.