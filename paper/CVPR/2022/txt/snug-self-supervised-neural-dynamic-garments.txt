Abstract
We present a self-supervised method to learn dynamic 3D deformations of garments worn by parametric hu-man bodies. State-of-the-art data-driven approaches to model 3D garment deformations are trained using super-vised strategies that require large datasets, usually obtained by expensive physics-based simulation methods or profes-sional multi-camera capture setups. In contrast, we propose a new training scheme that removes the need for ground-truth samples, enabling self-supervised training of dynamic 3D garment deformations. Our key contribution is to realize that physics-based deformation models, traditionally solved in a frame-by-frame basis by implicit integrators, can be recasted as an optimization problem. We leverage such optimization-based scheme to formulate a set of physics-based loss terms that can be used to train neural networks without precomputing ground-truth data. This allows us to learn models for interactive garments, including dynamic deformations and fine wrinkles, with a two orders of magni-tude speed up in training time compared to state-of-the-art supervised methods. 1.

Introduction
The efficient modeling of digital garments is an active area of research due to the large number of applications, including fashion design, e-commerce, virtual try-on, and video games. The traditional approach to this problem is through physics-based simulation [38], but the high com-putational cost required at run time hinders the deploy-ment of these techniques to real-world applications. Re-cently, learning-based methods [17,31,39,45,46,50,53, 55] have demonstrated that it is possible to closely approximate the accuracy of physics-based solutions. These methods use supervised learning strategies to find a function that outputs a deformed garment given an input body descrip-tor. During the training phase, the supervision is enforced by directly minimizing at a vertex level the difference be-tween the predicted garment and ground-truth 3D meshes.
Despite requiring hours of training, learning-based meth-ods are highly-efficient to evaluate at run time, therefore they potentially offer an attractive alternative to traditional physics-based solutions.
However, the need for large datasets in current super-vised methods is far from ideal. Ground-truth meshes must be obtained –for each combination of garment, body shape, and pose– via computationally-expensive simula-tions [37] or complex 3D scanning setups [40], which heav-ily hinders the scalability of current learning-based meth-ods. We observe that for similar image-based problems, self-supervised strategies have shown that it is possible to learn complex tasks without requiring ground-truth data
[41, 57]. Unfortunately, self-supervision for dynamic 3D clothing has not been explored.
In this work, we present a self-supervised method to learn dynamic deformations of 3D garments worn by para-metric human bodies. The key to our success is realizing that the solution to the equations of motion used in cur-rent physics-based methods can also be formulated as an optimization problem [34]. More specifically, we show that the per-time-step numerical integration scheme used to up-date the vertex position (e.g., backward Euler) in physics-based simulators, can be recast as an optimization problem, and demonstrate that the function for this minimization can become the central ingredient of a self-supervised learning scheme. Since this objective function includes both an iner-tial term and static term directly derived from the equations of motion, we are able to learn time-dependent and pose-dependent deformations without any ground-truth data.
The advantages of self-supervision go beyond removing the need for ground-truth data. By reformulating the learn-ing tasks in terms of physics-based intrinsic properties in-stead of explicit 3D surface similarity, we also mitigate the smoothing artifacts common in supervised methods where
L2 losses are used directly at the vertex level [39]. Ad-ditionally, self-supervised approaches also generalize bet-ter to test sequences outside the distribution of the training set. Finally, we also show how different material models can be easily formulated in our self-supervised framework, bringing the generalization capabilities of physics-based so-lutions (i.e., deform any material) to learning-based meth-ods, without requiring any precalculation or offline step.
All in all, our main contribution is a novel learning-based method capable of learning to dynamically deform garments using a self-supervised strategy. We demonstrate the superiority of our approach in terms of data require-ments, training time, and inference time, and we quanti-tatively and qualitatively compare our results with state-of-the-art supervised methods. 2.