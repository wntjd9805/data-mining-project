Abstract
Standard visual localization methods build a priori 3D model of a scene which is used to establish correspon-dences against the 2D keypoints in a query image. Stor-ing these pre-built 3D scene models can be prohibitively expensive for large-scale environments, especially on mo-bile devices with limited storage and communication band-width. We design a novel framework that compresses a scene while still maintaining localization accuracy. The scene is compressed in three stages: first, the database frames are clustered using pairwise co-visibility informa-tion. Then, a learned point selection module prunes the points in each cluster taking into account the final pose es-In the final stage, the features of the timation accuracy. selected points are further compressed using learned quan-tization. Query image registration is done using only the compressed scene points. To the best of our knowledge, we are the first to propose learned scene compression for visual localization. We also demonstrate the effectiveness and ef-ficiency of our method on various outdoor datasets where it can perform accurate localization with low memory con-sumption. 1.

Introduction
Visual localization aims to estimate the camera pose (i.e., position and rotation) of an RGB query image with respect to a known 3D scene. This is a fundamental prob-lem in 3D computer vision with various applications such as autonomous driving, augmented reality, indoor navigation, etc. Classical visual localization methods build 3D point cloud models of the scene beforehand, where each point is associated with one or more image descriptors, which are used to match against the 2D points detected in the query
*Equal contribution, â€ Corresponding authors
Project Page: sfu-gruvi-3dv.github.com/s_squeezer image. Once the 2D-3D correspondences are established, robust pose estimation [11, 19] can be applied to recover the camera pose. Retaining the pre-built 3D scene mod-els costs expensive memory usage for large-scale environ-ments, especially when deploying on mobile devices with limited storage. Thus, many methods have been proposed to compress 3D scene models to improve scalability while maintaining localization accuracy.
Previous 3D scene compression methods can be roughly divided into three categories. The first category [7, 39] compresses the descriptors of 3D points using quantiza-tion, inevitably sacrificing the distinctiveness of the fea-tures while offering only a limited amount of compres-sion. The second are learning-based visual localization methods [14, 16, 49]. Without explicitly storing a 3D scene model, they directly regress the camera pose or scene coor-dinates of each pixel via deep neural networks, which can be considered as a special type of scene compression as the network weights encode the 3D scene information. How-ever, these methods either have low accuracy or general-ize poorly. The third and the most common category com-presses the 3D models by carefully selecting a subset of 3D points based on certain handcrafted criteria which gener-ally include spatial coverage and visual distinctiveness of the selected points [6,23,27]. However, these criteria fail to directly consider the impact of the compression on the final pose estimation. Manually designing rules to select points is known to be a challenging problem for both absolute and relative pose estimation [13].
Motivated by the latest hierarchical localization meth-ods [36, 38] which lead the benchmarks for visual local-ization, we propose a novel hierarchical strategy for 3D scene compression. Specifically, we compress the 3D scene model in three levels. At the coarse level, we conduct database image clustering and divide the scene into multiple clusters, enabling us to compress the clusters individually.
This also speeds up localization by limiting the matching
of the query keypoints to a smaller set of 3D points. For the middle level, we solve a QP problem [27] to select 3D points from each cluster. However, different from [27], we design a novel differentiable point selection method that en-ables us to learn to select 3D points based on their 2D obser-vations and influence on the final pose estimation. To this end, we design a new multi-view observation fuser which learns to extract and aggregate features for each 3D point based on their ability to yield correct matches. With the extracted multi-view features we can learn the distinctive-ness scores and pairwise proximity of the points which are used to select a subset of points via a differentiable QP solver [1]. Our experiments show that our learned point selection performs better than the heuristic methods. At the finest level, we exploit the latest differentiable quantiza-tion method [12] to further compress the feature descriptors.
With these three levels of compression, our system outper-forms existing compression-based localization methods.
The contributions of our paper can be summarized as follows. At first, we propose a novel hierarchical pipeline for scene compression taking inspiration from the latest vi-sual hierarchical localization methods. Secondly, we pro-pose a novel differentiable point selection that learns dis-tinctiveness scores and pairwise proximity of scene points.
We train it alongside the feature matching and pose estima-tion, which enables the point selection to be based on the pose estimation accuracy instead of heuristic criteria. Ex-tensive experiments show that our method outperforms pre-vious methods on various benchmark datasets. 2.