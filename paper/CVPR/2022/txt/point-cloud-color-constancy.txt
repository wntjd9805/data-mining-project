Abstract 1.

Introduction
In this paper, we present Point Cloud Color Constancy, in short PCCC, an illumination chromaticity estimation al-gorithm exploiting a point cloud. We leverage the depth information captured by the time-of-ﬂight (ToF) sensor mounted rigidly with the RGB sensor, and form a 6D cloud where each point contains the coordinates and RGB intensi-ties, noted as (x,y,z,r,g,b). PCCC applies the PointNet archi-tecture to the color constancy problem, deriving the illumi-nation vector point-wise and then making a global decision about the global illumination chromaticity. On two pop-ular RGB-D datasets, which we extend with illumination information, as well as on a novel benchmark, PCCC ob-tains lower error than the state-of-the-art algorithms. Our method is simple and fast, requiring merely 16 × 16-size in-put and reaching speed over 140 fps (CPU time), including the cost of building the point cloud and net inference.
∗Equal contribution/†Corresponding author. Code & Data: https:
//github.com/xyxingx/Point-Cloud-Color-Constancy
There are over a billion smartphone cameras capturing photographs, where the procedure of perceiving true colors of the real world happens in the ISP pipeline. In general, a good-looking picture relies on a single CMOS sensor with a certain color ﬁlter array. On ﬂagship smartphones, there is a trend of leveraging two or more digital sensors to “com-pute” a higher-quality image. Such cases include {main camera, tele camera} [2], {main camera, near inferred sen-sor} [48], and {color camera and time-of-ﬂight (ToF) depth sensor} [45]. In this work, we proceed in this direction and use a color camera and a ToF sensor. A ToF depth sensor has been commonly employed for other applications; e.g., auto focus, bokeh effect and 3D face and skeleton scanning.
Here we show a novel task based on combing the RGB and
ToF sensors – RGB-D or point-cloud illumination estima-tion.
Illumination estimation refers to the task of estimating the normalized illumination chromaticity given a raw im-age, which itself is a core task in computational color con-stancy. Color constancy is an intrinsic property of the hu-man eye, which perceives the world independently of illu-mination color changes. For technical reasons and engineer-ing beneﬁts, in digital cameras, illumination estimation and a channel-wise rescaling are combined in the so-called auto-white balance (AWB), to realize “color constancy” compu-tationally. An accurate estimation, with proper rescaling factors than render a neutral surface white.
In common the use-case, illumination estimation only re-lies on a single raw format image. Here we advocate the use of depth sensor which casts the task into a 3D regression problem, which we show delivers more accurate results. We introduce depth based color constancy for the following rea-sons: 1. image statistics are steered by depth information
[30]. More specially, the surface geometry, the texture size and the signal-to-noise ratio varies with depth, which will inﬂuence the performance of illumination estimation algo-rithm; 2. abrupt depth changes correlate with non-smooth illumination distribution. Imagine walking from outside to an indoor room with a tungsten light. In a 2D image, the depth and illumination change dramatically in the area close to the door. As shown in Figure 1, the captured RGB-D images are reprojected to form the colored point cloud, on which the proposed method estimates the illumination for each point. Then these point predictions (Figure 1(C)) are averaged with learnt weights.
In a summary, the contributions of the paper are:
• We formulate the generic problem of illumination esti-mation in a 3D world, relaxing the over-simplistic as-sumptions about uniform illumination for a plain 2D image adopted in prior work .
• We present PCCC architecture, derived from PointNet, a novel point cloud net for the color constancy task.
We show its superior performance on illumination es-timation on three color constancy benchmarks. Side applications, e.g., local AWB is reported as well.
• PCCC operates well on the sparse-sampled thumbnail point clouds, which allows over 500 frames per sec-ond (include time of building point cloud) on a Nvidia
V100 GPU and is hardware friendly for a mobile SoC.
• We annotated the illumination groundtruth labels for three popular RGB-D datasets (NYU-v2 dataset [40],
Diode dataset [42] and ETH3D dataset [37]) and col-lected DepthAWB, a novel RGB-D dataset for the color constancy task. 2.