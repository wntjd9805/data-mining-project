Abstract
Large-scale pre-training has been proven to be cru-cial for various computer vision tasks. However, with the increase of pre-training data amount, model architecture amount, and the private/inaccessible data, it is not very efficient or possible to pre-train all the model architec-tures on large-scale datasets.
In this work, we investi-gate an alternative strategy for pre-training, namely Knowl-edge Distillation as Efficient Pre-training (KDEP), aim-ing to efficiently transfer the learned feature representa-tion from existing pre-trained models to new student mod-els for future downstream tasks. We observe that existing
Knowledge Distillation (KD) methods are unsuitable to-wards pre-training since they normally distill the logits that are going to be discarded when transferred to downstream tasks. To resolve this problem, we propose a feature-based
KD method with non-parametric feature dimension align-ing. Notably, our method performs comparably with su-pervised pre-training counterparts in 3 downstream tasks and 9 downstream datasets requiring 10× less data and 5× less pre-training time. Code is available at https:
//github.com/CVMI-Lab/KDEP. 1.

Introduction
With the booming of large-scale datasets [16, 37, 45, 55, 59], many computer vision tasks have benefitted sig-nificantly from pre-training in the past decade. In fact, it has been a de facto strategy to first pre-train on datasets like ImageNet [16] and then fine-tune on downstream tasks
[8, 24, 54, 66, 73], especially when the data of downstream tasks is scarce.
However, the increasing pre-training data scale and the inaccessibility of private data [59] render pre-training all architectures on large datasets not efficient or possible.
As well-trained deep neural networks are essentially con-∗Part of the work is done during an internship at ByteDance AI Lab.
†Equal contribution.
‡Corresponding author.
Figure 1. Transfer performance (averaged top-1 accuracy of four image classification tasks (details in Sec. 4)) compared to Super-vised Pre-training (SP), traditional KD method (logits KD, 1×1 conv), and KDEP (SVD+PTS) with different data amount (10% or 100% ImageNet-1K data) and training schedules. densed memory bank of datasets [3,20], we wonder whether the condensed data knowledge encoded into a pre-trained model can be leveraged to efficiently pre-train new architec-tures with only a relatively small set of pre-training data?
In this work, we propose Knowledge Distillation as Ef-ficient Pre-training (KDEP), transferring the feature extrac-tion capability of the teacher obtained from large-scale data, to the student model for solving future downstream tasks.
Note that KDEP is quite different from traditional Knowl-edge Distillation (KD) that only targets at distilling the knowledge of a given specific task to a student model.
Studies of existing KD methods for KDEP. Our empiri-cal studies show that existing KD methods such as logits
KD [29] (i.e. distilling the task-specific output logits) and feature-level KD [27] lead to inferior performance (see Fig-ure 1: “logits KD” and “1×1 conv”), indicating that exist-ing KD methods tailored to different tasks might be unable to fully leverage the knowledge condensed in the teacher model when pre-training new models with limited data and computation budget.
After further investigation, we conclude a potential issue
among channels are of great magnitude differences, and largely differ from those of normal DCNNs. This interferes the optimization of the network. To further boost feature learning, we design a Power Temperature Scaling (PTS) function to reduce the variance differences while preserv-ing the original relative magnitude, which tailors features from SVD for DCNNs. As illustrated in Figure 2e, with our SVD+PTS aligning method, the distilled student obtains feature representation similar to the original teacher’s (Fig-ure 2c) while matching the feature dimensions. Notably, our method adds no learnable parameters, does not rely on the task loss or the logits loss [29], and only use supervision from the teacher’s penultimate feature (after global average pooling), which is more general for feature representation learning and allows more potential pre-trained teachers.
Results. Our major findings of KDEP are summarized in
Figure 1: 1) Faster convergence. Our method achieves comparable or better transfer learning results with only 10% or 20% training time than supervised pre-training (SP) on the whole ImageNet-1K dataset. 2) Higher data-efficiency. With only 10% of ImageNet-1K unlabeled data (discard the labels) and an available pre-trained teacher model, our distilled student obtains better transfer learn-ing results than SP with 100% ImageNet-1K data. 3) Bet-ter Transferability. Given the same computation budget and data amount as SP, our method achieves higher transfer learning performance. With the proposed KDEP method, we could realize pre-training once and distilling it to all: distilling a pre-trained teacher (either to utilize an available pre-trained model or pre-train a certain architecture) to effi-ciently pre-train all other student models. 2.