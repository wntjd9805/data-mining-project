Abstract
In label-noise learning, estimating the transition matrix has attracted more and more attention as the matrix plays an important role in building statistically consistent classiﬁers.
However, it is very challenging to estimate the transition matrix T (x), where x denotes the instance, because it is unidentiﬁable under the instance-dependent noise (IDN). To address this problem, we have noticed that, there are psycho-logical and physiological evidences showing that we humans are more likely to annotate instances of similar appearances to the same classes, and thus poor-quality or ambiguous instances of similar appearances are easier to be mislabeled to the correlated or same noisy classes. Therefore, we pro-pose assumption on the geometry of T (x) that “the closer two instances are, the more similar their corresponding tran-sition matrices should be”. More speciﬁcally, we formulate above assumption into the manifold embedding, to effective-ly reduce the degree of freedom of T (x) and make it stably estimable in practice. The proposed manifold-regularized technique works by directly reducing the estimation error without hurting the approximation error about the estimation problem of T (x). Experimental evaluations on four synthetic and two real-world datasets demonstrate that our method is superior to state-of-the-art approaches for label-noise learning under the challenging IDN. 1.

Introduction
Label-noise learning has drawn more and more attention in the deep learning community, e.g., [3, 5, 11, 29, 50, 53].
The main reason is that accurately annotating large-scale datasets becomes extremely costly and sometimes even in-feasible [14]. An effective way is to collect such large-scale datasets from the crowd-sourcing platform [49] or online queries [4], which inevitably yield low-quality and noisy
∗Corresponding author. data. Thus, mitigating the side-effects of noisy labels be-comes a very crucial topic. The noise model can be catego-rized as the class-conditional noise (CCN) and the instance-dependent noise (IDN). In CCN, each instance from one class has a ﬁxed probability of being assigned to another.
While in IDN, the probability that an instance is mislabeled depends on both its class and features. In this paper, we fo-cus on the more promising IDN approach, which considers a more general noise and can cope with real-world noise.
The traditional label-noise learning methods can be divid-ed into two categories: algorithms with statistically incon-sistent classiﬁers and algorithms with statistically consistent classiﬁers. In the ﬁrst category, algorithms do not model the label noise distribution explicitly, they usually employ some heuristics to reduce the negative effects of the label noise [8–11]. Although such approaches often empirically work well, the learned classiﬁer from the data with label noise may not be statistically consistent and their reliabil-ity cannot be guaranteed. To address this limitation, the classiﬁer-consistent algorithms have been proposed. Specif-ically, recent studies showed that estimating the transition matrix plays an important role in building consistent classi-ﬁers for label-noise learning, as these methods can explicitly model the generation process of the noisy label [7, 35]. How-ever, it is very challenging to obtain the instance-dependent transition matrix (IDTM) for getting the noisy labels from the clean labels, because the IDTM T (x) as a function of in-stance x is unidentiﬁable under IDN without any constraint.
Existing methods have tried to deal with this challenging and ill-posed problem from two perspectives. First, they have simpliﬁed the complex problem of estimating a matrix-valued function T (x) for the general label noise into a prob-lem of estimating T (i.e., a ﬁxed matrix), which is known as
CCN. Then, some anchor points (training data that certainly belong to some speciﬁc classes) are adopted to easily esti-mate the transition matrix T [22,32]. Although such methods
Figure 1. The proposed instance-dependent label-noise learning framework. We train a classiﬁer in a statistically consistent manner through the proposed IDTM T (x), where T (xi) ∈ RK×K is estimated by the transition neural network. It is regularized by the manifold embedding to reduce the degree of freedom of T (x) and make it estimable in practice. In the manifold embedding L, the afﬁnity matrix Sij is obtained by ﬁnding the k-nearest neighbors in the instance feature space. Finally, we use the cross-entropy to train the classiﬁer assisted by T (x). have theoretical guarantees and have achieved success un-der some synthetic noisy labels or speciﬁc conditions, they are unable to cope with general real-world noise, i.e., IDN.
Second, several pioneer works considered strong assump-tions and focused on how to simplify T (x) and signiﬁcantly reduce its degree of freedom or complexity. For example, in part-dependent noise [44], it is assumed that T (x) is a convex combination of a predeﬁned number of ﬁxed transi-tion matrices, and their coefﬁcients come from non-negative matrix factorization. In such a way, the estimation problem becomes parametric, and the degree of freedom of T (x) can be reduced. The issue of those methods is that simplify-ing the form of T (x) too much will certainly cause a large approximation error.
To address the problem of estimating IDTM T (x) under
IDN, in this paper, we will not put any strong assumption on the form of T (x), but we will instead put some assump-tion on the geometry of T (x). More speciﬁcally, we have noticed that, there are psychological and physiological ev-idences [6, 24, 30, 36] showing that we humans are more likely to annotate instances of similar appearances to the same class, and thus poor-quality or ambiguous instances of similar appearances are susceptible to be mislabeled to correlated or same noisy classes. Therefore, according to the basic principle that the noisy class-posterior probability
P ( ¯Y = j|X = x) can be inferred by the latent clean class-posterior probability P (Y = i|X = x) and IDTM T (x), we propose an assumption that “the closer two instances are, the more similar their corresponding transition matrices will be”. This can be interpreted as that the instance adjacent relationships of one category in the feature space should be consistent with those in the transition matrix space.
Motivated by this practically useful assumption, we pro-pose to estimate T (x) by formulating the assumption into the manifold embedding as shown in Figure 1. Speciﬁcal-ly, we make use of the manifold assumption, and require that if xi and xj are close in the feature space, then T (xi) and T (xj) should also be close (in terms of a matrix norm).
Going along this line, though we do not reduce the complex-ity of T (x) directly since we do not further simplify it, we still effectively reduce the degree of freedom of the linear system P (¯yi|xi) = T (xi)P (yi|xi), i = 1, .., N and make
T (x) stably estimable in practice. Here, T (x) can be re-garded as practically stable, since adding such a smoothness assumption stops T (x) from changing too much in a tiny neighborhood and then it should be Lipschitz continuous.
Thus, it should be uniquely determined given inﬁnite data or the underlying data distribution. Finally, we conduct exten-sive experiments on various datasets, which illustrates the proposed method is superior to state-of-the-art approaches for label-noise learning under IDN.
The main contributions are summarized as follows:
• We are the ﬁrst to propose the practical assumption on the geometry of T (x) that “the closer two instances are, the more similar their corresponding transition matrices should be”, which aims to reduce the degree of freedom of T (x), and make it stably estimable in practice.
• We formulate the assumption into manifold embedding, which allows to keep the instance adjacent relationships in the features space to be consistent with those in the transition matrix space. By this way, the proposed manifold-regularized method can greatly reduce the estimation error without hurting much approximation error about the estimation problem of T (x).
• Extensive experiments on various datasets demonstrate superior classiﬁcation performances over current state-of-the-art methods on both synthetic IDN datasets (M-NIST, CIFAR10, CIFAR100) and two real-world noisy datasets (Clothing1M and Food101N). 2.