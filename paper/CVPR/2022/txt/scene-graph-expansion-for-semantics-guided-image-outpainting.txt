Abstract
In this paper, we address the task of semantics-guided image outpainting, which is to complete an image by gen-erating semantically practical content. Different from most existing image outpainting works, we approach the above task by understanding and completing image semantics at
In particular, we propose a novel the scene graph level. network of Scene Graph Transformer (SGT), which is de-signed to take node and edge features as inputs for model-ing the associated structural information. To better under-stand and process graph-based inputs, our SGT uniquely performs feature attention at both node and edge levels.
While the former views edges as relationship regularization, the latter observes the co-occurrence of nodes for guiding the attention process. We demonstrate that, given a partial input image with its layout and scene graph, our SGT can be applied for scene graph expansion and its conversion to a complete layout. Following state-of-the-art layout-to-image conversions works, the task of image outpainting can be completed with sufficient and practical semantics intro-duced. Extensive experiments are conducted on the datasets of MS-COCO and Visual Genome, which quantitatively and qualitatively confirm the effectiveness of our proposed SGT and outpainting frameworks. 1.

Introduction
Given an incomplete image or a partial image input, hu-mans generally are able to picture the context of the corre-sponding complete version. Such reasoning skill is largely based on our prior experience and knowledge observed from diverse images and their semantics. In the scope of machine learning, the objective is typically applied for the task of image completion, aiming to generate or predict rea-sonable missing image regions based on the observed in-put. In the areas of computer vision and image processing, several content creation applications such as object removal editing [21], image panorama creation [30], texture cre-Figure 1. Illustration of semantics-guided image outpainting.
Our work can be divided into stages of (a) Scene Graph Expansion (SGE), (b) Scene Graph to Layout (G2L), and (c) Layout to Image (L2I) conversions. The blue node and red edges in the scene graph indicate the generated objects and relationship, respectively. ation [24], and view expansion [29] are closely related to the aforementioned task.
Depending on where the missing parts are to be re-covered, the task of image completion is typically divided into two categories, image inpainting (also known as image hole-filling) and outpainting (also known as image extrap-olation). Compared to image inpainting, image outpainting needs to synthesize unknown regions in single-sided fash-ions and thus is considered to be more challenging. Based on image inpainting works [8,16,18,27,31], researchers ad-vance local and global GAN [8], Partial Convolution [16],
Gated Convolution [31] and edge information [18] for out-painting tasks [11, 17, 20, 23, 26, 29]. However, despite im-pressive performances, most existing approaches are not de-signed to predict novel semantic regions in the output im-ages. That is, they mainly focus on extending the surround-ing texture or completing the fractional objects, resulting in extrapolated image regions with repeating structures or patterns. It is not clear how to introduce novel semantics with reasonable relationships with the existing ones during outpainting. As a result, we choose to approach this chal-lenging semantics-oriented image outpainting problem by modeling and manipulating images at the semantic level.
In order to tackle the above task, a scene graph would be a desirable representation due to their ability in describ-ing the presence of semantic objects and their relationships in an image. Thus, based on recent works such as [9], [6] and [19], one can describe and categorize a given image into three levels. The first level is the image level, contain-ing pixel-level information. The second one is the layout level, which describes the locations/sizes of the objects of interest, including their corresponding category labels. The final level is the scene graph level, which describes seman-tic objects and their relationships (e.g., right of, throw) in an image. The higher the level is, the more abstract and semantic information it would contain.
In this paper, we choose to decompose the semantics-guided image outpainting task into three stages, as depicted in Figure 1. Given the scene graph extracted from the partial image and its layout, the first stage of scene graph expan-sion (SGE) utilizes the proposed Scene Graph Transformer (SGT), which uniquely performs node and edge-level atten-tion, for expanding the input scene graph. The following stage of G2L further transforms such an expanded scene graph into a complete layout. Finally, layout-to-image (L2I) models can be applied for producing the final image out-put. We note that both SGE and G2L stages utilize our proposed SGT module, taking scene graph data as inputs with unique objectives introduced to enforce the desirable object/relationship properties, as later discussed in Sect. 3.
The contributions of our work are highlighted as follows:
• We approach the task of semantics-guided image out-painting, which is able to synthesize novel yet semanti-cally practical objects with associated relationships for completing an image output.
• We propose a Scene Graph Transformer (SGT), which takes node and edge features with unique node-level and edge-level attention mechanisms for modeling the associated structural information.
• Expecting the sparsity of the object relationships in a scene graph, our SGT is designed to exploit the con-verse relationships between objects, so that semanti-cally practical nodes and their corresponding edges can be properly recovered or expanded. 2.