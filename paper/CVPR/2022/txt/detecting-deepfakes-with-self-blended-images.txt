Abstract
In this paper, we present novel synthetic training data called self-blended images (SBIs) to detect deepfakes. SBIs are generated by blending pseudo source and target images from single pristine images, reproducing common forgery artifacts (e.g., blending boundaries and statistical inconsis-tencies between source and target images). The key idea behind SBIs is that more general and hardly recognizable fake samples encourage classifiers to learn generic and robust representations without overfitting to manipulation-specific artifacts. We compare our approach with state-of-the-art methods on FF++, CDF, DFD, DFDC, DFDCP, and FFIW datasets by following the standard cross-dataset and cross-manipulation protocols. Extensive experiments show that our method improves the model generalization to unknown manipulations and scenes.
In particular, on
DFDC and DFDCP where existing methods suffer from the domain gap between the training and test sets, our approach outperforms the baseline by 4.90% and 11.78% points in the cross-dataset evaluation, respectively. Code is available at https://github.com/mapooon/
SelfBlendedImages. 1.

Introduction
The recent rapid advancement of generative adversarial networks [10, 25, 31, 32, 45, 51, 63] (GAN) in computer vi-sion has made it possible to generate realistic facial im-ages. In particular, techniques called deepfake manipulat-ing the identity, expression, or attributes of a subject are used for entertainment purposes, e.g., smartphone applica-tions or movies; however, they can also be used for mali-cious purposes, e.g., to create fake news or to falsify evi-dence. Therefore, the vision community is keenly working on deepfake detection techniques.
Most previous detection methods [8,16,26,30,36,48,53, 64] perform well on the in-dataset scenario where they de-tect forgeries they learned in training; however, some stud-ies [15,21,33,61] have found that the detection performance significantly drops in the cross-dataset scenario where fake
Figure 1. Overview of fake sample synthesis. Previous methods blend two distinct faces and generate artifacts based on a gap be-tween selected source and target images. By contrast, our method blends slightly changed faces from a single image and generate artifacts actively by transformations. In this example, we apply a color jitter, sharpening, resize, and translation to the source image and no transformations to the target image. samples are forged by unknown manipulations.
One of the most effective solutions to this problem is to train models with synthetic data, which encourages mod-els to learn generic representations for deepfake detection.
For example, facial regions are blurred to reproduce a qual-ity degradation of GAN-synthesized source images [41], blended images are generated from pairs of two pristine im-ages to reproduce blending artifacts [39, 65]. However, the quality of deepfakes has improved over the years, which has caused the former method to fail on recent bench-marks [42, 52]. Although the latter methods perform well on some datasets [2, 42], low-quality videos in more chal-lenging datasets [19, 20] where artifacts are hardly recog-(a) Landmark (b) Boundary (c) Color (d) Frequency
Figure 2. Typical artifacts on forged faces. We classify artifacts into four types, (a) landmark mismatch, (b) blending boundary, (c) color mismatch, and (d) frequency inconsistency. nizable owing to the high compression or extreme exposure lead them to unacceptable detection performance.
In this paper, we propose novel synthetic training data called self-blended images (SBIs) to detect deepfakes. The overviews of our method and previous methods [39, 65] are shown in Fig. 1. The key idea is that more hardly recogniz-able fake samples that contain common face forgery traces encourage models to learn more general and robust repre-sentations for face forgery detection. We analyze forged faces and define four typical artifacts motivated from pre-vious works (e.g., blending boundaries [39], source feature inconsistencies [65], and statistical anomalies in frequency domain [13]) as shown in Fig. 2. To synthesize these ar-tifacts based on our key idea, we develop a source-target generator (STG) and mask generator (MG). STG generates pairs of pseudo source and target images from single pris-tine images using simple image processing, and MG gen-erates various blending masks from facial landmarks of the input images. By blending the source and target images with the masks, we obtain SBIs. Training with SBIs en-courages the models to learn generic representations be-cause models learn the forgery traces we actively generate in STG. Moreover, our method improves training efficiency in terms of computational cost. Whereas successful previ-ous works [39, 65] use landmark nearest search for source-target pair selection, which is computationally expensive,
SBIs are generated without this process. Therefore, our method does not suffer from the large dataset size problem.
We evaluate our approach following the two evaluation protocols, cross-dataset evaluation and cross-manipulation evaluation.
In the cross-dataset evaluation, we train our model on FF++ [52] and evaluate it on CDF [42], DFD [2],
DFDC [19], DFDCP [20], and FFIW [67]. This experimen-tal setting is similar to that in real detection scenarios where defenders are exposed to unseen domains. Our approach surpasses or is at least comparable to the state-of-the-art methods on all test sets despite its simplicity. Especially, on DFDC and DFDCP where previous methods suffer from domain gaps between the training and test sets, our method outperforms the state-of-the-art unsupervised baseline [65] by 4.90% and 11.78% points, respectively.
In the cross-manipulation evaluation, we evaluate the generality of our model on unseen manipulation methods of FF++; DF [4],
F2F [56], FS [5], and NT [55]. Our approach achieves the AUC of 99.99%, 99.88%, 99.91%, and 98.79% on DF,
F2F, FS, and NT, respectively. Although the performance on FF++ becomes saturated, our method still outperforms the state of the art on whole FF++ (99.64% vs. 99.11%). 2.