Abstract
Affordance grounding, a task to ground (i.e., localize) action possibility region in objects, which faces the chal-lenge of establishing an explicit link with object parts due to the diversity of interactive affordance. Human has the abil-ity that transform the various exocentric interactions to in-variant egocentric affordance so as to counter the impact of interactive diversity. To empower an agent with such ability, this paper proposes a task of affordance grounding from ex-ocentric view, i.e., given exocentric human-object interac-tion and egocentric object images, learning the affordance knowledge of the object and transferring it to the egocentric image using only the affordance label as supervision. To this end, we devise a cross-view knowledge transfer frame-work that extracts affordance-speciﬁc features from exocen-tric interactions and enhances the perception of affordance regions by preserving affordance correlation. Speciﬁcally, an Affordance Invariance Mining module is devised to ex-tract speciﬁc clues by minimizing the intra-class differences originated from interaction habits in exocentric images.
Besides, an Affordance Co-relation Preserving strategy is presented to perceive and localize affordance by aligning the co-relation matrix of predicted results between the two views. Particularly, an affordance grounding dataset named
AGD20K is constructed by collecting and labeling over 20K images from 36 affordance categories. Experimental results demonstrate that our method outperforms the representa-tive models in terms of objective metrics and visual quality.
Code: github.com/lhc1224/Cross-View-AG. 1.

Introduction
The goal of affordance grounding is to locate the region of “action possibilities” of an object. For an intelligent
*This work was done during an internship at JD Explore Academy.
†Corresponding author. ‡ Equal contributions.
Figure 1. Observation. By observing the exocentric diverse inter-actions, the human learns affordance knowledge determined by the object’s intrinsic properties and transfer it to the egocentric view. agent, it is necessary to know not only what the object is but also to understand how it can be used [11]. Perceiving and reasoning about possible interactions in local regions of objects is the key to the shift from passive perception sys-tems to embodied intelligence systems that actively interact with and perceive their environment [1, 33, 34, 38]. It has a wide range of applications for robot grasping, scene under-standing, action prediction [12, 13, 19, 23, 28, 31, 47, 49].
As affordance is a dynamic property closely related to the interaction between humans and environment [13], it is difﬁcult to understand how to interact with objects and es-tablish an explicit link between the objects’ intrinsic prop-erties and affordances [29]. However, humans can easily perceive the object’s affordance region by observing exo-centric human-object interactions, and give an egocentric deﬁnition. As shown in Fig. 1, although different persons hold the racket in different positions due to their individual habits, the human observer can perceive swingable regions determined by the intrinsic properties (e.g., the long handle
(a) (b)
Figure 2. Motivation. (a) Exocentric interactions can be decomposed into affordance-speciﬁc features M and differences in individual habits E. (b) There are co-relations between affordances, e.g.“Cut with” inevitably accompanies “Hold” and is independent of the object category (knife and scissors). Such co-relation is common between objects. In this paper, we mainly consider extracting affordance-speciﬁc cues M from diverse interactions while preserving the affordance co-relations to enhance the perceptual capability of the network. structure) of the racket from a group of interacting images, despite the effect of individual differences, and transfer the knowledge to the egocentric view, thereby constructing a bridge between the object part and the affordance category.
To empower an agent with this ability to perceive the in-variant egocentric affordance from various exocentric inter-actions, this paper proposes a task of affordance grounding from exocentric view, i.e., given exocentric human-object interactions and egocentric object images, learning affor-dance knowledge and transferring it to object images by only using affordance labels as supervision. And in the test-ing stage, the output is the prediction of the affordance re-gion for a speciﬁc object with the input of an egocentric object image and a particular affordance label.
To address this problem, we propose a cross-view knowl-edge transfer framework to extract affordance-speciﬁc fea-tures from exocentric interactions and transfer them to ego-centric view. Speciﬁcally, we ﬁrst devise an Affordance
Invariance Mining (AIM) module to decompose the exo-centric human-object interactions into the affordance rep-resentations determined by objects’ intrinsic properties and the differences originated from individual habits (as shown in Fig. 2 (a)). We use low rank matrix decomposition
[10,18,22,24] to minimize the intra-class differences caused by diverse interactions to obtain affordance-speciﬁc cues.
Furthermore, there is a correlation between the object af-fordances (as shown in Fig. 2 (b)), which can be adopted to establish the link between different affordances to reduce the uncertainty caused by multiple affordances regions on the object. Therefore, we present a novel Affordance Co-relation Preserving (ACP) strategy to perceive and localize the affordance region by aligning the co-relation matrix of prediction results from two views.
Despite the advances in affordance learning, the existing datasets [8, 29, 32, 35, 43] still bear limitations in terms of affordance/object category, image quality, and scene com-plexity. To carry out a comprehensive study, this paper proposes an affordance grounding dataset named AGD20K, consisting of 20, 061 exocentric images and 3, 755 egocen-tric images from 36 affordance categories. The contrastive experiments against several representative methods are per-formed on the AGD20K dataset. The results demonstrate the superiority of our proposed method in capturing the in-trinsic property of objects and suppressing the interactive diversity of affordance.
Contributions: (1) We present a new affordance ground-ing from exocentric view task and establish a large-scale
AGD20K benchmark to facilitate the research for empow-ering the agent to capture affordance knowledge from exo-centric human-object interactions. (2) We propose a novel cross-view knowledge transfer framework for affordance grounding in which the affordance knowledge is acquired from exocentric human-object interactions and transferred to egocentric views while preserving the correlation be-tween affordances, thereby achieving better perception and localization of interactive affordance. (3) Experiments on the AGD20K dataset demonstrate that our method outper-forms state-of-the-art methods and can serve as a strong baseline for future research. 2.