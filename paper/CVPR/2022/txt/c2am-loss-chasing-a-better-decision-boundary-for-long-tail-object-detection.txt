Abstract
Long-tail object detection suffers from poor performance on tail categories. We reveal that the real culprit lies in the extremely imbalanced distribution of the classifier’s weight norm. For conventional softmax cross-entropy loss, such imbalanced weight norm distribution yields ill conditioned decision boundary for categories which have small weight norms. To get rid of this situation, we choose to maxi-mize the cosine similarity between the learned feature and the weight vector of target category rather than the inner-product of them. The decision boundary between any two categories is the angular bisector of their weight vectors.
Whereas, the absolutely equal decision boundary is sub-optimal because it reduces the model’s sensitivity to vari-ous categories. Intuitively, categories with rich data diver-sity should occupy a larger area in the classification space while categories with limited data diversity should occupy a slightly small space. Hence, we devise a Category-Aware
Angular Margin Loss (C2AM Loss) to introduce an adap-tive angular margin between any two categories. Specif-ically, the margin between two categories is proportional to the ratio of their classifiers’ weight norms. As a result, the decision boundary is slightly pushed towards the cat-egory which has a smaller weight norm. We conduct com-prehensive experiments on LVIS dataset. C2AM Loss brings 4.9∼5.2 AP improvements on different detectors and back-bones compared with baseline. 1.

Introduction
Object detection is one of the most essential tasks in computer vision [11, 16, 31, 32]. Modern object detec-tors [1, 3, 6, 12, 21, 22, 24, 31, 40, 44, 45] have achieved
Figure 1. (a) is the classifier’s weight norm distribution of a naive
Mask R-CNN model trained with LVIS v1.0 training split [13]. X-axis is the sorted category index based on the category frequency.
Y-axis shows the weight norm; (b) shows the precisions AP m of the first 100 categories. (c) shows the precisions of the last 100 categories. promising results on challenging PASCAL VOC [9] and
COCO [23] datasets. Both of these two benchmarks are cu-rated to keep the relative balance between categories. How-ever, in real-world scenarios, data always obeys the Zip-fian [29] distribution where a large number of tail categories have few samples. Although current detectors perform well on balanced datasets, they all suffer from severe perfor-mance degradation on tail classes when facing extremely imbalanced datasets. Thus, long-tail object detection re-mains a major challenge for researchers.
A model that minimizes empirical risk on long-tail train-ing datasets is seriously biased towards head categories since they contribute most of the training data. To overcome this issue, previous literatures typically adopt two types of measures, namely, data re-sampling [8, 14, 27, 33] and loss re-weighting [34, 35, 38, 39]. Data re-sampling pins hope
on adjusting the extremely imbalanced data distribution to a less imbalanced one by over-sampling the tail categories and under-sampling the head. Whereas, it only increases the occurrence frequency of tail categories. The data diversity remains unchanged, which will lead to over-fitting on tail classes. Besides, under-sampling the head classes has a risk of missing discriminative information. Loss re-weighting methods work by enhancing the loss of tail categories and weakening that of head categories. Both of these methods implicitly reshape the decision boundary and bring bene-fits to tail categories. Nevertheless, they adjust the decision boundary in an indirect way which may weaken their ef-fectiveness. What’s more, how they influence the decision boundary is not intuitive and geometrically interpretable.
Under the long-tail setting, we observe that the weight norm of the classifier also exhibits an extremely imbalanced distribution as shown in Fig. 1(a). This phenomenon has also been validated by previous literatures [20, 34, 35]. And we also notice the precision is highly related to the clas-sifier’s weight norm. As shown in Fig. 1(c), the weight norms of the last 100 categories are close to zero. And their precisions are almost zero. For categories that have large weight norms, their precisions vary in a reasonable range, as in Fig. 1(b). We demonstrate that the extremely imbal-anced weight norm distribution will deteriorate the decision boundary, leading to a near zero precision for categories which have small weight norms. For inner-product based softmax, the output logit (before softmax) of category i is given by ||Wi||2 · ||x||2 · cos(θi), where Wi, x, θi are the classifier weight, the feature and the angle between them, respectively. When ||Wi||2 is overwhelmingly large, the model has a high probability to predict a large score on cat-egory i. As a result, the categories with small weight norms are completely suppressed, which is fatal to their accuracy.
We will detailedly analyse how the extremely imbalanced weight norm distribution causes the ill conditioned decision boundary in the following section.
The cosine classifier has natural advantages for handling the ill conditioned decision boundary mentioned above. The decision boundary of two categories is the angular bisec-tor of the angle between two classifiers’ weight vectors, as shown in Fig. 2(b). Whereas, totally abandoning the weight norm information is suboptimal since it reduces the model’s sensitivity to different categories. Intuitively, cate-gories with rich data diversity should occupy a larger area in the angular classification space. And for categories with limited data diversity, it is beneficial to slightly shrink the angular classification space for learning a compact and in-trinsic feature representation. In other words, proper clas-sifier bias is profitable in long-tail object detection.
In this paper, we propose a Category-Aware Angular
Margin Loss (C2AM Loss) to adaptively adjust the decision boundary based on the weight norm distribution. Specifi-cally, it introduces a category-aware margin to any two cat-egories in the angular space. The angular margin is pro-portional to the ratio of the classifier’s weight norm. We can adaptively push the decision boundary towards cate-gories which have smaller weight norms to learn a more compact and intrinsic feature representation. Noting that al-though C2AM Loss manually introduces the classifier bias to the model, it will not generate ill conditioned decision boundary like the inner-product based softmax loss. C2AM
Loss utilizes a hyper-parameter α to control the strength of pushing the decision boundary. Besides, a convex function log(x) is utilized to ensure the margin will not become ex-cessively large. The above two measures guarantee the clas-sifier bias is maintained in a proper magnitude.
To validate the effectiveness of C2AM Loss, we conduct extensive experiments on the challenging long-tail object detection dataset LVIS (v0.5 and v1.0) [13]. Experimental results of various detectors (Mask R-CNN [15] and Cas-cade Mask R-CNN [1]) with different backbones (ResNet-50 and ResNet-101 [16]) all show the superiority of the pro-posed C2AM Loss. To be more specific, Mask R-50 with
C2AM Loss outperforms the baseline by 5.2 APm. The im-provements are mainly from rare categories (+11.9 AP m r ) and common categories (+6.8 AP m c ). We also compare our methods with other SOTA methods and the results show that our method is more competitive.
To sum up, this work makes the following three contri-butions: 1. We point out that the extremely imbalanced weight norm distribution under the long-tail setting yields ill conditioned decision boundary, which severely deteri-orates the performance. 2. We present a Category-Aware Angular Margin Loss (C2AM Loss) that can adaptively adjust the decision boundary for learning a more compact and intrinsic feature representation. 3. We conduct comprehensive experiments on long-tail object detection dataset LVIS (v0.5 and v1.0).
C2AM Loss brings obvious performance improvement (4.9%∼5.2% AP m) when compared with baseline and achieves new state-of-the-art on both LVIS v0.5 and v1.0. 2.