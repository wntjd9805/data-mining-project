Abstract
The size and shape of the receptive ﬁeld determine how the network aggregates local features, and affect the over-all performance of a model considerably. Many compo-nents in a neural network, such as depth, kernel sizes, and strides for convolution and pooling, inﬂuence the recep-tive ﬁeld. However, they still rely on hyperparameters, and the receptive ﬁelds of existing models result in suboptimal shapes and sizes. Hence, we propose a simple yet effective
Dynamically Optimized Pooling operation, referred to as
DynOPool, which learns the optimized scale factors of fea-ture maps end-to-end. Moreover, DynOPool determines the proper resolution of a feature map by learning the desirable size and shape of its receptive ﬁeld, which allows an oper-ator in a deeper layer to observe an input image in the op-timal scale. Any kind of resizing modules in a deep neural network can be replaced by DynOPool with minimal cost.
Also, DynOPool controls the complexity of the model by in-troducing an additional loss term that constrains computa-tional cost. Our experiments show that the models equipped with the proposed learnable resizing module outperform the baseline algorithms on multiple datasets in image classiﬁ-cation and semantic segmentation. 1.

Introduction
Despite the unprecedented success of deep neural net-works in various applications including computer vi-sion [12, 24, 39, 40], natural language processing [6, 33], robotics [21], and bioinformatics [16], the design of the op-timal network architecture is still a challenging problem.
While several handcrafted models exhibit impressive per-formance in various domains, there have been substantial efforts to identify the optimal neural network architecture with associated operations automatically [17, 18, 22, 41].
However, hand-engineered architectures are prone to be suboptimal and suffer from weak generalizability while the approaches based on neural architecture search either incur a huge amount of training cost or achieve minor improve-ment due to limited search space.
Researchers have been investigating powerful and efﬁ-cient operations applicable to deep neural networks, which include convolutions, normalizations, and activation func-tions. However, they have not paid much attention to pool-ing operations despite their simplicity and effectiveness in aggregating local features. The size and shape of a re-ceptive ﬁeld are critical; too small or large a receptive
ﬁeld may not be able to effectively recognize large or small objects, respectively. The receptive ﬁeld is deter-mined by several factors in deep neural networks such as the depth of a model, strides of operations, types of con-volutions, etc. To design an efﬁcient receptive ﬁeld of an operation, variants of convolution operations [5, 29, 43] or special architectures with multi-resolution branches [11, 44] are widely adopted. However, these approaches rely on delicately human-engineered hyperparameters or time-consuming neural architecture search [46, 47].
To alleviate the suboptimality of human-engineered ar-chitectures and operations, we propose Dynamically Opti-mized Pooling operation (DynOPool), which is a learnable resizing module that replaces standard resizing operations.
The proposed module ﬁnds the optimal scale factor of the receptive ﬁeld for the operations learned on a dataset, and, consequently, resizes the intermediate feature maps in a net-work to proper sizes and shapes. This relieves us from the delicate design of hyperparameters such as stride of convo-lution ﬁlters and pooling operators.
Our contributions are summarized as follows:
• Our work tackles the limitations of existing scaling op-erators in deep neural networks that depend on pre-determined hyperparameters. We point out the impor-tance of ﬁnding the optimal spatial resolutions and re-ceptive ﬁelds in intermediate feature maps, which are still under-explored in designing neural architectures.
• We propose DynOPool, a learnable resizing module that ﬁnds the optimal scale factors and receptive ﬁelds of intermediate feature maps. DynOPool identiﬁes the best resolution and receptive ﬁeld of a certain layer us-ing a learned scaling factor and propagates the infor-mation to the subsequent ones leading to scale opti-mization across the entire network. 3. Motivation
• We demonstrate that the model with DynOPool outper-forms the baseline algorithms on multiple datasets and network architectures in the image classiﬁcation and semantic segmentation tasks. It also exhibits desirable trade-offs between accuracy and computational cost.
Our paper is organized as follows. Section 2 presents ex-isting related works and Section 3 introduces our motivation for optimizing the size and shape of the receptive ﬁeld and feature map. We describe the technical details of DynOPool in Section 4 and experimental results in Section 5. Last, we conclude this work and discuss future works in Section 6. 2.