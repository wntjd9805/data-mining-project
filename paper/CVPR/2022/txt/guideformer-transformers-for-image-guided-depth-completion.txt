Abstract
Depth completion has been widely studied to predict a dense depth image from its sparse measurement and a sin-gle color image. However, most state-of-the-art methods rely on static convolutional neural networks (CNNs) which are not ﬂexible enough for capturing the dynamic nature of input contexts. In this paper, we propose GuideFormer, a fully transformer-based architecture for dense depth com-pletion. We ﬁrst process sparse depth and color guidance images with separate transformer branches to extract hier-archical and complementary token representations. Each branch consists of a stack of self-attention blocks and has key design features to make our model suitable for the task.
We also devise an effective token fusion method based on guided-attention mechanism. It explicitly models informa-tion ﬂow between the two branches and captures inter-modal dependencies that cannot be obtained from depth or color image alone. These properties allow GuideFormer to enjoy various visual dependencies and recover precise depth values while preserving ﬁne details. We evaluate
GuideFormer on the KITTI dataset containing real-world driving scenes and provide extensive ablation studies. Ex-perimental results demonstrate that our approach signiﬁ-cantly outperforms the state-of-the-art methods. 1.

Introduction
Guided depth completion is the task of converting sparse depth observations to dense depth maps with the corre-sponding color image. This task has been drawing more and more research attention, thanks to its wide range of ap-plications in the computer vision ﬁeld, e.g., 3D scene map-ping [26] and 3D object detection [22] for robotic percep-tion and autonomous driving. However, commercial depth-sensing cameras (e.g. LiDAR sensors) suffer from their inherent drawbacks, including specular surfaces, quantiza-tion, occlusion, and noise. These properties make depth completion a challenging problem. To tackle depth comple-tion, a variety of methods, mostly based on deep convolu-This research was supported by the Next Generation R&D Program (912706601), funded by the Agency for Defense Development (ADD).
∗ Equal contribution, † Corresponding author tional neural networks (CNNs), have been proposed. Early works deal only with sparse depth input to estimate dense depth via sparsity invariant CNN [30] or auxiliary vision tasks [13, 18]. Recent works have shown a great success in using multi-modal information, including color images
[10, 15, 20, 31, 40] and surface normal [23, 36]. These meth-ods have achieved state-of-the-art performances over the conventional methods using only depth input. Nevertheless,
CNN-based methods show fundamental limitations by their basic building block, i.e., the static convolutional layers.
The interaction between convolution kernels and inputs is content-independent. Applying the same kernels to any re-gion might not be ﬂexible for adapting diverse and disparate spatial contexts. Furthermore, the convolution is not effec-tive for modeling long-range dependencies. Recently, there are several attempts to develop content-adaptive CNNs for depth completion. ACMNet [40] constructs a graph prop-agation based network, and learns graph afﬁnities by con-sidering color and depth information. GuideNet [29] pro-poses a guided-convolution module, dynamically predict-ing content-adaptive convolution kernels from color im-ages. However, these methods still use CNNs as the back-bone, which leaves room for further improvement.
In this paper, we propose GuideFormer, a dual-branch architecture that takes full advantage of attention mecha-nisms for depth completion. Each branch, consisting of a stack of modiﬁed self-attention blocks, embeds hierarchical and complementary tokens from sparse depth and color im-ages. This allows our model to better capture adaptive intra-modal dependencies through the whole completion process.
An effective token fusion method, called guided-attention module (GAM), is also introduced by extending the stan-dard self-attention mechanism. It models explicit informa-tion exchange among the two complementary branches and captures inter-modal dependencies from the learned cross-modal similarity. To the best of our knowledge, we are the
ﬁrst to apply fully transformer-based architecture for depth completion task. Experimental results on the KITTI bench-mark [30] demonstrate the effectiveness of the proposed method, which outperforms the state-of-the-art methods.
Our contributions can be summarized as follows:
• We propose a dual-branch and fully transformer-based architecture for depth completion task. It learns input-adaptive token representations from the sparse depth and color guidance images, respectively through the whole completion process. This allows us to reason about diverse intra-modal dependencies better than the existing static CNN-based methods.
• We introduce a guided-attention module (GAM) by ex-tending the standard self-attention mechanism. It cap-tures inter-modal dependencies and models informa-tion ﬂow between depth and color tokens. We show that GAM is a more powerful method of fusing multi-modal information, compared to simple concatenation
[10] or guided-convolution module [29].
• Our method outperforms the recent state-of-the-art ap-proaches on the KITTI benchmark. We also provide extensive ablation studies with both quantitative and qualitative experimental analyses. 2.