Abstract
Self-Attention is a widely used building block in neu-ral modeling to mix long-range data elements. Most self-attention neural networks employ pairwise dot-products to specify the attention coefficients. However, these meth-ods require O(N 2) computing cost for sequence length N .
Even though some approximation methods have been in-troduced to relieve the quadratic cost, the performance of the dot-product approach is still bottlenecked by the low-rank constraint in the attention matrix factorization. In this paper, we propose a novel scalable and effective mixing building block called Paramixer. Our method factorizes the interaction matrix into several sparse matrices, where we parameterize the non-zero entries by MLPs with the data elements as input. The overall computing cost of the new building block is as low as O(N log N ). Moreover, all fac-torizing matrices in Paramixer are full-rank, so it does not suffer from the low-rank bottleneck. We have tested the new method on both synthetic and various real-world long se-quential data sets and compared it with several state-of-the-art attention networks. The experimental results show that
Paramixer has better performance in most learning tasks. 1.

Introduction
Transformer models have been widely used on many tasks such as text classification [28], text summarization, promoter region prediction [33], and image classification
[10]. The main engine in Transformer is the self-attention mechanism, which can work in parallel to mix long-range tokens in a long sequence. This fundamental innovation eliminated the sequential dependency in recurrent neural networks and was used as a building block for many pow-erful models, such as Bert [9], GPT [6] and Ernie [25].
However, the original self-attention is not scalable be-cause it requires computing and storing all pairwise dot-products, which incurs O(N 2) cost for sequence length N .
*Equal contribution.
â€ Corresponding author. zhirong.yang@ntnu.no
The scalability issue significantly restricted the application of neural models based on self-attention.
Various methods have been introduced to alleviate the quadratic cost of full attention. Some of them attempt to shorten the sequence length [2, 29], even though much in-formation is lost. Others try to break up the softmax by a certain kernel factorization. Another family of meth-ods sparsify the attention matrix with predefined attention
[3, 4, 7, 27, 33]. However, most Transformer variants stick to the dot-product self-attention, of which the expressive power is restricted by the low-rank bottleneck [5] because the dimensionality of the dot-product space is much smaller than the sequence length. Therefore, they cannot accu-rately model the transformation if the attention is intrinsi-cally high-rank.
This paper proposes a scalable and effective attention building block called Paramixer without dot-product and softmax. Our method directly parameterizes the mixing links in several sparse factors to form an attention ma-trix, where all factorizing matrices are full-rank. Therefore
Paramixer does not suffer from the low-rank bottleneck. We present two ways to specify the non-zero positions in each sparse factor. Both lead to an economical approximation of the full attention matrix, with the computing cost as low as
O(N log N ). As a result, our method can easily model very long sequential data.
We have tested Paramixer on various sequence data sets and compared it with many popular self-attention neural networks based on dot-products. The experimental results show that Paramixer gets the best performance on very long sequence tasks, including synthetic data inference, Genome classification, and character-level long document classifica-tion. Paramixer also achieves state-of-art accuracy on the public Long Range Arena benchmark tasks.
We organize the rest of the paper as follows. Section 2 investigates dot-product self-attention and its related work.
Section 3 introduces the development clue and model archi-tecture of Paramixer. The experimental settings and results are presented in Section 4, and we conclude the paper in
Section 5.
2.