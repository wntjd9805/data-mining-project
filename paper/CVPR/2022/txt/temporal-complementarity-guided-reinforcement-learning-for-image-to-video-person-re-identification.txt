Abstract
Image-to-video person re-identification aims to retrieve the same pedestrian as the image-based query from a video-based gallery set. Existing methods treat it as a cross-modality retrieval task and learn the common latent em-beddings from image and video modalities, which are both less effective and efficient due to large modality gap and re-dundant feature learning by utilizing all video frames. In this work, we first regard this task as point-to-set matching problem identical to human decision process, and propose a novel Temporal Complementarity-Guided Reinforcement
Learning (TCRL) approach for image-to-video person re-identification. TCRL employs deep reinforcement learning to make sequential judgments on dynamically selecting suit-able amount of frames from gallery videos, and accumu-late adequate temporal complementary information among these frames by the guidance of the query image, towards balancing efficiency and accuracy. Specifically, TCRL for-mulates point-to-set matching procedure as Markov deci-sion process, where a sequential judgement agent measures the uncertainty between the query image and all historical frames at each time step, and verifies that sufficient comple-mentary clues are accumulated for judgment (same or dif-ferent) or one more frames are requested to assist judgment.
Moreover, TCRL maintains a sequential feature extraction module with complementary residual detectors to dynam-ically suppress redundant salient regions and thoroughly mine diverse complementary clues among these selected frames for enhancing frame-level representation. Extensive experiments demonstrate the superiority of our method. 1.

Introduction
Person re-identification (Re-ID) is the task of search-ing the target samples from the gallery set which have the
It has been same identity with the given query. [5, 35].
â€  Equal contribution.
* Corresponding author.
Figure 1.
Image-to-video person Re-ID essentially belongs to point-to-set matching problem, identical to human decision pro-cess for pedestrian matching. Due to abundant redundant infor-mation and even noisy information within consecutive frames, the best similarity score between a query image and a gallery video is often not obtained when employing all frames of the video. widely studied in the computer vision community during the past few years, due to its large potential in the intelli-gent surveillance, video analysis and human-robot interac-tion, etc [5, 42]. It is a quite challenging task, derived from the dramatic variations in camera viewpoint, body pose, il-lumination, as well as the influence of cluttered background and partial occlusion.
In general, person Re-ID can be mainly divided into two categories: image-based Re-ID [8, 19, 43] and video-based Re-ID [3, 36, 39]. The main difference between them is that for the former, the query and gallery samples are both images, while the query and gallery samples are both videos for the latter. In these two categories, the samples to be matched are homogeneous. Recently, image-based and video-based Re-ID have achieved impressive progress, ben-efiting from the development of deep learning technique.
However, in many practical scenarios, person Re-ID re-quires to find the target pedestrian in numerous videos ac-cording to a query image. One situation is that, given an im-age of a criminal, the person Re-ID system should retrieve the criminal across multiple non-overlap video sequences.
Such brings out an emerging task, i.e., image-to-video (I2V) person re-identification [7, 28, 33].
Contrary to image and video based Re-ID, I2V Re-ID is more challenging due to the information asymmetry be-tween images and videos. Videos contain plenty of tem-poral information across time dimension, which results in feature distribution discrepancy and increasing the diffi-culty of measuring the similarity scores between image and video samples [25]. To tackle with this issue, existing I2V
Re-ID methods dedicate to 1) project images and videos into a common embedding space by distance metric learn-ing [27, 30, 33, 47, 48] or 2) propagate the temporal knowl-edge learned from the video representation network to the image representation network via temporal knowledge dis-tillation [7, 25, 28]. However, the aforementioned methods are both less effective and efficient. They simply treat I2V
Re-ID as a cross-modality retrieval task, and enforce image and video features to resemble each other even though the image is completely different from the video due to lack-ing temporal dimension. Moreover, as illustrated in Fig-ure 1, video sequences often mingle vast redundant appear-ance clues and noisy information, these methods directly exploit all frames of videos without discovering discrimi-native complementary information among them and avoid-ing the interference of noisy information, leading to poor feature representation and inefficient model.
In this work, we first regard this task as point-to-set matching problem that is the same with human decision process, and propose a novel Temporal Complementarity-Guided Reinforcement Learning (TCRL) approach for image-to-video person re-identification. TCRL exploits deep reinforcement learning to accumulate adequate com-plementary information from suitable amount of frames by making sequential judgements on the query images and the gallery videos, towards balancing efficiency and accuracy.
Concretely, TCRL formulates point-to-set matching proce-dure as Markov decision process, where a sequential judge-ment agent measures the uncertainty between the image feature and the frame-level feature containing the tempo-ral complementary information of all historical frames at each time step, and learns the optimal policy to make the judgment that the model have collected enough evidence for identifying the same pedestrian and distinguishing differ-ent pedestrians, or requests one more video frames to assist recognizing. In addition, TCRL designs a sequential feature extraction module with complementary residual detectors to improve the capacity of the frame-level feature by absorb-ing the diverse complementary information among these se-lected video frames. The complementary residual detec-tor learns the most salient features that have been activated in previous frames by a multi-head attention mechanism, which are then utilized as the salient convolutional ker-nel to estimate the suppression masks for other subsequent frames. The suppression masks restrain the common salient information and thoroughly discover the remaining poten-tial discriminative information of other subsequent frames.
Extensive experiments on two benchmarks demonstrate the effectiveness and efficiency of our method, surpassing the state-of-the-art methods by a large margin.
The main contributions of this paper are as following: (1) We first regard I2V Re-ID as point-to-set matching problem, and propose a novel Temporal Complementarity-Guided Reinforcement Learning (TCRL) approach, to-wards achieving both efficiency and accuracy. (2) We for-mulate point-to-set matching procedure as Markov decision process, and train an agent to make sequential judgments on adaptively selecting suitable amount of frames from gallery videos by the guidance of a query image for recognizing pedestrians. (3) We design a sequential feature extraction module with complementary residual detectors to dynami-cally suppress common salient information and thoroughly mine potential complementary clues among these selected video frames for enhancing the capacity of frame-level fea-tures of pedestrians. 2.