Abstract
Pose Guided Person Image Generation (PGPIG) is the task of transforming a person image from the source pose to a given target pose. Most of the existing methods only focus on the ill-posed source-to-target task and fail to cap-ture reasonable texture mapping. To address this problem, we propose a novel Dual-task Pose Transformer Network (DPTN), which introduces an auxiliary task (i.e., source-to-source task) and exploits the dual-task correlation to pro-mote the performance of PGPIG. The DPTN is of a Siamese structure, containing a source-to-source self-reconstruction branch, and a transformation branch for source-to-target generation. By sharing partial weights between them, the knowledge learned by the source-to-source task can effec-tively assist the source-to-target learning. Furthermore, we bridge the two branches with a proposed Pose Trans-former Module (PTM) to adaptively explore the correla-tion between features from dual tasks. Such correlation can establish the fine-grained mapping of all the pixels be-tween the sources and the targets, and promote the source texture transmission to enhance the details of the gen-erated target images. Extensive experiments show that our DPTN outperforms state-of-the-arts in terms of both
PSNR and LPIPS. In addition, our DPTN only contains 9.79 million parameters, which is significantly smaller than other approaches. Our code is available at: https:// github.com/PangzeCheung/Dual-task-Pose-Transformer-Network. 1.

Introduction
Pose Guided Person Image Generation (PGPIG) aims to generate person images with arbitrary given poses. It has various applications such as e-commerce, film special ef-fects, person re-identification [5–7, 19, 34, 35, 40, 41], etc.
Due to the significant changes in texture and geometry dur-*Corresponding Author
Figure 1. Visual comparison of our method with other approaches, including vanilla CNN based [22], attention based [45], optical flow based [24], and parsing map based [21] method. Compared with other methods, our model can generate more realistic images. ing the pose transfer, PGPIG is still a challenging task.
Driven by the improvement of generative models, e.g.,
Generative Adversarial Networks (GANs) [8] and Varia-tional Autoencoders (VAEs) [17], PGPIG has made great progress. However, early works [4, 22] are built on vanilla
Convolutional Neural Networks (CNNs), which lack the ca-pability to perform complex geometry transformations [13] (see Fig. 1 (c)). To tackle this problem, attention mech-anisms [30, 45] and optical flow [18, 24, 29] are applied to improve spatial transformation abilities. Some meth-ods [21,39] introduce additional labels such as human pars-ing maps to provide semantic guidance for pose variations.
However, the above mentioned methods solely focus on training the generator G on the Source-to-Target Task that transforms the source image xs from the source pose ps to the target pose pt: G(xs, ps, pt) = ˜xt. This is an ill-posed problem, making it arduous to train a robust genera-tor. Moreover, the existing methods cannot well capture the reasonable texture mapping between the source and target
images, especially when the person undergoes large pose changes. Therefore, those methods often produce unrealis-tic images, as shown in Fig. 1 (d)-(f).
In this paper, we seek to utilize an auxiliary task [26] to improve the ill-posed source-to-target transformation.
Here, we instantiate the auxiliary task as the Source-to-Source Task, which reconstructs the source image guided by source pose: G(xs, ps, ps) = ˜xs. We observe that si-multaneously learning the dual tasks (i.e., source-to-target task and source-to-source task) has the following two bene-fits: (1) Compared with the source-to-target task, the pixel-aligned source-to-source task is easier to learn because it does not require complex spatial transformations. By shar-ing weights between the dual tasks, the source-to-source task can not only exploit its knowledge to assist the source-to-target task, but also stabilize the training of the whole network. (2) Since the intermediate features in dual tasks are associated with their generated images ˜xs and ˜xt re-spectively, we can further explore the correlation between the dual tasks to establish the texture transformation from the sources to the targets. In this way, the natural source textures can be readily disseminated to enhance the details of the generated target image.
Based on these ideas, we propose a novel Dual-task Pose
Transformer Network (DPTN) for PGPIG. The architecture of DPTN is shown in Fig. 2. Specifically, our DPTN is of a Siamese structure, incorporating two branches: a self-reconstruction branch for the auxiliary source-to-source task and a transformation branch for the source-to-target task. These two branches share partial weights, and are trained simultaneously with different loss functions. By this means, the knowledge learned by the source-to-source task can directly assist the optimization of the source-to-target task. To explore the correlation between the dual tasks, we bridge the two branches with a novel Pose Trans-former Module (PTM). Our PTM consists of several Con-text Augment Blocks (CABs) and Texture Transfer Blocks (TTBs). CABs first selectively gather the information of the source-to-source task. Then TTBs gradually capture the fine-grained correlation between the features from the dual tasks. With the help of such correlation, TTBs can produc-tively promote the texture transmission from the real source image to the source-to-target task, enabling the synthetic image to preserve more source appearance details. (see
Fig. 1 (g)). In sum, the main contributions are:
• We propose a novel Dual-task Pose Transformer Net-work (DPTN), which introduces an auxiliary task (i.e., source-to-source task) by Siamese architecture and ex-ploits its knowledge to improve the PGPIG.
• We design a Pose Transformer Module (PTM) to ex-plore the dual-task correlation. Such correlation can not only establish the fine-grained mapping between the sources and the targets, but also effectively guide the source texture transmission to further refine the feature in the source-to-target task.
• Results on the two benchmarks, i.e., DeepFashion [20] and Market-1501 [43], have demonstrated that our method exhibits superior performance on PSNR and
LPIPS [42]. Moreover, our model only contains 9.79 million parameters, which is relatively 91.6% smaller than the state-of-the-art method SPIG [21]. 2.