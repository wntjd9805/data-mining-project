Abstract
Convolutional neural networks have made significant progresses in edge detection by progressively exploring the context and semantic features. However, local details are gradually suppressed with the enlarging of receptive fields. Recently, vision transformer has shown excellent capability in capturing long-range dependencies. Inspired by this, we propose a novel transformer-based edge de-tector, Edge Detection TransformER (EDTER), to extract clear and crisp object boundaries and meaningful edges by exploiting the full image context information and de-tailed local cues simultaneously. EDTER works in two stages.
In Stage I, a global transformer encoder is used to capture long-range global context on coarse-grained im-age patches. Then in Stage II, a local transformer encoder works on fine-grained patches to excavate the short-range local cues. Each transformer encoder is followed by an elaborately designed Bi-directional Multi-Level Aggrega-tion decoder to achieve high-resolution features. Finally, the global context and local cues are combined by a Feature
Fusion Module and fed into a decision head for edge pre-diction. Extensive experiments on BSDS500, NYUDv2, and
Multicue demonstrate the superiority of EDTER in compar-ison with state-of-the-arts. The source code is available at https://github.com/MengyangPu/EDTER. 1.

Introduction
Edge detection is one of the most fundamental problems in computer vision and has a wide variety of applications, such as image segmentation [8, 23, 39, 44, 45, 47], object detection [23], and video object segmentation [5, 57, 59].
Given an input image, edge detection aims to extract ac-curate object boundaries and visually salient edges.
It is challenging due to many factors including complex back-grounds, inconsistent annotations, and so on.
*Corresponding author.
Figure 1. Examples of edge detection. Our method, EDTER, extracts clear boundaries and edges by exploiting both global and local cues. (a, e): Input images from BSDS500 [1]. (b,f): Detected edges by EDTER. (c,d,g,h): Zoomed-in patches.
Edge detection is closely related to the context and se-mantic image cues.
It is thus crucial to obtain appropri-ate representation to capture both high and low level visual cues. Traditional methods [6,14,28,34,41,63] mostly obtain edges based on low-level local cues, e.g., color and texture.
Benefiting from the effectiveness of convolutional neural networks (CNNs) in learning semantic features, significant progress has been made for edge detection [3,4,29,48]. The
CNN features progressively capture global and semantic-aware visual concepts with the enlargement of the receptive fields, while many essential fine details are inevitably and gradually lost at the mean time. To include more details, methods in [22,36,37,65,66] aggregate the features of deep and shallow layers. However, such shallow features reflect mainly local intensity variation without considering seman-tic context, resulting in noisy edges.
Inspired by the recent success of vision transformers [9, 16, 61, 72], especially their capability of modeling long-range contextual information, we propose to tailor trans-formers for edge detection. Two main challenges, however, need to be solved. Firstly, transformers are often applied to patches with a relatively large size due to computation concerns, while coarse-grained patches are unfavorable for learning accurate features for edges. It is crucial to perform self-attention on fine-grained patches without increasing the computational burden. Second, as shown in Fig. 1 (d), ex-tracting precise edges from intersected and thin objects is challenging. So it is necessary to design an effective de-coder for generating edge-aware high-resolution features.
To address the above issues, we develop a two-stage framework (Fig. 2), named Edge Detection TransformER (EDTER), to explore global context information and ex-In stage I, we cavate fine-grained cues in local regions. split the image into coarse-grained patches and run a global transformer encoder on them to capture long-range global context. Then, we develop a novel Bi-directional Multi-Level Aggregation (BiMLA) decoder to generate high-resolution representations for edge detection. In stage II, we first divide the whole image into multiple sequences of fine-grained patches by sampling with a non-overlapping sliding window. Then a local transformer works on each sequence in turn to explore the short-range local cues. Afterward, all local cues are integrated and fed into a local BiMLA de-coder to achieve the pixel-level feature maps. Finally, the information from both stages is fused by a Feature Fusion
Module (FFM) and then is fed into a decision head to pre-dict the final edge map. With the above efforts, EDTER can generate crisp and less noisy edge maps (Fig. 1).
Our contributions are summarized as follows: (1) We propose a novel transformer-based edge detector, Edge De-tection TransformER (EDTER), to detect object contours and meaningful edges in natural images. To our best knowl-edge, it is the first transformer-based edge detection model. (2) EDTER is designed to effectively explore long-range global context (Stage I) and capture fine-grained local cues (Stage II). Moreover, we propose a novel Bi-directional
Multi-Level Aggregation (BiMLA) decoder to boost the in-(3) To effectively in-formation flow in the transformer. tegrate the global and local information, we use a Fea-ture Fusion Module (FFM) to fuse the cues extracted from
Stage I and Stage II. (4) Extensive experiments demonstrate the superiority of EDTER over the state-of-the-art methods on three well-known edge detection benchmarks, including
BSDS500, NYUDv2, and Multicue. 2.