Abstract
Image hashing is a principled approximate nearest neighbor approach to find similar items to a query in a large collection of images. Hashing aims to learn a binary-output function that maps an image to a binary vector. For optimal retrieval performance, producing balanced hash codes with low-quantization error to bridge the gap between the learn-ing stage’s continuous relaxation and the inference stage’s discrete quantization is important. However, in the exist-ing deep supervised hashing methods, coding balance and low-quantization error are difficult to achieve and involve several losses. We argue that this is because the existing quantization approaches in these methods are heuristically constructed and not effective to achieve these objectives.
This paper considers an alternative approach to learning the quantization constraints. The task of learning bal-anced codes with low quantization error is re-formulated as matching the learned distribution of the continuous codes to a pre-defined discrete, uniform distribution. This is equiv-alent to minimizing the distance between two distributions.
We then propose a computationally efficient distributional distance by leveraging the discrete property of the hash functions. This distributional distance is a valid distance and enjoys lower time and sample complexities. The pro-posed single-loss quantization objective can be integrated into any existing supervised hashing method to improve code balance and quantization error. Experiments confirm that the proposed approach substantially improves the per-formance of several representative hashing methods. 1.

Introduction
An important challenge associated with massive im-age datasets is to efficiently and effectively search for images containing semantically similar content in these datasets. Hashing is a principled approximate nearest neighbor search approach with applications in many do-mains, ranging from text or image retrieval [21, 46] to spam or duplicate-scene detection [8, 37]. Hashing approaches learn binary encoding of the original images so that the
“candidate” subset of images can be efficiently discovered from the binary-coding space. Binary codes are efficient to store and the high cost of pairwise distance calcula-tions in the high-dimensional space is reduced to the sig-nificantly lower cost of discrete Hamming distance calcu-lations. A Hamming-distance calculation only requires a bit-wise XOR and a bit-count operation, which can be effi-ciently computed in most conventional systems.
To ensure that the retrieved images are relevant, hash-ing methods learn hash functions that preserve the pair-wise similarity of the images in the discrete space. Su-pervised hashing methods additionally leverage the anno-tated similarity to learn the hash functions and achieve supe-rior retrieval performance compared to unsupervised hash-ing methods [4, 5, 18, 39, 47, 49, 53, 56, 59]. Since discrete optimization is intractable, these methods solve a relaxed problem that replaces the discrete constraint with a continu-ous output. The continuous output is “quantized” to obtain the binary during inference. Such a relaxation results in a discrepancy between the discrete and continuous optimiza-tions that must be compensated for in the learning process.
Two important criteria to consider are quantization error and coding balance [21, 51, 52]. Quantization error is the infor-mation loss when the discrete function is represented by a continuous function. Quantization error penalizes the cases of assigning “very” similar data points to binary codes with large distances [52]. Coding balance, on the other hand, encourages a uniform distribution of the images into the bi-nary codes, which helps reduce the time complexity of the retrieval operations in the worst and average scenarios [23].
Existing supervised hashing methods, especially those that are based on neural networks, include one or more to penalty terms, besides the similarity-preserving loss, force the continuous output as discrete as possible. How-ever, these relaxation schemes still introduce non-trivial
quantization error and coding unbalanced, which eventually leads to sub-optimal hash codes [51, 52]. Multiple penalty terms also lead to longer model trainings due to the time-consuming hyperparameter-tuning step.
This paper proposes a faster and more performant quan-tization approach for the deep supervised hashing methods.
First, we empirically show that low-quantization error and balance coding induce a uniform discrete distribution. The ultimate goal of hash-function learning is to project data into this uniform discrete distribution. Thus, we formulate the quantization objective as minimizing the distributional distance between the relaxed, continuous hash distribution and this uniform discrete distribution. The proposed formu-lation has two advantages: (i) achieving low-quantization error and coding balance is easier with this formulation and (ii) low-quantization error and coding balance are simulta-neously optimized in a unified formulation, thus reducing the number of hyperparameters to tune. Our main contri-butions are as follows:
• We achieve low quantization error and coding balance by minimizing the single-loss distributional distance between the learned hash distribution and the uniform discrete distribution. This new quantization objective can be used in conjunction with any existing deep su-pervised hashing methods to further improve their re-trieval performances and reduce their time-consuming hyperparameter tuning processes.
• We propose a low computation-and sample-complexity Sliced-Wasserstein-based distributional distance. The proposed distance, called HSWD, is the-oretically a valid distance with better computational ef-ficiency than other Wasserstein-distance types, includ-ing the original Sliced Wasserstein Distance.
• We demonstrate the efficiency and effectiveness of the proposed quantization technique in several well-known deep supervised hashing methods on various widely used real-world datasets using both quantitative and qualitative performance analysis.
Paper Organization: The rest of the paper is organized as follows. We review the related work in Section 2.
In
Section 3, we present the empirical analysis of the quan-tization error and coding balance, and the details of the proposed methodology. We evaluate the effectiveness of the proposed quantization approach in Section 4. Finally,
Section 5 presents remarks and concludes this paper. We present more details about experimental settings and results as well as supporting proofs in the supplementary material. 2.