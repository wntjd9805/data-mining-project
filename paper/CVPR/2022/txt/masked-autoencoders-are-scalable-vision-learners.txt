Abstract
This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our
MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that oper-ates only on the visible subset of patches (without mask to-kens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we ﬁnd that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task. Coupling these two de-signs enables us to train large models efﬁciently and ef-fectively: we accelerate training (by 3× or more) and im-prove accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla
ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer per-formance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior. 1.

Introduction
Deep learning has witnessed an explosion of archi-tectures of continuously growing capability and capacity
[33, 25, 57]. Aided by the rapid gains in hardware, mod-els today can easily overﬁt one million images [13] and begin to demand hundreds of millions of—often publicly inaccessible—labeled images [16].
This appetite for data has been successfully addressed in natural language processing (NLP) by self-supervised pre-training. The solutions, based on autoregressive language modeling in GPT [47, 48, 4] and masked autoencoding in
BERT [14], are conceptually simple: they remove a portion of the data and learn to predict the removed content. These methods now enable training of generalizable NLP models containing over one hundred billion parameters [4].
The idea of masked autoencoders, a form of more gen-eral denoising autoencoders [58], is natural and applicable in computer vision as well. Indeed, closely related research
Figure 1. Our MAE architecture. During pre-training, a large random subset of image patches (e.g., 75%) is masked out. The encoder is applied to the small subset of visible patches. Mask tokens are introduced after the encoder, and the full set of en-coded patches and mask tokens is processed by a small decoder that reconstructs the original image in pixels. After pre-training, the decoder is discarded and the encoder is applied to uncorrupted images (full sets of patches) for recognition tasks. in vision [59, 46] preceded BERT. However, despite signif-icant interest in this idea following the success of BERT, progress of autoencoding methods in vision lags behind
NLP. We ask: what makes masked autoencoding different between vision and language? We attempt to answer this question from the following perspectives: (i) Until recently, architectures were different. In vision, convolutional networks [34] were dominant over the last decade [33]. Convolutions typically operate on regular grids and it is not straightforward to integrate ‘indicators’ such as mask tokens [14] or positional embeddings [57] into con-volutional networks. This architectural gap, however, has been addressed with the introduction of Vision Transform-ers (ViT) [16] and should no longer present an obstacle. (ii) Information density is different between language and vision. Languages are human-generated signals that are highly semantic and information-dense. When training a model to predict only a few missing words per sentence, this task appears to induce sophisticated language under-standing. Images, on the contrary, are natural signals with heavy spatial redundancy—e.g., a missing patch can be re-covered from neighboring patches with little high-level un-Figure 2. Example results on ImageNet validation images. For each triplet, we show the masked image (left), our MAE reconstruction† (middle), and the ground-truth (right). The masking ratio is 80%, leaving only 39 out of 196 patches. More examples are in the appendix.
†As no loss is computed on visible patches, the model output on visible patches is qualitatively worse. One can simply overlay the output with the visible patches to improve visual quality. We intentionally opt not to do this, so we can more comprehensively demonstrate the method’s behavior.
Figure 3. Example results on COCO validation images, using an MAE trained on ImageNet (the same model weights as in Figure 2).
Observe the reconstructions on the two right-most examples, which, although different from the ground truth, are semantically plausible. derstanding of parts, objects, and scenes. To overcome this difference and encourage learning useful features, we show that a simple strategy works well in computer vision: mask-ing a very high portion of random patches. This strategy largely reduces redundancy and creates a challenging self-supervisory task that requires holistic understanding beyond low-level image statistics. To get a qualitative sense of our reconstruction task, see Figures 2 – 4. (iii) The autoencoder’s decoder, which maps the latent representation back to the input, plays a different role be-tween reconstructing text and images. In vision, the decoder reconstructs pixels, hence its output is of a lower semantic level than common recognition tasks. This is in contrast to language, where the decoder predicts missing words that contain rich semantic information. While in BERT the de-coder can be trivial (an MLP) [14], we found that for im-ages, the decoder design plays a key role in determining the semantic level of the learned latent representations.
Driven by this analysis, we present a simple, effective, and scalable form of a masked autoencoder (MAE) for visual representation learning. Our MAE masks random patches from the input image and reconstructs the missing patches in the pixel space. It has an asymmetric encoder-decoder design. Our encoder operates only on the visible subset of patches (without mask tokens), and our decoder is lightweight and reconstructs the input from the latent rep-resentation along with mask tokens (Figure 1). Shifting the mask tokens to the small decoder in our asymmetric encoder-decoder results in a large reduction in computation.
Under this design, a very high masking ratio (e.g., 75%) can achieve a win-win scenario: it optimizes accuracy while al-lowing the encoder to process only a small portion (e.g., 25%) of patches. This can reduce overall pre-training time by 3× or more and likewise reduce memory consumption, enabling us to easily scale our MAE to large models.
Our MAE learns very high-capacity models that gen-eralize well. With MAE pre-training, we can train data-hungry models like ViT-Large/-Huge [16] on ImageNet-1K with improved generalization performance. With a vanilla
ViT-Huge model, we achieve 87.8% accuracy when ﬁne-tuned on ImageNet-1K. This outperforms all previous re-sults that use only ImageNet-1K data. We also evaluate transfer learning on object detection, instance segmentation, and semantic segmentation. In these tasks, our pre-training achieves better results than its supervised pre-training coun-terparts, and more importantly, we observe signiﬁcant gains by scaling up models. These observations are aligned with those witnessed in self-supervised pre-training in NLP
[14, 47, 48, 4] and we hope that they will enable our ﬁeld to explore a similar trajectory.
Self-supervised learning approaches have seen signiﬁcant interest in computer vision, often focusing on different pre-text tasks for pre-training [15, 61, 42, 70, 45, 17]. Re-cently, contrastive learning [3, 22] has been popular, e.g.,
[62, 43, 23, 7], which models image similarity and dis-similarity (or only similarity [21, 8]) between two or more views. Contrastive and related methods strongly depend on data augmentation [7, 21, 8]. Autoencoding pursues a con-ceptually different direction, and it exhibits different behav-iors as we will present. 3. Approach
Our masked autoencoder (MAE) is a simple autoencod-ing approach that reconstructs the original signal given its partial observation. Like all autoencoders, our approach has an encoder that maps the observed signal to a latent representation, and a decoder that reconstructs the origi-nal signal from the latent representation. Unlike classical autoencoders, we adopt an asymmetric design that allows the encoder to operate only on the partial, observed signal (without mask tokens) and a lightweight decoder that re-constructs the full signal from the latent representation and mask tokens. Figure 1 illustrates the idea, introduced next.
Masking. Following ViT [16], we divide an image into reg-ular non-overlapping patches. Then we sample a subset of patches and mask (i.e., remove) the remaining ones. Our sampling strategy is straightforward: we sample random patches without replacement, following a uniform distribu-tion. We simply refer to this as “random sampling”.
Random sampling with a high masking ratio (i.e., the ra-tio of removed patches) largely eliminates redundancy, thus creating a task that cannot be easily solved by extrapolation from visible neighboring patches (see Figures 2 – 4). The uniform distribution prevents a potential center bias (i.e., more masked patches near the image center). Finally, the highly sparse input creates an opportunity for designing an efﬁcient encoder, introduced next.
MAE encoder. Our encoder is a ViT [16] but applied only on visible, unmasked patches. Just as in a standard ViT, our encoder embeds patches by a linear projection with added positional embeddings, and then processes the resulting set via a series of Transformer blocks. However, our encoder only operates on a small subset (e.g., 25%) of the full set.
Masked patches are removed; no mask tokens are used.
This allows us to train very large encoders with only a frac-tion of compute and memory. The full set is handled by a lightweight decoder, described next.
MAE decoder. The input to the MAE decoder is the full set of tokens consisting of (i) encoded visible patches, and (ii) mask tokens. See Figure 1. Each mask token [14] is a shared, learned vector that indicates the presence of a miss-Figure 4. Reconstructions of ImageNet validation images using an MAE pre-trained with a masking ratio of 75% but applied on inputs with higher masking ratios. The predictions differ plausibly from the original images, showing that the method can generalize. 2.