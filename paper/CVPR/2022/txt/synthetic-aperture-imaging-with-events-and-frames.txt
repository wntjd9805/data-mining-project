Abstract
The Event-based Synthetic Aperture Imaging (E-SAI) has recently been proposed to see through extremely dense occlusions. However, the performance of E-SAI is not con-sistent under sparse occlusions due to the dramatic de-crease of signal events. This paper addresses this problem by leveraging the merits of both events and frames, leading to a fusion-based SAI (EF-SAI) that performs consistently under the different densities of occlusions. In particular, we first extract the feature from events and frames via multi-modal feature encoders and then apply a multi-stage fusion network for cross-modal enhancement and density-aware feature selection. Finally, a CNN decoder is employed to generate occlusion-free visual images from selected fea-tures. Extensive experiments show that our method effec-tively tackles varying densities of occlusions and achieves superior performance to the state-of-the-art SAI methods.
Codes and datasets are available at https://github. com/smjsc/EF-SAI 1.

Introduction
The Event-based Synthetic Aperture Imaging (E-SAI)
[34, 35] has been recently proposed for occlusion removal, benefiting from the low latency and the high dynamic range of events.
It shows promising performance, espe-cially when facing extremely dense occlusions, as shown in Fig. 1. Unlike the frame-based SAI (F-SAI) collecting the light information from conventional frame-based cam-eras [8, 15, 16, 22, 33], the E-SAI collects the light infor-mation with events asynchronously triggered by brightness
*Equal contribution
†Corresponding author
The research was partially supported by the National Natural Science
Foundation of China under Grants 61871297, the Natural Science Founda-tion of Hubei Province, China under Grant 2021CFB467, the Fundamental
Research Funds for the Central University under Grant 2042020kf0019, and the National Natural Science Foundation of China Enterprise Innova-tion Development Key Project under Grant U19B2004.
E x t r e m e l y
D e n s e
D e n s e r
S p a r s e r
Occluded scenes
F-SAI+CNN
E-SAI+Hybrid EF-SAI (Ours)
Figure 1. Qualitative comparisons of F-SAI+CNN [27], E-SAI+Hybrid [35] and our proposed EF-SAI under different den-sities of occlusions. The EF-SAI can reconstruct high-quality occlusion-free images under either sparse or dense occlusions by exploiting the information from both events and frames. contrast [34, 35] in almost continuous viewpoints. Existing
E-SAI approaches have shown the superiority of restoring clear images from the occluded light field with extremely dense occlusions. However, their performance degrades dramatically when occlusions become sparse (see Fig. 1).
The principle of existing E-SAI is to predict the light field of occluded targets via accumulating events triggered by occlusion-to-target contrast, which is proportional to the intensity of occluded targets [35]. Thus, E-SAI achieves high performance in occluded scenes with dense occlu-sions, where the signal events caused by target-to-occlusion contrast is dominant. However, when occlusions become sparse, the performance of existing E-SAI will inevitably deteriorate as signal events decrease.
To deal with the SAI problem under sparse occlusions, one can alternatively employ the prior event-based imag-ing models [2, 17, 25] to firstly recover intensity frames from events triggered by targets and then utilize the F-SAI pipeline for occlusion removal. However, event cam-eras only respond to intensity contrasts theoretically until the brightness change reaches the triggering threshold [20].
Thus, the collected events may only contain light infor-mation from high contrast textures (mostly around sharp edges), leading to the missing of low contrast textures in the final SAI reconstructions if only based on events.
Motivated by the high performance of F-SAI in deal-ing with sparse occlusions, we propose a novel fusion-based SAI method (EF-SAI). The EF-SAI exploits inten-sity frames to address above problems of E-SAI, achiev-ing high-quality reconstruction performance invariant to the density of occlusions. However, F-SAI may suffer from performance deterioration when facing dense occlusions
[27, 34, 35]. Both E-SAI and F-SAI exhibit inconsistent performance under varying occlusion densities in the real-world scenarios. Thus, the main challenge of EF-SAI is straightforward, i.e., how to take the merits of both events and frames to bridge the gap between E-SAI and F-SAI, and finally achieve the see-through effects with performance in-variant to occlusion densities?
To this end, we propose a novel EF-SAI-Net for high-quality reconstruction based on the fusion of events and frames. The network is based on the Encoder-Decoder ar-chitecture to encode and fuse the multi-modal signals and then decode an intensity frame without occlusions. Specif-ically, a two-stage fusion mechanism in our network is pro-posed for feature enhancement and adaptive fusion, com-posed of a cross-modal enhancement module and a density-aware fusion module. The cross-modal enhancement mod-ule aims to suppress event noises and occlusion distur-bances by mutual compensations between features of dif-ferent modalities, i.e., events and frames. And we employ a cross-attention-based swin transformer to achieve this end.
On the other hand, events and frames have their advantages under specific conditions, e.g., different occlusion densities or lighting conditions. Thus, the density-aware fusion mod-ule aims to adaptively select the features with high confi-dence according to the occlusion densities. To this end, the multi-modal inputs are directly fed into a channel attention block to provide the information about occlusion densities, serving as a guidance for the feature selection procedure.
The main contributions of this paper are three-fold.
• We propose a novel fusion-based SAI, i.e., EF-SAI, which takes the merits of both E-SAI and F-SAI, to achieve high-quality imaging performance invariant to occlusion densities.
• We propose a deep neural network, i.e., EF-SAI-Net, to reconstruct the image of targets from the occluded events and frames, where a cross-attention module is introduced to suppress noises and disturbances, and a density-aware feature selection module is proposed for adaptive fusions.
• We train our proposed EF-SAI-Net on a new EF-SAI dataset which contains various targets under occlu-sions with different densities. Extensive evaluations show that our method is superior to existing SAI ap-proaches and exhibits consistent performance with re-spect to different occlusion densities. 2.