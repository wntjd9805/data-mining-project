Abstract
Due to the limited utilization of temporal relations in video re-id, the frame-level attention regions of mainstream methods are partial and highly similar. To address this problem, we propose a Salient-to-Broad Module (SBM) to enlarge the attention regions gradually. Speciﬁcally, in SBM, while the previous frames have focused on the the later frames tend to focus on most salient regions, broader regions.
In this way, the additional information in broad regions can supplement salient regions, incurring more powerful video-level representations. To further im-prove SBM, an Integration-and-Distribution Module (IDM) is introduced to enhance frame-level representations. IDM
ﬁrst integrates features from the entire feature space and then distributes the integrated features to each spatial lo-cation. SBM and IDM are mutually beneﬁcial since they enhance the representations from video-level and frame-level, respectively. Extensive experiments on four preva-lent benchmarks demonstrate the effectiveness and supe-riority of our method. The source code is available at https://github.com/baist/SINet. 1.

Introduction
In the past few years, video person re-identiﬁcation (re-id) has achieved favorable progress [33,39] with the help of
CNNs [11, 19]. However, further development of video re-id remains hindered because it is challenging to effectively utilize the rich temporal information among video frames, as pointed out in [26].
Recently, some approaches [41, 42, 44] try to exploit the temporal relations for a mutual enhancement between frames. To realize such enhancement, these methods mainly adopt self-attention mechanism [38] or Graph Convolution
Networks (GCNs) [5, 8] to encourage the information ﬂow among video frames. In this way, the ﬁnal frame-level fea-Figure 1. Comparison of existing methods and our method (SBM) with respect to Class Activation Maps (CAM) [32]. There is a clear salient-to-broad transition of activation maps (attention re-gions) in our method. Warmer color represents a higher value. tures will be more rich and recognizable. Although these methods have achieved encouraging performances in video re-id, they still have several intrinsic drawbacks.
First, for each frame, the concentration of these meth-ods is usually conﬁned in a salient but partial region. In fact, when a model has focused on a partial region that can recognize a pedestrian, it will not pay attention to other re-gions, which results in representations with limited power.
Obviously, this property should be avoided for a robust re-id model since it is desirable to use the complete charac-teristics of a given pedestrian. As shown in Figure 1(b), these methods pay almost all the attention to the upper clothes while ignoring the A-line skirt and other human parts. Maybe the black skirt is not as discriminative as the upper clothes in this particular example, but it is still a vital clue, especially when other pedestrians, if any, wear similar upper clothes. Therefore, enlarging the attention regions is of central importance to further enhance the robustness and discriminative ability of video embeddings.
Second, the utilization of temporal relations, i.e., mu-tual enhancement between frames, is limited. Speciﬁcally, these methods regard the temporal relations as mutual en-hancement or homogeneous information ﬂow across all
In this way, the frame-level embeddings will be frames. richer since they contain mutually enhanced information from other frames. However, such enhancement will also mix the frame-level embeddings, making them more similar to each other, even redundant. As illustrated in Figure 1(b), all four frames focus on nearly identical regions, indicat-ing that their embeddings are highly similar. The similar-ity or redundancy sacriﬁces the differences across frames, which limits further improvement in the ﬁnal temporal fu-sion stage. Therefore, to make better use of temporal rela-tions, it is desirable to leverage temporal cues from another perspective and encourage the differences between frames.
In this paper, we propose a Salient-to-Broad Module (SBM), which achieves the above two goals in a uniﬁed framework. SBM innovatively leverages the temporal re-lations to amplify the differences of frames, i.e., gradu-ally enlarges the attention regions of consecutive frames.
Speciﬁcally, we expect the pedestrians’ representations to be more informative and powerful, so they should contain as much foreground information as possible. While the pre-vious frames have focused on a salient but partial region, we require SBM to pay attention to a broader region for the later frame. In practice, SBM leverages temporal rela-tions via difference ampliﬁcation, which is implemented by properly broadening regions to be attended in later frames.
In summary, SBM realizes the salient-to-broad transition as shown in Figure 1(c). As a result, SBM makes frame-level features more complete and diverse, thus produces more in-formative video-level features after temporal fusion.
Moreover, we introduce an Integration-and-Distribution
Module (IDM) to assist our SBM. SBM increases the rep-resentation capability of video-level features by enhancing differences across frames. But the performance of SBM also depends on the richness of frame-level information. To this end, IDM will integrate and distribute the informative global features, enabling message passing across all frames.
The propagation is input-agnostic and is constructed with all information from input data. By doing this, IDM is re-ciprocal to SBM: IDM consolidates the frame-level repre-sentations, and SBM will enrich the video-level representa-tions. Thus, the combination of SBM and IDM will incur more powerful representations for video re-id.
SBM and IDM can be inserted into the backbone net-work together to form SINet. We carry out extensive exper-iments on four benchmarks to demonstrate the effectiveness of our method. Notably, SINet achieves 91.0% and 87.4% rank-1 accuracy on MARS and LS-VID, respectively, sur-passing the existing state-of-the-art models. 2.