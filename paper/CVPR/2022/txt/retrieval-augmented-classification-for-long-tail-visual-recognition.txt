Abstract
We introduce Retrieval Augmented Classification (RAC), a generic approach to augmenting standard image classi-fication pipelines with an explicit retrieval module. RAC consists of a standard base image encoder fused with a parallel retrieval branch that queries a non-parametric ex-ternal memory of pre-encoded images and associated text snippets. We apply RAC to the problem of long-tail clas-sification and demonstrate a significant improvement over previous state-of-the-art on Places365-LT and iNaturalist-2018 (14.5% and 6.7% respectively), despite using only the training datasets themselves as the external informa-tion source. We demonstrate that RAC’s retrieval mod-ule, without prompting, learns a high level of accuracy on tail classes.This, in turn, frees the base encoder to focus on common classes, and improve its performance thereon.
RAC represents an alternative approach to utilizing large, pretrained models without requiring fine-tuning, as well as a first step towards more effectively making use of external memory within common computer vision architectures. 1.

Introduction
Large Transformer [48] models have arrived in Com-puter Vision, with parameter counts and pretraining dataset size increasing rapidly [11,26,34,42,44,53]. The distributed representations learned by such models result in significant performance gains on a range of tasks, however come with the drawback of storing world knowledge implicitly within their parameters, making post-hoc modification [8] and in-terpretability [4] challenging. In addition, real-world data is long-tailed by nature, and implicitly storing every visual cue present in the world appears futile with current hard-ware constraints. As an alternative to this fully parametric approach, we propose augmenting standard classification pipelines with an explicit external memory, thus separating model performance from parameter count, and facilitating
*Part of this work was done when WY was with Amazon and CS was with The University of Adelaide. the dynamic addition and removal of information explicitly with no changes to model weights.
To evaluate our approach, we focus on the problem of
Long-Tail visual recognition, as it shares many of the prop-erties likely to be encountered by a general agent. Specifi-cally, the data distributions are highly skewed on a per-class basis, with a majority of classes containing a small number of samples. The number of samples in these small classes, commonly referred to as the “tail”, can far outweigh those in the relative minority of high sample classes (referred to as the “head”). In this situation, learning is challenging due to both the lack of information provided for tail classes, and the tendency for head classes to dominate the learning process. Long-tail learning is a well-studied [2, 20, 39] in-stance of the more general label shift problem [41], where the shift is static and known during both training and test-ing. Despite being well-studied, commonly occurring, and of great practical importance, classification performance on long-tail distributions lags significantly behind the state-of-the-art for better balanced classes [24].
Base approaches are largely variants of the same core idea—that of “adjustment”, where the learner is encouraged to focus on the tail of the distribution. This can be achieved implicitly, via over/under-weighting samples during train-ing [3, 12, 19, 22] or cluster-based sampling [6], or explic-itly via logit [10, 36, 57] or loss [21, 36] modification. Such approaches largely focus on consistency, ensuring minimiz-ing the training loss corresponds to a minimal error on the known, balanced, test distribution.
An alternative approach focuses on ensembling models.
Instead of disregarding knowledge of the test distribution, recent work [17, 52, 59] use ensembling models to induce invariance to the test distribution. This is typically done by training separate models under different losses or re-sampling techniques, and combining them at test time.
We introduce a third approach, Retrieval Augmented
Classification (RAC), motivated by the desire to explicitly store tail knowledge, as a retrieval-based augmentation to standard classification pipelines.
RAC’s retrieval module is multi-modal, making use of image representations as retrieval keys, and returning en-(a) The proposed RAC architecture (b) Retrieval module
Figure 1. (a) RAC overview. RAC consists of a retrieval module that augments a standard encoder B(·) with explicit external memory. (b) The retrieval module consists of external images I encoded by a fixed, pretrained image encoder E(·), and associated text T queried using an approximate k-NN and encoded via a text encoder T(·). The logits of the retrieval encoder are then combined with those of the base network. In our instantiation, B and E are ViT’s, and T is a BERT-like text encoder. coded textual information associated with each image. We place no limitation on the nature of this text; it may be the labels from a supervised training set, descriptions, captions etc. In the simplest case, the images in the index, and asso-ciated text, can be the images and labels from the dataset of interest alone.
RAC jointly trains a standard base encoder, and a sepa-rate retrieval branch. We demonstrate empirically that the retrieval branch learns, without explicit prompting, to focus on tail classes. This frees the base encoder from modelling these sparse classes, as they are already effectively repre-sented by the non-parametric memory of the retrieval mod-ule. This in turn allows the base encoder to achieve a higher level of performance on the head classes.
RAC achieves state-of-the-art performance on common
Long-Tail classification benchmarks, even out-performing approaches such as LACE [36] that are provably consistent with regard to the class-balanced error, and Bayes-optimal under Gaussian class priors. A major benefit of RAC is its ability to use large, pretrained models for inference (for in-dex and retrieval encoding), leveraging their rich represen-tations to improve the classification performance of a base learner. This broadens the applicability of such models due to the large cost of fine-tuning.
Our contributions are summarised as follows: 1. The first demonstration of effective external memory within long-tail visual recognition setting. 2. A novel method for Long-Tail classification that sig-nificantly improves on the current state-of-the-art. 3. Insight into the proposed method, with the reimple-mentation of strong baselines that also exceed current state-of-the-art. 2.