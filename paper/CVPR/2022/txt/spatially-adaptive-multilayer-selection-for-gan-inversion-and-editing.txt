Abstract 1.

Introduction
Existing GAN inversion and editing methods work well for aligned objects with a clean background, such as por-traits and animal faces, but often struggle for more diffi-cult categories with complex scene layouts and object oc-clusions, such as cars, animals, and outdoor images. We propose a new method to invert and edit such complex im-ages in the latent space of GANs, such as StyleGAN2. Our key idea is to explore inversion with a collection of layers, spatially adapting the inversion process to the difficulty of the image. We learn to predict the “invertibility” of dif-ferent image segments and project each segment into a la-tent layer. Easier regions can be inverted into an earlier layer in the generator’s latent space, while more challeng-ing regions can be inverted into a later feature space. Ex-periments show that our method obtains better inversion re-sults compared to the recent approaches on complex cate-gories, while maintaining downstream editability. Please refer to our project page at gauravparmar.com/sam_ inversion.
The recent advances of Generative Adversarial Net-works [19], such as ProGAN [29], the StyleGAN model family [31–33], and BigGAN [12], have revived the inter-est in GAN inversion and editing [13, 63]. In GAN editing pipelines, one first projects an image into the latent space of a pre-trained GAN, by minimizing the distance between the generated image and an input image. We can then change the latent code according to a user edit, and synthesize the output accordingly. The latent code can then be changed, in order to satisfy a user edit. The final output image is synthesized with the updated latent code. Several recent methods have achieved impressive editing results for real images [2, 8, 40, 62] using scribbles, text, attribute, and ob-ject class conditioning. However, existing methods work well for human portraits and animal faces but are less ap-plicable to more complex classes such as cars, horses, and cats. Compared to faces, these objects have more diverse visual appearance and cluttered backgrounds. In addition, they tend to be less aligned and more often occluded, all of which make inversion more challenging.
In this work, we aim to invert complex images better. We
build our method upon two key observations. (1) Spatially-adaptive invertibility: first, the inversion difficulty varies across different regions within an image.
Even if the entire image cannot be inverted in the early la-tent spaces (e.g., W and W + space of StyleGAN2 [33]), if we break the image into multiple segments, the easier re-gions can still be inverted in these latent spaces with high fidelity. For example, in Figure 1, while the car and sky regions are well-modeled by the LSUN CAR generator, shrubs and fences are not, as they appear less frequently in the dataset. Besides, both regions are occluded by the foreground car. (2) The trade-off between invertibility and editability: as noted by prior work [51, 65], the choice of layer can de-termine how precisely an image can be reconstructed and the range of downstream edits that can be performed. Early latent layers of a generative model (W , W +) are often un-able to reconstruct challenging images, but allow meaning-ful global and local editing. In contrast, inversion using later intermediate layers reconstructs the image more precisely at the cost of reduced editing capability. As invertibility in-creases in later layers, the editability decreases. The first two rows in Figure 1 show these trade-offs concretely for a real car image.
Considering the spatially-varying difficulty and the trade-off between editability and invertibility, we perform spatially-adaptive multilayer (SAM) inversion by choosing different features or latent spaces to use when inverting each image region. We train a prediction network to infer an invertibility map for an input image indicating the latent spaces to be used per segment as shown in the second col-umn of Figure 1. Our approach enables generating images very close to the target input images while maintaining the downstream editing ability.
We conduct experiments on multiple domains such as
FACES, CARS, HORSES, and CATS. The results show that our method can maintain editability while reconstruct-ing even challenging images more precisely. We measure reconstruction with standard metrics such as PSNR and
LPIPS. Whereas, the image quality and the editability are evaluated using a human preference study. Finally, we demonstrate the generality of our idea on different gener-ator architectures (StyleGAN2 [33], BigGAN-deep [12]), and different paradigms (optimization-based or encoder-based). 2.