Abstract
Code is available at https : / / github . com /
NVlabs/NP-CVP-MVSNet
Recent cost volume pyramid based deep neural net-works have unlocked the potential of efﬁciently leveraging high-resolution images for depth inference from multi-view stereo. In general, those approaches assume that the depth of each pixel follows a unimodal distribution. Boundary pixels usually follow a multi-modal distribution as they rep-resent different depths; Therefore, the assumption results in an erroneous depth prediction at the coarser level of the cost volume pyramid and can not be corrected in the reﬁne-ment levels leading to wrong depth predictions. In contrast, we propose constructing the cost volume by non-parametric depth distribution modeling to handle pixels with unimodal and multi-modal distributions. Our approach outputs mul-tiple depth hypotheses at the coarser level to avoid errors in the early stage. As we perform local search around these multiple hypotheses in subsequent levels, our ap-proach does not maintain the rigid depth spatial ordering and, therefore, we introduce a sparse cost aggregation net-work to derive information within each volume. We evaluate our approach extensively on two benchmark datasets: DTU and Tanks & Temples. Our experimental results show that our model outperforms existing methods by a large margin and achieves superior performance on boundary regions. 1.

Introduction
Multi-view stereo (MVS) aims to infer the 3D structure, represented by a depth map, of a scene from a set of images captured by a camera from multiple viewpoints. It is a fun-damental problem for the computer vision community and has been studied extensively for decades [13].
Recently, learning-based methods using a cost volume pyramid [3, 5, 22] have emerged as the preferred tech-nique to leverage high-resolution images leading to su-perior performance compared to other learning-based ap-proaches [23,24]. These approaches usually perform a local depth search around an initial depth estimate on a cost vol-ume built at the coarser level. Then, assuming each pixel follows a unimodal distribution, they estimate the depth for each pixel as the expectation of approximated contin-uous depth distributions by a few depth samples within a predeﬁned range [3, 5, 22]. While these approaches have achieved promising results, they tend to miss small objects and boundary regions with abrupt depth changes, where the
∗The work is done during an internship at NVIDIA
unimodal distribution assumption does not hold.
As shown in Fig. 1, the multi-scale depth estimation frameworks with unimodal distribution assumption per-forms badly on boundary pixels. It is mainly because ex-isting multi-scale approaches make an early decision at the coarse level as the depth in this level is represented as a sin-gle value based on the unimodal distribution assumption. If the estimated coarse depth is far from the actual depth, the error will be propagated to reﬁnement levels and can not be corrected via local depth search, resulting in incorrect depth prediction.
In this work, we address this problem by explicitly mod-eling the depth of each pixel at different resolutions using a multi-modal distribution. In particular, in contrast to meth-ods using a mixture density distribution [17], we use a non-parametric distribution to learn the probability of each depth hypothesis along the 3D visual ray. Our approach provides additional ﬂexibility compared to parametric approaches, especially in the coarse-to-ﬁne structure. We then guide the learning process using the depth distribution within its cor-responding depth patch at the highest resolution. Given the learned distribution, we build the cost volume for the next level by branching the depth hypotheses using the top K probabilities. Our approach achieves a better ground truth depth covering ratio than the existing approaches; however, it loses the relative spatial relationship due to the pixel-wise depth branching processing. To aggregate the infor-mation in our new cost volume structure, we propose a sparse cost aggregation network to preserve the relative spa-tial relationship. Extensive experiments on several bench-mark datasets demonstrate that our approach achieves su-perior performance, especially on boundary regions. On the
DTU dataset, our approach outperforms the current state-of-the-art multi-scale patchmatch based approach Patchmatch-Net [18], yielding up to a 32% lower error on boundary re-gions.
In summary, the contributions in this paper are as fol-lows.
• We propose a non-parametric depth probability distri-bution modeling, allowing us to handle pixels with uni-modal and multimodal distributions.
• We build a cost volume pyramid by branching the depth samples based on the modeled pixel-wise depth probability distribution.
• We apply a sparse cost aggregation network to process each cost volume to maintain rigid geometric spatial relation in the cost volume and avoid spatial ambiguity.
• Our approach outperforms previous approaches on boundary areas and becomes the new state of the art on the DTU dataset. 2.