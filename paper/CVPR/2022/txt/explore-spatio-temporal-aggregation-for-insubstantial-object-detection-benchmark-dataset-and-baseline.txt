Abstract
We endeavor on a rarely explored task named Insubstan-tial Object Detection (IOD), which aims to localize the ob-ject with following characteristics: (1) amorphous shape with indistinct boundary; (2) similarity to surroundings; (3) absence in color. Accordingly, it is far more challenging to distinguish insubstantial objects in a single static frame and the collaborative representation of spatial and tempo-ral information is crucial. Thus, we construct an IOD-Video dataset comprised of 600 videos (141,017 frames) covering various distances, sizes, visibility, and scenes captured by different spectral ranges. In addition, we develop a spatio-temporal aggregation framework for IOD, in which differ-ent backbones are deployed and a spatio-temporal aggrega-tion loss (STAloss) is elaborately designed to leverage the consistency along the time axis. Experiments conducted on
IOD-Video dataset demonstrate that spatio-temporal aggre-gation can significantly improve the performance of IOD.
We hope our work will attract further researches into this valuable yet challenging task. The code will be available at: https://github.com/CalayZhou/IOD-Video. 1.

Introduction
Recently, the emergence of deep learning based ap-proaches [21, 22, 58] has witnessed significant advance-ments of object detection. Nevertheless, they still face in-tractable problems on some insubstantial objects captured by multispectral cameras [26] under specific wavelength, e.g. smoke, steam and gas leak. Due to frequent occur-rences of smoke poisoning, fire accident, toxic gas leakage and explosion, it is urgent and crucial to realize real-time in-telligent monitoring as well as early warning for insubstan-tial objects. This research topic is fresh and challenging, as insubstantial objects are quite different from conventional objects from several aspects.
Classical paper what is an object [1] defined a measure
âˆ—Corresponding author.
Figure 1. Comparisons between the conventional object and in-substantial object. The insubstantial object is more similar to the background under MS, CC, SS and ED image cues. of objectness generic over classes which regards objects as standalone things with a well-defined boundary and center.
It is considered that any object has at least one of three dis-tinctive characteristics: (1) a well-defined closed boundary in space; (2) a different appearance from their surroundings; (3) sometimes it is unique within the image and stands out as salient. Based on the above observations, Alexe et al. [1] proposed four image cues to distinguish objects: Multi-scale Saliency (MS), Contrast (CC), Edge Density (ED) and
Superpixels Straddling (SS). MS [33] indicates that an ob-ject is the salient region with a unique appearance; CC re-flects the color dissimilarity between the foreground and background; ED measures the average edge magnitude as closed boundary characteristics; SS segments an image into
small regions with uniform color or texture. As illustrated in
Fig. 1, The foreground (train, in red box) in the left achieves relatively high scores for the four measurements, while the background (forest, in blue box) is the opposite. Consider-ing the gas leak (in green box) in the right, it is deprived of trichromatic information owing to the monochromaticity of infrared images. Moreover, the shape of gas leak changes over time and there is no fixed and clear boundary. In terms of above image cues, the gas leak is more similar to the background rather than the foreground. Accordingly, ma-ture algorithms for conventional object detection may fail in this special case, so that specific dataset and algorithm for insubstantial object are urgently needed.
To facilitate the study on this challenging problem, we collect a video-level insubstantial object detection (IOD-Video) dataset via multispectral camera under various scenes, including the smoke from chimney, hot steam, gas leak, etc. The object characteristics in IOD-Video dataset are summarized as follows: (1) indistinct boundary and amorphous shape; (2) the similarity to the background sur-roundings; (3) absence of color information and saliency.
Consequently, the collaborative representation of spatial and temporal features is crucial under the limitation of spa-tial information within a static frame.
Additionally, we develop a spatio-temporal aggregation framework for IOD task. Unlike conventional object de-tection, IOD should distinguish visual temporal variation more than static semantic appearance. Traditional video background subtraction methods such as Gaussian Mix-ture Model (GMM) [64] and Visual