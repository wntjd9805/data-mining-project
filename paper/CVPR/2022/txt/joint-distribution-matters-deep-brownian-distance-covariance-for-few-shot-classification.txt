Abstract
Few-shot classiﬁcation is a challenging problem as only very few training examples are given for each new task. One of the effective research lines to address this challenge fo-cuses on learning deep representations driven by a similar-ity measure between a query image and few support images of some class. Statistically, this amounts to measure the dependency of image features, viewed as random vectors in a high-dimensional embedding space. Previous meth-ods either only use marginal distributions without consid-ering joint distributions, suffering from limited represen-tation capability, or are computationally expensive though
In this paper, we propose harnessing joint distributions. a deep Brownian Distance Covariance (DeepBDC) method for few-shot classiﬁcation. The central idea of DeepBDC is to learn image representations by measuring the discrep-ancy between joint characteristic functions of embedded features and product of the marginals. As the BDC metric is decoupled, we formulate it as a highly modular and efﬁcient layer. Furthermore, we instantiate DeepBDC in two dif-ferent few-shot classiﬁcation frameworks. We make experi-ments on six standard few-shot image benchmarks, covering general object recognition, ﬁne-grained categorization and cross-domain classiﬁcation. Extensive evaluations show our DeepBDC signiﬁcantly outperforms the counterparts, while establishing new state-of-the-art results. The source code is available at http://www.peihuali.org/DeepBDC. 1.

Introduction
Few-shot classiﬁcation [15, 17] is concerned with a task where a classiﬁer can be adapted to distinguish classes un-seen previously, given only a very limited number of ex-amples of these classes. This is a challenging problem as scarcely labeled examples are far from sufﬁcient for learn-ing abundant knowledge and also likely lead to overﬁt-ting. One practical solution is based on the technique of
∗Equal contribution. †Corresponding author, peihuali@dlut.edu.cn. The work was supported by National Natural Science Foundation of China (61971086, 61806140), and CCF-Baidu Open Fund (2021PP15002000). meta-learning or learning to learn [12, 39], in which the episodic training is formulated to transfer the knowledge obtained on a massive meta-training set spanning a large number of known classes to the few-shot regime of novel classes. Among great advances that have been made, the line of metric-based methods attracts considerable research interest [15, 26, 33, 39], achieving state-of-the-art perfor-mance [45, 47] in recent years.
The primary idea of the metric-based few-shot classiﬁ-cation is to learn representations through deep networks, driven by the similarity measures between a query image and few support images of some class [33,47]. Statistically, the features of a query image (resp., support images) can be viewed as observations of a random vector X (resp., Y ) in a high-dimensional embedding space. Therefore, the similar-ity between images can be measured by means of probabil-ity distributions. However, modeling distributions of high-dimensional (and often few) features is hard and a common method is to model statistical moments. ProtoNet [33] and its variants (e.g., [26]) represent images by ﬁrst moment (mean vector) and use Euclidean distance or cosine simi-larity for metric learning. To capture richer statistics, sev-eral works study second moment (covariance matrix) [44] or combination of ﬁrst and second moments in the form of
Gaussians [20] for image representations, while adopting
Frobenius norm or Kullback-Leiberler (KL) divergence as similarity measures. However, these methods only exploit marginal distributions while neglecting joint distributions, limiting the performance of learned models.
In addition, the covariances can only model linear relations.
In general, the dependency between X and Y should be measured in light of their joint distribution fXY (x, y) [6].
Earth Mover’s Distance (EMD) is an effective method for measuring such dependency. As described in [29, Sec. 2.3],
EMD seeks an optimal joint distribution fXY (x, y), whose marginals are constrained to be given fX (x) and fY (y), so that the expectation of transportation cost is minimal.
In few-shot classiﬁcation, DeepEMD [47] proposes differen-tial EMD for optimal matching of image regions. Though achieving state-of-the-art performance, DeepEMD is com-putationally expensive [45], due to inherent linear program-Method
ProtoNet [33]
CovNet [44]
ADM [20]
DeepEMD [47]
Probability model
Mean vector
Covariance matrix
Gaussian distribution
Discrete distribution
DeepBDC (ours)
Characteristic function
Dis-similarity/similarity measure (cid:107)µX − µY (cid:107)2 or
µT
X µY (cid:107)µX (cid:107)(cid:107)µY (cid:107) (cid:107)ΣX − ΣY (cid:107)2
≥0
DKL(NµX ,ΣX ||NµY ,ΣY ) (cid:80) (cid:80) j lfxj ,yl=fxj , (cid:80)
|φXY (t, s)−φX (t)φY (s)|2 cpcq(cid:107)t(cid:107)1+p(cid:107)s(cid:107)1+q minfxj ,yl s.t. (cid:80) (cid:82) (cid:82) l fxj ,yl cxj ,yl
Rp
Rq j fxj ,yl=fyl for ∀j, l dtds
Joint distribution
Dependency
Latency
Accuracy (%) 1-shot 5-shot
No
No
No
Yes
Yes
N/A
Low 49.42 68.20
Linear
Low 49.64 69.45
N/A
N/A
Low 53.10 69.73
High 65.91 82.41
Nonlinear &
Independence
Low 67.34 84.46
Table 1. Comparison between our DeepBDC and the counterparts. To quantify the dependency between random vectors X and Y , moments based methods [20, 33, 44] only model marginal distributions, suffering from limited representation capability; though achieving state-of-the-art performance by considering joint distributions, DeepEMD [47] is computationally expensive. Our DeepBDC measures discrepancy between joint characteristic function and product of the marginals, which can be efﬁciently computed in closed-form, and model non-linear relations and fully characterizes independence. Note that for a random vector its characteristic function and probability distribution are equivalent in that they form a Fourier transform pair. Here we report accuracies of 5-way 1-shot/5-shot classiﬁcation on miniImageNet; our result is obtained by Meta DeepBDC and results of the counterparts are duplicated from respective papers. ming algorithm. Mutual information (MI) [3, 28] is a well-known measure, which can quantify the dependency of two random variables by KL-divergence between their joint dis-tribution and product of the marginals. Unfortunately, com-putation of MI is difﬁcult in real-valued, high-dimensional setting [2], and often involves difﬁcult density modeling or lower-bound estimation of KL-divergence [14].
In this paper, we propose a deep Brownian Distance
Covariance (DeepBDC) method for few-shot classiﬁcation.
The BDC metric, ﬁrst proposed in [35, 36], is deﬁned as the
Euclidean distance between the joint characteristic function and product of the marginals. It can naturally quantify the dependency between two random variables. For discrete observations (features), the BDC metric is decoupled so that we can formulate BDC as a pooling layer, which can be seamlessly inserted into a deep network, accepting feature maps as input and outputting a BDC matrix as an image rep-resentation. In this way, the similarity between two images is computed as the inner product between the corresponding two BDC matrices. Therefore, the core of our DeepBDC is highly modular and plug-and-play for different methodolo-gies of few-shot image classiﬁcation. Speciﬁcally, we in-stantiate our DeepBDC in meta-learning framework (Meta
DeepBDC), and in the simple transfer learning framework relying non-episodic training (STL DeepBDC). Contrary to covariance matrices, our DeepBDC can freely handle non-linear relations and fully characterize independence. Com-pared to EMD, it also considers joint distribution and above all, can be computed analytically and efﬁciently. Unlike MI, the BDC requires no density modeling. We present differ-ences between our BDC and the counterparts in Tab. 1.
Our contributions are summarized as follows. (1) For the ﬁrst time, we introduce Brownian distance covariance (BDC), a fundamental but largely overlooked dependency modeling method, into deep network-based few-shot clas-siﬁcation. Our work suggests great potential and future (2) We formulate applications of BDC in deep learning.
DeepBDC as a highly modular and efﬁcient layer, suitable for different few-shot learning frameworks. Furthermore, we propose two instantiations for few-shot classiﬁcation, i.e., Meta DeepBDC based on the meta-learning framework with ProtoNet as a blue print, and STL DeepBDC based on simple transfer learning framework without episodic train-ing. (3) We perform thorough ablation study on our meth-ods and conduct extensive experiments on six few-shot clas-siﬁcation benchmarks. The experimental results demon-strate that both of our two instantiations achieve superior performance and meanwhile set new state-of-the-arts. 2.