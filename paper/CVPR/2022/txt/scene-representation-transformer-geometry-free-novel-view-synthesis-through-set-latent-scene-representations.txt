Abstract
A classical problem in computer vision is to infer a 3D scene representation from few images that can be used to render novel views at interactive rates. Previous work fo-cuses on reconstructing pre-deﬁned 3D representations, e.g. textured meshes, or implicit representations, e.g. radiance
ﬁelds, and often requires input images with precise camera poses and long processing times for each novel scene.
In this work, we propose the Scene Representation Trans-former (SRT), a method which processes posed or unposed
RGB images of a new area, infers a “set-latent scene rep-resentation”, and synthesises novel views, all in a single feed-forward pass. To calculate the scene representation, we propose a generalization of the Vision Transformer to sets of images, enabling global information integration, and hence 3D reasoning. An efﬁcient decoder transformer parameter-izes the light ﬁeld by attending into the scene representation to render novel views. Learning is supervised end-to-end by minimizing a novel-view reconstruction error.
We show that this method outperforms recent baselines in terms of PSNR and speed on synthetic datasets, including a new dataset created for the paper. Further, we demonstrate that SRT scales to support interactive visualization and se-mantic segmentation of real-world outdoor environments using Street View imagery.
⇤Work done while at Google.
Contributions: MS: original idea & conceptualization, main implementa-tion, experiments, writing, organization, lead; HM: idea conceptualization, implementation, experiments, writing; EP: implementation, experiments, writing; UB: idea conceptualization, implementation, experiments, writ-ing; KG: dataset support, advising; NR: project-independent code; SV: project-independent code; ML: advising, writing; DD: advising, project-independent code; AD: advising; JU: advising; TF: datasets, advising, writing; AT: advising, writing.
Correspondence: srt@msajjadi.com & tutmann@google.com
Project website: srt-paper.github.io
Figure 1. Model overview – SRT encodes a collection of images into the scene representation: a set of latent features. Novel views are rendered in real-time by attending into the latent representation with light ﬁeld rays, see Fig. 2 for details. 1.

Introduction
The goal of our work is interactive novel view synthesis: given few RGB images of a previously unseen scene, we synthesize novel views of the same scene at interactive rates and without expensive per-scene processing. Such a system can be useful for virtual exploration of urban spaces [26], as well as other mapping, visualization, and AR/VR appli-cations [27]. The main challenge here is to infer a scene representation that encodes enough 3D information to render novel views with correct parallax and occlusions.
Traditional methods build explicit 3D representations, such as colored point clouds [1], meshes [31], voxels [36], octrees [42], and multi-plane images [52]. These representa-tions enable interactive rendering, but they usually require expensive and fragile reconstruction processes.
More recent work has investigated representing scenes with purely implicit representations [39]. Notably, NeRF represents the scene as a 3D volume parameterized by an
MLP [25], and has been demonstrated to scale to challeng-ing real-world settings [23]. However, it typically requires hundreds of MLP evaluations for the volumetric rendering of each ray, relies on accurate camera poses, and requires an expensive training procedure to onboard new scenes, as no parameters are shared across different scenes.
Follow-ups that address these shortcomings include re-projection based methods which still rely on accurate camera poses due to their explicit use of geometry, and latent models which are usually geometry-free and able to reason globally, an advantage for the sparse input view setting. However, those methods generally fail to scale to complex real-world datasets. As shown in Tab. 1, interactive novel view synthesis on complex real-world data remains a challenge.
We tackle this challenge by employing an encoder-decoder model built on transformers, learning a scalable implicit representation, and replacing explicit geometric operations with learned attention mechanisms. Different from a number of prior works that learn the latent scene representations through an auto-decoder optimization pro-cedure [38, 16], we speciﬁcally rely on an encoder architec-ture to allow for instant inference on novel scenes. At the same time, we do not rely on explicit locally-conditioned geometry [49, 41] to instead allow the model to reason glob-ally. This not only comes with the advantage of stronger generalization abilities (e.g., for rendering novel cameras further away from the input views, see Fig. 4), but also en-ables the model to be more efﬁcient, as global information is processed once per scene (e.g., to reconstruct 3D geom-etry and to resolve occlusions) in our model, rather than once or hundreds of times per rendered pixel, as in previous methods [49, 41, 44].
We evaluate the proposed model on several datasets of increasing complexity against relevant prior art, and see that it strikes a unique balance between scalability to complex scenes (Sec. 4), robustness to noisy camera poses (or no poses at all, Fig. 6), and efﬁciency in interactive applica-tions (Tab. 3). Further, we show a proof-of-concept for the downstream task of semantic segmentation using the learned representation on a challenging real-world dataset (Fig. 7). 2.