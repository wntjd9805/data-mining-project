Abstract
Scene graph generation (SGG) aims to capture a wide variety of interactions between pairs of objects, which is es-sential for full scene understanding. Existing SGG methods trained on the entire set of relations fail to acquire complex reasoning about visual and textual correlations due to var-ious biases in training data. Learning on trivial relations that indicate generic spatial configuration like ‘on’ instead of informative relations such as ‘parked on’ does not en-force this complex reasoning, harming generalization. To address this problem, we propose a novel framework for
SGG training that exploits relation labels based on their informativeness. Our model-agnostic training procedure imputes missing informative relations for less informative samples in the training data and trains a SGG model on the imputed labels along with existing annotations. We show that this approach can successfully be used in conjunction with state-of-the-art SGG methods and improves their per-formance significantly in multiple metrics on the standard
Visual Genome benchmark. Furthermore, we obtain con-siderable improvements for unseen triplets in a more chal-lenging zero-shot setting. 1.

Introduction
In this paper, we look at a structured vision-language problem, scene graph generation [26, 56], which aims to capture a wide variety of interactions between pairs of ob-jects in images. SGG can be seen as a step towards com-prehensive scene understanding and benefits several high-level visual tasks such as object detection/segmentation
[17, 20, 41], image captioning [1, 18, 22, 30], image/video retrieval [16], and visual question answering [2, 34].
In the literature, SGG is typically formulated as predicting a triplet of a localized subject-object pair connected by a re-lation (e.g. person wear shirt). Broadly, recent advances in
SGG have been obtained by extracting local and global vi-sual features in convolutional neural networks [23,45,68] or (a) An example of a scene graph with implicit and
Figure 1. explicit relations. (b) Per-class Recall@100 for an SGG model trained either on only explicit (orange) or implicit (blue) relations. graph neural networks [31, 56, 61] combined with language embeddings [36, 39] or statistical priors [65] for predicting relations between objects.
Despite the remarkable progress in this task, various factors (long-tail data distribution, language or reporting bias [38]) in the established SGG benchmarks (e.g. Visual
Genome [26]) have been shown to drive existing methods towards biased and inaccurate relation predictions [49, 50].
One major cause is that each subject-object pair is annotated with only one positive relation, which typically depends on the annotator’s preference and is subjective, while other plausible relations are treated as negative.1 For instance, a subject-object pair man–beach in Fig. 1a is only annotated with one relation as man on beach, even though other plau-sible relations are available such as standing on. Hence, the models [23, 29, 50, 65] trained on this data become bi-ased towards more frequently occurring labels, as reported in [49]. To alleviate the biased training, Tang et al. [49] employ counterfactual causality to force the SGG model to base its predictions only on visual evidence rather than the data bias. Wang et al. [54] propose a semi-supervised technique that jointly imputes the missing labels of subject– object pairs with no annotated relations to obtain more bal-anced triplet distributions. Suhail et al. [47] propose an en-1Note that both training and test sets of the standard benchmarks are subject to the similar biases.
ergy based method that can learn to model the joint proba-bility of triplets from few samples and thereby avoids gen-erating biased graphs with inconsistent structure.
While the recent methods [8, 47, 49, 54] successfully tackle the bias towards more frequently occurring labels, this paper studies another type of bias related to label infor-mativeness. It also manifests itself in missing annotations and has not been addressed in SGG before. In particular, we hypothesize that certain relation labels (implicit labels) are more informative than others (explicit labels) and train-ing on implicit labels improves a model’s ability to reason over complex correlations in visual and textual data.
Our key intuition comes from prior computational and cognitive models [32] and recent work [10] that catego-rize relations into explicit (or spatial) and implicit, based on whether the relation defines the relative spatial configu-ration between the two objects implicitly or explicitly (e.g. man standing on beach vs. man on beach in Fig. 1a). Ex-plicit relations are often easy to learn, e.g. from the spatial coordinates of subject–object pairs, thanks to their highly deterministic spatial arrangements, while implicit ones are often challenging due to the relative spatial variation and require deliberate reasoning. To test our hypothesis, we conduct experiments where we train a SGG model either only on explicit, or only implicit relations, and evaluate them on a test set including both types (i.e. zero-shot im-plicit or explicit relation classification), see Fig. 1b. Sur-prisingly, training only on implicit relations obtains good performance not only over implicit ones but also unseen ex-plicit ones (only 2% lower in average training on explicit relations and 4% lower when trained on all labels), while training only on explicit relations performs poorly on im-plicit relations (where the performance drops to 0.1% from 24.3%).2
In other words, training on implicit labels en-ables the model to better generalize to unseen explicit la-bels. However, due to partially annotated training data, many subject-object pairs are only labelled by explicit re-lations and their implicit relations are missing and obscured by the explicit ones.
Motivated by our analysis, we design a novel model-agnostic training procedure for SGG that jointly extracts more information from partially labeled data by mining the missing implicit labels, trains a SGG model on them and boosts its performance. In particular, our method involves a two stage training pipeline. The first stage trains a SGG model on a subset of training data including only annotated implicit relations, which allows the model to learn rich cor-relations in the data and encourages it to predict more infor-mative implicit labels in the next stage. The second stage includes an alternating procedure that imputes missing im-plicit labels on the subset of samples annotated with ex-plicit relations, followed by training on both the annotated 2We provide the full analysis in the results section and supplementary. and imputed labels, called label refinement. In this stage, a model is prone to confirm to its own (wrong) predictions to achieve a lower loss as observed in semi-supervised learn-ing (e.g. [3, 51]). To prevent such overfitting, we regular-ize the model by a latent space augmentation strategy. We demonstrate that our method yields significant performance gains in the SGG task for the standard and zero-shot settings on the Visual Genome [26] when applied to several existing scene graph generation models.
In short, our contributions are as following. We identify a previously unexplored issue, missing informative labels in the standard SGG benchmark and address this through a model agnostic training procedure based on alternating la-bel imputation and model training with effective regulariza-tion strategies. This method can be incorporated into state-of-the-art SGG models and boosts their performance by a significant margin. 2.