Abstract
Progress in 3D object understanding has relied on manu-ally “canonicalized” shape datasets that contain instances with consistent position and orientation (3D pose). This has made it hard to generalize these methods to in-the-wild shapes, e.g., from internet model collections or depth sen-sors. ConDor is a self-supervised method that learns to
Canonicalize the 3D orientation and position for full and partial 3D point clouds. We build on top of Tensor Field
Networks (TFNs), a class of permutation- and rotation-equivariant, and translation-invariant 3D networks. Dur-ing inference, our method takes an unseen full or partial 3D point cloud at an arbitrary pose and outputs an equiv-ariant canonical pose. During training, this network uses self-supervision losses to learn the canonical pose from an un-canonicalized collection of full and partial 3D point clouds. ConDor can also learn to consistently co-segment object parts without any supervision. Extensive quantitative results on four new metrics show that our approach out-performs existing methods while enabling new applications such as operation on depth images and annotation transfer. 1.

Introduction
Humans have the ability to recognize 3D objects in a wide variety of positions and orientations (poses) [40], even if objects are occluded. We also seem to prefer cer-tain canonical views [10], with evidence indicating that an object in a new pose is mentally rotated to a canonical pose [47] to aid recognition. Inspired by this, we aim to build scene understanding methods that reason about ob-jects in different poses by learning to map them to a canon-ical pose without explicit supervision.
Given a 3D object shape, the goal of instance-level canonicalization is to ﬁnd an equivariant frame of refer-ence that is consistent relative to the geometry of the shape under different 3D poses. This problem can be solved if we have shape correspondences and a way to ﬁnd a distinctive equivariant frame (e.g., PCA). However, it becomes signiﬁ-cantly harder if we want to operate on different 3D poses of different object instances that lack correspondences.
This category-level canonicalization problem has received much less attention despite tremendous interest in category-level 3D object understanding [8, 11, 14, 25, 26, 31, 56].
Most methods rely on data augmentation [23], or man-ually annotated datasets [3, 56] containing instances that are consistently positioned and oriented within each cat-egory [44, 48, 52]. This has prevented broader applica-tion of these methods to un-canonicalized data sources, such as online model collections [1]. The problem is fur-ther exacerbated by the difﬁculty of canonicalizing partial shape observations (e.g., from depth maps [36]), or symmet-ric objects that require an understanding of inter-instance part relationships. Recent work addresses these limitations using weakly-supervised [15, 38] or self-supervised learn-ing [13, 29, 43, 46], but cannot handle partial 3D shapes, or is limited to canonicalizing only orientation.
We introduce ConDor, a method for self-supervised category-level Canonicalization of the 3D pose of partial shapes.
It consists of a neural network that is trained on an un-canonicalized collection of 3D point clouds with in-consistent 3D poses. During inference, our method takes a full or partial 3D point cloud of an object at an arbitrary pose, and outputs a canonical rotation frame and translation vector. To enable operation on instances from different cate-gories, we build upon Tensor Field Networks (TFNs) [49], a 3D point cloud architecture that is equivariant to 3D rotation and point permutation, and invariant to translation. To han-dle partial shapes, we use a two-branch (Siamese) network with training data that simulates partiality through shape slicing or camera projection. We introduce several losses to help our method learn to canonicalize 3D pose via self-supervision. A surprising feature of our method is the (op-tional) ability to learn consistent part co-segmentation [6] across instances without any supervision (see Figure 1).
Given only the recent interest, standardized metrics for evaluation of canonicalization methods have not yet emerged. We therefore propose four new metrics that are designed to evaluate the consistency of instance- and category-level canonicalization, as well as consistency with manually pre-canonicalized datasets. We extensively eval-uate the performance of our method using these metrics by comparing with baselines and other methods [43, 46].
Quantitative and qualitative results on common shape cate-gories show that we outperform existing methods and pro-duce consistent pose canonicalizations for both full and par-tial 3D point clouds. We also demonstrate previously difﬁ-cult applications enabled by our method such as operation on partial point clouds from depth maps, keypoint annota-tion transfer, and expanding the size of existing datasets. To sum up, our contributions include:
• A self-supervised method to canonicalize the 3D pose of full point clouds from a variety of object categories.
• A method that can also handle partial 3D point clouds.
• New metrics to evaluate canonicalization methods, ex-tensive experiments, and new applications. 2.