Abstract
Recent video question answering benchmarks indicate that state-of-the-art models struggle to answer composi-tional questions. However, it remains unclear which types of compositional reasoning cause models to mispredict.
Furthermore, it is difficult to discern whether models ar-rive at answers using compositional reasoning or by lever-aging data biases.
In this paper, we develop a question decomposition engine that programmatically deconstructs a compositional question into a directed acyclic graph of sub-questions. The graph is designed such that each parent question is a composition of its children. We present AGQA-Decomp, a benchmark containing 2.3M question graphs, with an average of 11.49 sub-questions per graph, and 4.55M total new sub-questions. Using question graphs, we evaluate three state-of-the-art models with a suite of novel compositional consistency metrics. We find that models ei-ther cannot reason correctly through most compositions or are reliant on incorrect reasoning to reach answers, fre-quently contradicting themselves or achieving high accu-racies when failing at intermediate reasoning steps. 1.

Introduction
Compositional reasoning is fundamental to how humans represent visual events [20, 26, 32, 38]. For instance, Fig-ure 1 visualizes a video consisting of actions such as taking a picture and holding a bottle; the action holding a bottle involves an actor initially twisting the bottle and then later holding it. This ability to compose interactions and actions is reflected in the compositional nature of language people use to communicate about what they see [5,28]. To measure compositional reasoning of visual events, the computer vi-sion community has proposed multiple video benchmarks using question answering [12, 23, 39]. These benchmarks ask questions such as “Is a phone the first object that the
*Equal contribution
Figure 1. We introduce a question decomposition engine, which produces a DAG of sub-questions from a compositional question about visual events. A sub-question is designed to contain a sub-set of the original question’s reasoning steps. Our engine pro-duces a benchmark with 4.55M question answer pairs associated with 9.6K videos. We design handcrafted programs and templates for each sub-question as well as composition rules to compose sub-questions together. We analyze existing models using a suite of new compositional consistency metrics using our DAGs. Our
DAGs isolate which composition rules cause mispredictions (er-ror path is shown by pink arrows). They also highlight scenarios where models might exhibit self-contradiction (blue arrows). person is touching after taking a picture?”, where models need to compose actions (taking a picture) with relation-ships (touching) and objects (phone) to arrive at the correct answer. Using these benchmarks, researchers have recently concluded that state-of-the-art models [8, 22, 25] struggle to reason compositionally [12].
Unfortunately, existing benchmarks are unable to ex-plain why video question answering models struggle with
Table 1. We visualize our hand-designed sub-questions, which consist of a subset of the reasoning steps found in the AGQA bench-mark [12]. Each sub-question consists of a functional program and a natural language template. Parentheses indicate further categorization.
Sub-question type
Object exists
Relation exists
Interaction
Description
To verify if an object exists
To verify if a relationship exists
To verify if there is a particular relationship between person and an object
Example
Does a doorway exist?
Is the person holding something?
Is the person touching a dish?
Interaction temporal loc. A filter on an interaction type question
Exists temporal loc.
A condition on object/relationship exists question
First/last
Getting the first/last instance of the given object
Longest shortest action
Getting the longest/shortest action
Conjunction
Choose
Equals
Get a new exists question by combining two interaction questions with a conjunction
Compares between two objects, actions, relationships, or time lengths
Compares two objects and verifies if they are the same
Verifies if the given action is longer/shorter than the other one
Is the person holding a book while smiling at something?
Does a phone exist after looking in the mirror?
What is the first object that the person is above before walking through the doorway?
What does the person do for the longest amount of time?
Is the person in front of the mirror and behind the table while looking in the mirror?
Is the doorknob or the dish the first object that the person is holding?
Is the doorway the object they are interacting with while holding a dish? compositional reasoning. In Figure 1, a model incorrectly answers the root question as “no” instead of the correct an-swer of “yes.” However, this information does not explain what caused the model to err: Did the model struggle with words requiring temporal reasoning, such as first or after?
Did it fail at detecting the phone or identifying the relation-ship touching? Or did it struggle to compose the relation-ship with the object? Even if we assume the model had cor-rectly answered the question, it remains uncertain whether this behavior was due to proper compositional reasoning or a reliance on spurious correlations to “cheat.”
Not only do standard evaluation schemes fall short in this regard, but existing approaches for dissecting model be-havior also struggle to resolve this uncertainty. Attribution methods, such as GradCAM [35] or LIME [34], can high-light important aspects of the input data, but are agnostic to the structure of compositional reasoning. Approaches that rely on counterfactuals to illuminate model behavior, such as contrast sets [9], focus primarily on model decision boundaries by performing minor, local changes to the in-put. These local changes, however, cannot capture the full range of compositional reasoning steps required to answer compositional visual questions [12], which assess multiple, often interdependent, reasoning abilities at once.
In this paper, we develop a question decomposition en-gine that decomposes a compositional question into a di-rected acyclic graph (DAG) of sub-questions (see Figure 1).
A sub-question isolates a subset of the reasoning steps that the original question requires, exposing model performance on subsets of intermediate reasoning steps. This expo-sure enables us to identify difficult sub-questions and study which compositions cause models to struggle. It also allows us to test whether models are right for the right reasons. For instance, the root question mentioned earlier can not only decompose into intermediate reasoning steps that determine if the “the person was touching something after taking a pic-ture,” but also isolate basic perception capabilities, such as determining whether a “phone exists”.
Using our engine, we construct the AGQA-Decomp dataset1, which decomposes the 2.3M compositional ques-tions in the updated version2 of the recent balanced AGQA benchmark [12] to produce 1.62M unique sub-questions for 9.6K videos for a total of 4.55M sub-questions. To generate sub-questions, we hand-design 21 sub-questions, each with a functional program and natural language tem-plate (Table 1). To compose the sub-questions within a
DAG, we hand-design 13 composition rules (Table 2). Fi-nally, we create a suite of new metrics to evaluate compo-sitional reasoning. One of those metrics — internal consis-tency — measures whether models are self-consistent when they answer questions within a DAG. To enable this metric, we further hand-design 10 consistency rules between sub-questions (see Table 5 in the Supplementary).
We evaluate three state-of-the-art video question answer-ing models, HCRN [22], HME [8] and PSAC [25] using our
DAGs and metrics. Our analyses reveal that for a major-ity of compositional reasoning steps, models either fail to successfully complete the step or rely on faulty reasoning mechanisms. They frequently contradict themselves and achieve high accuracies even when failing at intermediate steps. Models even struggle when asked to choose between or compare two options, such as objects or relationships.
Finally, we find that for HCRN and PSAC, there is no cor-relation between internal consistency and accuracy across 1Project page: https://tinyurl.com/agqa-decomp 2AGQA 2.0: https://tinyurl.com/agqavideo
Figure 2. Our question decomposition engine expects a compositional root question as input and outputs a DAG of sub-questions. The root question has an associated functional program which explains the reasoning steps necessary to answer the question. We recursively iterate over the arguments of the function until we reach a leaf function. We design natural language templates for each leaf function, converting them into sub-questions. Once a leaf function is converted to a question, we return an indirect reference of the answer back to its parent.
The parent uses composition rules to combine the indirect references from its children to similarly generate questions.
DAGs. For HME, however, there is a weak negative correla-tion, suggesting that the model is frequently inaccurate and propagates this inaccuracy due to its internal consistency.
We believe that our decomposed question DAGs could fur-ther enable a host of future research directions: from pro-moting transparency through consistency to developing in-teractive model analysis tools. 2.