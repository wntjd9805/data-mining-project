Abstract
Continual learning (CL) aims to develop techniques by which a single model adapts to an increasing number of tasks encountered sequentially, thereby potentially leverag-ing learnings across tasks in a resource-efficient manner.
A major challenge for CL systems is catastrophic forget-ting, where earlier tasks are forgotten while learning a new task. To address this, replay-based CL approaches maintain and repeatedly retrain on a small buffer of data selected across encountered tasks. We propose Gradient Coreset
Replay (GCR), a novel strategy for replay buffer selection and update using a carefully designed optimization crite-rion. Specifically, we select and maintain a ’coreset’ that closely approximates the gradient of all the data seen so far with respect to current model parameters, and discuss key strategies needed for its effective application to the con-tinual learning setting. We show significant gains (2%-4% absolute) over the state-of-the-art in the well-studied of-fline continual learning setting. Our findings also effectively transfer to online / streaming CL settings, showing up to 5% gains over existing approaches. Finally, we demonstrate the value of supervised contrastive loss for continual learning, which yields a cumulative gain of up to 5% accuracy when combined with our subset selection strategy. 1.

Introduction
The field of continual learning (CL) [44] studies the training of models in an incremental fashion, to general-ize across a number of sequentially encountered scenarios or tasks, and to avoid the training & maintenance costs of one-off models. A key challenge in CL is the limited access to data from prior tasks; this results in catastrophic forget-ting [30], where training on subsequent tasks may poten-tially erase information in the model parameters pertaining to previous tasks. Approaches to address catastrophic for-Figure 1. When training CL models on S-Cifar100 with dif-ferent replay buffer sizes K ∈ [200, 500, 2000], the use of re-play buffers selected by GCR produces higher model accuracy and lower model forgetting than using reservoir-sampled replay buffers [6]. See text for more details. getting1 include modifications to the loss function (e.g., [23, 39]), to the network architecture (e.g., [18, 43]), and to the training procedure and data augmentation (e.g., [6]). In par-ticular, replay-based continual learning maintains a small data sketch from previous tasks, to be included in the train-ing mix throughout the lifetime learning of the model. Re-markably, as little as 1% of saved historical data, using random sampling, is enough to provide significant gains over other CL approaches [6]. This suggests that sophis-ticated methods for selecting compact data summaries or coresets may perform much better. However, previous ap-proaches to coreset in continual learning have focused on qualitative/diversity-based criteria [3, 47], or bi-level opti-mizations with significant computational costs and scaling limitations [4].
We present Gradient-based Coresets for Replay-based
CL (GCR), a principled, optimization-driven criterion for learning. selecting and updating coresets for continual
Specifically, we select a coreset that approximates the gra-dient of model parameters over the entirety of the data seen so far. We provide empirical evidence that coresets selected 1The papers cited, here and subsequently, under each paradigm or tech-nique are representative or recent papers, not necessarily the papers defin-ing or proposing said paradigm.
utilizing the above approach mitigate catastrophic forget-ting on par with, or better than previous methods. Fig-ure 1 illustrates how our coreset selection approach im-proves over random data selection by large margins, both in overall accuracy and in retaining of previous tasks. We explore the effect of better representation learning in the
CL setting by including a representation learning compo-nent [19] into our CL loss function. We conduct exten-sive experimentation against the state-of-the-art (SOTA) in the well-studied offline CL setting, where the current task’s data is available in entirety for iterative training (2-4% ab-solute gains over SOTA), as well as the online/streaming setting, where the data is only available in small batches and cannot be revisited (up to 5% gains over SOTA). In particular, we show that the benefit from our coreset selec-tion mechanism increases with increasing number of tasks, showing that GCR scales effectively with task count. Fi-nally, we demonstrate the superiority of GCR over other coreset methods for CL in a head-to-head comparison. 2.