Abstract
The paradigm of training models on massive data with-out label through self-supervised learning (SSL) and fine-tuning on many downstream tasks has become a trend re-cently. However, due to the high training costs and the un-consciousness of downstream usages, most self-supervised learning methods lack the capability to correspond to the diversities of downstream scenarios, as there are various data domains, different vision tasks and latency constraints on models. Neural architecture search (NAS) is one uni-versally acknowledged fashion to conquer the issues above, but applying NAS on SSL seems impossible as there is no label or metric provided for judging model selection.
In this paper, we present DATA, a simple yet effective NAS approach specialized for SSL that provides Domain-Aware and Task-Aware pre-training. Specifically, we (i) train a supernet which could be deemed as a set of millions of networks covering a wide range of model scales without any label, (ii) propose a flexible searching mechanism com-patible with SSL that enables finding networks of differ-ent computation costs, for various downstream vision tasks and data domains without explicit metric provided. Instan-tiated With MoCo v2, our method achieves promising re-sults across a wide range of computation costs on down-stream tasks, including image classification, object detec-tion and semantic segmentation. DATA is orthogonal to most existing SSL methods and endows them the ability of customization on downstream needs. Extensive experi-ments on other SSL methods demonstrate the generalizabil-ity of the proposed method. Code is released at https:
//github.com/GAIA-vision/GAIA-ssl.
*Corresponding author.
Figure 1. Illustration of how DATA works. We first build a super-net which is a set of many subnets, and train massive models si-multaneously in the regime of self-supervised learning. Then, we propose a unsupervised searching method that enables domain-aware and task-aware model selection without any label. The mechanism enables self-supervised models to fit various scenarios including point,edge and cloud covering different vision tasks like image classification, object detection, and segmentation. Network architectures in figure are ploted by software PlotNeuralNet [25]. 1.

Introduction
As is universally agreed that deep learning algorithms are data-hungry, how to leverage the exponentially growing unlabelled data from open-source has become a huge chal-lenge. Self-supervised learning (SSL), which utilizes the
inherent relationships of data to formulate supervision with-out manual annotation, has made remarkable progresses in both natural language processing (NLP) [14, 18, 34, 35] and computer vision (CV) [10, 19, 20] area. Despite its great success, to unleash the true power of SSL requires gigantic scale of data and unimaginable training budgets.
This brings about a side-effect that it is extremely expen-sive to train models of various architectures to cover the heterogeneous downstream needs, considering most scenar-ios are requesting for models of different scales and dif-ferent downstream vision tasks may desire different model architectures.
It is usually believed that neural architec-ture search (NAS) is designed for solving the issues above. labels are so indispensable for existing NAS
However, methods that it seems impossible to apply NAS in SSL, be-cause there is no clue for model selection if no label or met-ric is provided. These reflections leave us two problems: (1) Is it possible to train networks of distinctive archi-tectures simultaneously in SSL? Gladly, previous meth-ods [4, 5, 8, 44] have proved that training a supernet that comprises of millions of weight-sharing subnets is possible in the regime of supervised learning. Thus the hardship only lies in how to prevent the co-training of different networks from diverging when there is no strong and stable supervi-sion. Some recent studies [8, 49] interpret the process of
SSL of siamese based methods as a form of self-distillation that the query-branch works as a student and the teacher-branch works as a teacher. Thus if we stabilize the be-haviour of the teacher, we could provide a relatively steady knowledge source for the heterogeneous students. In this work, we build a supernet training mechanism for siamese based SSL that we fix the key-branch with the maximum architecture of supernet as a teacher and vary only the ar-chitectures of the query-branch. Experiments show that this ensures the efficiency of convergence and greatly im-proves the capability of feature representation of small sub-nets. More importantly, this design of training supernet in
SSL brings us the answer to the critical question below. (2) How to judge the quality of a network if no label or metric is provided? It is generally agreed that the big-ger the better works for deep neural networks when data is sufficient. Given a supernet that covers subnets of different sizes and the knowledge distillation behaviour of SSL, the distance between subnets and the maximum network natu-rally becomes a self-supervised metric for judging the qual-ity of networks. This metric works well especially when there is a budget constraint for subnets. More discussions about this assumption are placed in Sec 6
We further extend our exploration to enable the search-ing process to be aware of the type of downstream tasks that different tasks adopt different types of features for measur-ing the distance of student and teacher. This greatly mini-mizes the gap transferring to downstream tasks while keep-ing the searching strategy plug-and-play.
As shown in Figure 1, our approach enables training models of various sizes all in one go and searching appropri-ate models specialized for specific downstream tasks, com-putation constraints and data domains. This entire pipeline does not require any label for training or model selection.
Instantiated with MoCo v2 [11] , we validate our contribu-tions on evaluating our models on several standard of self-supervised benchmarks. We also combine our approach with other existing SSL methods [19,38,49] to demonstrate the generalizability. 2.