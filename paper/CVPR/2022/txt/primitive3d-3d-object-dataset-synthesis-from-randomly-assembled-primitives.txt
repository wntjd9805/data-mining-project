Abstract
Numerous advancements in deep learning can be at-tributed to the access to large-scale and well-annotated datasets. However, such a dataset is prohibitively expensive in 3D computer vision due to the substantial collection cost.
To alleviate this issue, we propose a cost-effective method for automatically generating a large amount of 3D objects with annotations. In particular, we synthesize objects sim-ply by assembling multiple random primitives. These ob-jects are thus auto-annotated with part labels originating from primitives. This allows us to perform multi-task learn-ing by combining the supervised segmentation with unsu-pervised reconstruction. Considering the large overhead of learning on the generated dataset, we further propose a dataset distillation strategy to remove redundant samples regarding a target dataset. We conduct extensive experi-ments for the downstream tasks of 3D object classification.
The results indicate that our dataset, together with multi-task pretraining on its annotations, achieves the best perfor-mance compared to other commonly used datasets. Further study suggests that our strategy can improve the model per-formance by pretraining and fine-tuning scheme, especially for the dataset with a small scale. In addition, pretraining with the proposed dataset distillation method can save 86% of the pretraining time with negligible performance degra-dation. We expect that our attempt provides a new data-centric perspective for training 3D deep models. 1.

Introduction
Deep learning has been shown to surpass prior state-of-the-art machine learning techniques in applications of 2D computer vision in the past several years [17–19]. Such suc-cess is often attributed to the ease of acquiring large-scale, richly-annotated and diverse 2D image datasets, e.g., Ima-geNet [16] and COCO [35]. In the field of 3D, a broad va-riety of applications, such as self-driving vehicles [9], aug-mented reality [22], and urban construction [7], can ben-*Equal contribution, Henghui was Xinke’s internship mentor.
†Corresponding author.
Figure 1. Compared to the CAD model and real-world data, the
Primitive3D dataset has richer annotations, lower collection cost, and higher sample diversity. These advantages help facilitate the learning of general knowledge for understanding 3D objects. efit from advancing 3D object understanding, which has long awaited high-quality datasets. Indeed, unlike the 2D counterparts, existing 3D object datasets are often limited in scale or lack of variety in annotation and instance diver-sity, thus hindering the advances of many applications.
The main reason for the limitation of 3D object datasets is the substantial cost related to data collection and anno-tation. In the current practice of real-world 3D object data acquisition, LiDAR or RGB-D scans often necessitate con-siderable labour and device costs [10, 14, 28], while image-derived data requires much computation efforts [33,51]. On the other hand, although synthesis datasets often consist of 3D CAD models with rich online data sources [6, 31, 63], they still involve manual design works and also have limited generalization for real-world objects recognition [55]. Ul-timately, tremendous human efforts have to be made in the annotation and maintenance of large-scale labeled datasets (e.g., voxel or point based label), especially considering the bulky size and the extra dimension of 3D data compared to 2D. To alleviate these problems, attempts have been made to automatically generate 3D objects via generative models built from existing datasets [2, 29, 40, 60]. Although these approaches are capable of generating high-quality 3D data
with fewer human interactions, they are task-oriented, com-putationally intensive, and may potentially incorporate bias from the training dataset. Therefore, it is critical to find a cheap 3D object data source as an alternative, which can derive large-scale, diverse and richly-annotated datasets, to push forward the development of 3D deep learning.
In our work, to obtain annotated 3D data at low cost, we introduce a learning-free method for the synthesis of pseudo 3D objects. Motivated by Constructive Solid Ge-ometry (CSG) scheme [48], we build a vast number of ran-dom objects from basic 3D shapes, i.e., primitives.
In a typical CSG scheme, 3D solids are often constructed with a tree representation, where the leaves are primitives and in-ternal nodes are boolean set operations. We randomize this tree-based construction by setting the parameters including tree structure, primitive parameters, boolean operations and rigid transformations as random variables. By uniformly sampling such variables, we derive a random tree, and a ran-dom object can be obtained by executing this tree from bot-tom to top. By tracking the original primitives after the con-struction, the object is automatically annotated with part-based labels. Furthermore, our analysis shows that such a method can generate objects with sufficient diversity, which allows the deep models to learn generalized representations from the resulting large-scale 3D dataset.
To exploit the abundance and annotations of the gener-ated objects, we provide a multi-task learning method for the point cloud of such objects. To grasp the object ge-ometry on a local and global level, the method combines two learning tasks: supervised segmentation of point clouds and unsupervised reconstruction of the original points. Re-garding the tremendous size of the generated dataset, we then present a method, dataset distillation, that is integrated into the learning process to relieve the computational bur-den. This method is accomplished by eliminating certain samples from the generated dataset to shrink its maximum mean discrepancy (MMD) to a target dataset.
In experiments, we evaluate the validity of our method by multiple object classification benchmarks. The results of the cross-dataset classification show that the features learned from our dataset can surpass those learned from other widely used 3D object datasets. Furthermore, pre-training by our method combined with fine-tuning can con-sistently improve the performance of downstream tasks with varying data size. Additionally, pretraining with dataset distillation can obtain comparable or even better per-formance than that with full dataset, reducing 86% of the pretraining time. In summary, our contributions are: 1. We present a cost-efficient method to generate a large amount of valid and diverse random 3D objects with part annotations automatically. The generated dataset as well as generation scripts will be released publicly. 2. We provide a multi-task learning method to train fea-ture encoders on our generated dataset with dense an-notations. We also propose an approach, dataset distil-lation, that can be optionally employed in the learning process to lower the computation cost. 3. Experiments show that our dataset serves as the best pretraining data for multiple downstream classification tasks in comparison to other commonly used datasets.
Our pretraining method can also consistently help the downstream tasks in achieving higher performance. 2.