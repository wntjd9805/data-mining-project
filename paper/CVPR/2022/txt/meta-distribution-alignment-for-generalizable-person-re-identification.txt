Abstract
Domain Generalizable (DG) person ReID is a challeng-ing task which trains a model on source domains yet gen-eralizes well on target domains. Existing methods use source domains to learn domain-invariant features, and assume those features are also irrelevant with target do-mains. However, they do not consider the target domain information which is unavailable in the training phrase of
DG. To address this issue, we propose a novel Meta Dis-tribution Alignment (MDA) method to enable them to share similar distribution in a test-time-training fashion. Specif-ically, since high-dimensional features are difficult to con-strain with a known simple distribution, we first introduce an intermediate latent space constrained to a known prior distribution. The source domain data is mapped to this la-tent space and then reconstructed back. A meta-learning strategy is introduced to facilitate generalization and sup-port fast adaption. To reduce their discrepancy, we fur-ther propose a test-time adaptive updating strategy based on the latent space which efficiently adapts model to un-seen domains with a few samples. Extensive experimental results show that our model outperforms the state-of-the-art methods by up to 5.2% R-1 on average on the large-scale and 4.7% R-1 on the single-source domain general-ization ReID benchmark. Source code is publicly available at https://github.com/haoni0812/MDA.git. 1.

Introduction
Person Re-identification (ReID) aims to match persons with the same ID across different camera views. Thanks to the development of deep convolutional neural networks (CNNs) [14], supervised ReID and unsupervised domain adaptation (UDA) [42] have achieved remarkable perfor-mance. However, they both need data of target domain for training. In real-world applications, the ReID system will inevitably search persons in unseen domains. Therefore,
*Corresponding author.
Figure 1. Illustration of our idea. Since target domain data (green) is unavailable during training, we cannot directly align source and target distributions. To address this issue, we align them to a prior distribution (purple) during training (source domains) and testing (target domains). Considering that high-dimensional ID features are difficult to constrain to a prior distribution, ID features are en-coded into latent embedding space. The same prior distribution and decoder guarantee the same generated feature distribution. domain generalization (DG) ReID has attracted extensive research attention in a practical setting.
Compared with supervised ReID and UDA setting, DG
ReID does not use target domain data for training. Only one or more labeled source datasets are available. Thus, most existing DG methods aim to learn domain invariant fea-ture through multiple source domains to generalize unseen domains. These methods explore generalization at feature-level based on disentanglement [13] or meta-learning [2, 3, 43]. However, a typical and effective cross-domain solution has been ignored in DG, that is, to align feature distributions across source and target domains. Methods based on dis-tribution alignment have not been researched because only source data is available for DG ReID. These method usually
need both source and target domain data for training. So we cannot align distributions based on the previous method.
Such observations reveal that it is challenging to align distributions directly. So we take a known prior distribution as the aligned goal, and ask: can we align source and tar-get distributions to the same known prior distribution dur-ing training and testing, respectively? However, constrain-ing the distribution of ID features to a known prior distribu-tion is difficult. Because ID feature is high-dimensional and contains ID information, its distribution is so complex that a known simple prior distribution cannot constrain it.
Thus, we adopt an encoding-decoding structure to en-code ID feature into latent space, whose latent embedding is low-dimensional and constrained to a known prior dis-tribution. Then we decode latent embedding through a de-coder. If the latent embedding distribution can also be close to the prior distribution during testing, we can treat latent embedding distributions during training and testing as the same. Note that the premise of the above conclusion is that the distance metric satisfies the triangle inequality, so we use Wasserstein distance instead of KL divergence to mea-sure the distance between distributions. Eventually, similar posterior distributions and the same decoder enable us to obtain aligned distributions. The main idea of our method is shown in Figure 1.
To further enhance the model generalization on unseen domains, we introduce a meta-learning strategy to simu-late the real train-test process. Specifically, we dynamically divide the source domain into a meta-train domain and a meta-test domain in each batch. The meta-train process is regarded as the training process of the source domain, and the meta-test simulates the situation of testing on the un-seen domain. In the meta-train stage, we use the meta-train domain to inner-update parameters. In the meta-test stage, we examine various generalization scenarios depending on the movement of inner loop, and perform a second-order updating on the original parameters. These two processes are performed alternately to improve generalization ability of the model on various unseen domains. During testing, we can further pull in latent embedding distributions across source and target domains by a test-train strategy at batch-level. We will only update the encoder in this process, to ensure the same decoder for source and target domains.
In summary, our contributions are three-fold:
• We propose a novel Meta Distribution Alignment (MDA) for DG ReID, which is a pioneering work on aligning distributions across source and target domains for DG ReID task.
• We design a meta-learning strategy to simulate the real train-test process, which improves the generalization of the model. A test-time adaptive updating strategy is further proposed to efficiently adapt the model to unseen domains with a few samples.
• We perform extensive experiments and achieve state-of-the-art performance on the large-scale DG ReID and single-source DG ReID. 2.