Abstract
The task of predicting future actions from a video is cru-cial for a real-world agent interacting with others. When anticipating actions in the distant future, we humans typi-cally consider long-term relations over the whole sequence of actions, i.e., not only observed actions in the past but also potential actions in the future. In a similar spirit, we pro-pose an end-to-end attention model for action anticipation, dubbed Future Transformer (FUTR), that leverages global attention over all input frames and output tokens to predict a minutes-long sequence of future actions. Unlike the pre-vious autoregressive models, the proposed method learns to predict the whole sequence of future actions in parallel de-coding, enabling more accurate and fast inference for long-term anticipation. We evaluate our method on two stan-dard benchmarks for long-term action anticipation, Break-fast and 50 Salads, achieving state-of-the-art results. 1.

Introduction
Long-term action anticipation from a video is recently emerging as an essential task for advanced intelligent sys-tems. It aims to predict a sequence of actions in the future from a limited observation of past actions in a video. While there exists a growing body of research on action anticipa-tion, most of the recent work focuses on predicting a sin-gle action in a few seconds [14–17, 30, 35–37]. In contrast, long-term action anticipation [2, 13, 19, 36] aims to predict a minutes-long sequence of multiple actions in the future.
This task is challenging since it requires learning long-range dependencies between past and future actions.
Recent long-term anticipation methods [13, 36] encode observed video frames into condensed vectors and decode them via recurrent neural networks (RNNs) to predict a se-quence of future actions in an autoregressive manner. De-spite the impressive performance on the standard bench-marks [21, 38], they have several limitations. First, the en-coder excessively compresses the input frame features so that fine-grained temporal relations between the observed frames are not preserved. Second, the RNN decoder is
Figure 1. Future Transformer (FUTR). The proposed method is an end-to-end attention neural network to anticipate actions in parallel decoding, leveraging global interactions between past and future actions for long-term action anticipation. limited in modeling long-term dependencies over the input sequence and also in considering global relations between past and future actions. Third, the sequential prediction of autoregressive decoding may accumulate errors from the precedent results and also increase inference time.
To resolve the limitations, we introduce an end-to-end attention neural network, Future Transformer (FUTR), for long-term action anticipation. The proposed method effec-tively captures long-term relations over the whole sequence of actions. i.e., not only observed actions in the past but also potential actions in the future. FUTR is an encoder-decoder structure [6,42] as illustrated in Fig. 1; the encoder learns to capture fine-grained long-range temporal relations between the observed frames from the past, while the decoder learns to capture global relations between upcoming actions in the future along with the observed features from the encoder.
Different from the previous autoregressive models, FUTR anticipates a sequence of future actions in parallel decod-ing, enabling more accurate and faster inference without er-ror accumulations. Furthermore, we employ an action seg-mentation loss for input frames to learn distinctive feature representations in the encoder. We evaluate FUTR on the
standard benchmarks for long-term action anticipation and achieve new state-of-the-art results on Breakfast and 50 Sal-ads. The main contribution of our paper is four-fold:
• We introduce an end-to-end attention neural net-work, dubbed FUTR, which effectively leverages fine-grained features and global interactions for long-term action anticipation.
• We propose to predict a sequence of actions in parallel decoding, enabling accurate and fast inference.
• We develop an integrated model that learns distinctive feature representation by segmenting actions in the en-coder and anticipating actions in the decoder.
• The proposed method sets a new state of the arts on standard benchmarks for long-term action anticipation,
Breakfast and 50 Salads. 2.