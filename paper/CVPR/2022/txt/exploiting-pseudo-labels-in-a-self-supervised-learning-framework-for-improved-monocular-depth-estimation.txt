Abstract
Image
Ground Truth
We present a novel self-distillation based self-supervised monocular depth estimation (SD-SSMDE) learning frame-work.
In the ﬁrst step, our network is trained in a self-supervised regime on high-resolution images with the pho-tometric loss. The network is further used to generate pseudo depth labels for all the images in the training set.
To improve the performance of our estimates, in the sec-ond step, we re-train the network with the scale invariant logarithmic loss supervised by pseudo labels. We resolve scale ambiguity and inter-frame scale consistency by intro-ducing an automatically computed scale in our depth la-bels. To ﬁlter out noisy depth values, we devise a ﬁltering scheme based on the 3D consistency between consecutive views. Extensive experiments demonstrate that each pro-posed component and the self-supervised learning frame-work improve the quality of the depth estimation over the baseline and achieve state-of-the-art results on the KITTI and Cityscapes datasets.
Depth Prediction
Error Map
] 8 1
[ f l e
S o d u e s
P
Figure 1. SD-SSMDE, our self-distillation framework for self-supervised monocular depth estimation. The teacher network (Self) trained in a self-supervised manner brings signiﬁcant im-provements over the baseline [18]. Our student model (Pseudo) is trained with pseudo labels generated with the previous model. The error is reduced, especially on the car on the right and on the entire left-hand side area of the image. In the error maps, small error is encoded with blue, while large error is encoded with red. 1.

Introduction
One of the long-lasting research ﬁelds of computer vi-sion is the accurate estimation of the 3D geometry of scenes.
This includes tasks such as depth and ego motion predic-tion, which have major importance in the perception sys-tem of real-world applications, such as robotics and auto-mated driving. While precise depth measurements can be directly obtained using specialized sensors such as LiDAR, they have several disadvantages such as high cost and re-duced output density. As an alternative, estimating depth from images captured from a moving monocular or binoc-ular system of cameras is attractive due to the lower cost and generally simpler setup. The stereo setup exhibits some limitations from a practical point of view as the stereo rig has to be carefully calibrated and synchronized. pared to aforementioned methods. With the recent advances in deep learning, the performance gap has been reduced, especially in a supervised setting. The prohibitively large cost of collecting high-quality ground truth has led to the emergence of self-supervised monocular depth estimation, which unlocks the power of large-scale unlabeled datasets.
Such approaches learn both the depth and ego motion, and embed 3D geometric constraints by using 3D reprojection models to synthesize consecutive images. More speciﬁ-cally, points from the target frame are back-projected in the camera coordinate system, displaced by the camera motion and reprojected onto adjacent source frames. In this way, the target image can be reconstructed from the source im-ages, and the photometric difference between the target and synthesized image will be minimized during training.
Monocular depth estimation is an inherently ill-posed problem and initial results had lower performance com-Self-supervised monocular depth estimation relies on several assumptions that are not always true and hinder the
learning performance. It assumes that the scene is rigid and the camera is moving, that all image regions can be recon-structed from the neighboring frames and that all surfaces are Lambertian, i.e. have constant brightness. However, dynamic objects or a static camera, occlusions and illumi-nation changes between consecutive views break these as-sumptions. Recent works address various issues [2,5,18,38] by designing masking techniques for ﬁltering errors dur-ing training, by using stereo images or external information such as semantic segmentation and optical ﬂow to guide the training process or improve the feature representation.
However, in the self-supervised setting, propagating correct training signals is still difﬁcult for all pixels, photometric loss can be high in occluded areas or for moving objects, and low in uniform texture areas or for repetitive structures.
In this paper, we propose a novel self-distillation based self-supervised learning framework for monocular depth es-timation (SD-SSMDE) that leads to signiﬁcant improve-ments when trained on monocular video and introduce the following contributions: (1) a two-stage self-distillation training strategy for monocular depth estimation: self-su-pervised ﬁrst stage to generate high resolution pseudo la-bels and a supervised second stage using a similar or a more lightweight network (2) a novel architecture for the depth network for more accurate results (3) we solve scale ambi-guity by incorporating the scale in pseudo labels, therefore depth predictions from the second stage are scaled and in-ter-frame scale-consistent (4) a ﬁltering strategy based on 3D consistency between consecutive views to ﬁlter out large errors in pseudo labels. We perform extensive experiments on the KITTI and Cityscapes datasets and demonstrate that the proposed network and two-stage training framework yield state-of-the-art results and surpass or achieve on par results with current approaches. 2.