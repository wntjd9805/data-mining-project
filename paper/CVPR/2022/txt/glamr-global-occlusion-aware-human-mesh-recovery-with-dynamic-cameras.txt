Abstract
We present an approach for 3D global human mesh re-covery from monocular videos recorded with dynamic cam-eras. Our approach is robust to severe and long-term occlu-sions and tracks human bodies even when they go outside the camera’s ﬁeld of view. To achieve this, we ﬁrst propose a deep generative motion inﬁller, which autoregressively in-ﬁlls the body motions of occluded humans based on visi-ble motions. Additionally, in contrast to prior work, our approach reconstructs human meshes in consistent global coordinates even with dynamic cameras. Since the joint re-construction of human motions and camera poses is under-constrained, we propose a global trajectory predictor that generates global human trajectories based on local body movements. Using the predicted trajectories as anchors, we present a global optimization framework that reﬁnes the predicted trajectories and optimizes the camera poses to match the video evidence such as 2D keypoints. Experi-ments on challenging indoor and in-the-wild datasets with dynamic cameras demonstrate that the proposed approach
*Work done during an internship at NVIDIA. outperforms prior methods signiﬁcantly in terms of motion inﬁlling and global mesh recovery. 1.

Introduction
Recovering ﬁne-grained 3D human meshes from monoc-ular videos is essential for understanding human behaviors and interactions, which can be the cornerstone for numer-ous applications including virtual or augmented reality, as-sistive living, autonomous driving, etc. Many of these ap-plications use dynamic cameras to capture human behaviors yet also require estimating human motions in global coor-dinates consistent with their surroundings. For instance, assistive robots and autonomous vehicles need a holistic understanding of human behaviors and interactions in the world to safely plan their actions even when they are mov-ing. Therefore, our goal in this paper is to tackle the impor-tant task of recovering global human meshes from monocu-lar videos captured by dynamic cameras.
However, this task is highly challenging for two main reasons. First, dynamic cameras make it difﬁcult to estimate human motions in consistent global coordinates. Existing human mesh recovery methods estimate human meshes in
the camera coordinates [67, 114] or even in the root-relative coordinates [45, 68]. Hence, they can only recover global human meshes from dynamic cameras by using SLAM to estimate camera poses [58]. However, SLAM can often fail for in-the-wild videos due to moving and dynamic objects.
It also has the problem of scale ambiguity, which often leads to camera poses that are inconsistent with the human mo-tions. Second, videos captured by dynamic cameras often contain severe and long-term occlusions of humans, which can be caused by missed detection, complete obstruction by objects and other people, or the person going outside the camera’s ﬁeld of view (FoV). These occlusions pose seri-ous challenges to standard human mesh recovery methods, which rely on detections or visible parts to estimate human meshes. Only a few works have attempted to tackle the oc-clusion problem in human mesh recovery [17, 37]. How-ever, these methods can only address partial occlusions of a person and fail to handle severe occlusions when the person is completely invisible for an extended period of time.
To tackle the above challenges, we propose Global
Occlusion-Aware Human Mesh Recovery (GLAMR), which can handle severe occlusions and estimate human meshes in consistent global coordinates – even for videos recorded with dynamic cameras. We start by using off-the-shelf methods (e.g., KAMA [34] or SPEC [47]) to estimate the shape and pose sequences (motions) of visible people in the camera coordinates. These methods also rely on multi-object tracking and re-identiﬁcation, which provide occlu-sion information, and the motion of occluded frames is not estimated. To tackle potentially severe occlusions, we pro-pose a deep generative motion inﬁller that autoregressively inﬁlls the local body motions of occluded people based on visible motions. The motion inﬁller leverages human dy-namics learned from a large motion database, AMASS [62].
Next, to obtain global motions, we propose a global trajec-tory predictor that can generate global human trajectories based on local body motions. It is motivated by the obser-vation that the global root trajectory of a person is highly correlated with the local body movements. Finally, using the predicted trajectories as anchors to constrain the solu-tion space, we further propose a global optimization frame-work that jointly optimizes the global motions and camera poses to match the video evidence such as 2D keypoints.
The contributions of this paper are as follows: (1) We propose the ﬁrst approach to address long-term occlusions and estimate global 3D human pose and shape from videos captured by dynamic cameras; (2) We propose a novel gen-erative Transformer-based motion inﬁller that autoregres-sively inﬁlls long-term missing motions, which consider-ably outperforms state-of-the-art motion inﬁlling methods; (3) We propose a method to generate global human trajec-tories from local body motions and use the generated tra-jectories as anchors to constrain global motion and camera optimization; (4) Extensive experiments on challenging in-door and in-the-wild datasets demonstrate that our approach outperforms prior state-of-the-art methods signiﬁcantly in tackling occlusions and estimating global human meshes. 2.