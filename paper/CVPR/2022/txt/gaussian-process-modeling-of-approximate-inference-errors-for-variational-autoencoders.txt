Abstract
Variational autoencoder (VAE) is a very successful gen-erative model whose key element is the so-called amor-tized inference network, which can perform test time in-ference using a single feed forward pass. Unfortunately, this comes at the cost of degraded accuracy in posterior ap-proximation, often underperforming the instance-wise vari-ational optimization. Although the latest semi-amortized approaches mitigate the issue by performing a few varia-tional optimization updates starting from the VAE’s amor-tized inference output, they inherently suffer from compu-tational overhead for inference at test time. In this paper, we address the problem in a completely different way by considering a random inference model, where we model the mean and variance functions of the variational posterior as random Gaussian processes (GP). The motivation is that the deviation of the VAE’s amortized posterior distribution from the true posterior can be regarded as random noise, which allows us to view the approximation error as uncer-tainty in posterior approximation that can be dealt with in a principled GP manner. In particular, our model can quan-tify the difficulty in posterior approximation by a Gaussian variational density. Inference in our GP model is done by a single feed forward pass through the network, significantly faster than semi-amortized methods. We show that our ap-proach attains higher test data likelihood than the state-of-the-arts on several benchmark datasets. 1.

Introduction
Variational Autoencoder (VAE) [14, 31] is a very suc-cessful generative model where a highly complex deep non-linear generative process can be easily incorporated. A key element of the VAE, the deep inference (a.k.a. encoder) network, can perform the test time inference using a sin-gle feed forward pass through the network, bringing sig-nificant computational speed-up. This feature, known as amortized inference, allows the VAE to circumvent other-wise time-consuming steps of solving the variational opti-mization problem for each individual instance at test time, required in the standard variational inference techniques, such as the stochastic variational inference (SVI) [9].
As suggested by the recent study [2], however, the amor-tized inference can also be a drawback of the VAE, specifi-cally the accuracy of posterior approximation by the amor-tized inference network is often lower than the accuracy of the SVI’s full variational optimization. There are two gen-eral approaches to reduce this amortization error. The first is to increase the network capacity of the inference model (e.g., flow-based models [13,37]). The other direction is the so-called semi-amortized approach [12, 15, 24, 27], where the key idea is to use the VAE’s amortized inference net-work to produce a good initial distribution, from which a few SVI steps are performed at test time to further reduce the amortization error, quite similar in nature to the test time model adaptation of the MAML [6] in multi-task (meta) learning. Although these models often lead to improved posterior approximation, they raise several issues: Train-ing the models for the former family of approaches is usu-ally difficult because of the increased model complexity; the latter approaches inadvertently suffer from computational overhead of additional SVI gradient steps at test time.
In this paper, we propose a novel approach to address these drawbacks. We retain the amortized inference frame-work similar to the standard VAE for its computational ben-efits, but consider a random inference model. Specifically, the mean and the variance functions of the variational pos-terior distribution are a priori assumed to be Gaussian pro-cess (GP) distributed. There are two main motivations for this idea. The first one stems from the suboptimality of the
VAE, where the estimated amortized inference network suf-fers from deviation from the true posteriors. This inaccu-racy can be viewed and modeled as uncertainty in the poste-rior approximation of the deterministic amortized inference network, suggesting the need for a principled Bayesian un-certainty treatment. The second intuition is that the devia-tion of the VAE’s variational posterior distributions from the true posteriors can be naturally regarded as random noise.
Whereas the semi-amortized approaches perform extra SVI gradient updates at test time to account for this noise, we model the discrepancy using a Bayesian neural network (GP), resulting in a faster and more accurate amortized model via principled uncertainty marginalization. Another benefit of the Bayesian treatment is that we can quantify the discrepancy in approximation, which can serve as indica-tors for goodness of posterior approximations. The infer-ence in our model is significantly faster than that of semi-amortized methods, accomplished by a single feed forward pass through the GP posterior marginalized inference net-work. We show that our approach attains higher test data likelihood scores than the state-of-the-art semi-amortized approaches and even the high-capacity flow-based encoder models on several benchmark datasets. 2.