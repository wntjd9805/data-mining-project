Abstract
We describe a method to extract persistent elements of a dynamic scene from an input video. We represent each scene element as a Deformable Sprite consisting of three components: 1) a 2D texture image for the entire video, 2) per-frame masks for the element, and 3) non-rigid deforma-tions that map the texture image into each video frame. The resulting decomposition allows for applications such as con-sistent video editing. Deformable Sprites are a type of video auto-encoder model that is optimized on individual videos, and does not require training on a large dataset, nor does it rely on pre-trained models. Moreover, our method does not require object masks or other user input, and discovers moving objects of a wider variety than previous work. We evaluate our approach on standard video datasets and show qualitative results on a diverse array of Internet videos. 1.

Introduction
When we observe a video of a dynamic scene, such as the bear video in Fig. 1, we do not see a disjoint set of pixels over time, but rather a bear walking in a zoo. However, computer vision methods often represent videos as 3D raster pixel grids. While this low-level representation is convenient for processing on hardware, it does not capture our intuitive notion of high-level objects moving through a 3D scene.
Moving layers are an alternative representation proposed in the seminal work of Wang & Adelson, in which scene elements are modeled as persistent image layers that trans-form over time to compose each video frame [34]. Such layered representations capture the idea that there are persis-tent motion groups that move smoothly in the scene, while still accounting for sharp edges that result from occlusions.
However, these classic methods were limited by machinery of the time to relatively simple motions and scenes.
Inspired by these classic ideas [10, 26, 34], we present a new approach that decomposes videos of complex dynamic scenes into sets of persistent motion groups. We do so by introducing Deformable Sprites (Fig. 1, center), representa-tion of motion groups across an entire video. A Deformable
Sprite for a motion group consists of three key components: 1) a canonical texture image, or sprite, describing the group’s appearance over all input frames, 2) masks locating the group in each input frame, and 3) a non-rigid geometric transfor-mation that maps each sprite into each frame. The resulting decomposition captures the correspondences of each motion group across the entire video, such that modiﬁcations of the
sprite can be propagated consistently throughout the video (Fig. 1, right).
We achieve this decomposition by ﬁtting the Deformable
Sprites representation to a video without any user input or even an a priori notion of what kind of objects will be present.
Instead, the decomposition is derived solely from image and motion cues present in the video. Our approach optimizes the Deformable Sprites for each video independently, and does not require training on a dataset. This freedom from the need for training data allows our method to handle videos of novel objects and categories that are not labeled in standard segmentation benchmarks.
Our approach can handle videos with moving camera and articulated or deformable objects. To capture such non-rigid motion, we model the transformation as composition of a homography with 2D spatial splines that evolve smoothly over time. This explicit parameterization has the beneﬁt of modeling non-rigid deformation with relatively few parame-ters while being continuous. The sprites and the masks are optimized through a convolutional neural network.
Our method has several advantages over recent ap-proaches for video decomposition, which do not recover persistent layer appearances [21,38], or require user inputs in the form of segmentation masks [20]. Although our method outputs a rich video decomposition, and not simply object masks, we evaluate it on standard video object segmenta-tion benchmarks where we obtain competitive results. On
DAVIS [25], we obtain decomposition results that are sim-ilar to recent approaches that require user mask initializa-tion [11], while being faster to optimize (30 minutes vs. 10 hours) due to the low dimensionality of our deformation model. We further demonstrate our approach on a variety of Internet clips, where off-the-shelf segmentation methods do not generalize to discover meaningful groupings. To our knowledge, we present the ﬁrst work that demonstrates video decomposition with a global texture model on in-the-wild videos without any user supervision. 2.