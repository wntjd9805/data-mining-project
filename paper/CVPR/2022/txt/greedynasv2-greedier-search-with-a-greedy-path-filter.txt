Abstract
Training a good supernet in one-shot NAS methods is difficult since the search space is usually considerably huge (e.g., 1321). In order to enhance the supernet’s evaluation ability, one greedy strategy is to sample good paths, and let the supernet lean towards the good ones and ease its eval-uation burden as a result. However, in practice the search can be still quite inefficient since the identification of good paths is not accurate enough and sampled paths still scatter around the whole search space. In this paper, we leverage an explicit path filter to capture the characteristics of paths and directly filter those weak ones, so that the search can be thus implemented on the shrunk space more greedily and efficiently. Concretely, based on the fact that good paths are much less than the weak ones in the space, we argue that the label of “weak paths” will be more confident and reliable than that of “good paths” in multi-path sampling. In this way, we thus cast the training of path filter in the positive and unlabeled (PU) learning paradigm, and also encour-age a path embedding as better path/operation represen-tation to enhance the identification capacity of the learned filter. By dint of this embedding, we can further shrink the search space by aggregating similar operations with similar embeddings, and the search can be more efficient and ac-curate. Extensive experiments validate the effectiveness of the proposed method GreedyNASv2. For example, our ob-tained GreedyNASv2-L achieves 81.1% Top-1 accuracy on
ImageNet dataset, significantly outperforming the ResNet-50 strong baselines. 1.

Introduction
Neural architecture search (NAS) aims to boost the per-formance of deep learning by seeking an optimal architec-ture in the given space, and it has achieved significant im-*Correspondence to: Shan You <youshan@sensetime.com>.
Figure 1. Performance of searched architectures w.r.t. different scales of search space. Left: retraining accuracies of models searched by GreedyNASv2 and baselines. Right: validation ac-curacies of searched models on supernets. provements in the sight of applications, such as image clas-sification [11, 26, 28, 32, 34] and object detection [2, 10].
One-shot NAS [11, 15, 23–25, 33, 34] stands out from the literature of NAS for the sake of its decent searching effi-ciency. Instead of exhaustively training each possible archi-tecture, one-shot NAS fulfills the searching in an only one-shot trial, where a supernet is leveraged to embody all can-didate architectures (i.e., paths). Each path can be param-eterized by the corresponding weights within the supernet, and thus gets trained, evaluated, and ranked. Typical uni-form sampling (SPOS) [11] is usually adopted to train the supernet because of the feasible single-path memory con-sumption and being friendly to large-scale datasets.
The architecture search space in NAS could be consid-erably huge (e.g., 1321). Equally treating different paths and uniformly sampling them from the supernet could lead to inappropriate training of the supernet, as the weak paths would disturb the highly-shared weights. Various sampling strategies have thus been proposed to address this issue, such as fair sampling [3] and Monte-Carlo tree search [23].
We are particularly interested in the strategy of multi-path sampling with rejection by GreedyNAS [34], which iden-tifies good paths from weak paths and then only greed-Figure 2. Left: the architecture of our path filter. Right: diagram of supernet training in GreedyNASv2. In GreedyNASv2, we adopt a path filter to filter weak paths from the uniformly-sampled paths, and the remained potentially-good paths are then used for optimization. The path filter is trained using weak paths identified by a validation set and unlabeled paths. ily updates those good ones; it is easy to implement and more suitable for large search spaces among these meth-ods. Working on the whole search space, GreedyNAS has to safely allocate only a medium level of partition of good paths (e.g., only 5 out of 10) to ensure a high probabil-ity of sampled paths being good. But the search will be-come infeasible and limited if the search space grows larger with more operation choices. Besides, GreedyNAS needs to maintain a candidate pool to recycle paths, which limits the number of stored paths, and many elite paths could be missed.
In this paper, we propose GreedyNASv2 to power the multi-path sampler with explicit search space shrinkage for one-shot NAS, which targets on a greedier search space with an tiny (e.g., only 1%) proportion of paths treated as
“good paths”. Since good paths are usually much less than weak paths, the probability of picking out a good path by a multi-path sampler could be smaller than that of sampling weak paths. If weak paths can be captured with confidence, we can easily screen them out from the searching space and execute a greedier search on the shrunk space. By doing so, the supernet only needs to focus on evaluating those not too bad paths (potentially-good paths), which benefits the over-all searching performance and efficiency simultaneously.
The key is then to learn a path filter to identify those weak paths from the entire architecture search space.
Though it is hard to find a good path, we can have high con-fidence about weak paths in the multi-path sampling. These identified weak paths with confidence can be regarded as positive examples to be thrown away. As a precaution, the remaining paths in the search space are taken as unlabeled examples, as they may contain both weak paths (positive examples to be thrown away) and good paths (negative ex-amples not to be thrown away). The learning of this path filter can thus be formulated as the Positive-Unlabeled (PU) learning problem [9, 17]. Once the path filter has been well trained, a given new path can be efficiently predicted to specify whether it is weak or not. A path embedding is also learned with the path filter to encode the path as a bet-ter path representation. Since the path embedding is learned in the weak/good sense, if two operations have similar em-beddings, it means that both operations have similar or even the same impact on discriminating paths, and they can be thus merged. This enables a greedy shrinkage of operations, which is expected to work together with the path shrinkage to boost the searching performance and efficiency further.
We conduct extensive experiments on ImageNet dataset to validate the effectiveness of our proposed GreedyNASv2.
Compared to the baseline methods SPOS and GreedyNAS, our proposed method achieves better performance with less search cost. To further investigate our superiority, we even search on a larger space, which has ∼ 104× architectures compared to the commonly-used MobileNetV2-SE search space, and the results show that our searched model out-performs state-of-the-art NAS models. The performance on different scales of search spaces are illustrated in Figure 1.
Besides, we also compare the searching performance on a recent benchmark NAS-Bench-Macro [23] for one-shot
NAS. Ablation studies show that our GreedyNASv2 effec-tively samples better architectures than uniform sampling and the multi-path sampler in GreedyNAS. 2.