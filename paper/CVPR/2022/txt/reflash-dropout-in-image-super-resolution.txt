Abstract
Dropout is designed to relieve the overfitting problem in high-level vision tasks but is rarely applied in low-level vi-sion tasks, like image super-resolution (SR). As a classic regression problem, SR exhibits a different behaviour as high-level tasks and is sensitive to the dropout operation.
However, in this paper, we show that appropriate usage of dropout benefits SR networks and improves the generaliza-tion ability. Specifically, dropout is better embedded at the end of the network and is significantly helpful for the multi-degradation settings. This discovery breaks our common sense and inspires us to explore its working mechanism. We further use two analysis tools – one is from a recent network interpretation work, and the other is specially designed for this task. The analysis results provide side proofs to our experimental findings and show us a new perspective to un-derstand SR networks. 1.

Introduction
Image super-resolution (SR) is a classic low-level vision task aiming at restoring a high-resolution image from a low-resolution input. Benefiting from the powerful convolu-tional neural networks (CNNs), deep SR networks [6–8, 23, 25,27,29,57–59] can easily fit the training data and achieve impressive results in a synthetic environment. To further extend their success to real-world images, researchers be-gin to design blind SR methods [30], which can deal with unknown downsampling kernels or degradations. Recent advances have made significant progress by enriching the data diversity [9, 49, 54, 55] and enlarging the model capac-ity [33, 48], but none of them has tried to improve the train-ing strategy. The overfitting problem will become promi-nent when the network scale increases significantly, result-*Equal contributions
†Corresponding author (e-mail: chao.dong@siat.ac.cn)
Figure 1. Dropout can significantly improve the performance of the models under the multi-degradation setting. It can even help
SRResNet outperform RRDB, while the latter has ten times more parameters. There are the PSNR (dB) results of ×4 SR models on Set5 with different degradations. For example, clean means input LR images without any degradations, noise means input LR images with noise. ing in a weak generalization ability. Then what kind of training strategy is suitable for the blind SR task? A simple yet surprising answer comes to our mind. It is dropout [20], which is originally designed to avoid overfitting and has been proved effective in high-level vision tasks.
In this work, we will dive into the usage of dropout and reflash it in super-resolution.
Dropout seems to be in conflict with SR in nature.
Specifically, the mechanism of dropout is to disable some units and produce a number of sub-networks randomly.
Each sub-network is able to give an acceptable result. How-ever, SR is a standard regression problem, where network features and channels all have contributions to the final out-put.
If we randomly discard some features or pixels, the output performance will drop severely. That is why we can-not see the application of dropout in SR, as well as other low-level vision tasks. From another perspective, overfit-ting is not a severe problem in conventional SR tasks; thus,
SR does not need dropout as well. However, this situation changes nowadays. First, overfitting has become a domi-nant problem for blind SR [30]. Simply increasing the data and network scale cannot continuously improve the gener-alization ability. Second, we have obtained a series of anal-ysis tools in the area of network interpretation, assisting us in finding better ways of application.
To study dropout, we begin with its usage in the con-ventional non-blind settings. After trying different dropout strategies, we can conclude detailed guidance of using dropout in SR. With appropriate usage of dropout, the per-formance of SR models can improve significantly in both in-distribution (seen in the training set) and out-distribution (unseen) data. Figure 1 shows the performance before and after dropout, where the most significant PSNR gap can reach 0.95 dB.
It is worth noting that dropout can help
SRResNet even outperform RRDB, while the latter has ten times more parameters. More importantly, adding dropout is only one line of code and has no sacrifice on computa-tion cost. The most appealing part of this paper does not lie in the experiments but in the following analysis. We adopt two novel interpretation tools, i.e., channel saliency map and deep degradation representation [31]) to analyze the behaviour of dropout. We find that dropout can equalize the importance of feature maps, which could inherently im-prove the generalization ability. There are also some other interesting observations, which all support our experimental results. We believe that these analyses can help us under-stand the working mechanism of SR networks and inspire more effective training strategies in the future. 2.