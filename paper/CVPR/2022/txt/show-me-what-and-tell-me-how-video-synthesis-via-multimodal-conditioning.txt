Abstract
Most methods for conditional video synthesis use a sin-gle modality as the condition. This comes with major lim-itations. For example, it is problematic for a model con-ditioned on an image to generate a specific motion trajec-tory desired by the user since there is no means to provide motion information. Conversely, language information can describe the desired motion, while not precisely defining the content of the video. This work presents a multimodal video generation framework that benefits from text and im-ages provided jointly or separately. We leverage the recent progress in quantized representations for videos and apply a bidirectional transformer with multiple modalities as in-puts to predict a discrete video representation. To improve video quality and consistency, we propose a new video token trained with self-learning and an improved mask-prediction algorithm for sampling video tokens. We introduce text aug-mentation to improve the robustness of the textual represen-tation and diversity of generated videos. Our framework can incorporate various visual modalities, such as segmen-tation masks, drawings, and partially occluded images. It can generate much longer sequences than the one used for training. In addition, our model can extract visual infor-mation as suggested by the text prompt, e.g., “an object in image one is moving northeast”, and generate correspond-ing videos. We run evaluations on three public datasets and a newly collected dataset labeled with facial attributes, achieving state-of-the-art generation results on all four1. 1.

Introduction
Generic video synthesis methods generate videos by sampling from a random distribution [56, 57]. To get more control over the generated content, conditional video syn-thesis works utilize input signals, such as images [9, 19], text or language [7, 30], and action classes [62]. This en-ables synthesized videos containing the desired objects as
*Work done during an internship at Snap Inc. 1Code: https://github.com/snap-research/MMVID and Webpage. specified by visual information or desired actions as speci-fied by textual information.
Existing works on conditional video generation use only one of the possible control signals as inputs [8, 30]. This limits the flexibility and quality of the generative process.
For example, given a screenplay we could potentially gen-erate several movies, depending on the decisions of the di-rector, set designer, and visual effect artist. In a similar way, a video generation model conditioned with a text prompt should be primed with different visual inputs. Addition-ally, a generative video model conditioned on a given im-age should be able to learn to generate various plausible videos, which can be defined from various natural language instructions. For example, to generate object-centric videos with objects moving [70], the motion can be easily defined through a text prompt, e.g., “moving in a zig-zag way,” while the objects can be defined by visual inputs. Thus, an interesting yet challenging question arises: Can we learn a video generation model that can support such behavior?
We tackle the question in this work and propose a new video synthesis model supporting diverse, multimodal con-ditioning signals. Our method consists of two phases. The first phase obtains discrete representations from images. We employ an autoencoder with a quantized bottleneck, in-spired by the recent success of two-stage image generation using quantized feature representations [21, 38, 47, 76]. The second phase learns to generate video representations that are conditioned on the input modalities, which can then be decoded into videos using the decoder from the first stage.
We leverage a bidirectional transformer, i.e., BERT [17], trained with a masked sequence modeling task, that uses to-kens from multimodal samples and predicts the latent rep-resentation for videos. Building such a framework requires solving several challenging problems. First, video consis-tency is a common problem among video generation meth-ods. Second, it is necessary to ensure that the correct textual information is learned. Third, training a transformer for im-age synthesis is computationally demanding [12], an issue that is even more severe in the time domain, as a longer se-quence of tokens needs to be learned. To solve these chal-lenges, we propose the following contributions: 1
• We introduce a bidirectional transformer with several new techniques to improve video generation: For train-ing, we propose the video token VID, which is trained via self-learning and video attention, to model temporal consistency; For inference, we improve mask-predict to generate videos with improved quality.
• We introduce text augmentation, including text dropout and pretrained language models for extracting textual embeddings, to generate diverse videos that are corre-lated with the provided text.
• We explore long sequence synthesis with the trans-former model to generate sequences with lengths that are much longer than the one used for training (Fig. 5).
We name our framework MMVID and show that a
MultiModal VIDeo generator can enable various applica-tions. The user can show what to generate using visual modalities and tell how to generate with language. We ex-plore two settings for multimodal video generation. The first involves independent multimodalities, such that there is no relationship between textual and visual controls (Fig. 3a and Fig. 4). The second one targets dependent multimodal generation, where we use text to obtain certain attributes from given visual controls (Fig. 3b and Fig. 4). The lat-ter case allows for more potential applications, in which language is not able to accurately describe certain image content that the user seeks to generate, but images can ef-ficiently define such content. We also show our model can use diverse visual information, including segmentation masks, drawings, and partially observed images (Fig. 4).
To validate our approach extensively, we conduct exper-iments on four datasets. In addition to three public datasets, we collect a new dataset, named Multimodal VoxCeleb, that includes 19, 522 videos from VoxCeleb [37] with 36 manu-ally labeled facial attributes. 2.