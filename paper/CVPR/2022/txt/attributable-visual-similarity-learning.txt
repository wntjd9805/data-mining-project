Abstract
This paper proposes an attributable visual similarity learning (AVSL) framework for a more accurate and ex-plainable similarity measure between images. Most exist-ing similarity learning methods exacerbate the unexplain-ability by mapping each sample to a single point in the em-bedding space with a distance metric (e.g., Mahalanobis distance, Euclidean distance). Motivated by the human se-mantic similarity cognition, we propose a generalized simi-larity learning paradigm to represent the similarity between two images with a graph and then infer the overall simi-larity accordingly. Furthermore, we establish a bottom-up similarity construction and top-down similarity inference framework to infer the similarity based on semantic hier-archy consistency. We ﬁrst identify unreliable higher-level similarity nodes and then correct them using the most co-herent adjacent lower-level similarity nodes, which simulta-neously preserve traces for similarity attribution. Extensive experiments on the CUB-200-2011, Cars196, and Stanford
Online Products datasets demonstrate signiﬁcant improve-ments over existing deep similarity learning methods and verify the interpretability of our framework. 1 1.

Introduction
Similarity learning is a fundamental task in the ﬁeld of computer vision, where most prevalent works (i.e., met-ric learning methods) employ a distance metric to measure the similarities between samples. They transform features into an embedding space and deﬁne the dissimilarity as the Euclidean distance in this space, where the objective is to cluster similar samples together and separate dissimi-lar ones apart from each other. While conventional meth-ods use hand-crafted features like SIFT [21] and LBP [1], deep metric learning methods employ convolutional neu-ral networks (CNNs) [16] to extract more representative features and demonstrate superior performance. In recent years, similarity learning has been widely applied to various
∗Corresponding author. 1Code: https://github.com/zbr17/AVSL.
Figure 1. The motivation of the proposed AVSL framework. Hu-mans recognize each image as a complex set of concepts and com-pare two images hierarchically [17]. For example, when infer-ring the similarity between two cars, humans usually ﬁrst compare high-level features such as shapes or colors and then turn to ﬁner features such as wheel structures when a coarse observation does not distinctly distinguish them. Motivated by this, we propose to employ a graph structure to decompose sample pairs into discrim-inative concept nodes, which is more consistent with how humans perceive the cognitive distance and is beneﬁcial to the attribution of the similarity measurement. vision tasks such as face recognition [11, 30, 35], person re-identiﬁcation [4,10,18,47], and image classiﬁcation [2,22].
The essential goal of visual similarity learning is to ob-tain a similarity measure that generalizes well to unseen data. It has been shown that the good generalization of the human visual system comes from the ability to parse ob-jects into parts and relations and learn the underlying con-cepts [17]. Humans also infer the similarity between two images hierarchically by ﬁrst comparing high-level features and then delving into lower-level features, as illustrated in
Figure 1. However, most existing similarity learning meth-ods simply project each sample to one single vector and em-ploy the Mahalanobis distance or Euclidean distance as the similarity function. They only use the top-level feature to
represent an image and directly compute the similarity with-out inference. Also, using a single vector for similarity mea-sure exacerbates the unexplainability caused by the black-box CNNs and leads to untraceable similarity measurement, i.e., we can hardly attribute the overall similarity to speciﬁc features. To alleviate this issue, some methods [3,33,59] at-tempt to extend neural network visualization techniques to deep metric learning and generate a saliency map for each image. Still, they treat the similarity computing model as a black box and can only explain it subjectively in a post hoc way, where the similarity computing process remains untraceable and unexplainable.
In this paper, we propose an attributable visual similarity learning (AVSL) framework to actively explain the learned similarity measurement. We generalize the prevalent met-ric learning paradigm to represent the similarity between images by a graph and then analyze it to infer the overall similarity. We use CNNs to extract hierarchical visual fea-tures in a bottom-up manner, where higher-level features encode more abstract concepts [45, 50] and can be regarded as a combination of low-level features [51, 52]. We further construct an undirected graph to represent the similarity be-tween images. We then propose a top-down similarity in-ference method based on hierarchy consistency. We start from high-level similarity nodes and rectify identiﬁed unre-liable nodes using adjacent low-level similarity nodes until reaching the lowest level, which is similar to how humans compare two objects from coarse to ﬁne. The overall simi-larity can be easily attributed to the effect of each similarity node corresponding to certain visual concepts. Our frame-work can be readily applied to existing deep metric learning methods with various loss functions and sampling strate-gies. Extensive experiments on the widely used CUB-200-2011 [37], Cars196 [15], and Stanford Online Products [24] datasets demonstrate that our AVSL framework can signif-icantly boost the performance of various deep metric learn-ing methods and achieve state-of-the-art results. We also conduct visualization experiments to demonstrate the at-tributability and interpretability of our method. 2.