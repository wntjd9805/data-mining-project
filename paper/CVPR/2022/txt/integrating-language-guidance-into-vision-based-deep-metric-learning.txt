Abstract
Deep Metric Learning (DML) proposes to learn met-ric spaces which encode semantic similarities as embed-ding space distances. These spaces should be transfer-able to classes beyond those seen during training. Com-monly, DML methods task networks to solve contrastive ranking tasks deﬁned over binary class assignments. How-ever, such approaches ignore higher-level semantic rela-tions between the actual classes. This causes learned em-bedding spaces to encode incomplete semantic context and misrepresent the semantic relation between classes, impact-ing the generalizability of the learned metric space. To tackle this issue, we propose a language guidance objec-tive for visual similarity learning. Leveraging language em-beddings of expert- and pseudo-classnames, we contextu-alize and realign visual representation spaces correspond-ing to meaningful language semantics for better semantic consistency. Extensive experiments and ablations provide a strong motivation for our proposed approach and show language guidance offering signiﬁcant, model-agnostic im-provements for DML, achieving competitive and state-of-the-art results on all benchmarks. Code available at github.com/ExplainableML/LanguageGuidance for DML. 1.

Introduction
Visual similarity learning with deep networks drives im-portant applications such as image retrieval [94, 115], face veriﬁcation [19, 63], clustering [6] or contrastive super-vised [49] and unsupervised representation learning [13, 39]. Deep Metric Learning (DML) has proven a useful and widely adopted framework to contextualize visual simi-larities by learning (deep) metric representation/embedding spaces in which a predeﬁned distance metric, such as the eu-clidean or cosine distance, has a strong connection to the ac-tual underlying semantic similarity of two samples [72, 91].
In most visual similarity tasks, transfer beyond just the training distribution and classes is crucial, which requires learned representation spaces to encode meaningful seman-tic context that generalizes beyond relations seen during training. However, the majority of DML methods introduce
Figure 1. Language-guidance for better semantic visual align-ment. We leverage language context to better align learned metric spaces with higher-level semantic relations and signiﬁcantly im-prove generalization performance. training paradigms based only around class labels provided in given datasets to deﬁne ranking tasks for networks to solve. This treats every class the same, with the arrange-ment of classes in embedding space solely derived from the class-label generated ranking tasks. In doing so, high-level semantic connections between different classes (e.g. sports cars vs. pickup trucks) can’t be accounted for, even though semantic context beyond what can be derived from purely discriminative class labels facilitates stronger gener-alization especially to novel classes [60, 66, 88, 123]. And while contextualization via e.g. the deﬁnition of hierarchies
[4, 10, 17, 22] can help reﬁne classlabels, such approaches commonly rely on predeﬁned rules or expert knowledge.
To address this problem, we propose to leverage the large corpora of readily available, large-scale pretrained natural language models 1 to provide task-independent contextual-ization for class labels and encourage DML models to learn semantically more consistent visual representation spaces.
Using language-based pretraining for contextualization of visual similarities is long overdue - for vision-based DML, pretraining (on ImageNet) has already become standard 1S.a. transformer language models [107] with strong zero-/fewshot generalization [8, 23, 64, 83, 84] across a large variety of applications. Pro-vided e.g. through public libraries such as huggingface [113].
[19, 66, 72, 75, 91, 92, 94, 95, 115] since it provides a strong and readily available starting point. This starting point transfers well to a multitude of downstream domains and ensures ranking tasks underlying most DML methods to be much better deﬁned initially, improving training and generalization performance (e.g. [72, 91]).
Such Ima-geNet pretraining is standard, and crucial, even for unsu-pervised DML [12, 45, 58, 120], and can similar be found in other areas of Deep Learning such as image detection
[35, 36, 40, 59, 62, 85, 87]. Thus, there is little reason to bottleneck DML to only leverage visual pretraining while disregarding the potential beneﬁts of language context.
To incorporate pretrained language models to facilitate visual similarity learning, we therefore propose language guidance (ELG) for DML. Given natural language class names, language embeddings and respective language sim-ilarities are computed, which are then used via distilla-tion to re-arrange and correct visual embedding relations learned by standard DML methods. However, natural lan-guage class names require expert knowledge. To circum-vent the need for such additional supervision, we further propose pseudolabel-guidance (PLG). Leveraging the ubiq-uitously used ImageNet pretraining in DML pipelines, we deﬁne a quasi-unique collection of natural language Ima-geNet pseudolabels for samples and classes essentially “for free”. Re-embedding these pseudolabels into pretrained language models gives access to a collection of less ﬁne-grained, but generically applicable pseudolabel similarities which can then similarly be used for language guidance with little changes in generalization performance. Exten-sive experimentation and ablations support the validity of our proposed approach and showcase signiﬁcant improve-ments to the generalization performance of DML models when using pretrained language models for additional vi-sual semantic reﬁnement, concurrently setting a new state-of-the-art with negligible overhead to training time. 2.