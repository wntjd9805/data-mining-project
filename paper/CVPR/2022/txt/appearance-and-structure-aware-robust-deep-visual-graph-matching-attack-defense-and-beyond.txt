Abstract
Despite the recent breakthrough of high accuracy deep graph matching (GM) over visual images, the robustness of deep GM models is rarely studied which yet has been revealed an important issue in modern deep nets, ranging from image recognition to graph learning tasks. We first show that an adversarial attack on keypoint localities and the hidden graphs can cause significant accuracy drop to deep GM models. Accordingly, we propose our defense strategy, namely Appearance and Structure Aware Robust
Graph Matching (ASAR-GM). Specifically, orthogonal to de facto adversarial training (AT), we devise the Appear-ance Aware Regularizer (AAR) on those appearance-similar keypoints between graphs that are likely to confuse. Ex-perimental results show that our ASAR-GM achieves bet-ter robustness compared to AT. Moreover, our locality at-tack can serve as a data augmentation technique, which boosts the state-of-the-art GM models even on the clean test dataset. Code is available at https://github.com/
Thinklab-SJTU/RobustMatch. 1.

Introduction
Graph matching (GM), as one of the most important re-search topics in the graph domain with wide applications in vision and pattern recognition, aims to find node-to-node correspondence among graphs. The matching of visual graphs has been intensively studied over the decades, such as image keypoint matching [42], scene graph discovery [6], and vision-text retrieval [46], especially since the recent ad-vances in combining deep neural networks and (visual) GM
[51]. Despite the success of deep GM, deep neural networks (DNN)s are found vulnerable to small input perturbations which are imperceptible to humans [3, 38]. For example, in image classification, carefully designed small perturbations on image pixels can fool neural classifiers [18], and in graph
*Correspondence author. This work was in part supported by China
Key Research and Development Program (2020AAA0107600), Shanghai
Municipal Science and Technology Major Project (2021SHZDZX0102).
Figure 1. The proposed imperceptible adversarial attacks. Left: input paired images and their graphs; Middle: adversarial pertur-bations on the pixel and locality; Right: induced adversarial data.
After being attacked, the appearance around the keypoints remains unchanged while the graph structure get perturbed: the red dotted line means the edge on the original graph being removed while the blue real line denotes the added edge on the new perturbed graph. domain, attackers perturb graph structures and its attributes to cause failures of graph learning tasks such as node clas-sification [10, 13, 37, 40, 59], community detection [7, 27], link prediction [32], etc. However, there is little work which considers the vulnerability of deep GM for vision, or more specifically matching image keypoints, which is recently a trending research topic [16, 17, 23, 24, 34, 42, 44, 48, 49, 57].
Since noise can be easily injected into images and graphs, a natural question that we would answer in our work is: How to design an effective adversarial attack on GM, perturbing images and their hidden graphs simultaneously?
In the context of visual GM, it is natural for the at-tacker to consider perturbing image pixels, which are di-rectly related to node features of visual graphs. Besides, deep GM also takes the keypoint locality and the induced hidden graphs as input. However, such way of adding or removing edges of the hidden graphs, as a common graph attack baseline [33, 56], is NOT feasible to visual GM: with the annotated keypoint locality, graph structure of GM is determined by certain domain knowledge, e.g., Delaunay triangulation [11], such that any operation on graph edges can be easily recovered. We instead focus on the location of keypoints. Among the common deep GM pipelines [44,51], the location property of keypoints in each graph is crucial to
Table 1. Comparisons of the attack and defense of vision and graph tasks. The column “Attack Object” denotes the specific perturbation object about the input; “Attack Type” denotes which way to perturb the input; the column “Similarity Metric” shows how to measure the similarity between clean and adversarial example, and “Defense Objective Function” is the minimization goal of the defender.
Task image classification [18] object tracking [22] node classification [10, 37] graph matching [33, 56] visual graph matching (ours)
Attack Object image image sequence graph graph visual graph
Attack Type flip pixels flip pixels inject nodes; add/delete edges add/delete edges flip pixels; perturb keypoint locality
Similarity Measure lp norm lp norm ratio of perturbed nodes(edges) ratio of perturbed edges lp norm
Defense Objective Function cross-entropy cross-entropy with smooth L1 cross-entropy pairwise cosine similarity binary cross-entropy the final matching performance since it affects how features of keypoints are extracted from the whole image through bi-linear interpolation and directly determines the graph struc-ture derived by Delaunay triangulation. However, the loca-tion of keypoints suffers from its inherent instability due to the randomness of human labeling or keypoint detec-tors, which means small yet malicious noise could be easily added without being detected. Therefore, we propose to perturb keypoint locality as an effective adversarial attack.
Towards defending against adversarial attacks, adversar-ial training (AT) [30] has become a widely-recognized prin-cipled defense mechanism by training models on adversar-ial examples while it suffers from lowering accuracy on clean test examples. Moreover, for graph learning, there are efforts in improving robustness against adversarial attacks in node classification [47, 54, 58], graph classification [25], community detection [21], etc. However, those defense mechanisms only focus on single graph learning tasks while
GM learns to analyze intersections among graphs such that these methods cannot be directly applied to GM.
Our defense mechanism derives from two insights. First, we show that adversarial attacks tend to confuse keypoints with similar appearance and those appearance-similar key-points usually occur in three cases: (i) shape similarity, e.g. the two ears of a cat; (ii) texture similarity, e.g. the wither and tail of a cat; (iii) structural symmetry, e.g. the four roof corners of a car. Such appearance-similarity depends on some prior in the dataset. For two graphs, if we can select these appearance-similar keypoints between them and ex-plicitly enlarge their disparity in the probabilistic space of model outputs, the model robustness could get enhanced.
Moreover, since our regularization strategy works in output space, which is orthogonal to AT that generates worst-case example in input space, we can further improve model ro-bustness by combining them together.
To this end, we take the initiative on studying the ro-bustness of visual GM. On the attacker’s side, we propose an effective keypoint locality attack and combine it with pixel attack to devise an even stronger attack. For de-fense, we analyze the attack pattern and discover that those appearance-similar keypoints can be inferred from the re-sult of our adversarial attack. Then we design a regulariza-tion term, namely Appearance Aware Regularizer (AAR), to regularize the discrepancy of features of keypoints which share similar appearance in the low-dimensional embedding space. Finally, we propose our defense strategy, namely
Appearance and Structure Aware Robust Graph Matching (ASAR-GM) on the basis of AT. The highlights are: 1) We analyze the vulnerability of deep (visual) graph matching (GM) under adversarial attacks and design an ef-fective locality attack, which perturbs the keypoint locations and hidden graph structure together. Moreover, stronger adversarial data is generated by combining our locality at-tack and pixel attack together. Our work differs from two recent GM attack/defense works as they only focus on adding/deleting the edges without manipulating on visual images as also considered in our method (see Table 1). 2) We propose our defense strategy, namely Appearance and Structure Aware Robust Graph Matching (ASAR-GM) to enhance robustness. Specifically, we show that adversarial attacks tend to utilize appearance-similar key-points among graphs to fool the matching of the model.
As such, we design a regularization term: Appearance
Aware Regularizer (AAR), to enlarge the disparity among appearance-similar keypoints in graph. Our AAR can be naturally integrated into the framework of AT, which brings better clean accuracy and robustness. 3) Experiments on real-world benchmarks validate the effectiveness of our attack on various deep GM base-lines [34,42,48] including the state-of-the-art NGMv2 [44].
Our attack also shows strong transferability in the black-box attack setting. For defense, ASAR-GM achieves better clean accuracy and robustness over defense baselines. 4) Last but importantly, while adversarial examples are often viewed a threat to DNN, our locality attack serves as a data augmentation to improve generalization ability of deep
GM because perturbations on locality induce various graph structures for training, making our model a new GM SOTA. 2.