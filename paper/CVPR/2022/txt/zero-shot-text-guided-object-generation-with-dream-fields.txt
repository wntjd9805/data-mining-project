Abstract 1.

Introduction
We combine neural rendering with multi-modal image and text representations to synthesize diverse 3D objects solely from natural language descriptions. Our method, Dream
Fields, can generate the geometry and color of a wide range of objects without 3D supervision. Due to the scarcity of diverse, captioned 3D data, prior methods only generate ob-jects from a handful of categories, such as ShapeNet. Instead, we guide generation with image-text models pre-trained on large datasets of captioned images from the web. Our method optimizes a Neural Radiance Field from many camera views so that rendered images score highly with a target caption ac-cording to a pre-trained CLIP model. To improve ﬁdelity and visual quality, we introduce simple geometric priors, includ-ing sparsity-inducing transmittance regularization, scene bounds, and new MLP architectures. In experiments, Dream
Fields produce realistic, multi-view consistent object geome-try and color from a variety of natural language captions. 1UC Berkeley, 2Google Research.
∗Work done at Google.
Correspondence to ajayj@berkeley.edu.
Project website and code: https://ajayj.com/dreamfields
Detailed 3D object models bring multimedia experiences to life. Games, virtual reality applications and ﬁlms are each populated with thousands of object models, each designed and textured by hand with digital software. While expert artists can author high-ﬁdelity assets, the process is painstak-ingly slow and expensive. Prior work leverages 3D datasets to synthesize shapes in the form of point clouds, voxel grids, triangle meshes, and implicit functions using generative mod-els like GANs [4, 21, 57, 65]. These approaches only sup-port a few object categories due to small labeled 3D shape datasets. But multimedia applications require a wide variety of content, and need both 3D geometry and texture.
In this work, we propose Dream Fields, a method to automatically generate open-set 3D models from natural language prompts. Unlike prior work, our method does not require any 3D training data, and uses natural language prompts that are easy to author with an expressive interface for specifying desired object properties. We demonstrate that the compositionality of language allows for ﬂexible creative control over shapes, colors and styles.
A Dream Field is a Neural Radiance Field (NeRF) trained to maximize a deep perceptual metric with respect to both
the geometry and color of a scene. NeRF and other neural 3D representations have recently been successfully applied to novel view synthesis tasks where ground-truth RGB photos are available. NeRF is trained to reconstruct images from multiple viewpoints. As the learned radiance ﬁeld is shared across viewpoints, NeRF can interpolate between viewpoints smoothly and consistently. Due to its neural representation,
NeRF can be sampled at high spatial resolutions unlike voxel representations and point clouds, and are easy to optimize unlike explicit geometric representations like meshes as it is topology-free.
However, existing photographs are not available when creating novel objects from descriptions alone. Instead of learning to reconstruct known input photos, we learn a radi-ance ﬁeld such that its renderings have high semantic similar-ity with a given text prompt. We extract these semantics with pre-trained neural image-text retrieval models like CLIP [46], learned from hundreds of millions of captioned images. As
NeRF’s volumetric rendering and CLIP’s image-text repre-sentations are differentiable, we can optimize Dream Fields end-to-end for each prompt. Figure 1 illustrates our method.
In experiments, Dream Fields learn signiﬁcant artifacts if we naively optimize the NeRF scene representation with textual supervision without adding additional geometric con-straints (Figure 3). We propose general-purpose priors and demonstrate that they greatly improve the realism of results.
Finally, we quantitatively evaluate open-set generation per-formance using a dataset of diverse object-centric prompts.
Our contributions include:
•
•
•
Using aligned image and text models to optimize NeRF without 3D shape or multi-view data,
Dream Fields, a simple, constrained 3D representation with neural guidance that supports diverse 3D object generation from captions in zero-shot, and
Simple geometric priors including transmittance regu-larization, scene bounds, and an MLP architecture that together improve ﬁdelity. 2.