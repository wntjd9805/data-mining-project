Abstract
Prevalent semantic segmentation solutions, despite their different network designs (FCN based or attention based) and mask decoding strategies (parametric softmax based or pixel-query based), can be placed in one category, by con-sidering the softmax weights or query vectors as learnable class prototypes. In light of this prototype view, this study un-covers several limitations of such parametric segmentation regime, and proposes a nonparametric alternative based on non-learnable prototypes. Instead of prior methods learning a single weight/query vector for each class in a fully para-metric manner, our model represents each class as a set of non-learnable prototypes, relying solely on the mean fea-tures of several training pixels within that class. The dense prediction is thus achieved by nonparametric nearest proto-type retrieving. This allows our model to directly shape the pixel embedding space, by optimizing the arrangement be-tween embedded pixels and anchored prototypes. It is able to handle arbitrary number of classes with a constant amount of learnable parameters.We empirically show that, with FCN based and attention based segmentation models (i.e., HR-Net, Swin, SegFormer) and backbones (i.e., ResNet, HRNet,
Swin, MiT), our nonparametric framework yields compel-ling results over several datasets (i.e., ADE20K, Cityscapes,
COCO-Stuff), and performs well in the large-vocabulary situation. We expect this work will provoke a rethink of the current de facto semantic segmentation model design. 1.

Introduction
With the renaissance of connectionism, rapid progress has been made in semantic segmentation. Till now, most of state-of-the-art segmentation models [15, 34, 49, 135] were built upon Fully Convolutional Networks (FCNs) [79]. De-spite their diversiﬁed model designs and impressive results, existing FCN based methods commonly apply parametric softmax (
) over pixel-wise features for dense pre-diction (Fig. 1(a)). Very recently, the vast success of Trans-former [105] stimulates the emergence of attention based
*Corresponding author: Wenguan Wang.
Figure 1. Different sematic segmentation paradigms: (a-b) para-metric vs (c) nonparametric. Modern segmentation solutions, no matter using (a) parametric softmax or (b) query vectors for mask decoding, can be viewed as learnable prototype based methods that learn class-wise prototypes in a fully parametric manner. We in-stead propose a nonparametric scheme (c) that directly selects sub-cluster centers of embedded pixels as prototypes, and achieves per-pixel prediction via nonparametric nearest prototype retrieving. segmentation solutions. Many of these ‘non-FCN’ models, like [118, 139], directly follow the standard mask decoding regime, i.e., estimate softmax distributions over dense visual embeddings (extracted from patch token sequences). Inter-estingly, the others [20, 100] follow the good practice of
Transformer in other ﬁelds [11, 82, 113] and adopt a pixel-query strategy (Fig. 1(b)): utilize a set of learnable vectors
) to query the dense embeddings for mask prediction. (
They speculate the learned query vectors can capture class-wise properties, however, lacking in-depth analysis.
Noticing there exist two different mask decoding strate-gies, the following questions naturally arise: (cid:182) What are the relation and difference between them? and (cid:183) If the learn-able query vectors indeed implicitly capture some intrinsic properties of data, is there any better way to achieve this?
Tackling these two issues can provide insights into mod-ern segmentation model design, and motivate us to rethink the task from a prototype view. The idea of prototype based classiﬁcation [31] is classical and intuitive (which can date back to the nearest neighbors algorithm [23] and ﬁnd evi-dence in cognitive science [60, 91]): data samples are classi-ﬁed based on their proximity to representative prototypes of classes. With this perspective, in §2, we ﬁrst answer ques-tion (cid:182) by pointing out most modern segmentation methods, from softmax based to pixel-query based, from FCN based to attention based, fall into one grand category: parametric models based on learnable prototypes. Consider a segmenta-tion task with C semantic classes. Most existing efforts seek to directly learn C class-wise prototypes – softmax weights or query vectors – for parametric, pixel-wise classiﬁcation.
Hence question (cid:183) becomes more fundamental: (cid:184) What are the limitations of this learnable prototype based parametric paradigm? and (cid:185) How to address these limitations?
Driven by question (cid:184), we ﬁnd there are three critical limi-tations: First, usually only one single prototype is learned per class, insufﬁcient to describe rich intra-class variance. The prototypes are simply learned in a fully parametric manner, without considering their representative ability. Second, to map a H×W×D image feature tensor into a H×W×C seman-tic mask, at least D×C parameters are needed for prototype learning. This hurts generalizability [115], especially in the large-vocabulary case; for instance, if there are 800 classes and D = 512, we need 0.4M learnable prototype parameters alone. Third, with the cross-entropy loss, only the relative relations between intra-class and inter-class distances are op-timized [89, 111, 134]; the actual distances between pixels and prototypes, i.e., intra-class compactness, are ignored.
As a response to question (cid:185), in §3, we develop a nonpa-rametric segmentation framework, based on non-learnable prototypes.Speciﬁcally, building upon the ideas of prototype learning [116, 133] and metric learning [40, 64], it is fully aware of the limitations of its parametric counterpart. Inde-pendent of speciﬁc backbone architectures (FCN based or attention based), our method is general and brings insights into segmentation model design and training. For model design, our method explicitly sets sub-class centers, in the pixel embedding space, as the prototypes. Each pixel data is predicted to be in the same class as the nearest prototype, without relying on extra learnable parameters. For training, as the prototypes are representative of the dataset, we can directly pose known inductive biases (e.g., intra-class com-pactness, inter-class separation) as extra optimization crite-ria and efﬁciently shape the whole embedding space, instead of optimizing the prediction accuracy only. Our model has three appealing advantages: First, each class is abstracted by a set of prototypes, well capturing class-wise characteristics and intra-class variance. With the clear meaning of the pro-totypes, the interpretability is also enhanced – the prediction of each pixel can be intuitively understood as the reference of its closest class center in the embedding space [3, 7].
Second, due to the nonparametric nature, the generalizabil-ity is improved. Large-vocabulary semantic segmentation can also be handled efﬁciently, as the amount of learnable prototype parameters is no longer constrained to the number of classes (i.e., 0 vs D×C). Third, via prototype-anchored metric learning, the pixel embedding space is shaped as well-structured, beneﬁting segmentation prediction eventually.
By answering questions (cid:182)-(cid:185), we formalize prior methods within a learnable prototype based, parametric framework, and link this ﬁeld to prototype learning and metric learning.
We provide literature review and related discussions in §4.
In §5.2, we show our method achieves impressive results over famous datasets (i.e., ADE20K [140], Cityscapes [22],
COCO-Stuff [10]) with top-leading FCN based and attention based segmentation models (i.e., HRNet [108], Swin [78],
SegFormer [118]) and backbones (i.e., ResNet [45], HRNet
[108], Swin [78], MiT [118]). Compared with the paramet-ric counterparts, our method does not cause any extra com-putational overhead during testing while reduces the amount of learnable parameters. In §5.3, we demonstrate our method consistently performs well when increasing the number of semantic classes from 150 to 847. Accompanied with a set of ablative studies in §5.4, our extensive experiments verify the power of our idea and the efﬁcacy of our algorithm.
Finally, we draw conclusions in §6. This work is expec-ted to open a new venue for future research in this ﬁeld. 2. Existing Semantic Segmentation Models as
Parametric Prototype Learning
Next we ﬁrst formalize the existing two mask decod-ing strategies mentioned in §1, and then answer question (cid:182) from a uniﬁed view of parametric prototype learning.
Parametric Softmax Projection. Almost all FCN-like and many attention-based segmentation models adopt this strat-egy. Their models comprise two learnable parts: i) an en-coder φ for dense visual feature extraction, and ii) a classi-ﬁer ρ (i.e., projection head) that projects pixel features into the semantic label space. For each pixel example i, its em-bedding i ∈ RD, extracted from φ, is fed into ρ for C-way classiﬁcation: p(c|i) = exp(w(cid:62) c i) c(cid:48)=1 exp(w(cid:62) c(cid:48) i) (cid:80)C
, (1) where p(c|i) ∈[0, 1] is the probability that i being assigned to class c. ρ is a pixel-wise linear layer, parameterized by
W= [w1, · · ·, wC] ∈ RC×D; wc ∈ RD is a learnable projec-tion vector for c-th class; the bias term is omitted for brevity.
Parametric Pixel-Query. A few attention-based segmen-tation networks [118, 139] work in a more ‘Transformer-like’ manner: given the pixel embedding i ∈ RD, a set of C query vectors, i.e., E= [e1, · · ·, eC] ∈ RC×D, are learned to generate a probability distribution over the C classes: p(c|i) = exp(ec ∗ i) c(cid:48)=1 exp(ec(cid:48) ∗ i) (cid:80)C
, (2) where ‘∗’ is inner product between (cid:96)2-normalized inputs.
φ
φ
LCE (Eq. 7)
LPPC (Eq. 11)
LPPD (Eq. 12)
Figure 2. Architecture illustration of our non-learnable prototype based nonparametric segmentation model during the training phase.
Prototype-based Classiﬁcation. Prototype-based classiﬁ-cation [31, 33] has been studied for a long time, dating back to the nearest neighbors algorithm [23] in machine learn-ing and prototype theory [60, 91] in cognitive science. Its prevalence stems from its intuitive idea: represent classes by prototypes, and refer to prototypes for classiﬁcation. Let
{pm}M m=1 be a set of prototypes that are representative of their corresponding classes {cpm∈{1, · · ·, C}}m. For a data sample i, prediction is made by comparing i with {pm}m, and taking the class of the winning prototype as response:
ˆci = cpm∗ , with m∗ = arg min
{(cid:104)i, pm(cid:105)}M m=1, m (3) where i and {pm}m are embeddings of the data sample and prototypes in a feature space, and (cid:104)·, ·(cid:105) stands for the dis-tance measure, which is typically set as (cid:96)2 distance (i.e.,
||i−pm||) [123], yet other proximities can be applied.
Further, Eqs. 1-2 can be formulated in a uniﬁed form: p(c|i) = exp(−(cid:104)i, gc(cid:105)) c(cid:48)=1 exp(−(cid:104)i, gc(cid:48) (cid:105)) (cid:80)C
, (4) where gc ∈ RD can be either wc in Eq. 1 or ec in Eq. 2.
With Eqs. 3-4, we are ready to answer questions (cid:182)(cid:183). Both the two types of methods are based on learnable prototypes; they are parametric models in the sense that they learn one prototype gc, i.e., linear weight wc or query vector ec, for each class c (i.e., M=C ). Thus one can consider softmax pro-jection based methods ‘secretly’ learn the query vectors. As for the difference, in addition to different distance measures (i.e., inner product vs cosine similarity), pixel-query based methods [118, 139] can feed the queries into cross-attention decoder layers for cross-class context exchanging, rather than softmax projection based counterparts only leveraging the learned class weights within the softmax layer.
With the uniﬁed view of parametric prototype learning, a few intrinsic yet long ignored issues in this ﬁeld unfold:
First, prototype selection [36] is a vital aspect in the de-sign of a prototype based learner – prototypes should be typ-ical for their classes. Nevertheless, existing semantic seg-mentation algorithms often describe each class by only one prototype, bearing no intra-class variation. Moreover, the prototypes are directly learned in a fully parametric man-ner, without accounting for their representative ability.
Second, the amount of the learnable prototype parameters, i.e., {gc∈RD}C c=1, grows with the number of classes. This may hinder the scalability, especially when a large number of classes are present. For example, if there are 800 classes and the pixel feature dimensionality is 512, at least 0.4M parameters are needed for prototype learning alone, making large-vocabulary segmentation a hard task. Moreover, if we want to represent each class by ten prototypes, instead of only one, we need to learn 4M prototype parameters.
Third, Eq. 3 intuitively shows that prototype based learn-ers make metric comparisons of data [8]. However, existing algorithms often supervise dense segmentation representa-tion by directly optimizing the accuracy of pixel-wise pre-diction (e.g., cross-entropy loss), ignoring known inductive biases [83, 84], e.g., intra-class compactness, about the fea-ture distribution. This will hinder the discrimination poten-tial of the learned segmentation features, as suggested by many literature in representation learning [76, 95, 114].
After tackling question (cid:184), in the next section we will detail our non-learnable prototype based nonparametric segmenta-tion method, which serves as a solid response to question (cid:185). 3. Non-Learnable Prototype based Nonpara-metric Semantic Segmentation
We build a nonparametric segmentation framework that conducts dense prediction by a set of non-learnable class pro-totypes, and directly supervises the pixel embedding space via a prototype-anchored metric learning scheme (Fig. 2).
Non-Learnable Prototype based Pixel Classiﬁcation. As normal, an encoder network (FCN based or attention based), i.e., φ, is ﬁrst adopted to map the input image I ∈ Rh×w×3, to a 3D feature tensor I∈RH×W×D. For pixel-wise C-way classiﬁcation, rather than prior semantic segmentation mod-els that automatically learn C class weights {wc ∈ RD}C c=1 (cf. Eq. 1) or C queries vectors {ec ∈ RD}C c=1 (cf. Eq. 2), we refer to a group of CK non-learnable prototypes, i.e.,
{pc,k∈ RD}C,K c,k=1, which are based solely on class data sub-centers. More speciﬁcally, each class c ∈ {1, · · ·, C} is rep-resented by a total of K prototypes {pc,k}K k=1, and proto-type pc,k is determined as the center of k-th sub-cluster of training pixel samples belonging to class c, in the embed-ding space φ. In this way, the prototypes can comprehen-sively capture characteristic properties of the corresponding classes, without introducing extra learnable parameters out-side φ. Analogous to Eq. 3, the category prediction of each pixel i ∈ I is achieved by a winner-take-all classiﬁcation:
ˆci = c∗, with (c∗, k∗) = arg min (c,k)
{(cid:104)i, pc,k(cid:105)}C,K c,k=1, (5) where i ∈ RD stands for the (cid:96)2-normalized embedding of pixel i, i.e., i ∈ I, and the distance measure (cid:104)·, ·(cid:105) is deﬁned as the negative cosine similarity, i.e., (cid:104)i, p(cid:105) = −i(cid:62)p.
With this exemplar-based reasoning mode, we ﬁrst de-ﬁne the probability distribution of pixel i over the C classes: p(c|i) = exp(−si,c) c(cid:48)=1exp(−si,c(cid:48)) (cid:80)C
, with si,c = min{(cid:104)i, pc,k(cid:105)}K k=1, (6) where the pixel-class distance si,c ∈[−1, 1] is computed as the distance to the closest prototype of class c. Given the groundtruth class of each pixel i, i.e., ci ∈ {1, · · ·, C}, the cross-entropy loss can be therefore used for training:
LCE = − log p(ci|i)
= − log exp(−si,ci ) exp(−si,ci )+(cid:80) exp(−si,c(cid:48) ) c(cid:48)(cid:54)=ci (7)
.
In our case, Eq. 7 can be viewed as pushing pixel i closer to the nearest prototype of its corresponding class, i.e., ci, and further from other close prototypes of irrelevant classes, i.e., c(cid:48) (cid:54)= ci. However, only adopting such training objective is not enough, due to two reasons. First, Eq. 7 only considers pixel-class distances, e.g., si,c, without addressing within-class pixel-prototype relations, e.g., (cid:104)i, pci,k(cid:105). For example, for discriminative representation learning, pixel i is expec-ted to be pushed further close to a certain prototype (i.e., a particularly suitable pattern) of class ci, and, distant from other prototypes (i.e., other irrelevant but within-class pat-terns) of class ci. Eq. 7 cannot capture this nature. Second, as the pixel-class distances are normalized across all classes (cf. Eq. 6), Eq. 7 only optimizes the relative relations between intra-class (i.e., si,ci) and inter-class (i.e., {si,c(cid:48)}c(cid:48)(cid:54)=ci) dis-tances, instead of directly regularizing the cosine distances between pixels and classes. For example, when the intra-class distance si,ci of pixel i is relatively smaller than other inter-class distances {si,c(cid:48)}c(cid:48)(cid:54)=ci, the penalty from Eq. 7 will be small, but the intra-class distance si,ci might still be large
[89, 134]. Next we ﬁrst elaborate on our within-class online clustering strategy and then detail our two extra training ob-jectives which rely on prototype assignments (i.e., cluster-ing results) and address the above two issues respectively.
Within-Class Online Clustering. We approach online clu-stering for prototype selection and assignment: pixel sam-ples within the same class are assigned to the prototypes belonging to that class, and the prototypes are then updated according to the assignments. Clustering imposes a natural bottleneck [55] that forces the model to discover intra-class discriminative patterns yet discard instance-speciﬁc details.
Thus the prototypes, selected as the sub-cluster centers, are typical of the corresponding classes. Conducting clustering online makes our method scalable to large amounts of data, instead of ofﬂine clustering requiring multiple passes over the entire dataset for feature computation [13].
Formally, given pixels I c = {in}N n=1 in a training batch that belong to class c (i.e., cin = c), our goal is to map the pixels I c to the K prototypes {pc,k}K k=1 of class c. We denote this pixel-to-prototype mapping as Lc = [lin ]N n=1 ∈
{0, 1}K×N , where lin = [lin,k]K k=1∈ {0, 1}K is the one-hot assignment vector of pixel in over the K prototypes. The optimization of Lc is achieved by maximizing the similarity n=1∈ RD×N , and between pixel embeddings, i.e., X c= [in]N k=1∈ RD×K: the prototypes, i.e., P c= [pc,k]K
Tr(Lc(cid:62)P c(cid:62)X c), max
Lc s.t. Lc∈ {0, 1}K×N , Lc(cid:62)1K = 1N , Lc1N = (8)
N
K 1K , where 1K denotes the vector of all ones of K dimensions.
The unique assignment constraint, i.e., Lc(cid:62)1K = 1N , en-sures that each pixel is assigned to one and only one pro-totype. The equipartition constraint, i.e., Lc1N = N
K 1K, enforces that on average each prototype is selected at least
N
K times in the batch [13]. This prevents the trivial solu-tion: all pixel samples are assigned to a single prototype, and eventually beneﬁts the representative ability of the pro-totypes. To solve Eq. 8, one can relax Lc to be an element of the transportation polytope [2, 24]:
Tr(Lc(cid:62)P c(cid:62)X c) + κh(Lc), max
Lc s.t. Lc∈ RK×N
+
, Lc(cid:62)1K = 1N , Lc1N = (9)
N
K 1K , where h(Lc)=(cid:80) n,k−lin,k log lin,k is an entropy, and κ > 0 is a parameter that controls the smoothness of distribution.
With the soft assignment relaxation and the extra regular-ization term h(Lc), the solver of Eq. 9 can be given as [24]:
Lc = diag(u) exp (cid:0) P c(cid:62)X c
κ (cid:1)diag(v), (10) where u ∈ RK and v ∈ RN are renormalization vectors, computed by few steps of Sinkhorn-Knopp iteration [24].
Our online clustering is highly efﬁcient on GPU, as it only in practice, involves a couple of matrix multiplications; clustering 10K pixels into 10 prototypes takes only 2.5 ms.
Pixel-Prototype Contrastive Learning. With the assign-n=1 ∈ [0, 1]K×N , we on-ment probability matrix Lc = [lin ]N line group the training pixels I c = {in}N n=1 into K proto-types {pc,k}K k=1 within class c. After all the samples in cur-rent batch are processed, each pixel i is assigned to ki-th prototype of class ci, where ki = arg maxk{li,k}K k=1 and li,k ∈ li. It is natural to derive a training objective for pro-totype assignment prediction, i.e., maximize the prototype assignment posterior probability. This can be viewed as a pixel-prototype contrastive learning strategy, and addresses
the ﬁrst limitation of Eq. 7: exp(i(cid:62)pci,ki /τ )
, (11)
LPPC = −log exp(i(cid:62)pci,ki/τ )+(cid:80) p−∈P− exp(i(cid:62)p−/τ ) (cid:14)pci,ki, and the temperature τ con-where P −= {pc,k}C,K c,k=1 trols the concentration level of representations. Intuitively,
Eq. 11 enforces each pixel embedding i to be similar with its assigned (‘positive’) prototype pci,ki, and dissimilar with other CK −1 irrelevant (‘negative’) prototypes P −. Com-pared with prior pixel-wise metric learning based segmenta-tion models [111], which consume numerous negative pixel samples, our method only needs CK prototypes for pixel-prototype contrast computation, neither causing large mem-ory cost nor requiring heavy pixel pair-wise comparison.
Pixel-Prototype Distance Optimization. Building upon the relative comparison over pixel-class/-prototype distances,
Eq. 7 and Eq. 11 inspire inter-class/-cluster discrinimitive-ness, but less consider reducing the intra-cluster variation, i.e., making pixel features of the same prototype compact.
Thus a compactness-aware loss is used for further regulari-zing representations by directly minimizing the distance be-tween each embedded pixel and its assigned prototype:
LPPD = (1 − i(cid:62)pci,ki )2. (12)
Note that both i and pci,ki are (cid:96)2-normalized. This training objective minimizes intra-cluster variations while maintain-ing separation between features with different prototype as-signments, making our model more robust against outliers.
Network Learning and Prototype Update. Our model is a nonparametric approach that learns semantic segmenta-tion by directly optimizing the pixel embedding space φ.
It is called nonparametric because it constructs prototype hypotheses directly from the training pixel samples them-selves. Thus the parameters of the feature extractor φ are learned through stochastic gradient descent, by minimizing the combinatorial loss over all the training pixel samples:
LSEG = LCE + λ1LPPC + λ2LPPD. (13)
Meanwhile, the non-learnable prototypes {pc,k}C,K c,k=1 are not learned by stochastic gradient descent, but are computed as the centers of the corresponding embedded pixel sam-ples. To do so, we let the prototypes evolve continuously by accounting for the online clustering results. Particularly, after each training iteration, each prototype is updated as: pc,k ← µpc,k + (1 − µ)¯ic,k, (14) where µ ∈ [0, 1] is a momentum coefﬁcient, and ¯ic,k in-dicates the (cid:96)2-normalized, mean vector of the embedded training pixels, which are assigned to prototype pc,k by on-line clustering. With the clear meaning of the prototypes, our segmentation procedure can be intuitively understood as retrieving the most similar prototypes (sub-class centers).
Fig. 3 provides prototype retrieval results for person and car with K = 3 prototypes for each. The prototypes are associ-Figure 3. Visualization of pixel-prototype similarity for person (top) and car (bottom) classes. Please refer to §3 for details. ated with different colors (i.e., red, green, and blue). For each pixel, its distance to the closest prototype is visualized using the corresponding prototype color. As can be seen, the prototypes well correspond to meaningful patterns within classes, validating their representativeness. 4.