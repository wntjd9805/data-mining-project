Abstract
Video summarization has recently engaged increasing attention in computer vision communities. However, the scarcity of annotated data has been a key obstacle in this task. To address it, this work explores a new solution for video summarization by transferring samples from a corre-lated task (i.e., video moment localization) equipped with abundant training data. Our main insight is that the anno-tated video moments also indicate the semantic highlights of a video, essentially similar to video summary. Approx-imately, the video summary can be treated as a sparse, redundancy-free version of the video moments.
Inspired by this observation, we propose an importance Propaga-It tion based collaborative Teaching Network (iPTNet). consists of two separate modules that conduct video sum-marization and moment localization, respectively. Each module estimates a frame-wise importance map for indi-cating keyframes or moments. To perform cross-task sam-ple transfer, we devise an importance propagation module that realizes the conversion between summarization-guided and localization-guided importance maps. This way crit-ically enables optimizing one of the tasks using the data from the other task. Additionally, in order to avoid error ampliﬁcation caused by batch-wise joint training, we de-vise a collaborative teaching scheme, which adopts a cross-task mean teaching strategy to realize the joint optimiza-tion of the two tasks and provide robust frame-level teach-ing signals. Extensive experiments on video summarization benchmarks demonstrate that iPTNet signiﬁcantly outper-forms previous state-of-the-art video summarization meth-ods, serving as an effective solution that overcomes the data scarcity issue in video summarization. 1.

Introduction
In recent years, with the popularization of video-sharing platforms, the number of videos that record activities of daily living has witnessed an explosive growth. Techniques that help people quickly browse videos and search the key
*Corresponding author.
Figure 1. The video summarization and video moment localiza-tion models can be jointly optimized using the proposed cross-task sample transfer. information become a valuable research topic. Video sum-marization [5, 20, 46], a technology that automatically ex-tracts representative segments from untrimmed videos to concisely depict the original video content, has attracted in-creasing attention from both academia and industries.
The earlier work in video summarization mostly adopts hand-crafted heuristics to attain certain properties of frames (e.g., diversity, representativeness) [11, 31, 34, 42, 49, 52].
These modeling paradigms are recently less used since the revival of deep learning techniques [2, 30, 50, 53, 56, 75, 85].
Speciﬁcally, the latest video summarization models have been empowered by recurrent neural networks (RNN) [24, 86, 87, 93] and the attention mechanism [29, 46, 70], which drastically advance the state-of-the-art.
Most of these methods employ a data-hungry super-vised learning setting for training [24, 46, 93]. Despite the performance gains achieved by these efforts, current re-search has been still suffering from the scarcity of large-scale annotated video summaries, which require extensive time and effort to construct. Some weakly-supervised ap-proaches [5, 11, 31] have been proposed to alleviate this problem. However, they not only require additional video auxiliary information, but are still hard to achieve compet-itive results. Heretofore, learning video summarization un-der limited data remains an untapped problem.
To address this problem, this work explores the idea of cross-task sample transfer from related tasks, particularly the video moment localization task that aims to temporally spot the video segments corresponding to an arbitrary sen-tence query. The idea is illustrated in Figure 1. Note that video moment localization is query-driven. We ob-serve that the user-provided queries usually describe the key events in the video, thus the task is essentially correlated with video summarization. Considering the compactness of video summaries, they can be approximately regarded as sparse, redundancy-free version of the video moments, which shed light on transferring the abundant annotations in moment localization for helping the video summariza-tion model. In light of this, we explore a collaborative op-timization scheme for these two tasks. Nevertheless, it is non-trivial to achieve cross-task sample transfer satisfacto-rily. Overlooking the domain gap between the two tasks will inevitably cause the learned model to suffer from col-laborative signals with domain bias and lead to performance degradation. Moreover, batch-wise joint training of the two models makes the optimization susceptible to current batch noise. This easily causes the error of one task to spread to another task, resulting in error ampliﬁcation and failure to provide stable and robust collaborative signals.
In this paper, we propose an importance Propagation based collaborative Teaching Network (iPTNet), as shown in Figure 2. It consists of four parts: the video summariza-tion module (SM), the moment localization module (LM), the importance propagation module (PM), and the collab-orative teaching module (TM). To be more speciﬁc, SM and LM do the job of video summarization / moment local-izaiton respectively, under the supervision of correspond-ing task-related data and annotations. The main function-ality of PM is to connect the two frame-wise importance maps generated by SM and LM. The collaborative teach-ing module implements a cross-task mean teaching strategy and enforces the soundness of our main assumption (i.e., the ground-truth video summaries can be approximately ex-panded into video moments via some inter-frame propaga-tions). The main contributions of this work are summarized as below:
• To the best of our knowledge, this is the ﬁrst work that utilizes a second correlated task (i.e., video moment localization) with sufﬁcient training data to help the training of video summarization. Through jointly opti-mizing two models, it surmounts the obstacle of insuf-ﬁcient annotated video summaries without the require-ment of additional annotations or any auxiliary video information.
• To fully harness the ensembles of training data from two tasks, we devise an importance propagation al-gorithm, which realizes the conversion between the summarization-guided and localization-guided impor-tance maps and thereby accomplishes cross-task sam-ple transfer during model optimization.
• To avoid the error ampliﬁcation caused by batch-wise joint training, we propose a collaborative teaching scheme based on a cross-task mean-teaching strategy for the modules SM and LM.
Extensive experiments conducted on video summariza-tion benchmark datasets demonstrate that iPTNet signiﬁ-cantly outperforms the state-of-the-art methods. The code and data of this work have been released to facilitate further research1. 2.