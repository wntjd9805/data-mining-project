Abstract
Recently, vision-language joint representation learning has proven to be highly effective in various scenarios.
In this paper, we specifically adapt vision-language joint learning for scene text detection, a task that intrinsically involves cross-modal interaction between the two modal-ities: vision and language, since text is the written form of language. Concretely, we propose to learn contextu-alized, joint representations through vision-language pre-training, for the sake of enhancing the performance of scene text detectors. Towards this end, we devise a pre-training architecture with an image encoder, a text encoder and a cross-modal encoder, as well as three pretext tasks: image-text contrastive learning (ITC), masked language model-ing (MLM) and word-in-image prediction (WIP). The pre-trained model is able to produce more informative repre-sentations with richer semantics, which could readily bene-fit existing scene text detectors (such as EAST and PSENet) in the down-stream text detection task. Extensive experi-ments on standard benchmarks demonstrate that the pro-posed paradigm can significantly improve the performance of various representative text detectors, outperforming pre-vious pre-training approaches. The code and pre-trained models will be publicly released. 1.

Introduction
Scene text detection is a fundamental yet challenging task in computer vision, which requires the model to pre-dict bounding boxes or polygons for each text instance in an image. For years, scene text detection methods based on deep learning have been extensively studied and widely adopted in both the academia and the industry due to its high research value and broad real-world applications. Recently, substantial advances have been observed, while grand chal-lenges are still remained [32].
*Both authors contributed equally to this work.
Figure 1. Comparisons of pre-training paradigms for scene text (a) Conventional SynthText pre-training: Region su-detection. pervisions (e.g., bounding box annotations, ground-truth masks) are used to train the image backbone and detector head. The fine-tuning pipeline is the same as the pre-training pipeline. (b) Pre-training with text supervision: Text annotations are utilized as the supervisions through an encoder-decoder framework. (c) Our pre-training model includes an image encoder, a text encoder and a cross-modal encoder, learned through three pretext tasks.
Different pre-training strategies have been proposed to learn better representations in natural language process-ing [8] and computer vision [14], usually relying on various pretext tasks. To accelerate the training procedure and en-hance the generalization capability, pre-training techniques have also largely been applied to scene text detection meth-ods. Most of the early attempts employ ImageNet [7] pre-training as done in general object detection. Nevertheless, an obvious domain gap exists between natural images from
ImageNet and scene text images, which might result in lim-ited performance gain after fine-tuning.
Therefore, researchers presented methods [16, 28] that fine-tune models pre-trained on synthetic text datasets, such as SynthText [12], curved SynthText [31] and Unreal-Text [34]. Most of recent text detection models that pre-trained on synthetic datasets outperform those pre-trained on ImageNet, however, they still suffer from a domain gap between synthetic and read-world data, which usually cause text-like textures to be falsely detected.
To tackle this issue, Wan et al. [51] propose STKM for pre-training via mining text knowledge without using region supervision. By adopting a text-recognition-like pipeline, STKM has proven to be effective on down-stream text detection task across different methods and datasets.
However, STKM utilizes a character-level decoding pro-cess, which makes it hard to effectively exploit context in-formation in the lexicon.
In addition, the single-stream pipeline is essentially a unidirectional mapping (from vision modality to language modality), thus can not sufficiently make use of the interaction between vision and text to learn informative representations.
We address these major challenges by introducing a novel Vision-Language Pre-Training paradigm for boost-ing Scene Text Detectors, termed VLPT-STD, together with three novel pre-training objectives, which enable en-coding richer information and learning discriminative rep-Importantly, by imposing a mutual align-resentations. ment between the two modalities, our method can better exploit text knowledge and achieve improved visual rep-resentations. As shown in Fig. 1, we compare different pre-training paradigms for scene text detection. Note that, although our approach only requires image-level text an-notations like STKM, it is designed from a very different perspective where fine-grained cross-modality interaction is adopted to align unimodal embeddings for learning better representations.
Inspired by vision-language pre-training approaches [4, 20–22], we utilize self-attention and cross-attention mod-ules to build a unified architecture together with three care-fully designed pre-training objectives. The image and text unimodal representations are aligned first via contrastive learning, and then attend to fine-grained text regions via pre-training tasks of masked language model and word-in-image prediction. Consequently, the pre-trained backbone can be fine-tuned for various text detectors to significantly improve the detection performance.
Specifically, the image (or text) embeddings are firstly extracted from an image (or text) encoder, and then fed to cross-attention blocks for fine-grained cross-modal inter-actions via various pre-training tasks. By designing var-ious pre-training objectives, we encourage the encoder to attend text regions in image data from cross-modal cues.
The whole model can be trained end-to-end and the visual backbone can be transferred to different text detectors. In addition, the proposed paradigm requires only image-level text annotations, whose labeling cost is much cheaper than conventional region annotations, especially for curve text labeling. Extensive experiments are conducted on various text detectors and datasets to demonstrate the effectiveness of the pre-trained backbone.
In summary, the main contributions of this paper are three-fold:
• We propose a novel vision-language joint learning framework for pre-training the visual backbones of scene text detectors, which is a conceptually simple and flexible framework to enable mutual alignment be-tween visual and textual representations.
• We devise three pretext tasks to encourage fine-grained vision-language interactions.
In particular, a novel
Word-in-Image Prediction (WIP) task is designed with hard example sampling strategy for learning discrimi-native representations.
• Extensive experiments demonstrate the effectiveness of our approach.
In particular, with three classical scene text detection methods: EAST, PSENet and DB, our approach has shown consistent and considerable improvements on five text detection datasets over con-ventional and STKM pre-training techniques. 2.