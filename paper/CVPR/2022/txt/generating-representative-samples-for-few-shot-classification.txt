Abstract
Few-shot learning (FSL) aims to learn new categories with a few visual samples per class. Few-shot class repre-sentations are often biased due to data scarcity. To mit-igate this issue, we propose to generate visual samples based on semantic embeddings using a conditional vari-ational autoencoder (CVAE) model. We train this CVAE model on base classes and use it to generate features for novel classes. More importantly, we guide this VAE to strictly generate representative samples by removing non-representative samples from the base training set when training the CVAE model. We show that this training scheme enhances the representativeness of the generated samples and therefore, improves the few-shot classifica-tion results. Experimental results show that our method improves three FSL baseline methods by substantial mar-gins, achieving state-of-the-art few-shot classification per-formance on miniImageNet and tieredImageNet datasets for both 1-shot and 5-shot settings. Code is available at: https://github.com/cvlab-stonybrook/ fsl-rsvae. 1.

Introduction
Few-shot learning (FSL) methods aim to learn useful representations with limited training data. They are ex-tremely useful for situations where machine learning solu-tions are required but large labelled datasets are not triv-ial to obtain (e.g. rare medical conditions [49, 71], rare animal species [75], failure cases in autonomous systems
[42,43,58]). Generally, FSL methods learn knowledge from a fixed set of base classes with a surplus of labelled data and then adapt the learned model to a set of novel classes for which only a few training examples are available [73].
Many FSL methods [10, 23, 39, 65, 65, 77, 82] employ a prototype-based classifier for its simplicity and good per-formance. They aim to find a prototype for each novel class such that it is close to the testing samples of the same class and far away from testing samples for other classes. How-*Work done outside of Amazon
Figure 1. Representative Samples. We refer representative sam-ples to the “easy-to-recognize” samples that faithfully reflect the key characteristics of the category. We identify those samples and then use them to train a VAE model for feature generation, con-ditioned on class-representative semantic embeddings. We show that the generated data significantly improves few-shot classifica-tion performance. ever, it is challenging to estimate a representative prototype just from a few available support samples [37,79]. An effec-tive strategy to enhance the representativeness of the proto-type is to employ textual semantic embeddings learned via
NLP models [13, 46, 52, 53] using large unsupervised text corpora [77, 82]. These semantic embeddings implicitly associate a class name, such as “Yorkshire Terriers”, with the class representative semantic attributes such as “small-est dog” or “long coat” [1] ( Fig. 1), providing strong and unbiased priors for category recognition.
For the most part, current FSL methods focus on learn-ing to adaptively leverage the semantic information to com-plete the original biased prototype estimated from the few available samples. For example, the recent FSL method of
Zhang et al. [82] learns to fuse the primitive knowledge and attribute features into a representative prototype, depending on the set of given few-shot samples. Similarly, Xing et al. [77] propose a method that computes an adaptive mix-ture coefficient to combine features from the visual and tex-tual modalities. However, learning to recover an arbitrarily biased prototype is challenging due to the drastic variety of the possible combinations of few-shot samples.
In this paper, we propose a novel FSL method to ob-tain class-representative prototypes. Inspired by zero-shot learning (ZSL) methods [4, 18, 85], we propose to gen-erate visual features via a variational autoencoder (VAE) model [66] conditioned on the semantic embedding of each class. This VAE model learns to associate a distribution of features to a conditioned semantic code. We assume that such association generalizes across the base and novel classes [3, 47]. Therefore, the model trained with sufficient data from the base classes can generate novel-class fea-tures that align with the real unseen features. We then use the generated features together with the few-shot samples to construct class prototypes. We show that this strategy achieves state-of-the-art results on both miniImageNet and tieredImageNet datasets. It works exceptionally well for 1-shot scenarios where our method outperforms state-of-the-art methods [76, 80] by 5 ∼ 6% in terms of classification accuracy.
Moreover, to enhance the representativeness of the pro-totype, we guide the VAE to generate more representative samples. Here we refer representative samples to the “easy-to-recognize” samples that faithfully reflect the key char-acteristics of the category (see Fig. 1). The embeddings of these representative samples often lie close to their cor-responding class centers, which are particularly useful for constructing class-representative prototypes.
Specifically, we guide the VAE model to generate repre-sentative samples by selecting only representative data from the base classes for training it. In essence, our VAE model is trained to model the data distribution of the training set.
As the training set contains only representative data, the trained VAE model outputs samples that are also represen-tative. Specifically, to select those representative features, we first assume that the feature vectors of each class fol-low a multivariate Gaussian distribution and estimate this distribution for each base class. Based on these distribu-tions, we compute the probability of each sample belonging to its corresponding category to measure the representative-ness for the sample. We filter out the non-representative samples and train the VAE using only representative sam-ples. Interestingly, we show that the representativeness of the training set highly corresponds to the accuracy of the few-shot classifier. We obtain the highest accuracy when training the VAE with the most representative samples. In this case, we only use a small percentage of the whole train-ing set, e.g., 10% for the case of miniImagenet dataset, to obtain the best results. Our analyses show that this approach consistently improves the FSL classification performance by 1 ∼ 2% across all benchmarks for three different base-lines [10, 39, 65].
Our main contributions can be summarized as follows:
• We are the first to use a VAE-based feature generation approach conditioned on class semantic embeddings for few-shot classification.
• We propose a novel sample selection method to col-lect representative samples. We use these samples to train a VAE model to obtain reliable data points for constructing class-representative prototypes.
• Our experiments show that our methods achieve state-of-the-art performance on two challenging datasets, tieredImageNet and miniImageNet.
We summarize related FSL works in Section 2. Section 3 provides a rundown of our approach. Section 4 reports the main results obtained with our method. In section 5, we provide multiple analyses to clarify different aspects of our methods. 2.