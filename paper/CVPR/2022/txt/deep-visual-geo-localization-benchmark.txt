Abstract
In this paper, we propose a new open-source benchmark-ing framework for Visual Geo-localization (VG) that allows to build, train, and test a wide range of commonly used ar-chitectures, with the flexibility to change individual compo-nents of a geo-localization pipeline. The purpose of this framework is twofold: i) gaining insights into how differ-ent components and design choices in a VG pipeline im-pact the final results, both in terms of performance (re-call@N metric) and system requirements (such as execu-tion time and memory consumption); ii) establish a system-atic evaluation protocol for comparing different methods.
Using the proposed framework, we perform a large suite of experiments which provide criteria for choosing back-bone, aggregation and negative mining depending on the use-case and requirements. We also assess the impact of engineering techniques like pre/post-processing, data aug-mentation and image resizing, showing that better perfor-mance can be obtained through somewhat simple proce-dures: for example, downscaling the images’ resolution to 80% can lead to similar results with a 36% savings in ex-traction time and dataset storage requirement. Code and trained models are available at dataset storage require-ment. https://deep-vg-bench.herokuapp.com/. 1.

Introduction
The task of coarsely estimating the place where a photo was taken based on a set of previously visited locations is called Visual (Image) Geo-localization (VG) [35, 40, 81] or
Visual Place Recognition (VPR) [20, 42] and it is addressed using image matching and retrieval methods on a database of images of known locations. We are witnessing a rapid growth of this field of research, as demonstrated by the in-creasing number of publications [2,10,14,21–23,27,29,34, 35,40,42,44,55,58,69,70,73–76,78,82], but this expansion
Vanilla
Resize (80%)
Data augm. (brightness = 2)
Pred. refinement (nearest crop)
PCA (2048)
CRN [35]
R@1 63.4 64.3 68.6 67.0 56.6 68.8
Table 1. Example of how results can be influenced by little train or test time changes to the VG pipeline. Recall@1 for a ResNet-18 with NetVLAD trained on Pitts30k and tested on Tokyo24/7.
Results are thoroughly discussed in later sections. is accompanied by two major limitations: i) A focus on single metric optimization, as it is common practice to compare results solely based on the recall on chosen datasets and ignoring other factors such as execution time, hardware requirements, and scalability. All these as-pects are important constraints in the design of a real-world
VG system. For instance, one might gladly accept a 5% drop in accuracy if this leads to a 90% decrease of descrip-tors size as the resulting reduction in memory requirements enables a better scalability. Similarly, computational time and descriptor dimensionality are crucial constraints in real-time applications, given a target hardware platform. ii) A lack of a standardized framework to train and test
VG models. It is common practice to perform direct com-parisons among off-the-shelf methods that use different se-tups (e.g., data augmentation, initialization, training dataset, etc.) [35, 64, 80], which can hide the improvement (or lack thereof) obtained by algorithmic changes and it does not al-low to pinpoint the impact of each individual component.
Table 1 shows how some simple engineering choices can have big effects on the recall metric.
Although previous benchmarks for VPR [80] and the re-lated task of Visual Localization [56, 62] offer interesting insights, they do not address the aforementioned issues. For these reasons, we propose a new open-source benchmark that provides researchers with an all-inclusive tool to build, train, and test a wide range of commonly used VG archi-tectures, offering the flexibility to change each component of a geo-localization pipeline. This allows to rigorously ex-amine how each element of the system influences the final
results while providing information computed on-the-fly re-garding the number of parameters, FLOPs, descriptors di-mensionality, etc.
Using our framework, we run numerous experiments aiming to understand which components are the most suit-able for a real-world application, and derive good practices depending on the target dataset and one’s hardware avail-ability. For example, we find that ResNet-50 [28] provides a good trade-off between accuracy, FLOPs and model size, and that Visual Transformers can successfully replace the
CNN backbones and achieve better geo-localization perfor-mances when trained on larger datasets. Furthermore, we observed that partial negative mining and reduced resolu-tion yield important decrease in computations without sig-nificantly compromising the performance, or even yielding gains in some cases.
The benchmark’s software and models are hosted at https://deep-vg-bench.herokuapp.com/. 2.