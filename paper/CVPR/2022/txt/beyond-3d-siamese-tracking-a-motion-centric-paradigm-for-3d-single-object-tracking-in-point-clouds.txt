Abstract 3D single object tracking (3D SOT) in LiDAR point clouds plays a crucial role in autonomous driving. Cur-rent approaches all follow the Siamese paradigm based on appearance matching. However, LiDAR point clouds are usually textureless and incomplete, which hinders ef-fective appearance matching. Besides, previous meth-ods greatly overlook the critical motion clues among tar-In this work, beyond 3D Siamese tracking, we in-gets. troduce a motion-centric paradigm to handle 3D SOT from a new perspective. Following this paradigm, we pro-pose a matching-free two-stage tracker M2-Track. At the 1st-stage, M 2-Track localizes the target within successive frames via motion transformation. Then it refines the tar-get box through motion-assisted shape completion at the 2nd-stage. Extensive experiments confirm that M 2-Track significantly outperforms previous state-of-the-arts on three large-scale datasets while running at 57FPS (∼ 8%, ∼ 17% and ∼ 22% precision gains on KITTI, NuScenes, and
Waymo Open Dataset respectively). Further analysis veri-fies each component’s effectiveness and shows the motion-centric paradigm’s promising potential when combined with appearance matching. Code will be made available at https://github.com/Ghostish/Open3DSOT. 1.

Introduction
Single Object Tracking (SOT) is a basic computer vi-sion problem with various applications, such as autonomous driving [22, 38, 39] and surveillance system [32]. Its goal is to keep track of a specific target across a video sequence, given only its initial state (appearance and location).
Existing LiDAR-based SOT methods [8,10,23,26,43,44] all follow the Siamese paradigm, which has been widely
* Corresponding author
Figure 1. Top. Previous Siamese approaches obtain a canoni-cal target template using the previous target box and search for the target in the current frame according to the matching similar-ity, which is sensitive to distractors. Bottom. Our motion-centric paradigm learns the relative target motion from two consecutive frames and then robustly localizes the target in the current frame via motion transformation. adopted in 2D SOT since it strikes a balance between per-formance and speed. During the tracking, a Siamese model searches for the target in the candidate region with an ap-pearance matching technique, which relies on the features of the target template and the search area extracted by a shared backbone (see Fig.1(a)).
Though the appearance matching for 3D SOT shows satisfactory results on KITTI dataset [9], we observe that
KITTI has the following proprieties: i) the target’s motion between two consecutive frames is minor, which ensures no drastic appearance change; ii) there are few/no distractors in the surrounding of the target. However, the above character-istics do not hold in natural scenes. Due to self-occlusion, significant appearance changes may occur in consecutive
LiDAR views when objects move fast, or the hardware only supports a low frame sampling rate. Besides, the negative samples grow significantly in dense traffic scenes. In these scenarios, it is not easy to locate a target based on its ap-pearance alone (even for human beings).
Is the appearance matching the only solution for LiDAR
SOT? Actually, Motion Matters. Since the task deals with a dynamic scene across a video sequence, the target’s move-ments among successive frames are critical for effective tracking. Knowing this, researchers have proposed various 2D Trackers to temporally aggregate information from pre-vious frames [2, 34]. However, the motion information is rarely explicitly modeled since it is hard to be estimated un-der the perspective distortion. Fortunately, 3D scenes keep intact information about the object motion, which can be easily inferred from the relationships among annotated 3D bounding boxes (BBoxes)1. Although 3D motion matters for tracking, previous approaches have greatly overlooked it. Due to the Siamese paradigm, previous methods have to transform the target template (initialized by the object point cloud in the first target 3D BBox and updated with the last prediction) from the world coordinate system to its own ob-ject coordinate system. This transformation ensures that the shared backbone extracts a canonical target feature, but it adversely breaks the motion connection between consecu-tive frames.
Based on the above observations, we propose to tackle 3D SOT from a different perspective instead of sticking to the Siamese paradigm. For the first time, we introduce a new motion-centric paradigm that localizes the target in sequential frames without appearance matching by explic-itly modeling the target motion between successive frames (Fig. 1(b)). Following this paradigm, we design a novel two-stage tracker M2-Track (Fig. 2). During the track-ing, the 1st-stage aims at generating the target BBox by predicting the inter-frame relative target motion. Utilizing all the information from the 1st-stage, the 2nd-stage refines the BBox using a denser target point cloud, which is aggre-gated from two partial target views using their relative mo-tion. We evaluate our model on KITTI [9], NuScenes [3] and Waymo Open Dataset (WOD) [29], where NuScenes and WOD cover a wide variety of real-world environments and are challenging for their dense traffics. The experiment results demonstrate that our model outperforms the exist-ing methods by a large margin while running faster than the previous top-performer [44]. Besides, the performance gap becomes even more significant when more distractors exist in the scenes. Furthermore, we demonstrate that our method can directly benefit from appearance matching when inte-grated with existing methods.
In summary, our main contributions are as follows: 1) A 1This is greatly held for rigid objects (e.g., cars), and it is approximately true for non-rigid objects (e.g., pedestrian). novel motion-centric paradigm for real-time LiDAR SOT, which is free of appearance matching. 2) A specific second-stage pipeline named M2-Track that leverages the motion-modeling and motion-assisted shape completion. 3) State-of-the-art online tracking performance with significant im-provement on three widely adopted datasets (i.e., KITTI,
NuScenes and Waymo Open Dataset). 2.