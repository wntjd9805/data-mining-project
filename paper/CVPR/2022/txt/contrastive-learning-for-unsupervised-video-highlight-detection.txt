Abstract
Video highlight detection can greatly simplify video browsing, potentially paving the way for a wide range of ap-plications. Existing efforts are mostly fully-supervised, re-quiring humans to manually identify and label the interest-ing moments (called highlights) in a video. Recent weakly supervised methods forgo the use of highlight annotations, but typically require extensive efforts in collecting external data such as web-crawled videos for model learning. This observation has inspired us to consider unsupervised high-light detection where neither frame-level nor video-level an-notations are available in training. We propose a simple contrastive learning framework for unsupervised highlight detection. Our framework encodes a video into a vector rep-resentation by learning to pick video clips that help to dis-tinguish it from other videos via a contrastive objective us-ing dropout noise. This inherently allows our framework to identify video clips corresponding to highlight of the video.
Extensive empirical evaluations on three highlight detec-tion benchmarks demonstrate the superior performance of our approach. 1.

Introduction
Video highlight detection aims to automatically find the interesting moments (called “highlights”) within videos.
With the explosion of video content in the past decade, it has become even more important due to its broad range of applications, such as video retrieval, recommendation, browsing, and editing.
In recent years, there has been significant progress in video highlight detection. Existing efforts predominantly focus on the fully-supervised sce-nario [9, 14, 19, 31, 36, 42, 47, 48], which requires manual annotation of the highlight moments in the training videos.
Since manual annotations are expensive to obtain, recent weakly supervised methods [3, 6, 18, 28, 29, 43] make use of
Code: https://github.com/tkbadamdorj/CHD.
Figure 1. Highlight detection using contrastive learning. Our model identifies highlight clips in a video because they form bet-ter clusters for contrastive learning. We use t-SNE [39] to visual-ize a video and its nearest neighbors using non-highlight clips and highlight clips. Each point represents a video clip, and each color represents a video. We see that poor clusters in the non-highlight clip embedding space (left) are clearer in the highlight clip embed-ding space (right). Ellipses are drawn around the clusters to show that video clusters that are not separable in the non-highlight clip embedding space (red and dark blue, yellow and light blue) are separated in the highlight clip embedding space. The green, yel-low, and pink videos also form tighter clusters. Thus, our model learns to pick highlight clips to do well on the contrastive learning task. See text for detailed explanation. video-level labels such as video category as a weak super-vision signal. However, the state-of-the-art methods often rely on large-scale external data. For example, LM [43] employs 10 million Instagram videos to train their model.
Recent advances in contrastive learning have helped close the gap between unsupervised models and their super-vised counterparts [5, 8]. The goal of contrastive learning is to learn an unsupervised representation of some data that is useful for downstream tasks. For images, the task is usu-ally classification. In this domain, a model is trained to map
two randomly transformed versions of the same image (e.g. random cropping, color distortion) close together in an em-bedding space, while mapping the same image farther from other images that are also randomly transformed [5]. Thus, we can view contrastive learning as a clustering task: we want to form a cluster for each image where the samples within each cluster are randomly transformed versions of the original image.
While visual transformations are straightforward, it is unclear what kinds of transformations to apply in discrete domains such as language. Recent work [8] has found that dropout [35] can be used as a random transformation to learn superior unsupervised sentence embeddings through contrastive learning. Dropout is a general transformation that can be applied to any network.
Inspired by this idea, we propose a novel and simple un-supervised1 framework for video highlight detection. In the highlight detection task, we break down each video into fixed length clips (e.g. 100 frames). Then we use a pre-trained feature extractor such as a C3D action recognition model [38] to obtain a vector representation of each clip.
Thus, we represent an input video by a sequence of vectors.
We interchangeably use “clip” to refer to the vector repre-sentation of a clip. Our unsupervised framework picks clips to produce a single vector embedding for the entire video.
We apply dropout within the network as our transformation, and learn to map two embeddings of the same video with a different series of dropouts close together, while mapping the video farther from other videos.
Our model learns to pick highlight clips (interesting mo-ments) since they contain more information about the video content. Let us motivate this claim through the example in
Fig. 1. We learn to cluster a video close to itself under ran-dom dropout perturbations. We claim that highlight clips contain more information about the video itself, thus better clusters are formed if our model picks highlight clips to pro-duce a video embedding. In this figure, we show the same set of videos represented purely by non-highlight clips (left) and highlight clips (right) using t-SNE [39]. Each video is shown using a different color, and each point represents a highlight/non-highlight clip after we apply a series of ran-dom dropouts within the network. The non-highlight clips do not form good clusters: the clusters often overlap, and each cluster is also not compact. On the other hand, we can clearly differentiate between the different videos when us-ing highlight clips. This means that in order to do well on the contrastive learning task, our model will learn to pick highlight clips. We experimentally prove this claim in Sec-tion 4.3. 1We use unsupervised to indicate that the method does not have ac-cess to any manually annotated training data or video-level labels. Weakly supervised methods typically utilize video-level label (category) or video length information (short web videos) for supervision.
In short, our main contribution is a novel unsupervised framework for video highlight detection based on con-trastive learning. Empirical evaluation on three widely-used highlight detection benchmarks demonstrate the superior performance of our approach. In many cases, it performs on par or better than the state-of-the-art methods that make use of large amounts of external data. 2.