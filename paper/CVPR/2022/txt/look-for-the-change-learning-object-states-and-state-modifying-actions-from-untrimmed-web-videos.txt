Abstract
Human actions often induce changes of object states such as “cutting an apple”, “cleaning shoes” or “pouring coffee”. In this paper, we seek to temporally localize ob-ject states (e.g. “empty” and “full” cup) together with the corresponding state-modifying actions (“pouring coffee”) in long uncurated videos with minimal supervision. The contributions of this work are threefold. First, we develop a self-supervised model for jointly learning state-modifying actions together with the corresponding object states from an uncurated set of videos from the Internet. The model is self-supervised by the causal ordering signal, i.e. initial ob-ject state → manipulating action → end state. Second, to cope with noisy uncurated training data, our model incor-porates a noise adaptive weighting module supervised by a small number of annotated still images, that allows to ef-ﬁciently ﬁlter out irrelevant videos during training. Third, we collect a new dataset with more than 2600 hours of video and 34 thousand changes of object states, and manually an-notate a part of this data to validate our approach. Our re-sults demonstrate substantial improvements over prior work in both action and object state-recognition in video. 1.

Introduction
Human actions often induce changes of the state of an object, as illustrated in Figure 1. Examples include “cut-ting an apple”, “cleaning shoes”, “tying a tie” or “ﬁlling-up a cup with coffee”. People can easily recognize such ac-tions and the resulting changes of object states [12], for ex-ample, when watching instructional videos. Furthermore, people can reproduce the actions in their environment, e.g. 1Czech Institute of Informatics, Robotics and Cybernetics at the Czech
Technical University in Prague. 3D´epartement d’informatique de l’ENS, ´Ecole normale sup´erieure,
CNRS, PSL Research University, 75005 Paris, France.
Figure 1. Examples of object states and state-modifying ac-tions learnt by our model from a dataset of long uncurated web videos. In each example the top row shows: the initial state in the video (left), the state-modifying action (middle), and the end-state (right). The bottom row shows video frames sampled from the en-tire video with their corresponding timestamps. It illustrates the difﬁculty of ﬁnding the correct temporal localization of the object states and the actions in the entire video. when following a recipe from a cooking video. However, artiﬁcial system with similar cognitive abilities is yet to be developed. Existing methods for recognizing object states and state-modifying actions address small-scale setups (5 objects and short manually curated videos) [3] or controlled environments [18]. At the same time, progress on auto-matic understanding of causal relations between actions and object states in the wild would be a major step in embod-ied video understanding and robotics. However, the task is challenging given the large amount and variability of exist-ing object-action pairs as well as the difﬁculty of manually collecting and annotating video data for it.
In this paper, we investigate whether the learning of ob-ject states and corresponding state-modifying actions can be scaled-up to noisy uncurated videos from the web while
using only minimal supervision. The contribution of this work is threefold as we outline below.
First, we develop a self-supervised model for jointly learning state-modifying actions and object states from an uncurated set of videos obtained from a video search en-gine. We explore the causal ordering in the video as a free supervisory signal and use it to discover the changing states of objects and state-modifying actions. We deﬁne it by the sequence of initial object state → manipulating action → end state, as illustrated in Figure 1. While the prior work on this problem [3] was limited to closed-form linear clas-siﬁers, our model is amenable to large-scale learning using stochastic gradient descent and supports non-linear multi-layer models.
Second, to cope with noisy uncurated data that may in-clude a large proportion of irrelevant videos (e.g. videos of
Apple laptops when learning “cutting an apple”), our model incorporates a noise adaptive weighting module that allows to ﬁlter out irrelevant videos. This noise adaptive weight-ing module is supervised by a small number of still images depicting the two states of the object, which are easy to col-lect using currently available image search engines. This attention mechanism allows us to scale our method to noisy uncurated data, as we show by experimental results.
Third, we collect a new “ChangeIt” dataset with more than 2600 hours of video and 34 thousand changes of object states. We manually annotate a portion of this data for eval-uation. To validate our approach, we show results on this new uncurated dataset as well as on the existing smaller cu-rated video dataset from [3]. We ablate key components of our method and demonstrate substantial improvements over prior work both in action and object state localization. The dataset, the code, and a trained model are publicly available. 2.