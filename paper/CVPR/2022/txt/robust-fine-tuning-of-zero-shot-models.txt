Abstract
Large pre-trained models such as CLIP or ALIGN offer con-sistent accuracy across a range of data distributions when performing zero-shot inference (i.e., without fine-tuning on a specific dataset). Although existing fine-tuning methods substantially improve accuracy on a given target distribu-tion, they often reduce robustness to distribution shifts. We address this tension by introducing a simple and effective method for improving robustness while fine-tuning: ensem-bling the weights of the zero-shot and fine-tuned models (WiSE-FT). Compared to standard fine-tuning, WiSE-FT pro-vides large accuracy improvements under distribution shift, while preserving high accuracy on the target distribution.
On ImageNet and five derived distribution shifts, WiSE-FT improves accuracy under distribution shift by 4 to 6 percent-age points (pp) over prior work while increasing ImageNet accuracy by 1.6 pp. WiSE-FT achieves similarly large ro-bustness gains (2 to 23 pp) on a diverse set of six further distribution shifts, and accuracy gains of 0.8 to 3.3 pp com-pared to standard fine-tuning on commonly used transfer learning datasets. These improvements come at no addi-tional computational cost during fine-tuning or inference. 1.

Introduction
A foundational goal of machine learning is to develop mod-els that work reliably across a broad range of data distri-∗⋆These authors contributed equally. ArXiv version: 2109.01903. butions. Over the past few years, researchers have pro-posed a variety of distribution shifts on which current al-gorithmic approaches to enhance robustness yield little to no gains [68, 95]. While these negative results highlight the difficulty of learning robust models, large pre-trained mod-els such as CLIP [79], ALIGN [44] and BASIC [75] have recently demonstrated unprecedented robustness to these challenging distribution shifts. The success of these models points towards pre-training on large, heterogeneous datasets as a promising direction for increasing robustness. However, an important caveat is that these robustness improvements are largest in the zero-shot setting, i.e., when the model per-forms inference without fine-tuning on a target distribution.
In a concrete application, a zero-shot model can be fine-tuned on extra application-specific data, which often yields large performance gains on the target distribution. However, in the experiments of Radford et al. [79] and Pham et al. [75], fine-tuning comes at the cost of robustness: across several natural distribution shifts, the accuracy of their fine-tuned models is lower than that of the original zero-shot model. This leads to a natural question: Can zero-shot models be fine-tuned without reducing accuracy under distribution shift?
As pre-trained models are becoming a cornerstone of ma-chine learning, techniques for fine-tuning them on down-stream applications are increasingly important. Indeed, the question of robustly fine-tuning pre-trained models has re-cently also been raised as an open problem by several au-thors [3, 9, 75, 79]. Andreassen et al. [3] explored several fine-tuning approaches but found that none yielded models
Figure 1. (Top left) Zero-shot CLIP models exhibit moderate accuracy on the reference distribution (x-axis, the target for fine-tuning) and high effective robustness (accuracy on the distribution shifts beyond the baseline models). In contrast, standard fine-tuning—either end-to-end or with a linear classifier (final layer)—attains higher accuracy on the reference distribution but less effective robustness. (Top
[0, 1]. (Bottom) On five right) Our method linearly interpolates between the zero-shot and fine-tuned models with a mixing coefficient α distribution shifts derived from ImageNet (ImageNetV2, ImageNet-R, ImageNet Sketch, ObjectNet, and ImageNet-A), WiSE-FT improves average accuracy relative to both the zero-shot and fine-tuned models while maintaining or improving accuracy on ImageNet.
∈ with improved robustness at high accuracy. Furthermore,
Taorio et al. [95] demonstrated that no current algorithmic robustness interventions provide consistent gains across the distribution shifts where zero-shot models excel.
In this paper, we conduct an empirical investigation to un-derstand and improve fine-tuning of zero-shot models from a distributional robustness perspective. We begin by mea-suring how different fine-tuning approaches (last-layer vs. end-to-end fine-tuning, hyperparameter changes, etc.) af-fect the accuracy under distribution shift of the resulting fine-tuned models. Our empirical analysis uncovers two key issues in the standard fine-tuning process. First, the robustness of fine-tuned models varies substantially under even small changes in hyperparameters, but the best hyper-parameters cannot be inferred from accuracy on the target distribution alone. Second, more aggressive fine-tuning (e.g., using a larger learning rate) yields larger accuracy improve-ments on the target distribution, but can also reduce accuracy under distribution shift by a large amount.
Motivated by the above concerns, we propose a robust way of fine-tuning zero-shot models that addresses the afore-mentioned trade-off and achieves the best of both worlds: increased performance under distribution shift while main-taining or even improving accuracy on the target distribution relative to standard fine-tuning. In addition, our method simplifies the choice of hyperparameters in the fine-tuning process.
Our method (Figure 1) has two steps: first, we fine-tune the zero-shot model on the target distribution. Second, we com-bine the original zero-shot and fine-tuned models by linearly interpolating between their weights, which we refer to as weight-space ensembling. Interpolating model parameters is a classical idea in convex optimization dating back decades (e.g., see [76, 82]). Here, we empirically study model in-terpolation for non-convex models from the perspective of distributional robustness. Interestingly, linear interpolation in weight-space still succeeds despite the non-linearity in the activation functions of the neural networks.
Weight-space ensembles for fine-tuning (WiSE-FT) substan-tially improve accuracy under distribution shift compared to prior work while maintaining high performance on the target distribution. Concretely, on ImageNet [17] and five of the natural distribution shifts studied by Radford et al. [79],
WiSE-FT applied to standard end-to-end fine-tuning im-proves accuracy under distribution shift by 4 to 6 percentage points (pp) over prior work while maintaining or improving the ImageNet accuracy of the fine-tuned CLIP model. Rel-ative to the zero-shot model, WiSE-FT improves accuracy under distribution shift by 1 to 9 pp. Moreover, WiSE-FT improves over a range of alternative approaches such as regularization and evaluating at various points throughout fine-tuning. These robustness gains come at no additional computational cost during fine-tuning or inference.
While our investigation centers around CLIP, we observe sim-ilar trends for other zero-shot models including ALIGN [44],
BASIC [75], and a ViT model pre-trained on JFT [21]. For instance, WiSE-FT improves the ImageNet accuracy of a fine-tuned BASIC-L model by 0.4 pp, while improving aver-age accuracy under distribution shift by 2 to 11 pp.
To understand the robustness gains of WiSE-FT, we first study WiSE-FT when fine-tuning a linear classifier (last layer) as it is more amenable to analysis. In this linear case, our procedure is equivalent to ensembling the outputs of two models, and experiments point towards the complementarity of model predictions as a key property. For end-to-end fine-tuning, we connect our observations to earlier work on the phenomenology of deep learning. Neyshabur et al. [71] found that end-to-end fine-tuning the same model twice yielded two different solutions that were connected via a linear path in weight-space along which error remains low, known as linear mode connectivity [25]. Our observations suggest a similar phenomenon along the path generated by
WiSE-FT, but the exact shape of the loss landscape and con-nection between error on the target and shifted distributions are still open problems (analysis in Appendix A).
In addition to the aforementioned ImageNet distribution shifts, WiSE-FT consistently improves robustness on a di-verse set of six additional distribution shifts including: (i) geographic shifts in satellite imagery and wildlife recogni-tion (WILDS-FMoW, WILDS-iWildCam) [6, 13, 47], (ii) reproductions of the popular image classification dataset
CIFAR-10 with a distribution shift (CIFAR-10.1 and CIFAR-10.2) [60,81], and (iii) datasets with distribution shift induced by temporal perturbations in videos (ImageNet-Vid-Robust and YTBB-Robust) [86]. Beyond the robustness perspective,
WiSE-FT also improves accuracy compared to standard fine-tuning, reducing the relative error rate by 4-49% on a range of seven datasets: ImageNet, CIFAR-10, CIFAR-100 [52],
Describable Textures [14], Food-101 [10], SUN397 [101], and Stanford Cars [51]. Even when fine-tuning data is scarce, reflecting many application scenarios, we find that WiSE-FT improves performance.
Overall, WiSE-FT is simple, universally applicable in the problems we studied, and can be implemented in a few lines of code. Hence we encourage its adoption for fine-tuning zero-shot models. 2.