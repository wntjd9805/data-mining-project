Abstract
Affine registration is indispensable in a comprehensive medical image registration pipeline. However, only a few studies focus on fast and robust affine registration algo-rithms. Most of these studies utilize convolutional neural networks (CNNs) to learn joint affine and non-parametric registration, while the standalone performance of the affine subnetwork is less explored. Moreover, existing CNN-based affine registration approaches focus either on the local mis-alignment or the global orientation and position of the in-put to predict the affine transformation matrix, which are sensitive to spatial initialization and exhibit limited gener-alizability apart from the training dataset. In this paper, we present a fast and robust learning-based algorithm, Coarse-to-Fine Vision Transformer (C2FViT), for 3D affine medi-cal image registration. Our method naturally leverages the global connectivity and locality of the convolutional vision transformer and the multi-resolution strategy to learn the global affine registration. We evaluate our method on 3D brain atlas registration and template-matching normaliza-tion. Comprehensive results demonstrate that our method is superior to the existing CNNs-based affine registration methods in terms of registration accuracy, robustness and generalizability while preserving the runtime advantage of the learning-based methods. The source code is available at https://github.com/cwmok/C2FViT. 1.

Introduction
Rigid and affine registration is crucial in a variety of medical imaging studies and has been a topic of active re-search for decades. In a comprehensive image registration framework, the target image pair is often pre-aligned based on a rigid or affine transformation before using deformable (non-rigid) registration, eliminating the possible linear and large spatial misalignment between the target image pair.
Solid structures such as bones can be aligned well with rigid and affine registration [29, 37].
In conventional im-age registration approaches, inaccurate pre-alignment of the
Figure 1. Comparisons of different architectures for affine regis-tration. The concatenation-based (VTN-Affine [46]) and Siamese network (ConvNet-Affine [11]) approaches are based on convolu-tional neural networks, while our proposed C2FViT is based on vi-sion transformers. For brevity, we illustrate 1-level C2FViT only.
Local and global operations are in green and purple, respectively. image pair may impair the registration accuracy or impede the convergence of the optimization algorithm, resulting in sub-optimal solutions [47]. The success of recent learning-based deformable image registration approaches has largely been fueled [3,9,11,17,19,20,34–36] by accurate affine ini-tialization using conventional image registration methods.
While the conventional approaches excel in registration per-formance, the registration time is dependent on the degree of misalignment between the input images and can be time-consuming with high-resolution 3D image volumes. To fa-cilitate real-time automated image registration, a few stud-ies [21, 22, 40, 46] have been proposed to learn joint affine and non-parametric registration with convolutional neural networks (CNNs). However, the standalone performance of
the affine subnetwork compared to the conventional affine registration algorithm is less explored. Moreover, consider-ing that affine transformation is global and generally targets the possible large displacement, we argue that CNNs are not the ideal architecture to encode the orientation and absolu-tion position of the image scans in Cartesian space or affine parameters due to the inductive biases embedded into the architectural structure of CNNs.
In this paper, we analyze and expose the generic inabil-ity and limited generlizability of CNN-based affine regis-tration methods in cases with large initial misalignment and unseen image pairs apart from the training dataset. Moti-vated by the recent success of vision transformer models
[10, 12, 41, 43, 44], we depart from the existing CNN-based approaches and propose a coarse-to-fine vision transformer (C2FViT) dedicated to 3D medical affine registration. To the best of our knowledge, this is the first learning-based affine registration approach that considers the non-local de-pendencies between input images when learning the global affine registration for 3D medical image registration.
The main contributions of this work are as follows:
• we quantitatively investigate and analyze the registra-tion performance, robustness and generalizability of existing learning-based affine registration methods and conventional affine registration methods in 3D brain registration;
• we present a novel learning-based affine registration algorithm, namely C2FViT, which leverages convo-lutional vision transformers with the multi-resolution strategy. C2FViT outperforms the recent CNN-based affine registration approaches while demonstrating su-perior robustness and generalizability across datasets;
• the proposed learning paradigm and objective func-tions can be adapted to a variety of parametric regis-tration approaches with minimum effort.
We evaluate our method on two tasks: template-matching normalization to MNI152 space [13–15] and 3D brain atlas registration in native space. Results demonstrate that our method not only achieves superior registration per-formance over existing CNN-based methods, but the trained model also generalizes well to an unseen dataset beyond the training dataset, reaching the registration performance of conventional affine registration methods. 2.