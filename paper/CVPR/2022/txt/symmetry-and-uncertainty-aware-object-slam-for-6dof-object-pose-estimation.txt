Abstract
We propose a keypoint-based object-level SLAM frame-work that can provide globally consistent 6DoF pose es-timates for symmetric and asymmetric objects alike. To the best of our knowledge, our system is among the ﬁrst to utilize the camera pose information from SLAM to pro-vide prior knowledge for tracking keypoints on symmetric objects – ensuring that new measurements are consistent with the current 3D scene. Moreover, our semantic key-point network is trained to predict the Gaussian covariance for the keypoints that captures the true error of the predic-tion, and thus is not only useful as a weight for the resid-uals in the system’s optimization problems, but also as a means to detect harmful statistical outliers without choos-ing a manual threshold. Experiments show that our method provides competitive performance to the state of the art in 6DoF object pose estimation, and at a real-time speed. Our code, pre-trained models, and keypoint labels are available https://github.com/rpng/suo_slam. 1.

Introduction
Object pose estimation in 6 degrees of freedom (DoF) plays a key role in a variety of down-stream applica-tions (e.g., autonomous driving, robotic navigation, ma-nipulation, and augmented reality), and has been exten-sively studied in computer vision and robotics communi-ties [5, 14, 17, 22, 26, 28, 33]. Some methods rely on RGB input [15,22,31,32,37], while others utilize additional depth input to improve the performance [15,29,32,35]. Some deal with a single view [22, 28, 35], while others utilize multiple views to enhance the results [2, 3, 5, 14, 15, 29]. In particu-lar, multi-view methods can be further categorized into of-ﬂine structure from motion (SfM) – where all the frames are given at once [3, 14] – and the online SLAM styles, where frames are provided sequentially and real-time per-Figure 1. Our proposed method leverages the detected keypoints of asymmetric objects and the 3D scene created from the SLAM system to consistently track the keypoints of symmetric objects.
Given the current camera pose estimated from asymmetric objects’ keypoints, the projections of the existing 3D keypoints into the current image act as informative prior input to guide the network in predicting keypoints with consistent symmetry over time. formance is expected [5, 29]. This paper focuses on image-based 6DoF pose estimation for multiple objects in the con-text of an online monocular SLAM system.
A typical multi-view 6DoF pose estimation method can be decomposed into the single-view estimation stage and the multi-view enhancement stage. While pose estimates from multiple views can be fused for better performance [3, 5, 14], handling extreme inconsistency – e.g., those caused by rotational symmetry of objects – is still challenging. It is also unreliable to manually tune the thresholds for out-lier rejection and assign residual weights for nonlinear opti-mization. To tackle these challenges, in this paper, we pro-pose a symmetry and uncertainty-aware 6DoF object pose estimation method which fuses semantic keypoint measure-ments from all views within a SLAM framework. The main contributions of this work are:
• We design a keypoint-based object SLAM system that jointly estimates the globally-consistent object and camera poses in real time – even in the presence of incorrect detections and symmetric objects.
• We propose a method able to consistently predict and track 2D semantic keypoints for symmetric objects over time, which leverages the projection of existing 3D keypoints into the current image as an informative prior input to the keypoint network.
• We develop a method to train the keypoint network to estimate the uncertainty of its predictions such that the uncertainty measure quantiﬁes the true error of the keypoints, and signiﬁcantly improves object pose esti-mation in the object SLAM system.
The rest of this paper is organized as follows: After brieﬂy reviewing the related literature in Sec. 2, we describe our method in detail in Sec. 3 – including the keypoint de-tector and how it is used in the entire system. A thorough evaluation of our framework is presented in Sec. 4 before concluding in Sec. 5. 2.