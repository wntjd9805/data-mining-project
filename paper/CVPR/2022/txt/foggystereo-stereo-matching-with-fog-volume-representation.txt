Abstract
Stereo matching in foggy scenes is challenging as the scattering effect of fog blurs the image and makes the matching ambiguous. Prior methods deem the fog as noise and discard it before matching. Different from them, we propose to explore depth hints from fog and improve stereo matching via these hints. The exploration of depth hints is designed from the perspective of rendering. The rendering is conducted by reversing the atmospheric scattering pro-cess and removing the fog within a selected depth range.
The quality of the rendered image reflects the correctness of the selected depth, as the closer it is to the real depth, the clearer the rendered image is. We introduce a fog volume representation to collect these depth hints from the fog. We construct the fog volume by stacking images rendered with depths computed from disparity candidates that are also used to build the cost volume. We fuse the fog volume with cost volume to rectify the ambiguous matching caused by fog. Experiments show that our fog volume representation significantly promotes the SOTA result on foggy scenes by 10% ∼ 30% while maintaining a comparable performance in clear scenes. 1.

Introduction
Stereo matching is a pixel-wise labeling task relying on discriminative features to achieve accurate results. The dis-criminative features can be well extracted by existing meth-ods in clear scenes [3, 4, 7, 15, 16]. However, it is unavoid-able to encounter foggy or foggy-like scenes in the real world. The fog blurs the image and makes the features in-discriminative for stereo matching. The ambiguous match-ing result caused by fog restricts the application of stereo matching.
Prior methods deem fog as a noise and discard it to im-prove matching results [12,26,27,36]. Different from them, we propose to take advantage of fog and explore depth hints for stereo matching. The intuitive observation comes from the fog rendering process. During rendering, fog is ac-(a) The process and results of rendering. (b) The distribution of SSIM ∼ Depth.
Figure 1. The Visualization of depth hints from fog. (a) We reverse the atmospheric scattering process by removing the fog among dif-ferent depth ranges. Only the depth near the ground truth leads to a clear image. We deem this observation as the depth hint. (b)
We further illustrate the distribution of rendered image quality in depth. We measure the image quality via the structural similarity (SSIM) metric. We find that the closer the depth candidate is to the ground truth, the better the rendered image quality is. cumulated along the light path between objects and cam-era following the physical atmospheric scattering process
[24,34,38]. Different depths will lead to different brightness and blur the image at different levels. So when we render the image by reversing the atmospheric scattering process, fog is removed within a selected depth range. As presented in Fig. 1a, only the depth close to the real depth will lead to a clear image. In other words, the quality of the rendered image indicates the correctness of depth used in the render-ing process, which is illustrated in Fig. 1b.
Based on the above observation, we introduce a fog vol-ume representation to collect depth hints from the fog. The fog volume is built along with the cost volume using the same disparity candidates. When we sample a disparity can-didate for the cost volume, we also validate its correctness by the fog volume. The fog volume representation is con-structed in three steps. We first learn the parameters of the atmospheric scattering process from the left image, includ-ing the global atmospheric light and the atmospheric atten-uation coefficient. Then we render a series of left images with atmospheric parameters and sampled disparity candi-dates by reversing the atmospheric scattering process. Fi-nally, the rendered images are stacked together to build our fog volume. We use a 3D convolutional network on the fog volume to learn to validate the sampled disparities.
Our fog volume provides great depth hints in foggy areas where existing cost volume loses effectiveness due to image degradation. The cost volume, instead, is more suitable in good visible areas [3, 7, 11]. In order to take advantage of both kinds of volumes, we fuse them through the volume uncertainty. The volume uncertainty is computed from the variance of two volumes along the disparity dimension.
We validate our method on both synthetic and natural foggy scenes. Our method outperforms the state-of-the-art approaches in foggy scenes by more than 10% while keeping comparable performance in clear scenes. We test the ability of our method in different depth ranges and fog thicknesss to demonstrate the potential application of our method in the real world. 2.