Abstract
Visual relation understanding plays an essential role for holistic video understanding. Most previous works adopt a multi-stage framework for video visual relation detec-tion (VidVRD), which cannot capture long-term spatio-temporal contexts in different stages and also suffers from inefﬁciency.
In this paper, we propose a transformer-based framework called VRDFormer to unify these de-coupling stages. Our model exploits a query-based ap-proach to autoregressively generate relation instances. We speciﬁcally design static queries and recurrent queries to enable efﬁcient object pair tracking with spatio-temporal contexts. The model is jointly trained with object pair detection and relation classiﬁcation. Extensive experiments on two benchmark datasets, ImageNet-VidVRD and VidOR, demonstrate the effectiveness of the proposed VRDFormer, which achieves the state-of-the-art performance on both relation detection and relation tagging tasks. The code is released at https://github.com/zhengsipeng/
VRDFormer_VRD. 1.

Introduction
Video visual relation detection (VidVRD) [32] aims to detect all relation instances in the video. Each instance contains a subject, an object and their relationship, as well as the spatial and temporal locations of the subject and the object. This task has attracted more and more attention in recent years, as it serves as a bridge to connect basic vision tasks (e.g. object detection [5, 12, 54] and tracking [11, 47]) with more complicated video semantic understanding tasks (e.g. captioning [43] and VideoQA [25]).
One typical approach for VidVRD [30, 32, 36] is to decompose the task in a multi-stage pipeline. These works, as illustrated in Figure 1, ﬁrstly employ off-the-shelf object detectors [27, 54] to detect and track objects in a video, and then, enumerate every two object tracklets and use temporal sliding window to obtain tracklet pairs. Finally, they ﬁlter
*Qin Jin is the corresponding author.
Figure 1. Existing VidVRD methods adopt a multi-stage pipeline.
They suffer from limited spatio-temporal contexts, overly sampled tracklet pairs, and independently optimized modules. out invalid tracklet pairs, then predict relation types for the remained ones using region-of-interests (RoI) features cropped from pre-computed CNN feature maps [3, 27].
We consider that there are three major limitations in such multi-stage framework. First, spatio-temporal contexts are not well exploited in tracklet pair generation. In fact, spatio-temporal contexts not only can enhance the model’s ability to localize objects, but also provide valuable information to infer the presence or absence of a relation. For example, the detection of the subject/object when occlusion occurs can get help from the temporal context. The relation reasoning can beneﬁt from the spatial context. Although context has been widely adopted for the ﬁnal relation classiﬁcation step [39, 44], it has not been well explored in the detection of relation instances in video. Therefore, object detection and tracking might not be very accurate in these methods, resulting in accumulated errors to the following stages.
Second, in previous works, each module is independently trained. However, object detection, tracking and relation classiﬁcation are highly correlated and could promote each other through joint learning. Last but not least, since tracklet pairs are exhaustively generated, many of them do not have meaningful relations, which not only harms
computational efﬁciency but also inﬂuences classiﬁcation performances.
To address above limitations, in this work, we propose a uniﬁed transformer-based video visual relation detection
It consists of a video framework named VRDFormer. encoding module and a query-based relation instance gen-eration module to detect relations in an autoregressive manner. Speciﬁcally, we adopt a query-based approach to detect and track object pairs. We propose two types of queries for object pairs generation in videos, namely static and recurrent queries. The static queries detect new object pairs in each frame which can aggregate the spatial contexts via transformer attention mechanism, while the recurrent queries aggregate the temporal contexts across frames to track previously detected object pairs. We keep all tracklet pairs in a memory and use a transformer-based model to classify relations for each tracklet pair which reserves long-term spatio-temporal history. The whole model is end-to-end trained by the object pair detection and relation classiﬁcation tasks jointly. We conduct extensive experi-ments on two benchmark datasets to evaluate the model.
VRDFormer achieves the state-of-the-art performance in relation detection and relation tagging on the two datasets.
In summary, our contributions are as follows:
• We propose a uniﬁed one-stage model VRDFormer for video visual relation detection (VidVRD), which is able to perform tracklet pair generation and relation classiﬁcation simultaneously.
• We design static queries and recurrent queries to ag-gregate spatio-temporal contexts, which enable more convenient temporal association for object pairs across frames and more effective relation classiﬁcation.
• We carry out extensive experiments and analysis on two benchmark datasets and achieve the state-of-the-art performance on both datasets. 2.