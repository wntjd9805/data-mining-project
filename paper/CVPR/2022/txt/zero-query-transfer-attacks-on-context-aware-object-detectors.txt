Abstract
Adversarial attacks perturb images such that a deep neural network produces incorrect classification results.
A promising approach to defend against adversarial at-tacks on natural multi-object scenes is to impose a context-consistency check, wherein, if the detected objects are not consistent with an appropriately defined context, then an at-tack is suspected. Stronger attacks are needed to fool such context-aware detectors. We present the first approach for generating context-consistent adversarial attacks that can evade the context-consistency check of black-box object de-tectors operating on complex, natural scenes. Unlike many black-box attacks that perform repeated attempts and open themselves to detection, we assume a “zero-query” setting, where the attacker has no knowledge of the classification decisions of the victim system. First, we derive multiple at-tack plans that assign incorrect labels to victim objects in a context-consistent manner. Then we design and use a novel data structure that we call the perturbation success prob-ability matrix, which enables us to filter the attack plans and choose the one most likely to succeed. This final attack plan is implemented using a perturbation-bounded adver-sarial attack algorithm. We compare our zero-query attack against a few-query scheme that repeatedly checks if the vic-tim system is fooled. We also compare against state-of-the-art context-agnostic attacks. Against a context-aware de-fense, the fooling rate of our zero-query approach is signifi-cantly higher than context-agnostic approaches and higher than that achievable with up to three rounds of the few-query scheme. 1.

Introduction
Despite achieving significant performance gains on a va-riety of vision and language tasks, deep neural networks (DNNs) are vulnerable to adversarial attacks [49]. One
*Corresponding author
Figure 1. For natural scenes containing multiple objects, applying an eva-sion attack on an individual object (e.g., crosswalk Ñ boat) violates the context: A boat and a stop-sign rarely occur together. A context-aware detector can detect this attack. In this work, we perturb multiple objects in a context-consistent way (e.g., crosswalk Ñ boat, stop-sign Ñ water) in a single attempt. The combination (person, boat, water) does not violate context and thus fools even a context-aware detector. of the most popular adversarial approaches is the class of perturbation-bounded evasion attacks [9, 19, 35, 41]. Here, an attacker can make a model yield arbitrarily wrong classi-fication results by adding imperceptible perturbations to the input image. These attacks are quite practical and can be performed at test time without needing access to the train-ing data. The vast majority of work in this area has focused on attacking classifiers trained on datasets like ImageNet,
MNIST, CIFAR-10, and CIFAR-100, where the classifier attempts to recognize one dominant object in a given im-age.
In contrast, we are primarily concerned with object detectors [24,29,39,42,44] that localize and recognize mul-tiple objects in an image, which is the case in most natural images. Such detectors often take a holistic view of an im-age, rather than considering it as a collection of arbitrary objects [55, 57]. The objects in natural scene images form a context that can help identify the scene or given the scene we are likely to find the objects that conform to the scene
context. For instance, a boat is unlikely to co-occur with a stop sign, and much more likely to co-occur with wa-ter. Leveraging this observation, some recent attack [7] and defense [28, 62] mechanisms have been proposed that take image context into account. Context-aware defense meth-ods in [28, 62] can detect attacks that are inconsistent with the scene context, as shown in Figure 1. To evade these defenses, changing one object in the image is insufficient.
The context-aware attack method in [7] uses the knowledge of co-occurrence between different object classes in com-plex images to generate a sequence of transferable attacks for black-box object detectors; however, this method needs a few queries to test which attack plan is successful. We present, for the first time, a zero-query attack algorithm that changes multiple objects simultaneously in a context-consistent manner thereby creating a holistic adversarial scene that can overcome context-aware defenses.
In this work, we consider “zero-query” attacks (ZQA) that refer to a setting in which the attacker has no feedback channel to access the classification decisions of the victim system. This setting is extremely useful in practice because in many applications the victim system is inaccessible to the attacker; even if the victim system is accessible, the at-tacker’s communications can be monitored, and thus draw suspicion. ZQA, on the other hand, is a truly stealthy attack.
The attacker can only implement an attack plan once by per-turbing multiple objects in a given scene and submitting the perturbed image to the victim system. Furthermore, we as-sume that the victim system is explicitly context-aware; that is, it will examine the list of detected objects and determine whether that list is “context-consistent” or not. If yes, then the detector will not suspect an attack. If not, it will sus-pect that the image has been perturbed by an attacker. Our
ZQA approach is able to subvert more sophisticated multi-label object detectors that either implicitly or explicitly take context relationships across objects into account while per-forming their inference. In fact, accounting for context is what makes it possible to achieve high success rates in a single attempt.
Several approaches exist for scene context modeling [2, 17, 22, 38, 50].
In this paper, we restrict our attention to object co-occurrence, which is the most fundamental ap-proach to modeling semantic context. The context model is represented by a co-occurrence graph (or equivalently the co-occurrence matrix) that is computed for a given set of images. We consider a list of objects as context-consistent only if the corresponding labels form a fully connected sub-graph within the co-occurrence graph.
The main contributions of this paper are as follows.
‚ We develop an architecture for designing attacks on multi-object scenes that fool context-aware object detec-tors. Our detectors explicitly use object co-occurrence to model scene context.
‚ We propose an approach for zero-query context-aware at-tacks that generate adversarial scenes to fool a context-aware detector in a single step.
‚ We introduce the concept of a perturbation success prob-ability matrix (PSPM) that models the probability of suc-cessfully perturbing a given target object to a given vic-tim object in the white box setting. We use the PSPM to refine our attack plans, essentially choosing the one which is most likely to succeed. We show that the PSPM-guided attacks improve the fooling rate even in a black-box setting.
‚ We show experimentally that the fooling rate of ZQA is significantly higher than that achieved by a context-agnostic black-box attack. Furthermore, we compare our results against a possible “few-query” strategy [7] that repeatedly enhances the attack plan, while observing the detector output, until the detector is fooled. For the Pas-cal VOC dataset [18], the ZQA attacks provide fooling rates comparable to 5-query and 3-query attacks in the white-box and black-box settings, respectively. 2.