Abstract
Graph neural networks (GNN) are powerful models for many graph-structured tasks. Existing models often assume that the complete structure of the graph is available dur-ing training. In practice, however, graph-structured data is usually formed in a streaming fashion so that learning a graph continuously is often necessary. In this paper, we bridge GNN and lifelong learning by converting a continual graph learning problem to a regular graph learning prob-lem so GNN can inherit the lifelong learning techniques developed for convolutional neural networks (CNN). We propose a new topology, the feature graph, which takes features as new nodes and turns nodes into independent graphs. This successfully converts the original problem of node classification to graph classification. In the experi-ments, we demonstrate the efficiency and effectiveness of feature graph networks (FGN) by continuously learning a sequence of classical graph datasets. We also show that
FGN achieves superior performance in two applications, i.e., lifelong human action recognition with wearable devices and feature matching. To the best of our knowledge, FGN is the first method to bridge graph learning and lifelong learn-ing via a novel graph topology. Source code is available at https://github.com/wang-chen/LGL. 1.

Introduction
Graph neural networks (GNN) have received increas-ing attention and proved useful for many tasks with graph-structured data, such as citation, social, and protein networks
[51]. However, graph data is sometimes formed in a stream-ing fashion and real-world datasets are continuously evolving over time. Thus, learning a streaming graph is expected in many cases [45]. For example, in a social network, the num-ber of users often grows over time and we expect that the model can learn continually with new users. In this paper, we extend graph neural networks to lifelong learning, which is also known as continual or incremental learning [26]. (a) Regular graph G. (b) Feature graph GF .
Figure 1. We introduce feature graph network (FGN) for lifelong graph learning. A feature graph takes the features as nodes and turns nodes into graphs, resulting in a graph predictor instead of the node predictor. This makes the lifelong learning techniques for CNN applicable to GNN, as the new nodes in a regular graph become individual training samples. Take the node a with label za in the regular graph G as an example, its features xa = [1, 0, 0, 1] are nodes {a1, a2, a3, a4} in feature graph GF a . The feature adja-cency is established via feature cross-correlation between a and its neighbors N (a) = {a, b, c, d, e} to model feature “interaction.”
Lifelong learning often suffers from “catastrophic for-getting” if the models are simply updated with new sam-ples [34]. Although some strategies have been developed to alleviate the forgetting problem for convolutional neural networks (CNN), it is still difficult to directly apply them to graph networks. This is because in the lifelong learning setting, the graph size can increase over time and we have to drop old data to save memory for learning new knowl-edge. However, the existing graph model cannot directly overcome this difficulty. For example, graph convolutional networks (GCN) require the entire graph for training [20].
SAINT [57] requires pre-processing for the entire dataset.
Sampling strategies [7, 13, 57] easily forget old knowledge when learning new knowledge.
Recall that regular CNNs are trained in a mini-batch man-ner where the model can take samples as independent inputs
[23]. Our question is: can we convert a graph task into a traditional CNN-like classification problem, so that (I) nodes can be predicted independently and (II) the lifelong learning techniques developed for CNN can be easily adopted for
GNN? This is not straightforward as node connections can-not be modeled by a regular CNN-like classification model.
To solve this problem, we propose to construct a new graph topology, the feature graph in Figure 1, to bridge GNN to lifelong learning. It takes features as nodes and turns nodes into graphs. This converts node classification to graph clas-sification where the node increments become independent training samples, enabling natural mini-batch training.
The contribution of this paper includes: (1) We introduce a novel graph topology, i.e., feature graph, to convert a prob-lem of growing graph to an increasing number of training samples, which makes existing lifelong learning techniques developed for CNN applicable to GNN. (2) We take the cross-correlation of neighbor features as the feature adja-cency matrix, which explicitly models feature “interaction,” that is crucial for many graph-structured tasks. (3) Feature graph is of constant computational complexity with the in-creased learning tasks. We demonstrate its efficiency and effectiveness by applying it to classical graph datasets. (4)
We also demonstrate its superiority in two applications, i.e., distributed human action recognition based on subgraph clas-sification and feature matching based on edge classification. 2.