Abstract
Convolutional neural networks (CNNs) rely on the depth of the architecture to obtain complex features. It results in computationally expensive models for low-resource IoT de-vices. Convolutional operators are local and restricted in the receptive field, which increases with depth. We explore partial differential equations (PDEs) that offer a global re-ceptive field without the added overhead of maintaining large kernel convolutional filters. We propose a new feature layer, called the Global layer, that enforces PDE constraints on the feature maps, resulting in rich features. These con-straints are solved by embedding iterative schemes in the network. The proposed layer can be embedded in any deep
CNN to transform it into a shallower network. Thus, result-ing in compact and computationally efficient architectures achieving similar performance as the original network. Our experimental evaluation demonstrates that architectures with global layers require 2 − 5× less computational and storage budget without any significant loss in performance. 1.

Introduction
Convolutional neural networks (CNNs) have been the backbone for recent advances in image recognition [19], object detection [33], and other applications [31] interfac-ing the image modalities. Convolutional filters with lim-ited receptive fields act on localized input regions to gener-ate low-level features. Features used for decision-making are complex functions of these low-level features, achieved through the composition of many such convolutional oper-ators applied in sequence, resulting in deep networks with high inference/train time and large model size.
Recent works [3, 5] have explored neural networks in-spired by ordinary differential equations (ODEs), offer-ing richer representation than their discrete counterparts.
Resnets [8] can be viewed as a discretized form of ODEs.
The final architecture based on these continuous layers leads to higher computational cost in comparison to their discrete counterpart [2], namely due to the costly fixed point solvers.
In contrast, we explore novel constraints on the feature maps, based on partial differential equations (PDEs) that offer simi-lar rich representation but with shallower neural networks. In addition, we provide efficient and scalable solvers to provide computational and storage savings.
Proposed Method. We explore a hybrid approach wherein we modify discrete models by embedding a new layer with a global receptive field that operates on the input feature map and computes complex compositions of these low-level features. We call this layer the Global feature layer. It ap-proximately solves a PDE constraint that couples the input and output feature maps. In a typical discrete model, at every input resolution, the same convolutional block is applied repeatedly m times. We modify this structure by keeping only one convolutional block and replacing the m − 1 blocks with a single global feature layer (see Figure 1). Thus, reduc-ing the deep neural network to a much shallower network without any significant performance loss. It leads to smaller models with low computational and storage costs. In addi-tion, it improves both the train and inference times.
By keeping at least one block from the original architec-ture, we are incorporating the signature of this architecture.
It allows the application of this generic global feature layer to any architecture. Also, since a good start for any itera-tive solver implies smaller steps to reach the solution, this original block helps to initialize the PDE solution.
Estimated Savings. Suppose a Resnet architecture con-structed with three resolutions has m residual blocks, and for the three resolutions, the compute cost is = {c1, c2, c3} respectively. The total compute cost for operating this net-work is m × (c1 + c2 + c3). Global residual block re-places m − 1 residual blocks with just one global block and assuming that the cost of this global block is similar, i.e.
= {c1, c2, c3}, then the cost to operate the modified network is 2 × (c1 + c2 + c3). Given that m > 2, the modified net-work can lead to computational savings over Resnet. Similar conclusions can be drawn for storage savings.
Motivational Example. To motivate our approach, we ap-ply the Global feature layer to the Resnet32 [8] architecture, where at each feature map resolution, the same block repeats 5 times. With the experimental setup described in the sec-tion 4, we train three models on the CIFAR-10 dataset: (a)
Figure 1. Replacing repeated blocks in a given CNN architecture with the Global layer for compute and model savings.
Resnet32 : same architecture as used in [8], (b) ODE based
Resnet32, i.e., MDEQ [2] modified to match feature map configuration as in Resnet32, and (c) ResNet32-Global : re-placed repeated blocks with global layers. Table 1 shows that both Resnet32 and MDEQ have similar performance. Note that MDEQ is significantly costly in terms of the floating-point multiply-add operations (MACs). In contrast, the pro-posed Resnet32-Global results in a much smaller model and a significantly lower computational footprint without any hit in the performance. This experiment clearly shows that the
Global layer results in the following benefits: 1. Shallow network. Resnet32-Global has ≈ 3× less depth. 2. Less storage. Resnet32-Global has ≈ 3× less parameters. 3. Less compute. Resnet32-Global uses ≈ 5× less MACs. 4. Readily embedable in any network.
Table 1. CIFAR-10 : Comparison between discrete Resnet32, ODE based Resnet32 (MDEQ [2]), and our PDE embedded Resnet32-Global. We compute the depth as the number of blocks in the network. Train and Inference time denote the cost of processing one pass of the train and test dataset on a V100 GPU. Supplementary
Table 16 lists results for Resnet (m = 2) and CIFAR-100 dataset.
Accuracy #Params #MACs
Resnet32
MDEQ 92.49% 460K 92.28% 1.1M
Resnet32-Global 91.93% 162K 70M 1.5B 15M
Train
Time(s) 78 409 24
Inference
Time(s) 4.45 23.32 1.91
Depth 15
-6
Contributions.
• Proposed a Global feature layer that imposes PDE con-straints on the input and output feature maps. Embedding this layer in deep networks results in their shallower vari-ants with a smaller footprint with similar performance.
• Embedded the proposed global layer in many existing
CNN architectures and conducted an extensive empirical study on benchmark image recognition datasets to show computational and storage savings.
• Proposed an efficient and approximate PDE solver to em-bed in the neural network wherein model accuracy can be traded-off for the computational budget.
• We provide pseudo-code for the Global layer that is readily deployable in any popular deep learning li-brary. Our PyTorch implementation is available at https://github.com/anilkagak2/PDE_GlobalLayer. 2.