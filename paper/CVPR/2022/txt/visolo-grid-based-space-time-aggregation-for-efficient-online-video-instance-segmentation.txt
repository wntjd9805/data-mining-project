Abstract
For online video instance segmentation (VIS), fully uti-lizing the information from previous frames in an efficient manner is essential for real-time applications. Most previ-ous methods follow a two-stage approach requiring addi-tional computations such as RPN and RoIAlign, and do not fully exploit the available information in the video for all subtasks in VIS. In this paper, we propose a novel single-stage framework for online VIS built based on the grid structured feature representation. The grid-based features allow us to employ fully convolutional networks for real-time processing, and also to easily reuse and share fea-tures within different components. We also introduce co-operatively operating modules that aggregate information from available frames, in order to enrich the features for all subtasks in VIS. Our design fully takes advantage of pre-vious information in a grid form for all tasks in VIS in an efficient way, and we achieved the new state-of-the-art ac-curacy (38.6 AP and 36.9 AP) and speed (40.0 FPS) on
YouTube-VIS 2019 and 2021 datasets among online VIS methods. The code is available at https://github. com/SuHoHan95/VISOLO. 1.

Introduction
Video instance segmentation, introduced in [33], extends instance segmentation in the image domain to the video do-main by adding instance tracking. Given a video, all ob-jects in the video need to be located and classified, gener-ating spatio-temporal pixel masks for all objects. VIS is attracting a lot of attention as it is an essential technique for holistic video understanding with various applications such as video editing, autonomous navigation of robots and cars, and augmented reality.
Recently introduced offline methods for VIS solve the problem through mask propagation and transformers [2, 13,
Figure 1. Comparisons of the quality and the speed of previous video instance segmentation methods on the YouTube-VIS 2019 dataset. The orange and the blue markers indicate online and offline methods, respectively. Our framework (VISOLO) is the fastest and the most accurate among online methods, and is ap-proaching the performance of offline methods. 16, 31]. Although these methods show good performance, they cannot be used for real-time applications as it operates in an offline manner requiring the entire video to be pro-cessed before making predictions.
In this paper, we tackle the online video instance seg-mentation problem where video frames are processed se-quentially. While processing videos online is beneficial robot navigation, it is for many VIS applications, e.g. more challenging compared to the offline approaches as the changes in object appearance over time and the occlusions caused by multiple objects should be handled without infor-mation from the future frames.
For online VIS, many algorithms perform frame-level classification and segmentation without fully utilizing the information of previous frames. However, the inconsis-tency of object categories and the errors in object masks de-grade the performance of VIS when tracking instances from frame-level results. For example, MaskTrack R-CNN [33] uses the classification and segmentation results of Mask R-CNN [10], and only uses the previous frame information for tracking. SipMask [4] and SG-Net [20] improve the image-level segmentation performance without using the tempo-ral cues that are available from previous frames. Although
CrossVIS [34] uses temporal information to enhance the instance features for segmentation and tracking during the training, it does not use temporal information during the in-ference and for classification. STMask [14] uses the infor-mation from the previous frame, but it only uses an adjacent frame for segmentation.
In this paper, we propose a framework that exploits infor-mation from previous frames not only for tracking but also for classification and segmentation, which is beneficial for increasing the overall VIS performance. The motivation of our new design is that fully utilizing the available informa-tion from past frames for all subtasks is important, as online
VIS cannot access the whole video like offline VIS.
In addition, real-time processing is important for online
VIS applications. Therefore, it is necessary to come up with a framework that can fully utilize the information from previous frames while running real-time. CompFeat [9] re-cently proposed temporal and spatial attention modules that aggregate temporal features for segmentation and classifi-cation with non-local operation [28]. However, it requires heavy computation as it employs the two-stage framework based on Mask-RCNN and includes additional encoder to obtain key and value features as in STM [24].
To this end, we introduce a novel real-time video in-stance segmentation framework called VISOLO. As the name suggests, our work builds upon recently introduced single-stage image instance segmentation SOLO [29, 30], which divides input image into uniform grids and outputs per-grid semantic category scores and instance masks. Us-ing the grid representation for single-stage VIS has several advantages. It enhances the speed by employing a fully con-volutional network structure and getting rid of intermediate stages like RPN [27] and RoI-Align [10]. It also becomes easier to manage and store features in the grid structure, en-abling the addition of extra modules to share the features for multiple subtasks (classification, segmentation and track-ing), which in turn upgrades the overall VIS performance.
To fully take advantage of the grid-structured represen-tation, we add a memory matching module that computes the similarity between grids of different frames. The com-puted grid similarity is then used for instance matching. By storing the grid-structured feature maps of previous frames in memory, the grid similarity can be computed at any time through the memory matching module. This enables us to gain robustness against occlusions and reappearance.
Furthermore, we propose additional modules called the temporal aggregation and the score reweighting modules, which utilize the information from the previous frames to enhance both the classification and the segmentation per-formance by using the stored feature maps with a marginal overhead. In VISOLO, subtask heads (classification, seg-mentation and tracking) operate interdependently using grid-structured features, so they can share features effec-tively and be optimized as a whole network.
Technical contributions of our work are as follows:
• We propose a novel online video instance segmenta-tion framework built upon the grid structured represen-tation of SOLO [29, 30]. With the grid structure, we can build a single-stage VIS algorithm and avoid com-putation heavy processes like RPN and RoIAlign in two-stage methods, achieving real-time performance.
• We introduce novel modules – memory matching, tem-poral aggregation, and score reweighting modules, all of which take advantage of easily manageable grid structured features. With the memory that stores fea-tures of previous frames, these modules work collabo-ratively to enrich the features for each subtask of VIS, resulting in the overall VIS performance gain.
• We achieve the new state-of-the-art accuracy on the
YouTube-VIS 2019 and 2021 datasets [33] (38.6 AP and 36.9 AP) compared to all other online VIS meth-ods. Our method also runs in real-time (40.0 FPS), which is the fastest among online algorithms (Fig. 1). 2.