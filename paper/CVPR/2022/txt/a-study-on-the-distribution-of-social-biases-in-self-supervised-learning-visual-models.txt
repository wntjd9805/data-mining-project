Abstract
Deep neural networks are efficient at learning the data distribution if it is sufficiently sampled. However, they can be strongly biased by non-relevant factors implicitly incor-porated in the training data. These include operational biases, such as ineffective or uneven data sampling, but also ethical concerns, as the social biases are implicitly present—even inadvertently, in the training data or explic-itly defined in unfair training schedules.
In tasks having impact on human processes, the learning of social biases may produce discriminatory, unethical and untrustworthy consequences. It is often assumed that social biases stem from supervised learning on labelled data, and thus, Self-Supervised Learning (SSL) wrongly appears as an efficient and bias-free solution, as it does not require labelled data.
However, it was recently proven that a popular SSL method also incorporates biases. In this paper, we study the biases of a varied set of SSL visual models, trained using ImageNet data, using a method and dataset designed by psychologi-cal experts to measure social biases. We show that there is a correlation between the type of the SSL model and the num-ber of biases that it incorporates. Furthermore, the results also suggest that this number does not strictly depend on the model’s accuracy and changes throughout the network.
Finally, we conclude that a careful SSL model selection pro-cess can reduce the number of social biases in the deployed model, whilst keeping high performance. The code is avail-able at https://github.com/vpulab/SB-SSL. 1.

Introduction
Supervised Deep Learning models currently constitute the state-of-the-art in the fields of computer vision and natu-ral language processing. However, the recent developments
[17, 24] in the field of Self-Supervised Learning (SSL) —a type of unsupervised learning, are slowly closing the per-formance gap gained via human guidance usually provided in the shape of target labels. SSL methods aim at solving a pre-formulated pretext task—defined by automatically gen-erated labels, whose solution is expected to require high-level understanding of the data in order to learn descriptive feature embeddings with strong transferability potential.
Human social biases are a well-studied and, in some cases, numerically quantifiable phenomenon [23] that causes unjustified prejudices against social groups based, among others, on aspects such as age, gender and race.
Whereas one cannot assign prejudices or preferences to deep learning approaches as these are highly subjective characteristics attributed solely to humans, deep learning methods can wrongly correlate certain concepts if the la-beled training data distribution is biased itself [45]. In prac-tice, this leads to the replication of social biases. Sev-eral cases have been studied and reported, including: an incorrect gender prediction based on the contextual cues (i.e., location - kitchen, office), rather than on visual ev-idence associated with the described person [30], a fewer number of automatic high-paying job recommendations for female candidates than for male ones [20] and a promo-tion of biased suggestions in the dating/political decision-making context [7]. Anticipating these situations, institu-tional initiatives are being developed internationally to ex-tinguish social biases from the training data, as declared in the Ethics Guidelines for a Trustworthy AI issued by the Eu-ropean Commission, and regulate the use of machine learn-ing methods with potential human implications, as stated in numerous US bills [2, 3, 18] and the legislative documents of other countries [1, 4–6].
Previously, it was demonstrated that supervised learn-ing models are prone to implicitly learn biases from the datasets containing them [9, 27, 30], as these human bi-ases are encapsulated in the target labels. For instance, it has been shown that the earlier versions of ImageNet [21] exposed an imbalanced distribution regarding skin colors, ages and genders, leading to the under-representation of cer-tain groups [43]. Furthermore, datasets collecting raw com-ments scraped from the web [10, 40] contain explicit biases against certain social groups [31].
SSL approaches, being unsupervised, are expected to be
unaffected by biases-bearing labels. However, as they re-quire large amounts of training data that often prevents its curation, it is not unlikely that the data itself contains some social human biases. In fact, results for a recent study [37], suggest that two of the state-of-the-art unsupervised learn-ing models also contain association biases learned from the data, only in this case it cannot be explained by the choice of class labels, as the unsupervised models do not leverage this information in the training process. This result indi-cates that at least one top performing SSL model [15] might implicitly learn social biases while training to solve the tar-geted pretext task. Hence, data should be handled carefully, as the neural network’s capacity to avoid inadvertent perpet-uation of undesirable social biases is an important quality to consider, alongside classification accuracy, in the design of deep learning models.
This paper addresses this phenomenon and attempts to answer the questions: what is the origin of the biases in the
SSL setting? What affects the model’s proneness to learn an implicit social bias? What is the relationship between the model’s accuracy and the biases it learns? Whereas a preliminary work addresses the first question and hypothe-sizes on the origins of implicit social biases in a couple of unsupervised models [37], to our knowledge, this is the first attempt to study a wider and more varied set of SSL models.
In particular, the contributions of this paper are:
• We study the association biases acquired by 11 SSL models that share the same ResNet-50 [29] architec-ture, and vary in terms of pretext task and, thus, their accuracy after transfer learning. The results of this study suggest that the nature of the pretext task influ-ences the number and nature of incorporated biases, and that contrastive models are more prone to acquire biased associations that are implicit in the data.
• We also perform an analysis of biases acquired in the embeddings at different layers of the models, show-ing that the number and strength of the biases vary at different model depths. The results of the per-layer analysis suggest that a careful consideration of bias in transfer learning applications can improve the trade-off between bias and accuracy, as the accuracy achieved using embeddings from highly-biased layers is not far from the accuracy achieved by a less-biased embed-dings layer. 2.