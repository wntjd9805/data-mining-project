Abstract
Generating speech-consistent body and gesture move-ments is a long-standing problem in virtual avatar creation.
Previous studies often synthesize pose movement in a holis-tic manner, where poses of all joints are generated simul-taneously. Such a straightforward pipeline fails to gen-erate fine-grained co-speech gestures. One observation is that the hierarchical semantics in speech and the hierar-chical structures of human gestures can be naturally de-scribed into multiple granularities and associated together.
To fully utilize the rich connections between speech audio and human gestures, we propose a novel framework named
Hierarchical Audio-to-Gesture (HA2G) for co-speech ges-ture generation. In HA2G, a Hierarchical Audio Learner extracts audio representations across semantic granular-ities. A Hierarchical Pose Inferer subsequently renders the entire human pose gradually in a hierarchical man-ner. To enhance the quality of synthesized gestures, we de-velop a contrastive learning strategy based on audio-text alignment for better audio representations. Extensive ex-periments and human evaluation demonstrate that the pro-posed method renders realistic co-speech gestures and out-performs previous methods in a clear margin. Project page: https://alvinliu0.github.io/projects/HA2G. 1.

Introduction
When communicating with other people, we spon-taneously make co-speech gestures to help convey our thoughts. Such non-verbal behaviors supplement speech in-formation, making the content clearer and more understand-able to listeners [11, 48, 62]. Psycho-linguistic studies also suggest that virtual avatars with plausible speech gestures are more intimate and trustworthy [60]. Therefore, actuat-ing embodied AI agents such as social robots and digital humans with expressive body movements and gestures is of great importance to facilitating human machine interaction
[55, 56]. To this end, researchers have explored the task of co-speech gesture synthesis [1,2,8,19,25–27,41,53,68,69], which aims at generating a sequence of human gestures given the speech audio and transcripts as input.
Traditionally, the task is tackled through building one-to-one correspondences between speech and unit gesture pairs [12, 13, 32, 47]. Such pipelines require huge human efforts, making them inapplicable to general scenarios of unseen speech. Recent studies leverage deep learning to solve this problem by training a neural network to map a compact representation of audio [1, 25, 26, 41, 53] and text [3, 8, 35, 68, 69] to holistic human pose sequence. How-ever, such a straightforward approach fails to capture the micro-scale motions and cross-modal information, e.g., the subtle finger movements and the rich meanings contained in speech audio. The problem of how to learn the fine-grained cross-modal association remains unsolved.
In order to fully exploit the rich multi-modal semantics, we identify two important observations from a human ges-ture study [48]: 1) Different types of co-speech gestures are related to distinct levels of audio information. For example, the metaphorical gestures are strongly associated with the high-level speech semantics (e.g., when depicting a ravine, one would moving two outstretched hands apart and saying
“gap”), while the low-level audio features of beat and vol-ume lead to the rhythmic gestures. 2) The dynamic patterns of different human body parts in co-speech gestures are not the same, such as the flexible fingers and relatively still up-per arms. Thus it is improper to generate the upper body pose as a whole like previous studies [1–3,25,26,41,53,68].
Inspired by the discussions above, we develop the Hier-archical Audio-to-Gesture (HA2G) pipeline, which gener-ates diverse co-speech gestures. Our key insight is to build hierarchical cross-modal associations across multiple lev-els between tri-modal information and generate gestures in a coarse-to-fine manner. Specifically, two modules are de-vised, namely the Hierarchical Audio Learner, and the Hi-erarchical Pose Inferer. In the Hierarchical Audio Learner, we argue that features extracted from different levels of the audio backbone capture different meanings. Additionally, text information can further strengthen the audio embed-ding through contrastive learning for more discriminative representations. Afterwards, based on the hypothesis that different levels of audio information contribute to different body joint movements, we associate the multi-level audio features with the hierarchical structure of human body in the Hierarchical Pose Inferer.
In particular, the associa-tion is achieved in correlation with speaking styles encoded from speaker appearances. The hierarchy of human upper limb is predicted in a coarse-to-fine manner from shoul-ders to fingers like a tree structure by cascading multiple
In addition, we propose bi-directional GRU generators. a novel physical regularization to enhance the realness of generated poses. Experiments demonstrate that our method synthesizes realistic and smooth co-speech gestures.
To summarize, our main contributions are three-fold: (1)
We propose the Hierarchical Audio Learner to extract hier-archical audio features and render discriminative represen-tations through contrastive learning. (2) We propose the Hi-erarchical Pose Inferer to learn associations between multi-level features and human body parts. Human poses are thus generated in a cascaded manner. (3) Extensive experiments show that HA2G can generate fine-grained co-speech ges-tures, which outperform state-of-the-art methods on both objective evaluations and subjective human studies. 2.