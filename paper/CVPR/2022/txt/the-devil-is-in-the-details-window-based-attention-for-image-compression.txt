Abstract
Learned image compression methods have exhibited su-perior rate-distortion performance than classical image compression standards. Most existing learned image com-pression models are based on Convolutional Neural Net-works (CNNs). Despite great contributions, a main draw-back of CNN based model is that its structure is not de-signed for capturing local redundancy, especially the non-repetitive textures, which severely affects the reconstruction quality. Therefore, how to make full use of both global structure and local texture becomes the core problem for learning-based image compression. Inspired by recent pro-gresses of Vision Transformer (ViT) and Swin Transformer, we found that combining the local-aware attention mech-anism with the global-related feature learning could meet the expectation in image compression.
In this paper, we first extensively study the effects of multiple kinds of atten-tion mechanisms for local features learning, then introduce a more straightforward yet effective window-based local at-tention block. The proposed window-based attention is very flexible which could work as a plug-and-play component to enhance CNN and Transformer models. Moreover, we pro-pose a novel Symmetrical TransFormer (STF) framework with absolute transformer blocks in the down-sampling en-coder and up-sampling decoder. Extensive experimental evaluations have shown that the proposed method is ef-fective and outperforms the state-of-the-art methods. The code is publicly available at https://github.com/
Googolxx/STF. 1.

Introduction
Image compression is a fundamental and long-standing research topic in image processing field. With the ever in-creasing visual applications, lossy image compression is a
* Corresponding Author
Figure 1. Comparison of image reconstruction by different meth-ods, including CNN + window-attention (CNN-att), Symmetrical
TransFormer + window-attention (STF-att), and previous SOTA
CNN model (MBT). The upper shows results optimized for MSE, while the lower is optimized for MS-SSIM. It is obvious that the proposed window-based attention is effective for both supervi-sion, especially with the STF model, showing that the proposed window-based attention model could internally allocate more bits on high contrast areas and achieve better visual quality. The met-rics are [bpp↓/PNSR↑/MS-SSIM↑]. vital technique for storing images and videos efficiently in limited hardware resources. Classical lossy image compres-sion standards including JPEG [44], JPEG2000 [40], BPG
[8], and VVC [9] follow a similar coding scheme: trans-forming, quantization, and entropy coding. However, these image compression standards rely heavily on the hand-crafted rules, which means they are not expected to be of the ultimate solution for image compression.
In recent years, the learned image compression based on variational auto-encoder (VAE) [24] has achieved bet-ter rate-distortion [41] performance than conventional lossy image compression methods on metrics of Signal-to-Noise
Ratio (PSNR) and Multi-Scale-Structural Similarity Index
Measure (MS-SSIM) [46], showing great potential for prac-tical compression use. Here, we would briefly introduce the general pipeline [5] of the VAE-based methods. For en-coding, the VAE-based image compression methods use a linear and nonlinear parametric analysis transform to map the images to a latent code space. After quantization, en-tropy estimation modules predict the distributions of latents, then the lossless Context-based Adaptive Binary Arithmetic
Coding (CABAC) [30] or Range Coder [31] compresses the latents into the bit stream. Meanwhile, hyper-prior [6], auto-regressive [34] priors and Gaussian Mixture Model (GMM) [15] allow the entropy estimation modules to more precisely predict distributions of latents, and achieve bet-ter rate-distortion (RD) performance. For decoding, loss-less CABAC or Range Coder decompresses the bit stream, then the decompressed latents are mapped to reconstructed images by a linear and nonlinear parametric synthesis trans-form. Further, there are also some works [21,27,48] design-ing post-processing networks for better quality of recon-struction. Combining above sequential units, those mod-els could be trained end-to-end. Although great progresses have been achieved, one core problem of above CNN-based model is that the original convolutional layer is designed for the high-level global feature distillation, rather than the low-level local detail restoration. As shown in the right side of Fig.1, even the SOTA CNN model is still affected by the weak local detail learning ability which would inevitably limit further performance improvement.
Inspired by the success of attention mechanism in nat-ural language processing (NLP) and computer vision tasks such as the image classification and semantic segmentation, many researchers apply the non-local attention mechanism to guide the adaptive processing of latent features, which could help the compression algorithm allocate more bits to challenging areas (i.e., edges, textures) for better RD perfor-mance. However, such non-local attentions still not change the intrinsic global-aware character of the CNN structure.
Recent studies [13, 18, 29, 42] have demonstrated that trans-former [43] can be successfully applied to vision tasks with competitive even better performance compared with con-volutional neural networks (CNNs). Those attention-based networks, such as Vision Transformer [18] and Swin Trans-former [29] take the advantages of attention mechanism to capture global dependency. However, we intuitively find that the global semantic information in image compression is not as effective as in other computer vision tasks. Instead, spatially neighboring elements have stronger correlation.
Following above discussions, this paper explores to ad-dress the detail missing problem from two aspects, i.e., studying the local-aware attention mechanism and intro-ducing a novel transformer-based framework. First, we comprehensively study how to combine the neural networks with the attention mechanism for designing local-aware lossy image compression architectures. Through conduct-ing a set of comparative experiments based on a global at-tention mechanism and a local attention mechanism, we have verified our aforementioned guess, that the local at-tention is more suitable for the local texture reconstruction.
Afterward, we present a flexible attention module combined with neural networks to capture correlations among spa-tially neighboring elements, namely window-attention. As shown in Fig.1, the proposed attention module could work as a plug-and-play component to enhance CNN and Trans-former models. Second, although the transformer-based models have achieved great success in a variety of computer vision tasks, there are still great challenges applying trans-former model in image compression, e.g., no up-sampling units, fixed attention model. To this end, we propose a novel Symmetrical TransFormer (STF) framework with ab-solute transformer blocks in down-sampling encoder and up-sampling decoder, which may be the first exploration of designing up-sampling transformer, especially for the im-age compression task. Extensive experimental results show that our methods are superior to the state-of-the-art (SOTA) image compression methods in essential metrics. The main contributions of this paper are summarized as follows:
• We extensively study the local-aware attention mech-anism, and find that it is crucial to combine the global structure learned by neural networks and the local tex-ture mined by the attention units.
• We present a flexible window-based attention module to capture correlations among spatially neighboring el-ements, which could work as a plug-and-play compo-nent to enhance CNN or Transformer models.
• We design a novel Symmetrical TransFormer (STF) framework with absolute transformer blocks in both down-sampling encoder and up-sampling decoder.
• Extensive experimental evaluations have shown that the proposed methods are effective and outperform the
SOTA image compression methods. 2.