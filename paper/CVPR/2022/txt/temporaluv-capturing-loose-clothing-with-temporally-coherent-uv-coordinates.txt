Abstract
We propose a novel approach to generate temporally co-herent UV coordinates for loose clothing. Our method is not constrained by human body outlines and can capture loose garments and hair. We implemented a differentiable pipeline to learn UV mapping between a sequence of RGB inputs and textures via UV coordinates. Instead of treating the UV coordinates of each frame separately, our data gen-eration approach connects all UV coordinates via feature matching for temporal stability. Subsequently, a generative model is trained to balance the spatial quality and tempo-ral stability.
It is driven by supervised and unsupervised losses in both UV and image spaces. Our experiments show that the trained models output high-quality UV coordinates and generalize to new poses. Once a sequence of UV co-ordinates has been inferred by our model, it can be used to flexibly synthesize new looks and modified visual styles.
Compared to existing methods, our approach reduces the computational workload to animate new outfits by several orders of magnitude. 1.

Introduction
In image or video generation tasks [37, 44] that involve people, it is crucial to obtain accurate representations of the 3D human shape and appearance to efficiently generate modified content. In this context, UV coordinates are a pop-ular 2D representation that establish dense correspondences between 2D images and 3D surface-based representations of the human body. UV coordinates go beyond skeleton landmarks to encode human pose and shape, and are widely used in image/video editing, augmented reality, and human-computer interaction [12, 14, 15]. In this paper, we tackle video generation of people, with a focus on efficiency and capturing loose clothing. Unlike previous works [33,40,42] which use large networks to capture motion and appearance, we train a model to generate temporally coherent UV coor-dinates. We use a single, fixed texture to store appearance information so that our model can solely focus on learning
UV dynamics.
Human body UV coordinates can be derived indirectly from estimates of 3D shape models [6, 19, 26, 29] like
SMPL [23]. Alternatively, direct estimation methods like
DensePose [2] and UltraPose [43] bypass intermediate 3D models to directly output UV coordinates from a single
RGB image. The convenience of direct methods has led to DensePose being widely used in animation and editing applications [25, 27, 28, 49]. Nevertheless, the UV coor-dinates obtained from SMPL and DensePose approximate only human body silhouettes in tight clothing. They do not capture loose clothing, such as long skirts or wide pants (see comparisons in Figure 6 and 7). In addition, the meth-ods for UV estimation work only on individual images. For video inputs, they are applied frame-by-frame [32,47] with-out considering the temporal relationship between frames.
As such, the UV coordinates are inconsistent over time, so any re-targeted sequences will shift and jitter.
In this paper, we focus on improving the spatial coverage and temporal coherence of UV coordinates generated from a sequence of 2D images. We target the ability to retain the full body plus clothing silhouette for arbitrary styles of clothing. Our approach is agnostic to the UV source, which we demonstrate via inputs from both DensePose [2] and
SMPL model estimates [19]. For temporal coherence, we aim at achieving the point-to-point correspondences among different frames via UV coordinate maps, so that video se-quences can be generated with one fixed texture.
A core challenge of learning a model for extended and temporally coherent UV coordinates lies in the lack of data for direct supervision. Hence, we propose a novel learning scheme that combines both supervised and unsupervised components. We first pre-process a sequence of UV coor-dinates obtained from DensePose or SMPL via spatial ex-tension and temporal stabilization to obtain training data for an initial training stage. We then shift the learning gradu-ally from supervised, with the pre-processed data, to unsu-Figure 1. a) Our method generates temporally coherent UV coordinates that capture loose clothing from off-the-shelf human pose UV estimates such as SMPL and DensePose [2, 23]. b) Generated UV coordinates allow us to recover entire sequences from a constant texture map. c) Virtual try-on and modifications of the look can be easily achieved with minimal computation via a simple lookup. pervised, driven by a differentiable UV mapping pipeline between the texture and image space.
Our results demonstrate that using loss terms formulated in both UV and image space are crucial for generating high-quality UV coordinates with temporal coherence. As our generator does not take RGB images as input, the UV co-ordinates generated from our trained model can be directly paired with different texture maps to generate virtual try-on videos with a very simple lookup step. This is device-independent and orders of magnitude more efficient than other methods, which generate video outputs by evaluating neural networks. To summarize, our main contributions are
• a model-agnostic method to extend UV coordinates to capture the complete appearance of the human body,
• an approach to train neural networks that generate completed and temporally coherent UV coordinates without the need for ground truth, and
• a highly efficient way to generate virtual try-on videos with arbitrary clothing styles and textures. 2.