Abstract
Multimodal learning helps to comprehensively under-stand the world, by integrating different senses. Accord-ingly, multiple input modalities are expected to boost model performance, but we actually find that they are not fully exploited even when the multimodal model outperforms its uni-modal counterpart. Specifically, in this paper we point out that existing multimodal discriminative models, in which uniform objective is designed for all modalities, could remain under-optimized uni-modal representations, caused by another dominated modality in some scenar-ios, e.g., sound in blowing wind event, vision in drawing picture event, etc. To alleviate this optimization imbal-ance, we propose on-the-fly gradient modulation to adap-tively control the optimization of each modality, via mon-itoring the discrepancy of their contribution towards the learning objective. Further, an extra Gaussian noise that changes dynamically is introduced to avoid possible gen-eralization drop caused by gradient modulation. As a re-sult, we achieve considerable improvement over common fusion methods on different multimodal tasks, and this sim-ple strategy can also boost existing multimodal methods, which illustrates its efficacy and versatility. The source code is available at https://github.com/GeWu-Lab/OGM-GE_CVPR2022. 1.

Introduction
People perceive the world by collaboratively utilizing multiple senses: eyes to look, ears to listen, and hands to touch. Such a multimodal way can provide more com-prehensive information from different aspects. Inspired by the multi-sensory integration ability of humans [11], mul-timodal data, collected from different sensors, tend to be more considered in machine learning. In recent years, mul-†Equal contribution. *Corresponding author. timodal learning has exhibited a clear advantage in improv-ing the performance of previous uni-modal tasks as well as addressing new challenging problems, such as action recog-nition [10, 23, 29, 37], audio-visual speech recognition [33], and visual question answering [2, 19, 41].
Multimodal data usually provides more views compared with uni-modal one, accordingly learning with multimodal data should match or outperform the uni-modal case. How-ever, according to the recent study [39], multimodal models that optimize the uniform learning objective for all modal-ities with joint training strategy can be inferior to the uni-modal model in some situations. Such a phenomenon vio-lates the intention of improving model performance through integrating information from multiple modalities. Previ-ous researchers claimed that various modalities tend to con-verge at different rates, leading to uncoordinated conver-gence problem [20, 35, 39]. To cope with this problem, some methods aid the training of multimodal models with the help of additional uni-modal classifiers or pre-trained models [8,39]. Hence, they inevitably bring extra efforts on training additional neural modules.
However, we further find that even when the multimodal models outperform the uni-modal ones, they still cannot fully exploit the potential of multiple modalities. As shown in Figure 1, the joint multimodal model achieves the best event classification performance on VGGSound [7], but the performance of visual and audio modality within it is clearly worse than that in the visual-only and audio-only model, respectively1. This interesting observation suggests under-optimized representation in both modalities. We consider the reason could be that, in some multimodal scenarios, the dominated modality [31] with better performance (e.g., sound of wind blowing, vision of playing football, etc.) will suppress the optimization of the other one. Moreover, as il-1Here, the visual-only and the audio-only are the models that trained with a single modality. To evaluate the uni-modal encoders of the multi-modal model, we observe the performance through fixing the joint-trained uni-modal encoder and finetuning a uni-modal classifier only.
(a) (b) (c)
Figure 1. Performance of the uni-modal models, joint-trained multimodal model, and multimodal model with our proposed OGM-GE on the validation set of the VGGSound [7] dataset. (a) Performance of audio modality. (b) Performance of visual modality. (c) Performance of the multimodal model. Best viewed in color. lustrated in Figure 1(a) and (b), there is another noticeable observation that accuracy drops more remarkably in visual modality compared with audio case, which is consistent with the fact that VGGSound, as a curated sound-oriented dataset, prefers audio modality, even the sound sources are guaranteed to be visible. Generally speaking, such dataset preferences will lead to one modality is usually dominant, resulting in this phenomenon of optimization imbalance.
Aiming to solve the problem above, we first analyze the imbalance phenomenon from the optimization perspective, and find that the modality with better performance con-tributes to lower joint discriminative loss then dominates the optimization progress via propagating limited gradient over the other modality, thus leading to the under-optimized situation. Then, to ease the situation, we propose to control the optimization process of each modality via the On-the-fly Gradient Modulation (OGM) strategy. Specifically, the contribution discrepancy between different modalities to the learning objective is dynamically monitored during train-ing progress, which is then exploited to adaptively modulate the gradients, offering more efforts on the under-optimized modality. However, the modulated gradient may lower the intensity of the stochastic gradient noise, which has been proven to have a positive correlation with generalization ability [21]. Hence, we further introduce extra Gaussian noise that changes dynamically to achieve Generalization
Enhancement (GE). After applying our method of OGM-GE on the multimodal learning task of VGGSound in Fig-ure 1, we obtain the consistent performance boost for under-optimized uni-modal representation, i.e., the blue curves in
Figure 1(a) and (b). More than that, the visual modality gains more improvement. As a result, our method is no-ticeably superior to the conventional one in the multimodal learning setting, as shown in Figure 1(c). To comprehen-sively demonstrate the effectiveness of OGM-GE, we test it in various multimodal tasks on different datasets, which brings consistent improvements, working with both vanilla fusion strategies and existing multimodal methods.
To summarize, our contributions are as follows:
• We find the optimization imbalance phenomenon that the performance of the joint multimodal model is lim-ited due to the under-optimized representations, and then analyze it from the optimization perspective.
• The OGM-GE method is proposed to solve the opti-mization imbalance problem by controlling the opti-mization process of each modality dynamically as well as enhancing generalization ability.
• The proposed OGM-GE can be plugged in not only vanilla fusion strategies but also existing multimodal frameworks and brings consistent improvement, indi-cating its promising versatility. 2.