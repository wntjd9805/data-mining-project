Abstract
Domain generalization (DG) aims to help models trained on a set of source domains generalize better on unseen tar-get domains. The performances of current DG methods largely rely on sufficient labeled data, which are usually costly or unavailable, however. Since unlabeled data are far more accessible, we seek to explore how unsupervised learning can help deep models generalize across domains.
Specifically, we study a novel generalization problem called unsupervised domain generalization (UDG), which aims to learn generalizable models with unlabeled data and analyze the effects of pre-training on DG. In UDG, models are pre-trained with unlabeled data from various source domains before being trained on labeled source data and eventu-ally tested on unseen target domains. Then we propose a method named Domain-Aware Representation LearnING (DARLING) to cope with the significant and misleading heterogeneity within unlabeled pretraining data and severe distribution shifts between source and target data. Surpris-ingly we observe that DARLING can not only counterbal-ance the scarcity of labeled data but also further strengthen the generalization ability of models when the labeled data are insufficient. As a pretraining approach, DARLING shows superior or comparable performance compared with
ImageNet pretraining protocol even when the available data are unlabeled and of a vastly smaller amount compared to
ImageNet, which may shed light on improving generaliza-tion with large-scale unlabeled data. 1.

Introduction
Deep neural network based approaches have achieved striking performance in tasks where training and test data share similar distribution [23, 24]. However, under con-siderable distribution shifts, they can significantly fail [4, 15, 27, 49, 56]. To address this problem, the literature in
†Equal contribution, *Corresponding author domain generalization (DG) proposes algorithms that have access to labeled data from multiple domains or environ-ments during training and generalize well to unseen test do-mains [18, 32, 36, 37, 43, 63].
Sufficient labeled data are crucial for current DG meth-ods to learn domain invariant features, which are proved to be generalizable to unseen domains [1, 43, 54, 64]. A com-mon and effective approach to learning discriminative fea-tures in DG is to enlarge the available data space with aug-mentations of source domains [5, 69, 70]. With sufficient source data and strong augmentations, even empirical risk minimization (ERM) can outperform previous state-of-the-art methods [21]. Nevertheless, both augmentation-based methods and carefully hyperparameter tuned ERM assume adequate labeled data from multiple domains available for representation learning.
As manually labeled data can be costly or unavailable while unlabeled data are far more accessible, we study a novel generalization problem called unsupervised domain generalization (UDG). UDG aims to unsupervised learn dis-criminative representations that generalize well across do-mains and thus reduce the dependence of DG methods on labeled data. Specifically, models are trained with unla-beled heterogeneous data before finetuned and evaluated on labeled data, so that methods for UDG can be easily assem-bled with current DG methods as pretraining and study how pre-training affects models’ generalization ability.
In the field of unsupervised learning [22, 50, 65], con-trastive learning (CL) advances in discriminative represen-tation learning for downstream tasks compared to its coun-terparts [6, 23, 57]. Actually, the objective of CL, which is to maximize the similarity between a given image and its variant under disturbance while contrasting with nega-tives [16, 34, 66], agrees with the target of DG. However, current CL only learns robust representations against pre-defined perturbation under independent and identically dis-tributed (I.I.D) hypothesis [3, 26, 28] and fails to consider severe distribution shifts across domains beyond predefined perturbation types [45, 67]. With samples from various do-mains as negative pairs, current CL methods leverage both
domain-related (i.e., features irrelevant to categories) and category-discriminative features to push their representa-tions away. Furthermore, in UDG, the distribution shifts across domains in training data are significant and can not be fully counterweighed via data transformations (for in-stance, one can hardly transform a dog in sketch to photo).
The strong heterogeneity induces models to leverage the domain-related features to distinguish one sample from its negatives [2, 52] and thus, hinders the learning of an invari-ant representation space where dissimilarity across domains is minimized [41,43]. Thus current contrastive learning can not perfectly handle the UDG problem.
To address this problem, we propose Domain-Aware
Representation LearnING (DARLING), a novel contrastive learning algorithm for UDG which unifies objectives of
DG and contrastive learning. To force the model to ignore domain-related features, we select valid sources of nega-tive samples for any given queue according to the similarity between different domains. Specifically, the more similar two domains are, the more likely two samples in a negative pair are selected from these two domains, respectively. Intu-itively, consider samples from two domains with enormous differences in distribution, the domain-related features of which are discriminative enough to distinguish them from each other and, in turn, boost variance across domains in the representation space. On the contrary, if a negative pair of samples comes from a single domain and shares identical domain-related features, domain-irrelevant representations are learned to contrast them.
As shown in Section 4, the proposed unsupervised pre-training protocol achieves a significant improvement in gen-eralization even with raw ERM finetuning, indicating that the UDG problem gives an effective and enlightening com-plementary to supervised methods for DG. We further show that DARLING outperforms state-of-the-art counterparts by a considerable margin with quantitative and qualitative ex-periments. Moreover, prepositive unsupervised learning can be considered as a protocol of pretraining. Although initialization of weights pretrained on ImageNet shows un-paralleled effectiveness on independent and identically dis-tributed (I.I.D.) tasks, we argue that it lacks rationality for the DG problem. Since ImageNet can be considered as a set of data sampled from latent domains [55, 68], the distri-bution shifts across domains are not as significant as most
DG datasets [25,36,46], resulting in insufficient heterogene-ity for models to learn a generalizable representation space.
Thus the protocol of unsupervised pretraining on heteroge-neous unlabeled data is a reasonable alternative to initial-ization with weights pretrained on ImageNet for DG. 2.