Abstract
In real-world applications, it is important and desirable to learn a model that performs well on out-of-distribution (OOD) data. Recently, causality has become a power-ful tool to tackle the OOD generalization problem, with the idea resting on the causal mechanism that is invari-ant across domains of interest. To leverage the generally unknown causal mechanism, existing works assume a lin-ear form of causal feature or require sufﬁciently many and diverse training domains, which are usually restrictive in practice. In this work, we obviate these assumptions and tackle the OOD problem without explicitly recovering the causal feature. Our approach is based on transformations that modify the non-causal feature but leave the causal part unchanged, which can be either obtained from prior knowl-edge or learned from the training data in the multi-domain scenario. Under the setting of invariant causal mecha-nism, we theoretically show that if all such transformations are available, then we can learn a minimax optimal model across the domains using only single domain data. Noticing that knowing a complete set of these causal invariant trans-formations may be impractical, we further show that it suf-ﬁces to know only a subset of these transformations. Based on the theoretical ﬁndings, a regularized training procedure is proposed to improve the OOD generalization capability.
Extensive experimental results on both synthetic and real datasets verify the effectiveness of the proposed algorithm, even with only a few causal invariant transformations. 1.

Introduction
The success of many machine learning algorithms with empirical risk minimization (ERM) relies on the indepen-dent and identically distributed (i.i.d.) hypothesis that train-ing and test data originate from a common distribution. In
∗Equal contribution. Work was done during an internship at Huawei
Noah’s Ark Lab.
†Corresponding author. practice, however, data in different domains or environ-ments are often heterogeneous, due to changing circum-stances, selection bias, and time-shifts in the distributions
[47, 57]. Accessing data from all the domains of interest, on the other hand, is expensive or even impossible. Con-sequently, the problem of learning a model that generalizes well on the unseen target distributions is a practically impor-tant but also challenging task and has gained much research attention in the past decades [6, 7, 17, 49, 68].
Since data from some domains are unavailable, assump-tions or prior knowledge on the unseen domains are gen-erally required to achieve a guaranteed out-of-distribution (OOD) generalization performance. Recently, causality has become a powerful tool to tackle the OOD problem
[2, 51, 55, 57]. This is based on the assumption or obser-vation that the underlying causal mechanism is invariant in general, even though the data distributions may vary with domains. It has been shown that a model would perform well across different domains in the minimax sense if such a causal mechanism is indeed captured.
To capture the invariant causal mechanism, existing works have assumed a particular form of the causal dia-gram [25,46,48,57,67], which may be restrictive in practice and is untestable from the observed data. Other works try to recover the so-called “causal feature” from the data to im-prove the OOD generalization performance [10, 22, 44, 55].
These works usually assume a linear form of causal fea-ture [10, 22, 44, 57] or that there are sufﬁciently many and diverse training domains so that the causal feature could be identiﬁed via certain invariant properties [2, 50, 55]. In the absence of these assumptions, existing methods such as invariant risk minimization [2] can fail to capture the in-variance or recover the causal feature even in simple exam-ples [30]. In real applications like image classiﬁcation, the linearity assumption may not hold, and it may be expensive or even impossible to ensure that the available domains are indeed sufﬁcient. As such, the identiﬁability issue of causal feature can hardly be resolved in practice.
In this paper, we obviate the aforementioned assump-tions and propose a new approach to learn a robust model for OOD generalization under the invariant causal mecha-nism assumption. We do not try to explicitly recover the causal feature; rather, we directly learn a model that takes advantage of the invariant properties. Our approach is based on the observation that though the explicit functional form of the causal feature is generally unknown and maybe also hard to learn, we often have some prior knowledge on the transformations that the causal feature is invariant to, i.e., transformations that modify the input data but do not change their causal feature. For example, the shape of digit in an image from MNIST dataset [38] can be treated as a causal feature when predicting the digit, while ﬂipping or rotation does not change causal meanings. A detailed discussion on this issue is given in Section 3.5. We refer to these transfor-mations as causal invariant transformations (CITs).
Theoretically, we prove that given complete prior knowl-edge of CITs, it is feasible to learn a model with OOD generalization capability using only single domain data.
Speciﬁcally, we show that if all the CITs are known, then minimizing the loss over all the causally invariant trans-formed data, which are obtained by applying the CITs to data from the given single domain, would lead to the de-sired model that achieves a minimax optimality across all the domains of interest. Noticing that obtaining all CITs may be impractical, we further show that, for the purpose of
OOD generalization, it sufﬁces to know only an appropriate subset of CITs, referred to as causal essential set and is for-mally deﬁned in Deﬁnition 2. The learned model is then shown to generalize to different domains if it is invariant to transformations in a causal essential set. This is different from existing works [59, 65] that demonstrate an improved i.i.d. generalization capability from invariance properties.
Following these theoretical results, we propose to reg-ularize training with the discrepancy between the model outputs of the original data and their transformed versions from the CITs in the causal essential set, to enhance OOD generalization. The CITs can be viewed as data augmenta-tion operations; in this sense, our theoretical results reveal the rationale behind data augmentation in OOD problems.
Experiments on both synthetic and real-world benchmark datasets, including PACS [40] and VLCS [17], verify our theoretical ﬁndings and demonstrate the effectiveness of the proposed algorithm in terms of OOD performance. Notice-ably, in some experiments, we use CycleGAN to learn the transformations between different environments, which are then used as our CITs. This is in contrast with [78] which conjectured that source-to-source transformation could pro-vide little help to domain generation tasks in their approach. 2.