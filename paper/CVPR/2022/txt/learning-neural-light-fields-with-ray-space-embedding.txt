Abstract 1.

Introduction
Neural radiance ﬁelds (NeRFs) produce state-of-the-art view synthesis results, but are slow to render, requiring hundreds of network evaluations per pixel to approximate a volume rendering integral. Baking NeRFs into explicit data struc-tures enables efﬁcient rendering, but results in large memory footprints and, in some cases, quality reduction. Additionally, volumetric representations for view synthesis often struggle to represent challenging view dependent effects such as dis-torted reﬂections and refractions. We present a novel neural light ﬁeld representation that, in contrast to prior work, is fast, memory efﬁcient, and excels at modeling complicated view dependence. Our method supports rendering with a single network evaluation per pixel for small baseline light
ﬁelds and with only a few evaluations per pixel for light
ﬁelds with larger baselines. At the core of our approach is a ray-space embedding network that maps 4D ray-space into an intermediate, interpolable latent space. Our method achieves state-of-the-art quality on dense forward-facing datasets such as the Stanford Light Field dataset. In addition, for forward-facing scenes with sparser inputs we achieve results that are competitive with NeRF-based approaches while providing a better speed/quality/memory trade-off with far fewer network evaluations.
∗ This work was done while Benjamin was an intern at Meta.
View synthesis is an important problem in computer vision and graphics. Its goal is to photorealistically render a scene from unobserved camera poses, given a few posed input im-ages. Existing approaches solve this problem by optimizing some underlying representation of the scene’s appearance and geometry and then rendering this representation from novel views.
View synthesis has recently experienced a renaissance with an explosion of interest in neural scene representations
— see Tewari et al. [52,53] for a snapshot of the ﬁeld. Neural radiance ﬁelds [30] are perhaps the most popular of these neural representations, and methods utilizing them have re-cently set the state-of-the-art in rendering quality for view synthesis. A radiance ﬁeld is a 5D function that maps a 3D point x and 3D direction (cid:126)ω (with only 2 degrees of freedom) to the radiance leaving x in direction (cid:126)ω, as well as the density of the volume at point x. A neural radiance ﬁeld or NeRF represents this function with a neural network. Because vol-ume rendering a NeRF is differentiable, it is straightforward to optimize by minimizing the difference between ground truth views at known camera poses and their reconstructions.
The main drawback of neural radiance ﬁelds is that vol-ume rendering requires many samples and thus many neural network evaluations per ray to approximate a volume render-ing accurately. Thus, rendering from a NeRF is usually quite slow. Various approaches exist for baking or caching neural
radiance ﬁelds into explicit data structures to improve efﬁ-ciency [11, 15, 39, 66]. Some approaches, concurrent to this work, learn color and density directly on a voxel grid, which improves both training and rendering speed [31,50,65]. How-ever, the storage cost for explicit representations is much higher than a NeRF. Further, the baking procedure itself sometimes leads to a loss in resulting view synthesis quality for baking methods. Other methods aim to reduce the num-ber of neural network evaluations per ray by representing radiance only on surfaces [17, 32]. These methods predict new images with only a few evaluations per ray. However, their quality is contingent on either ground truth geome-try or reasonable geometry estimates, which are not always available.
A light ﬁeld [12, 20] is the integral of a radiance ﬁeld. It maps ray parameters directly to the integrated radiance along that ray. Thus, only one look-up of the underlying represen-tation is required to determine the color of a ray; hence one evaluation per pixel, unlike hundreds of evaluations required by a NeRF. A common assumption for light ﬁelds is that this integral remains the same no matter the ray origin (i.e., radi-ance is constant along rays), which holds when the convex hull of the scene geometry does not contain any viewpoints used for rendering [20]. Given this assumption, a light ﬁeld is a function of a ray on a 4D ray space.
In this paper, we show how to learn neural light ﬁelds.
Since coordinate-based neural representations have been successfully employed to learn radiance ﬁelds from a set of ground truth images, one might think that they could be useful for representing and learning light ﬁelds as well.
However, we show that learning light ﬁelds is signiﬁcantly more challenging than learning radiance ﬁelds. Using the same neural network architecture as in NeRF to parameter-ize a light ﬁeld leads to poor interpolation quality for view synthesis.
While a radiance ﬁeld is a 5D function, an essential ingre-dient of NeRF is that it learns the scene geometry as a density
ﬁeld in 3D space. Additionally, a NeRF’s learned appear-ance is closer to a 3D function than a 5D function since the network is late-conditioned on viewing directions [67]. This makes NeRFs easy to optimize but also means that they can struggle to represent complex view-dependent effects such as reﬂections and refractions [59] which violate multi-view color constraints.
On the other hand, we face the problem of learning a func-tion deﬁned on a 4D ray space from only partial observations
— input training images only cover a few 2D slices of the full 4D space. At the same time, NeRF enjoys multiple observa-tions of most 3D points. Further, light ﬁelds do not entail any form of scene geometry, which allows them to capture complex view dependence but poses signiﬁcant challenges in interpolating unseen rays in a geometrically meaningful way. Existing methods address these issues by sacriﬁcing view interpolation [8], leveraging data driven priors [47], or relying on strong supervision signals [22].
In order to deal with these challenges, we employ a novel ray-space embedding network that re-maps the input ray-space into an embedded latent space. This facilitates both the registration of rays observing same 3D points and the interpolation of unobserved rays, which leads to better mem-orization and view synthesis at the same time. The embed-ding network alone already provides state-of-the-art view synthesis quality for densely sampled inputs (such as the
Stanford light ﬁelds [58]). However, it does not interpolate well in sparser input sequences (such as those from Real
Forward-Facing [30]). Thus, we represent such scenes with a set of local light ﬁelds, where each local light ﬁeld is less prone to large depth and texture changes. Each local light
ﬁeld has to learn a simpler embedding at the price of several more network evaluations per ray.
We evaluate our method for learning neural light ﬁelds in sparse and dense regimes, with and without subdivisions.
We compare to state-of-the-art view-synthesis methods and show that our approach achieves comparable or better view synthesis quality in both regimes, in a fraction of render time that other methods require, while still maintaining their small memory footprint (Figure 1).
In summary, our contributions are: 1. A novel neural light ﬁeld representation that employs a ray-space embedding network and achieves state-of-the-art quality for small-baseline view synthesis without any geometric constraints. 2. A subdivided neural light ﬁeld representation for large baseline light ﬁelds that leads to a good trade-off in terms of the number of network queries vs. quality, which can be optimized to achieve comparable performance to NeRF [30] and NeX [59] for sparse real-world scenes. 3. Improved capture of view-dependent appearance in both sparse and dense regimes (e.g., complicated reﬂections and refractions) that existing volume-based methods such as NeRF [30] and NeX [59] struggle to represent. 2.