Abstract
Human-annotated attributes serve as powerful semantic embeddings in zero-shot learning. However, their annotation process is labor-intensive and needs expert supervision. Cur-rent unsupervised semantic embeddings, i.e., word embed-dings, enable knowledge transfer between classes. However, word embeddings do not always reflect visual similarities and result in inferior zero-shot performance. We propose to discover semantic embeddings containing discriminative visual properties for zero-shot learning, without requiring any human annotation. Our model visually divides a set of images from seen classes into clusters of local image regions according to their visual similarity, and further imposes their class discrimination and semantic relatedness. To associate these clusters with previously unseen classes, we use external knowledge, e.g., word embeddings and propose a novel class relation discovery module. Through quantitative and quali-tative evaluation, we demonstrate that our model discovers semantic embeddings that model the visual properties of both seen and unseen classes. Furthermore, we demonstrate on three benchmarks that our visually-grounded semantic embeddings further improve performance over word embed-dings across various ZSL models by a large margin. Code is available at https://github.com/wenjiaXu/VGSE 1.

Introduction
Semantic embeddings aggregated for every class live in a vector space that associates different classes even when visual examples of these classes are not available. Therefore, they facilitate the knowledge transfer in zero-shot learning (ZSL) [1,28,42,59] and are used as side-information in other computer vision tasks like fashion trend forecast [4, 23, 64], face recognition and manipulation [11, 27, 29], and domain adaptation [10, 24].
Human annotated attributes [19, 36, 55], characteristic properties of objects annotated by human experts, are widely
Figure 1. Human-annotated attributes (left) are labor-intensive to collect, and may neglect some local visual properties shared between classes. We propose to discover semantic embeddings via visually clustering image patches and predicting the class relations. used as semantic embeddings [61, 62]. However, obtaining attributes is often a labor-intensive two-step process. First, domain experts carefully design an attribute vocabulary, e.g., color, shape, etc., and then human annotators indicate the presence or absence of an attribute in an image or a class (as shown in Figure 1). The labeling effort devoted to human-annotated attributes hinders its applicability of performing zero-shot learning for more datasets in realistic settings [30].
Previous works tackle this problem by using word em-beddings for class names [31, 38], or semantic embeddings from online encyclopedia articles [3, 39, 67]. Though they model the semantic relation between classes without using human annotation, some of these relations may not be visu-ally detectable by machines, resulting in a poor performance in zero-shot learning. Similarly, discriminative visual cues may not all be represented in those semantic embeddings.
To this end, we propose the Visually-Grounded Semantic
Embedding (VGSE) Network to discover semantic embed-dings with minimal human supervision (we only use cate-gory labels for seen class images). Our network explicitly explores visual clusters that relate image regions from dif-ferent categories, which is useful for knowledge transfer between classes under zero-shot learning settings (see our
learnt clusters in Figure 1). To fully unearth the visual prop-erties shared across different categories, our model discovers semantic embeddings by assigning image patches into var-ious clusters according to their visual similarity. Besides, we further impose class discrimination and semantic related-ness of the semantic embeddings, to benefit their ability in transferring knowledge between classes in ZSL.
To sum up, our work makes the following contribu-tions. (1) We propose a visually-grounded semantic embed-ding (VGSE) network that learns visual clusters from seen classes, and automatically predicts the semantic embeddings for each category by building the relationship between seen and unseen classes given unsupervised external knowledge sources. (2) On three zero-shot learning benchmarks (i.e.
AWA2, CUB, and SUN), our learned VGSE semantic em-beddings consistently improve the performance of word em-beddings over five SOTA methods. (3) Through qualitative evaluation and user study, we demonstrate that our VGSE embeddings contain rich visual information like fine-grained attributes, and convey human-understandable semantics that facilitates knowledge transfer between classes. 2.