Abstract
Recently, table structure recognition has achieved im-pressive progress with the help of deep graph models. Most of them exploit single visual cues of tabular elements or sim-ply combine visual cues with other modalities via early fu-sion to reason their graph relationships. However, neither early fusion nor individually reasoning in terms of multi-ple modalities can be appropriate for all varieties of table
Instead, different modali-structures with great diversity. ties are expected to collaborate with each other in different patterns for different table cases. In the community, the im-portance of intra-inter modality interactions for table struc-ture reasoning is still unexplored. In this paper, we define it as heterogeneous table structure recognition (Hetero-TSR) problem. With the aim of filling this gap, we present a novel
Neural Collaborative Graph Machines (NCGM) equipped with stacked collaborative blocks, which alternatively ex-tracts intra-modality context and models inter-modality in-teractions in a hierarchical way. It can represent the intra-inter modality relationships of tabular elements more ro-bustly, which significantly improves the recognition perfor-mance. We also show that the proposed NCGM can mod-ulate collaborative pattern of different modalities condi-tioned on the context of intra-modality cues, which is vital for diversified table cases. Experimental results on bench-marks demonstrate our proposed NCGM achieves state-of-the-art performance and beats other contemporary methods by a large margin especially under challenging scenarios. 1.

Introduction
Table structure recognition (TSR) aims to recognize the table internal structure to the machine readable data mainly logical structure [18, 46] and presented in two formats: physical structure [2, 13, 20, 22, 27, 30, 31, 34, 35, 40, 45].
More concretely, logical structure only focuses on whether two table elements belong to the same row, column or cells (i.e., logical relationships), while the physical one contains
*Equal contribution. †Contact person.
Illustration of motivation of the proposed NCGM.
Figure 1. (a) Early fusion-based method. The multiple modalities of table elements are fused before modeling their relationships. (b) Late fusion-based method. The multiple modalities are modeled on their intra-modality relationships which are then fused for final re-sults prediction. Due to lack of collaboration, for a distorted table case, previous methods cannot well extract the row relations (con-nected by blue lines) for an anchor element (yellow) with some true relation lost (green dotted line). (c) Our proposed NCGM.
Different modalities are built into graphs with collaboration, which well accommodate the distorted table case. not only logical relationships but also physical coordinates of cell boxes. The recognized tabular structure is essential to many downstream applications [12, 17]. Although many previous algorithms [2,13,18,20,22,30,31,34,35,40,45,46] have achieved impressive progress in the community, TSR is still a challenging task due to two factors of complicated tables. The interior factor is complex table structure where spanning cell occupies at least two columns or rows, while exterior one is table distortion incurred by capture device.
Intuitively, table elements (text segment bounding boxes or table cells) commonly have inherent relationships and natural graph structure. Therefore, recent methods [2, 30, 34] attempt to attack the problem via constructing vi-sual cues of table elements as graphs and applying the deep graph model, such as Graph Convolutional Networks (GCN) [15] to reason their relationships. To introduce richer table information, several methods [20, 30, 34] con-catenate the visual features with other modalities of fea-tures, such as geometry features, as a whole input to the graph model, as shown in Fig. 1 (a). Nevertheless, the re-lational inductive biases of different modalities would be highly discrepant, which makes naively early-fused modali-ties unable to deal with all table structures of great diversity.
Besides, the intra-modality relationships would negatively affect each other when reasoning specific table structures.
For example, the coordinates of table would dominate when recognizing a regular table, but they would become unreli-able when processing distorted table cases. Instead, another alternative way is to individually model intra-modality re-lationships between table elements and combine them by a late-fusion strategy (Fig. 1 (b)). Unfortunately, the dis-entangled reasoning in terms of intra-modality interactions would introduce the curtailment of inter-modality interac-tions. This dilemma leads to the following question: can different modalities collaborate with each other rather than interfering under different table scenarios? We define this practical problem as heterogeneous table structure recogni-tion (Hetero-TSR), which still lacks investigation.
In this work, we propose a novel Neural Collaborative
Graph Machines (NCGM) tailored for this problem, as il-lustrated in Fig. 1 (c). Concretely, we adopt text segment bounding boxes as table elements in our method and ex-tract their multi-modality feature embeddings from appear-ance, geometry and content dimensionality separately. To obtain the corresponding graph context and explore their interactions, we go beyond the standard attention model and propose a basic collaborative block with two successive modules, i.e., Ego Context Extractor (ECE) and Cross Con-text Synthesizer (CCS). Among, ECE plays a role that dy-namically generates graph context for the samples of each modality while the subsequent CCS is in charge of fus-ing and modulating inter-modality interactive information for different table cases. We stack this elemental block multiple times. Through this way, the intra-modality con-text generation and inter-modality collaboration can be con-ducted alternatively in a hierarchical way, which enables intra-inter modality interactions to be generated constantly from the low layer to the top one. In other words, the low-level contextual information in multiple modalities and the high-level one can collaborate with each other throughout the whole network, which is similar to the human percep-tion process [1, 26]. The yielded collaborative graph em-beddings enable our method to achieve better performance compared to other TSR methods, especially under more challenging scenarios, as clearly validated by extensive ex-perimental results. To sum up, our contributions are in the four folds:
• We investigate the importance of collaboration be-tween different modalities in TSR and propose the
Hetero-TSR problem. To our best knowledge, we are the first to research the collaborative patterns between modality interaction for predicting table structure.
• We coin a novel NCGM tailored for Hetero-TSR prob-lem, which consists of collaborative blocks alterna-tively conducting intra-modality context extraction and inter-modality collaboration in a hierarchical way.
• Experimental results on public benchmarks demon-strate that our method significantly outperforms the state-of-the-arts.
• We release a synthesizing method to augment existing benchmarks to more challenging ones. Under more challenging scenarios, our method can achieve at most 11% improvement than the second best method. 2.