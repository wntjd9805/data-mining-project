Abstract 1.

Introduction
Natural language-based communication with mobile de-vices and home appliances is becoming increasingly popu-lar and has the potential to become natural for communi-cating with mobile robots in the future. Towards this goal, we investigate cross-modal text-to-point-cloud localization that will allow us to specify, for example, a vehicle pick-In particular, we propose up or goods delivery location.
Text2Pos, a cross-modal localization module that learns to align textual descriptions with localization cues in a coarse-to-fine manner. Given a point cloud of the environment,
Text2Pos locates a position that is specified via a natural language-based description of the immediate surroundings.
To train Text2Pos and study its performance, we construct
KITTI360Pose, the first dataset for this task based on the re-cently introduced KITTI360 dataset. Our experiments show that we can localize 65% of textual queries within 15m dis-tance to query locations for top-10 retrieved locations. This is a starting point that we hope will spark future develop-ments towards language-based navigation.
“Alexa, hand me over my special delivery at the sidewalk in front of the yellow building next to the blue bus stop.”
Authors of this paper, the future. Hopefully.
Future mobile robots, such as autonomous vehicles and delivery drones, will need to cooperate with humans to co-ordinate actions and plan their trajectories. In this paper we tackle large scale position localization of the target position based on natural-language-based position descriptions, as needed for, e.g., for goods delivery or for vehicle pickup.
For self-localization within a map, mobile agents rely on visual localization methods [4, 22, 35, 41, 56]. These methods match observed images either to a database of geo-tagged images [4, 16, 45] or point-cloud-based maps [33, 35, 39], often obtained using structure-from-motion tech-niques [38, 40]. By contrast, in this paper, we study language-based localization of any location, which, impor-tantly, does not require the user to be physically present at the target location. This would, for example, allow us to explain the pick-up position or delivery location through text/voice to a robo-taxi via natural language based com-munication, that is preferable to humans. Our method can also be seen as complementary to GPS localization meth-ods, e.g., when a GPS tag is too coarse, unavailable, or language-based communication is more convenient.
As the main contribution of this paper, we formalize the task of language-based localization and provide the first dataset and methods for this task. In this problem setting, we assume an intelligent agent is given access to the map of
the environment that comes in the form of a 3D point cloud and object instance labels. While there are several ways of acquiring point clouds, we rely on LiDAR point clouds, readily available in modern automotive [8, 43, 52] and ur-ban [26] datasets, On the query side, we assume a textual description of the query position surroundings, such as the one shown in Fig. 1. The task is then to provide the most likely position estimate based on this query.
To study this challenging problem, we need a dataset that (i) provides a point-cloud-based representation of the environment and (ii) provides labels in the form of query positions and their corresponding textual descriptions, ex-tracted from their immediate surroundings. We build on the recently proposed KITTI360 dataset [52], which pro-vides nine scenes (city districts), covering 80 km of driv-ing data.
Importantly, this dataset provides semantic and instance-level annotations of the point cloud, which we use to automate the generation of query position descriptions.
We obtain the KITTI360Pose dataset by randomly sampling query positions and by generating corresponding textual de-scriptions. For each query position we automatically gener-ate multiple descriptions based on a natural language tem-plate that specifies spatial relations of surrounding instances to the query position, together with their semantic classes and appearance. We generate 43, 381 such descriptions for 14, 934 sampled positions, which we split by scene (repre-senting a city district) to obtain our train/test splits.
We use this dataset to train and evaluate our proposed
Text2Pos model that performs coarse-to-fine localization.
In the coarse localization step, we retrieve sub-regions of the map that likely contain our target position. To this end, our network learns to align the encoded query with the point clouds, representing these sub-regions. We finally refine the position estimate within retrieved candidate regions using our matching-based fine localization module. Our experi-ments show that we can localize such randomly generated positions within KITTI360 scenes with 65% recall for top-10 queries, demonstrating that localizing positions based on textual descriptions is feasible.
In summary, our main contributions are: we (i) intro-duce and formalize the task of 3D point-cloud based local-ization based on textual descriptions. To this end, we (ii) provide KITTI360Pose, the first public dataset for this task, based on the KITTI360 dataset, together with our method for automated mining of positions and corresponding tex-tual descriptions. We (iii) provide a coarse-to-fine baseline model for the localization task, that learns to align objects which are mentioned in the text with object instances in the point cloud and thoroughly evaluate and ablate the perfor-mance on this challenging new task. We believe this work is the first step towards natural language-based communica-tion with future mobile robots, such as delivery drones and self-driving taxis. 2.