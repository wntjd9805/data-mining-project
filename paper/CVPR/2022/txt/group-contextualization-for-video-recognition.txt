Abstract
Learning discriminative representation from the complex spatio-temporal dynamic space is essential for video recog-nition. On top of those stylized spatio-temporal compu-tational units, further refining the learnt feature with ax-ial contexts is demonstrated to be promising in achieving this goal. However, previous works generally focus on uti-lizing a single kind of contexts to calibrate entire feature channels and could hardly apply to deal with diverse video activities. The problem can be tackled by using pair-wise spatio-temporal attentions to recompute feature response with cross-axis contexts at the expense of heavy computa-tions. In this paper, we propose an efficient feature refine-ment method that decomposes the feature channels into sev-eral groups and separately refines them with different axial contexts in parallel. We refer this lightweight feature cal-ibration as group contextualization (GC). Specifically, we design a family of efficient element-wise calibrators, i.e.,
ECal-G/S/T/L, where their axial contexts are information dynamics aggregated from other axes either globally or lo-cally, to contextualize feature channel groups. The GC mod-ule can be densely plugged into each residual layer of the off-the-shelf video networks. With little computational over-head, consistent improvement is observed when plugging in
GC on different networks. By utilizing calibrators to em-bed feature with four different kinds of contexts in parallel, the learnt representation is expected to be more resilient to diverse types of activities. On videos with rich tempo-ral variations, empirically GC can boost the performance of 2D-CNN (e.g., TSN and TSM) to a level comparable to the state-of-the-art video networks. Code is available at https://github.com/haoyanbin918/Group-Contextualization.
*Hao Zhang is the corresponding author.
Figure 1. Perspective/axial preference of different video activities.
The scene change caused by quick camera movement yearns for global context for recognizing the soccer highlight “Corner kick”.
“Arm wrestling”, “Bee keeping” and “Ice skating” can be easily recognized even by a single keyframe. Whereas, the Something-Something activity examples (middle) rely much on temporal re-lations. The group activities, i.e., “Blocked shot” and “Layup”, require a model to localize sub-activities. 1.

Introduction
The 3D spatio-temporal nature of video signals allows video content to be flexibly analyzed from different per-spectives or axes. Specifically, the signals can be trans-formed along various dimensions to capture the activities underlying a video. For example, in Figure 1, the soccer highlight “Corner kick” may require projection of the 3D video signal into a 1D vector to globally summarize the quick camera movement causing scene change. The less temporal activity categories “Arm wrestling”, “Bee keep-ing” and “Ice skating” occurring in near static scenes pre-fer a 2D image representation to capture spatial perspective for classification. By contrast, the video events “Moving something across a surface until it falls down” and “Mov-ing something and something closer to each other” require modeling of temporal relations over time axis. When it
is about classifying sub-activities such as “Blocked shot” and “Layup” in a lengthy basketball video, the localized 3D spatio-temporal analysis is preferred. These observations show the necessity of adjusting the features (C channels) of a 3-dimensional T ×H ×W video tensor with a perspective aligned with video activities.
Feature contextualization [13, 16, 23, 29, 47, 50] is a technique that makes full use of axial contexts (e.g., spa-tial, temporal) to calibrate plain video features obtained from convolutional filters of CNN models (e.g., C3D [41],
I3D [3], P3D [33]). Generally, axial contexts are referred to as information aggregated from other axes towards the features. For example, the global spatial context within the whole time length can be obtained by squeezing the tensor along time axis [50], while in contrast the global tempo-ral context is acquired through shrinking along space axes
[23, 29]. However, due to the large diversity of video activ-ities, it is obvious that a single context cannot fit all activity cases. Given the activity “Moving something and something closer to each other” in Figure 1 as an example, globally aggregating contents along the time axis will harm the time order information, underemphasizing the subtle movement between objects. Also, the global aggregation will mess up sub-activities in the basketball highlights, diminishing the characteristics localized to sub-activities such as “dunk” and “foul”. In these cases, the existing works [16,23,29,50], which focus on calibrating image/video features with only a specific global axial context, may under-perform due to the lack of versatility in representing various activities. On the other hand, projecting multiple axial contexts to a feature will increase the computation cost. Some works [13, 47] try to pairwisely attend each feature point of the 3D video ten-sor from local to global receptive field to adaptively decide the perspectives depending on context. Nevertheless, these works suffer from the heavy computation burden. The re-sulting network is not lightweight, and cannot densely plug into the existing network backbone. The most current work temporal difference network (TDN) [45] shows strong per-formance on activities that require both short-term and long-term dependencies through pairwisely computing the tem-poral differences with short and long intervals of time.
This paper addresses the limitation of feature contextu-alization for video recognition. Specifically, a novel fea-ture contextualization paradigm, i.e, group contextualiza-tion (GC), is presented to derive feature representation that is generic to different activities and with lightweight com-putation. The GC module decomposes the channels into several paralleled groups and applies different feature con-textualization operations on them respectively. As such, the calibrated feature is versatile for it integrating feature dy-namics aggregated from different perspectives and poten-tially can recognize a wide variety of activities. The com-putational overload is kept to a minimum level by apply-Figure 2. The workflow of the proposed group contextualization.
“GAP” denotes global average pooling. ing group convolution [30, 42, 49]. As each output channel only relates to the input channels within a group, only 1 g C 2 (g is the number of divided groups) channel interactions is required, instead of C 2 of a standard convolution. Capital-izing on efficiency in computation, group convolution also allows us to analyze how a network exploits axial contexts in different layers for different activities.
The workflow of GC module is illustrated in Figure 2.
Particularly, the input CNN feature X ∈ RT ×H×W ×C is split into four groups (group-1/2/3/4 with the size of
T × H × W × 1 4 C) along the channel dimension. To achieve separate contextualization, we accordingly design four element-wise calibrators1 (ECals) to calibrate the four channel groups globally along space-time (ECal-G), glob-ally along space (ECal-S), globally along time (ECal-T), and locally (ECal-L) in a small neighborhood in parallel. In this architecture, all ECals share the similar cascaded struc-ture of “GAP/None+FC/Conv+Sigmoid” for efficiency, and achieve feature calibration with element-wise multiplica-tion. Here, the global feature calibrations (ECal-G/S/T) per-form feature pooling along a specific axis and contextual-ization on the different axis with the customized operators.
In fact, the feature aggregation compresses the global in-formation in an axis onto the other so that it enlarges the receptive field to the entire axial range. For example, ECal-T squeezes the input T × H × W × 1 4 C feature along the space axes, resulting in a T × 1 × 1 × 1 4 C contextual fea-ture. In this case, when conducting temporal convolution on the resulted context feature, the global spatial content contributes the attention weight computation of each times-tamp, which can benefit the recognition of video activities requiring long-range temporal relation (e.g., the video clips in Figure 1(middle)). In contrast, if we directly convolve the given feature map without any pooling operation, the global view narrows to a local neighbourhood (ECal-L). This per-spective is particularly useful to localize sub-activities in lengthy video, e.g., “Blocked shot”, “Layup” in the basket-ball video shown in Figure 1. Finally, by separately per-1In this paper, feature adjustment, refinement and calibration, as well as their noun and verb forms, are used interchangeably.
forming feature refinement with ECal-G/S/T/L on those de-composed channel groups, we can reweight the input fea-ture with multiple axial perspectives, resulting in a more discriminative representation Y.
We summarize our contributions as below:
• Group contextualization. We propose a new regime named group contextualization (GC) for video fea-ture refinement. GC encompasses a set of element-wise calibrators (ECal-G/S/T/L) to explicitly model multi-axial contexts and separately refine video feature groups in parallel.
• Computation-efficient. All ECal variants are de-signed in an efficient manner, and the channel decom-position as in group convolution moderates the extra computational cost incurred in feature calibration. For example, when averagely splitting the channels into four groups, GC only introduces 5.3%/1.3% extra pa-rameters/FLOPs to the original TSN backbone.
• Significant performance gain. We verify that our
GC module not only significantly improves the video recognition performance for several feedforward video networks (i.e., TSN, TSM and GST), but also can work together with the temporal difference network (TDN) leading to a notable performance gain. 2.