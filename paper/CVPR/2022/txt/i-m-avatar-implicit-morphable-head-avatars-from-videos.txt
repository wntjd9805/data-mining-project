Abstract
Traditional 3D morphable face models (3DMMs) pro-vide fine-grained control over expression but cannot eas-ily capture geometric and appearance details. Neural volumetric representations approach photorealism but are hard to animate and do not generalize well to unseen ex-pressions. To tackle this problem, we propose IMavatar (Implicit Morphable avatar), a novel method for learn-ing implicit head avatars from monocular videos. Inspired by the fine-grained control mechanisms afforded by con-ventional 3DMMs, we represent the expression- and pose-related deformations via learned blendshapes and skinning fields. These attributes are pose-independent and can be used to morph the canonical geometry and texture fields given novel expression and pose parameters. We employ ray marching and iterative root-finding to locate the canon-ical surface intersection for each pixel. A key contribution is our novel analytical gradient formulation that enables end-to-end training of IMavatars from videos. We show quanti-tatively and qualitatively that our method improves geome-try and covers a more complete expression space compared to state-of-the-art methods. Code and data can be found at https://ait.ethz.ch/projects/2022/IMavatar/. 1.

Introduction
Methods to automatically create animatable personal avatars in unobtrusive and readily available settings (i.e., from monocular videos) have many applications in VR/AR games and telepresence. Such applications require faith-ful renderings of the deforming facial geometry and ex-pressions, detailed facial appearance and accurate recon-struction of the entire head and hair region. Conventional methods [14, 16, 18, 19, 43, 45, 47, 49] based on morphable mesh models [2, 28, 38] can fit 3DMM shape and texture parameters to images for a given subject. However, such mesh-based approaches suffer from an inherent resolution-Figure 1. IMavatar. We learn personal avatars from RGB videos, represented by canonical shape, texture, and deformation fields.
An implicit morphing formulation allows generalization to unseen expressions and poses outside the training distribution. memory trade-off, and cannot handle topological changes caused by hair, glasses, and other accessories. Recent meth-ods build on neural radiance fields [32] to learn personal-ized avatars [17, 36, 37] and yield high-quality images, es-pecially if the generated expressions are close to the train-ing data. A key challenge for building animatable facial avatars with implicit fields is the modeling of deformations.
Previous work either achieves this by conditioning the im-plicit representation on expressions [17] or via a separate displacement-based warping field [36, 37]. Such under-constrained formulations limit the generalization ability, re-quiring large numbers of training poses.
In this paper, we propose Implicit Morphable avatar (IMavatar), a novel approach for learning personalized,
generalizable and 3D-consistent facial avatars from monoc-ular videos, see Fig. 1. The proposed method combines the fine-grained expression control provided by 3DMMs with the high-fidelity geometry and texture details offered by resolution-independent implicit surfaces, taking advan-tage of the strengths of both methods. IMavatars are mod-eled via three continuous implicit fields, parametrized by multilayer perceptrons (MLPs), that represent the geometry, texture, and pose- and expression-related deformations. In-spired by FLAME [28], we represent the deformation fields via learned expression blendshapes, linear blend skinning weights, and pose correctives in the canonical space. The blendshapes and weights are then used to warp canonical points to the deformed locations given expression and pose conditions. This pose and expression invariant formulation of shape and deformations improves generalization to un-seen poses and expressions, resulting in a model of implicit facial avatars that provides fine-grained and interpretable control.
To learn from monocular videos with dynamically de-forming faces, the mapping from pixels to 3D locations on the canonical surface is required. To this end, we aug-ment the root-finding algorithm from SNARF [9] with a ray marcher akin to that of IDR [60], yielding the canonical sur-face correspondence for each pixel. The color prediction for each image location is then given by the canonical texture network, which formulates our primary source of supervi-sion: a per-pixel image reconstruction objective.
To obtain gradients for the canonical point, we observe that the location of the surface-ray intersection is implic-itly defined by the geometry and deformation network with two constraints: 1. the canonical point must lie on the sur-face and 2. its deformed location must be on the marched ray. Given these equality constraints, we derive an analytic gradient for the iteratively located surface intersection via implicit differentiation, which allows end-to-end training of the geometry and deformation networks from videos.
We compare our method with several hard baselines and state-of-the-art (SOTA) methods using image similarity and expression metrics. To evaluate the generated geometry un-der different expressions and poses, we construct a synthetic dataset containing 10 subjects. We quantitatively show that our method produces more accurate geometry and gener-alizes better to unseen poses and expressions. When ap-plied to real video sequences, IMavatar reconstructs the tar-get poses and expressions more accurately than that state of the art (SOTA) and qualitatively achieves more accurate geometry and better extrapolation ability over poses and ex-pressions.
In summary, we contribute:
• a 3D morphing-based implicit head avatar model with detailed geometry and appearance that generalizes across diverse expressions and poses,
• a differentiable rendering approach that enables end-to-end learning from videos, and
• a synthetic video dataset for evaluation. 2.