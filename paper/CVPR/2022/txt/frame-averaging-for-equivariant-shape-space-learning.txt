Abstract
The task of shape space learning involves mapping a train set of shapes to and from a latent representation space with good generalization properties. Often, real-world col-lections of shapes have symmetries, which can be defined as transformations that do not change the essence of the shape. A natural way to incorporate symmetries in shape space learning is to ask that the mapping to the shape space (encoder) and mapping from the shape space (decoder) are equivariant to the relevant symmetries. In this paper, we present a framework for incorporating equivariance in en-coders and decoders by introducing two contributions: (i) adapting the recent Frame Averaging (FA) framework for building generic, efficient, and maximally expressive Equiv-ariant autoencoders; and (ii) constructing autoencoders equivariant to piecewise Euclidean motions applied to dif-ferent parts of the shape. To the best of our knowledge, this is the first fully piecewise Euclidean equivariant au-toencoder construction. Training our framework is simple: it uses standard reconstruction losses, and does not require the introduction of new losses. Our architectures are built of standard (backbone) architectures with the appropriate frame averaging to make them equivariant. Testing our framework on both rigid shapes dataset using implicit neu-ral representations, and articulated shape datasets using mesh-based neural networks show state of the art general-ization to unseen test shapes, improving relevant baselines by a large margin. In particular, our method demonstrates significant improvement in generalizing to unseen articu-lated poses. 1.

Introduction
Learning a shape space is the task of finding a latent representation to a collection of input training shapes that generalizes well to unseen, test shapes. This is often done within an autoencoder framework, namely an encoder Φ :
X → Z, mapping an input shape in X (in some 3D repre-sentation) to the latent space Z, and a decoder Ψ : Z → Y ,
*Work done during an internship at NVIDIA. mapping latent representations in Z back to shapes Y (pos-sibly in other 3D representation than X).
Many shape collections exhibit symmetries. That is, transformations that do not change the essence of the shape.
For example, applying an Euclidean motion (rotation, re-flection, and/or translation) to a rigid object such as a piece of furniture will produce an equivalent version of the object.
Similarly, the same articulated body, such as an animal or a human, can assume different poses in space.
A natural way to incorporate symmetries in shape space learning is to require the mapping to the latent space, i.e., the encoder, and mapping from the latent space, i.e., the de-coder, to be equivariant to the relevant symmetries. That is, applying the symmetry to an input shape and then encoding it would result in the same symmetry applied to the latent code of the original shape. Similarly, reconstructing a shape from a transformed latent code will result in a transformed shape.
The main benefit in imposing equivariance in shape space learning is achieving a very useful inductive bias: If the model have learned a single shape, it can already gener-alize perfectly to all its symmetric versions! Unfortunately, even in the presumably simpler setting of a global Eu-clidean motion, building an equivariant neural network that is both expressive and efficient remains a challenge. The only architectures that were known to be universal for Eu-clidean motion equivariant functions are Tensor Field Net-works [17, 49] and group averaging [8, 57] both are compu-tationally and memory intensive. Other architectures, e.g.,
Vector Neurons [15] are efficient computationally but are not known to be universal.
In this paper, we present a novel framework for building equivariant encoders and decoders for shape space learning that are flexible, efficient and maximally expressive (i.e., universal). In particular, we introduce two contributions: (i) we adapt the recent Frame Averaging (FA) framework [39] to shape space learning, showing how to efficiently build powerful shape autoencoders. The method is general, easily adapted to different architectures and tasks, and its training only uses standard autoencoder reconstruction losses with-out requiring the introduction of new losses. (ii) We con-struct what we believe is the first autoencoder architecture that is fully equivariant to piecewise Euclidean transforma-tions of the shape’s parts, e.g., articulated human body.
We have tested our framework on two types of shape learning implicit representations of space learning tasks: shapes from real-life input point clouds extracted from sequences of images [42], and learning mesh deforma-tions of human (body and hand) and animal shape spaces
[1, 6, 31, 62]. In both tasks, our method produced state of the art results when compared to relevant baselines, often showing a big margin compared to the runner-up, justifying the efficacy of the inductive bias injected using the frame-averaging and equivariance. 2.