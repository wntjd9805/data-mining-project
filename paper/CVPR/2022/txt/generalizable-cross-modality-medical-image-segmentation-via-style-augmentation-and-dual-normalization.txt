Abstract
For medical image segmentation, imagine if a model was only trained using MR images in source domain, how about its performance to directly segment CT images in target do-main? This setting, namely generalizable cross-modality segmentation, owning its clinical potential, is much more challenging than other related settings, e.g., domain adap-tation. To achieve this goal, we in this paper propose a novel dual-normalization model by leveraging the aug-mented source-similar and source-dissimilar images dur-ing our generalizable segmentation. To be specific, given a single source domain, aiming to simulate the possible appearance change in unseen target domains, we first uti-lize a nonlinear transformation to augment source-similar and source-dissimilar images. Then, to sufficiently ex-ploit these two types of augmentations, our proposed dual-normalization based model employs a shared backbone yet independent batch normalization layer for separate nor-malization. Afterward, we put forward a style-based selec-tion scheme to automatically choose the appropriate path in the test stage. Extensive experiments on three publicly available datasets, i.e., BraTS, Cross-Modality Cardiac, and Abdominal Multi-Organ datasets, have demonstrated that our method outperforms other state-of-the-art domain generalization methods. Code is available at https:// github.com/zzzqzhou/Dual-Normalization.
*Corresponding author: Yinghuan Shi. This work was supported by National Key Research and Development Program of China (2019YFC0118300), NSFC Major Program (62192783), CAAI-Huawei
MindSpore Project (CAAIXSJLJJ-2021-042A), China Postdoctoral Sci-ence Foundation Project (2021M690609), and Jiangsu Natural Science
Foundation Project (BK20210224).
†Ziqi Zhou and Yinghuan Shi are with the State Key Laboratory for
Novel Software Technology and National Institute of Healthcare Data Sci-ence, Nanjing University, China. Lei Qi is with the School of Computer
Science and Engineering, Southeast University, China. Xin Yang and
Dong Ni are with the National-Regional Key Technology Engineering Lab-oratory for Medical Ultrasound, School of Biomedical Engineering, Health
Science Center, the Medical Ultrasound Image Computing Lab, the Mar-shall Laboratory of Biomedical Engineering, Shenzhen University, China. (a) Example slices of T2 & T1. (b) Performance.
Figure 1. (a) Example slices from BraTS dataset; (b) Comparison of our method with “DeepAll” and “DoFE” methods on the Cross-center prostate segmentation task and the Cross-modality brain tumor segmentation task. 1.

Introduction
In recent years, profound progress in medical image seg-mentation has been achieved by deep convolutional neural networks [22, 30, 35]. Benefiting from these recent efforts, the accuracy of segmentation on medical images has now been substantially improved. Despite their success, the dis-tribution shift between training (or labeled) and test (or un-labeled) data usually results in a severe performance degen-eration during the deployment of trained segmentation mod-els. The reason for distribution shift typically come from different aspects, e.g., different acquisition parameters, var-ious imaging methods or diverse modalities.
To fight against domain shift, several practical settings have been investigated, among which unsupervised domain adaptation (UDA) based segmentation [6,14,44] is the most popular one. Specifically, in UDA setting, by assuming that test or unlabeled data could be observed, the model is firstly trained on labeled source domain (i.e., training set) along with unlabeled target domain (i.e., test set), by reduc-ing their domain gap. Then, the trained model is employed to segment the images from target domain. Nevertheless, these UDA based models require that target domain could 1
be observed and even allowed to be trained. This prerequi-site sometimes is difficult or infeasible to satisfy in the real application. For example, to protect personal privacy infor-mation, target domain (or test set) in some institutes cannot be directly accessed.
To alleviate the requirement of target domain in UDA, we consider a more feasible yet challenging setting, do-main generalization (DG), to achieve generalizable med-ical image segmentation against domain shift. We no-tice that most existing DG models merely perform well in cross-central setting with small variations between do-mains, whereas the large domain shift (e.g., cross-modality) is seldom investigated that could largely deteriorate their performance [25, 26, 40].
Specifically,
We now illustrate two types of generalizable segmen-tation scenarios (i.e., cross-center and cross-modality) to clarify our motivation. in Figure 1b, we show results of our method (denoted as “Ours”), “DeepAll” baseline and a state-of-the-art cross-center DG method (“DoFE”) [40] on two different DG tasks. The first task is the cross-center prostate segmentation task [27]. As il-lustrated in Figure 1b, all of these methods achieve rela-tively high Dice scores (> 80%) on this task, and gaps of different methods are quite small. However, when we apply these three methods to BraTS dataset [29], a cross-modality brain tumor segmentation dataset, “DeepAll” and
“DoFE” methods show a drastic degradation on Dice scores (< 40%), while our method still achieves a competitive
Dice score (> 50%). The reason of this large performance degradation lies in large domain shift. For example, in Fig-ure 1a, we illustrate T2- and T1-weighted images in BraTS dataset [29]. The brain tumors that need to be segmented are delineated with red curves. It is obvious that T2 and T1 modalities show large distinct appearances. Accordingly, we notice that cross-modality DG task is more challenging than cross-center DG task due to the former should tackle larger domain shift. In this paper, we aim to deal with DG task with large domain shift (e.g., cross-modality task), and most previous DG methods are not designed for this.
Our setting owns its clinical meaning. For example, large distribution shift, caused by some unpredictable fac-tors (e.g., interference from light source) during imaging, poses challenge to current generalization methods. Also, in some cases, data scarcity occurs in target domain makes
UDA becomes hard to realize. In a nut shell, we intend to develop a robust method for realizing domain distribution shift insensitive modeling.
Based on above motivations, we propose a generaliz-able cross-modality medical image segmentation method trained on a single source domain (e.g., CT) and directly applied to unseen target domain (e.g., MRI) without any re-training. We notice that in medical images, modality dis-crepancy usually manifests in gray-scale distribution dis-crepancy. Being aware of this fact, we wish to simulate possible appearance changes in unseen target domains. In order to tackle this challenging cross-modality DG task, we introduce a module that can randomly augment source do-main into different styles. To be specific, we utilize B´ezier
Curves [31] as transformation functions to generate two groups of images: one group of images are similar to source domain images (i.e., source-similar domain), and another group of images have a large distribution gap with source domain images (i.e., source-dissimilar domain). Then, we introduce a segmentation model with a dual-normalization module to preserve style information of source-similar do-main and source-dissimilar domain. Finally, a style-based path selection module is developed to help target domain images select the best normalization path to achieve opti-mal segmentation results. The main contributions of this paper are summarized as:
• We propose a deep dual-normalization model to tackle a more challenging DG task, i.e., generalizable cross-modality segmentation, that could directly segment the images from unseen target domains without re-training.
• We enhance the diversity of source domain via generat-ing source-similar and source-dissimilar images based on
B´ezier Curves and develop a dual-normalization network for effective exploitation. Besides, we propose the style-based path selection scheme in the test stage.
• Extensive experiments demonstrate our effectiveness. On
BraTS dataset, our method achieves the Dice of 54.44% and 57.98% on T2 and T1CE source domains, respec-tively, which is quite close to UDA [5] (59.30% on T2 source domain) as our upper bound . On both Cross-Modality Cardiac and Abdominal Multi-Organ datasets, our method outperforms the state-of-the-art DG methods. 2.