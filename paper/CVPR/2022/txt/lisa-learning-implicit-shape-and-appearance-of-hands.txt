Abstract
This paper proposes a do-it-all neural model of human hands, named LISA. The model can capture accurate hand shape and appearance, generalize to arbitrary hand sub-jects, provide dense surface correspondences, be recon-structed from images in the wild, and can be easily an-imated. We train LISA by minimizing the shape and ap-pearance losses on a large set of multi-view RGB image se-quences annotated with coarse 3D poses of the hand skele-ton. For a 3D point in the local hand coordinates, our model predicts the color and the signed distance with respect to each hand bone independently, and then combines the per-bone predictions using the predicted skinning weights. The shape, color, and pose representations are disentangled by design, enabling ﬁne control of the selected hand param-eters. We experimentally demonstrate that LISA can ac-curately reconstruct a dynamic hand from monocular or multi-view sequences, achieving a noticeably higher qual-ity of reconstructed hand shapes compared to baseline ap-proaches. Project page: https:// www.iri.upc.edu/ people/ ecorona/ lisa/ .
† Work performed during internship with Reality Labs, Meta. 1.

Introduction
Since the thumb opposition enabled grasping around 2 million years ago [25], humans interact with the physical world mainly with hands. The problems of modeling and tracking human hands have therefore naturally received a considerable attention in computer vision [43]. Accurate and robust solutions to these problems would unlock a wide range of applications in, e.g., human-robot interaction, pros-thetic design, or virtual and augmented reality.
Most research efforts related to modeling and tracking human hands, e.g., [8, 20, 21, 29, 38, 70], rely on the MANO hand model [53], which is deﬁned by a polygon mesh that can be controlled by a set of shape and pose parameters.
Despite being widely used, the MANO model has a low res-olution and does not come with texture coordinates, which makes representing the surface color difﬁcult.
The related ﬁeld of modeling and tracking human bod-ies has been relying on parametric meshes as well, with the most popular model being SMPL [31] which suffers from similar limitations as the MANO model. Recent approaches for modeling human bodies, e.g., [1, 4, 10, 15, 34, 54, 61], rely on articulated models based on implicit representa-tions, such as Neural Radiance Field [35] or Signed Dis-tance Field (SDF) [46]. Such representations are capable of representing both shape and appearance and able to capture
ﬁner geometry compared to approaches based on paramet-ric meshes. However, it is yet to be explored how well im-plicit representations apply to articulated objects such as the human hand and how they generalize to unseen poses.
We explore articulated implicit representations for mod-eling human hands and make the following contributions: 1. We introduce LISA, the ﬁrst neural model of human hands that can capture accurate hand shape and appear-ance, generalize to arbitrary hand subjects, provide dense surface correspondences (via predicted skinning weights), be reconstructed from images in the wild, and easily animated. 2. We show how to train LISA by minimizing shape and appearance losses on a large set of multi-view RGB image sequences annotated with coarse 3D poses of the hand skeleton. 3. The shape, color and pose representations in LISA are disentangled by design, enabling ﬁne control of se-lected aspects of the model. 4. Our experimental evaluation shows that LISA sur-passes baselines in hand reconstruction from 3D point clouds and hand reconstruction from RGB images. 2.