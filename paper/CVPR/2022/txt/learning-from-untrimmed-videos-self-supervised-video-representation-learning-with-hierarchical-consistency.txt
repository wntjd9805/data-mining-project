Abstract
Natural videos provide rich visual contents for self-Yet most existing approaches for supervised learning. learning spatio-temporal representations rely on manually trimmed videos, leading to limited diversity in visual pat-terns and limited performance gain. In this work, we aim to learn representations by leveraging more abundant in-formation in untrimmed videos. To this end, we propose to learn a hierarchy of consistencies in videos, i.e., visual consistency and topical consistency, corresponding respec-tively to clip pairs that tend to be visually similar when separated by a short time span and share similar topics when separated by a long time span. Specifically, a hi-erarchical consistency learning framework HiCo is pre-sented, where the visually consistent pairs are encouraged to have the same representation through contrastive learn-ing, while the topically consistent pairs are coupled through a topical classifier that distinguishes whether they are topic-related. Further, we impose a gradual sampling algorithm for proposed hierarchical consistency learning, and demon-strate its theoretical superiority. Empirically, we show that not only HiCo can generate stronger representations on untrimmed videos, it also improves the representation qual-ity when applied to trimmed videos. This is in contrast to standard contrastive learning that fails to learn appropri-ate representations from untrimmed videos. 1.

Introduction
Self-supervised learning is of crucial importance in com-puter vision, and has shown remarkable potential in learning powerful spatio-temporal representations using unlabelled
âˆ—Corresponding authors.
Project page: https://hico-cvpr2022.github.io/.
Figure 1. (a) An example of untrimmed video with the interview, competition and the stadium of Sumo Wrestling. It shows the hier-archical consistency present in untrimmed videos. As can be seen, clips with short temporal distance share similar visual elements, while clips with long temporal distance, despite their dissimilar visual contents, share a same topic. (b, c) Linear evaluation of con-ventional contrastive learning (CL), i.e., SimCLR [7], and HiCo on
HMDB51 [27] and UCF101 [49], with pretraining respectively on the original (trimmed) and untrimmed version of Kinetics-400 [5]. videos. Current state-of-the-art approaches on unsupervised video representation learning are typically based on the con-trastive learning framework [13, 44, 45], which encourages the representations of the clips from the same video to be close and those from different videos to be as far away from each other as possible [7, 21].
In most approaches, they are trained on manually trimmed videos such as Kinetics-400 [5]. However, it is labor-intensive and time-consuming to collect such a large-scale trimmed video dataset, and the
trimming process may also bring in certain human bias into the data. In contrast, natural videos carry more abundant and diverse visual contents, and they are easier to obtain.
Hence, this work sets out to exploit the natural untrimmed videos for video representation learning.
Directly learning generalized and powerful representa-tions from untrimmed videos is not a trivial problem, as empirical results both in Fig. 1(b, c) and in [13](Tab.4 and Tab.6) demonstrate that directly applying contrastive learning on untrimmed videos yields worse representa-tions than on trimmed videos. One possible reason is that the temporally-persistent hypothesis [13] followed by the standard video contrastive learning framework and verified on trimmed videos is no longer sufficient for untrimmed videos. Ideally, the temporally-persistent hypothesis learns an invariant representation for all the clips in the video. This may be plausible for trimmed videos, and even for clips with short temporal distance in untrimmed ones, where a certain level of visual similarity or visual consistency ex-ists. Yet it could be overly strict for temporally distant clips in untrimmed videos with less or no visual consistency ex-ists, since they are only related by the same topic, i.e., they are topically consistent. In fact, we spot a hierarchical rela-tion between the two consistencies existing in untrimmed videos. Specifically, visually consistent pairs are always topically consistent, while topically consistent pairs are not necessarily visually consistent. An example of the hierar-chical consistency is visualized in Fig. 1(a).
In this paper, we present a novel framework for learning strong representations from untrimmed videos. By exploit-ing the hierarchical consistencies existing in untrimmed videos, i.e., the visual consistency and the topical consis-tency, our framework HiCo for Hierarchical Consistency learning can leverage the more abundant semantic patterns in natural videos. We design two hierarchical tasks, re-spectively for learning the two consistencies. For visually consistent learning, we apply standard contrastive learning on clips with a small maximum temporal distance, and en-courage temporally-invariant representations. For topical consistency learning, we propose a topic prediction task, instead of a strict invariant mapping, the representations are only required to group different topics. Considering the hierarchical nature of consistencies, we also include the visually consistent pairs in topical consistency learning, while exclude topically consistent pairs for visual consis-tency learning. Due to the complexity of the hierarchical tasks, we further introduce a gradual sampling that gradu-ally increases the training difficulty for positive pairs to help optimization and improve generalization, which we show its superior both theoretically and empirically.
Extensive experiments on multiple downstream tasks show that employing HiCo can learn a strong and gener-alized video representation from untrimmed videos, with a convincing gap of 12.8% and 12.5% on the downstream action recognition task respectively on HMDB51 [27] and
UCF101 [49] compared with the standard contrastive learn-ing. We also demonstrate the capability of HiCo to learn a better representation from trimmed videos. 2.