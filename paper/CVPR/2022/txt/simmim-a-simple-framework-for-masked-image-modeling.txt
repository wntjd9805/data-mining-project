Abstract
This paper presents SimMIM, a simple framework for masked image modeling. We have simplified recently pro-posed relevant approaches, without the need for special designs, such as block-wise masking and tokenization via discrete VAE or clustering. To investigate what makes a masked image modeling task learn good representations, we systematically study the major components in our frame-work, and find that the simple designs of each component have revealed very strong representation learning perfor-mance: 1) random masking of the input image with a mod-erately large masked patch size (e.g., 32) makes a pow-erful pre-text task; 2) predicting RGB values of raw pix-els by direct regression performs no worse than the patch classification approaches with complex designs; 3) the pre-diction head can be as light as a linear layer, with no worse performance than heavier ones. Using ViT-B, our approach achieves 83.8% top-1 fine-tuning accuracy on
ImageNet-1K by pre-training also on this dataset, surpass-ing previous best approach by +0.6%. When applied to a larger model with about 650 million parameters, SwinV2-H, it achieves 87.1% top-1 accuracy on ImageNet-1K using only ImageNet-1K data. We also leverage this approach to address the data-hungry issue faced by large-scale model training, that a 3B model (SwinV2-G) is successfully trained to achieve state-of-the-art accuracy on four representative vision benchmarks using 40× less labelled data than that in previous practice (JFT-3B). The code is available at https://github.com/microsoft/SimMIM . 1.

Introduction
“What I cannot create, I do not understand.”
— Richard Feynman
“Masked signal modeling” is one such task that learns to create: masking a portion of input signals and trying to predict these masked signals. In NLP, following this phi-*Equal. Zhenda, Yutong, Zhuliang are long-term interns at MSRA.
Figure 1. An illustration of our simple framework for masked lan-guage modeling, named SimMIM. It predicts raw pixel values of the randomly masked patches by a lightweight one-layer head, and performs learning using a simple ℓ1 loss. losophy, self-supervised learning approaches built on the masked language modeling tasks have largely repainted the field [2,12,30], i.e., learning very large-scale language mod-els by using huge amounts of unlabeled data has been shown to generalize well to a broad range of NLP applications.
In computer vision, although there are pioneers leverag-ing this philosophy for self-supervised representation learn-ing [13, 57, 58], in previous years, this line of work was almost buried by the contrastive learning approaches [8, 20, 48]. The different difficulties of applying this task to the language and visual domains can be explained by the dif-ferences between two modalities. One of the differences is that images exhibit stronger locality: pixels that are close to each other tend to be highly correlated [25], so the task can be done by duplicating close pixels rather than by seman-tic reasoning. Another difference is that visual signals are raw and low-level, while text tokens are human-generated high-level concepts. This raises a question of whether the prediction of low-level signals is useful for high-level vi-sual recognition tasks. A third difference is that the visual signal is continuous, and the text token is discrete. It is un-known how classification-based masked language modeling approaches can be adapted to handle continuous visual sig-nals well.
Until recently, there have been trials that attempt to bridge modality gaps and resolve the obstacles, by introduc-ing several special designs, for example, by converting con-tinuous signals into color clusters [7], by patch tokenization using an additional network [1], or by a block-wise masking strategy to break short-range connections [1], etc. Through these special designs, the learned representations proved to be well transferable to several visual recognition tasks.
In contrast to requiring special complex designs, in this paper, we present a simple framework which aligns well with the nature of visual signals, as shown in Figure 1, and is able to learn similar or even better representations than previously more complex approaches: random masking of input image patches, using a linear layer to regress the raw pixel values of the masked area with an ℓ1 loss. The key designs and insights behind this simple framework include:
• Random masking is applied on image patches, which is simple and convenient for vision Transformers. For masked pixels, either larger patch size or higher mask-ing ratio can result in a smaller chance of finding vis-ible pixels that are close. For a large masking patch size of 32, the approach can achieve competitive per-formance in a wide range of masking ratios (10%-70%). For a small mask patch size of 8, the mask-ing ratio needs to be as high as 80% to perform well.
Note that the preferred masking ratios are very differ-ent from that in the language domain, where a small masking ratio of 0.15 is adopted as default. We hypoth-esize that different degrees of information redundancy in two modalities may lead to the different behaviors.
• A raw pixel regression task is used. The regression task aligns well with the continuous nature of visual signals, which possesses ordering property. This sim-ple task performs no worse than the classification ap-proaches with classes specially defined by tokeniza-tion, clustering, or discretization.
• An extremely lightweight prediction head (e.g., a lin-ear layer) is adopted, which achieves similarly or slightly better transferring performance than that of heavier prediction heads (e.g., an inverse Swin-B). The use of an extremely lightweight prediction head brings a remarkable speedup in pre-training. In addition, we note that a broad range of target resolutions (e.g., 122-962) perform competitive with the highest 1922. While heavier heads or higher resolutions generally result in greater generation capability, this greater capability does not necessarily benefit down-stream fine-tuning tasks.
Though simple, the proposed SimMIM approach is very effective for representation learning. Using ViT-B, it achieves 83.8% top-1 fine-tuning accuracy on ImageNet-1K, surpassing previous best approach ( [1]) by +0.6%.
SimMIM has also shown to be scalable to larger mod-els: with a SwinV2-H model (658M parameters) [31], it achieves 87.1% top-1 accuracy on ImageNet-1K classifi-cation, which is the highest number among methods that use ImageNet-1K data only. This result encourages the use of self-supervised learning to address the increasing data-hungry problem caused by quickly rising model capacity.
In fact, with the help of SimMIM, we successfully trained a SwinV2-G model with 3 billion parameters [31] using
∼40× smaller data than that of Google’s JFT-3B dataset, and set new records on several representative benchmarks: 84.0% top-1 accuracy on ImageNet-V2 classification [40], 63.1/54.4 box/mask mAP on COCO object detection [6,29], 59.9 mIoU on ADE20K semantic segmentation [49, 60], and 86.8% top-1 accuracy on Kinetics-400 action recogni-tion [26, 33].
While in recent years we have witnessed an increasing overlap between NLP and computer vision in both basic modeling and learning algorithms, as well as in multi-modal applications, which aligns well with how human brains achieve general intelligence capabilities, we hope that our demonstration of “masked signal modeling” in computer vi-sion can drive this trend a bit further and encourage deeper interaction of different AI fields. 2.