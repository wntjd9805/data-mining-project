Abstract
Temporal representation is the cornerstone of mod-ern action detection techniques. State-of-the-art methods mostly rely on a dense anchoring scheme, where anchors are sampled uniformly over the temporal domain with a dis-cretized grid, and then regress the accurate boundaries. In this paper, we revisit this foundational stage and introduce
Recurrent Continuous Localization (RCL), which learns a fully continuous anchoring representation. Speciﬁcally, the proposed representation builds upon an explicit model con-ditioned with video embeddings and temporal coordinates, which ensure the capability of detecting segments with arbi-trary length. To optimize the continuous representation, we develop an effective scale-invariant sampling strategy and recurrently reﬁne the prediction in subsequent iterations.
Our continuous anchoring scheme is fully differentiable, al-lowing to be seamlessly integrated into existing detectors, e.g., BMN [20] and G-TAD [41]. Extensive experiments on two benchmarks demonstrate that our continuous represen-tation steadily surpasses other discretized counterparts by
∼2% mAP. As a result, RCL achieves 52.92% mAP@0.5 on
THUMOS14 and 37.65% mAP on ActivtiyNet v1.3, outper-forming all existing single-model detectors. 1.

Introduction
Temporal Action Localization (TAL) that localizes tem-poral boundaries of actions with speciﬁc categories in untrimmed videos [6, 14, 15], is at the core of several down-stream tasks such as video classiﬁcation [12], video cap-tioning [44] and video editing [13]. This challenging prob-lem has been deeply studied in recent years [20, 21, 32, 40, 41], as the large scale variation problems is very serious, causing sophisticated feature designs to capture both local and global information, and thus inspired many extensions such as UNet-like architecture [22], local context [28], and proposal-relations [41, 42, 45]. Prior works [9, 40] take in-spiration from image detection [29, 30] and are carried out by densely spanning temporal anchors and predicting their corresponding scores. Other challenges include the fact that (a) the overall anchor distribution (b) the unbalanced scale distribution
Figure 1. The tIoU histogram of training anchors from BMN [20].
The red numbers are the positive percentage higher than the cor-responding tIoU threshold. These anchors mainly cover the long segments, which results in a missing detection for short instances. the deﬁnition of an action’s temporal boundaries are often ambiguous [2]. The ambiguity and uncertainty also hinder the convergence for localization optimization, and brings an illogical empirical observation that the classiﬁcation-based detectors [20, 41] usually achieve better performance than regression-based methods [9].
While numerous efforts have been made towards solv-ing the above challenges, recent approaches still suffer from a major limitation: they mainly leverage a discretized anchoring representation. For example, existing bottom-up methods [21, 32, 46] utilize the discretized boundary classiﬁcation and a well-tuned post-processing to compose temporal segments, which can not be trained in an end-to-end manner. Recently, many works utilize the pre-deﬁned temporal anchor to represent the temporal hypothe-sis, e.g., the sliding-windows paradigm [33] and the multi-scale anchors [9,40]. These methods show excellent perfor-mance with faster speed and have the ability to handle large duration segments.
In contrast to representing complete segments, some anchor-free methods, e.g., AFSD [19], leverage center-point representations to directly regress the start and end time, and the latest studies [35, 39] utilize transformer decoder to bi-match the segments with action queries. In general, different representation methods usu-ally steer the detectors to perform well in different aspects.
For example, the bottom-up representation is usually more accurate for ﬁne-grained localization. The anchor-based
How to represent a temporal segment ? background
LongJump 6.2s background frontground background start: hold: end: starting n o i t a r u d starting n o i t a r u d
FFN
FFN
FFN
FFN
LongJump, (3.1, 9.3) background background background
Transformer decoder action queries (a)Boundary Classification 
+Post-processing (b) Multi-scale Anchors
Regression (c) Temporal Grid 
Classification (d) Anchor-free 
Regression (e) Transformer-based
Adaptive Anchors (f) Continuous 
Representation (ours)
Figure 2. The typical temporal representation methods. (a) the bottom-up representation [21, 46]. (b) the multi-scale anchor representa-tion [9, 40]. (c) the grid-based representation [20,43]. (d) the anchor-free representation [19]. (e) the transformer-based representation [35] (f) the proposed continuous representation. Best viewed in color. representation achieves better completeness and is easy to optimize with the tIoU supervision. The anchor-free rep-resentations avoid the need for an anchoring design and are usually quite efﬁcient. The transformer has shown powerful abilities with set matching loss from action queries. Notic-ing that different representations and their anchoring opti-mization are usually heterogeneous, but their performances essentially depend on the anchor distribution and the rank-ing quality between the anchors. As shown in Figure 1, the discretized anchoring representation [20] can only pro-vide coarse proposals, causing seriously missed detection for short-term segments.
To address this issue, we introduce a novel anchoring representation that is efﬁcient, expressive, and fully contin-uous, as depicted in Figure 2(f). Our key idea is to directly regress conﬁdence scores from continuous anchor points us-ing deep neural networks. Thus we can extract precise seg-ments by searching local maximum in the continuous func-tion.
In this work, we present Recurrent Continuous Localiza-tion (RCL), an explicit model conditioned with video em-beddings and temporal coordinates. Our approach uses the concept of a Continuous Anchoring Representation (CAR) to achieve high ﬁdelity action detection. Unlike com-mon anchor-based detection techniques, which discretize the segments into a regular grid for measurement [20], we produce an estimation in the continuous ﬁeld. The proposed continuous representation can be intuitively understood as a learned position-conditioned classiﬁer for which the conﬁ-dence scores are jointly determined by the video features and the temporal coordinates.
The proposed RCL can serve as a generic plug-in module into various prevalent temporal action localization frame-works, including BMN [20] and G-TAD [41]. Extensive ex-periments on the THUMOS14 [15] and ActivityNet v1.3 [6] show that RCL substantially improves various detectors by 2 ∼ 4% mAP. In particular, we improve a strong BMN de-tector by about 1.8% average mAP and 5.9% recall, reach-ing 37.65% mAP on ActivityNet v1.3.
The innovations of this article are as follows:
• We propose a continuous anchoring representation method, which uniﬁes and extends existing anchor-based detector into a continuous regression problem in 2D coordinates.
• To optimize the continuous representation, we develop an effective scale-invariant sampling strategy, which provide accurate ranking scores for short-term seg-ments.
• With an iterative optimization method, our model adaptively focus on target region and provide a reﬁned estimation.
• Our model obtain state-of-the-art results in quantita-tive comparisons on the THUMOS14 [15] and Ac-tivityNet v1.3 [6] datasets, with 52.92% mAP@0.5, 37.65% mAP, respectively. 2.