Abstract
Audiovisual Event (AVE) localization requires the model to jointly localize an event by observing audio and visual information. However, in unconstrained videos, both in-formation types may be inconsistent or suffer from severe background noise. Hence this paper proposes a novel cross-modal background suppression network for AVE task, op-erating at the time- and event-level, aiming to improve lo-calization performance through suppressing asynchronous audiovisual background frames from the examined events and reducing redundant noise. Specifically, the time-level background suppression scheme forces the audio and visual modality to focus on the related information in the temporal dimension that the opposite modality considers essential, and reduces attention to the segments that the other modal considers as background. The event-level background sup-pression scheme uses the class activation sequences pre-dicted by audio and visual modalities to control the fi-nal event category prediction, which can effectively sup-press noise events occurring accidentally in a single modal-ity. Furthermore, we introduce a cross-modal gated atten-tion scheme to extract relevant visual regions from complex scenes exploiting both global visual and audio signals. Ex-tensive experiments show our method outperforms the state-of-the-art methods by a large margin in both supervised and weakly supervised AVE settings. 1 1.

Introduction
Event location and action recognition [3, 10, 30] have become increasingly important in understanding and ana-lyzing video content, with most methods relying on optical flow and RGB features processing. However, audio can also provide valuable clues for understanding holistic video con-tent [11, 22, 37]. To comprehensively realize how to com-bine audio and visual modalities and understand the video contents, Tian et al. [32] introduce the audio-visual event
*Corresponding author. 1The source code and pre-trained models are publicly available at: https://github.com/marmot-xy/CMBS
Figure 1. An illustration example of AVE task. An audio-visual event will be identified when it is both audible and visible. In this example, only when we see the car and hear the engine sound at the same time (the 2nd and 3rd segments) can we determine an audio-visual event ”running car” happened. (AVE) localization task, where the model determines the presence of an event and localizes its boundary in the tem-poral dimension when the event is both audible and visible at the same time. Figure 1 demonstrates that this way of judging an audio-visual event is necessary, as only simul-taneously observing the car and hearing the engine sound indicates that the car is running. Compared with the tradi-tional video event localization, the challenges of the AVE task mainly exist in the following aspects [1, 21, 38]: i).
Merging the complementary audio and visual features while preserving the simultaneously modal-specific information is not trivial. ii). Sudden noise and complex background existing in the unconstrained videos will hinder the predic-tions of the event categories. iii). AVE requires the event to be audible and visible, while unsynchronized audio and visual information mislead the event boundary prediction.
Early models mainly focused on solving the first chal-lenge by simply fusing the information of the two modali-ties after processing each modality independently [14,32] or aligning audio and visual information and then fusing them by cross attention [4, 34, 36]. However, problems such as event category detection errors caused by the sudden noise existing in single mode or inaccurate temporal event local-ization caused by unsynchronized audio and visual infor-mation are still an open research case.
Unlike previous methods, we consider the problem of audio-visual event localization from the viewpoint of cross-modal background suppression, which can effectively al-leviate challenges (ii) and (iii).