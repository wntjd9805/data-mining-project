Abstract
The networks trained on the long-tailed dataset vary re-markably, despite the same training settings, which shows the great uncertainty in long-tailed learning. To alleviate the uncertainty, we propose a Nested Collaborative Learn-ing (NCL), which tackles the problem by collaboratively learning multiple experts together. NCL consists of two core components, namely Nested Individual Learning (NIL) and
Nested Balanced Online Distillation (NBOD), which focus on the individual supervised learning for each single expert and the knowledge transferring among multiple experts, re-spectively. To learn representations more thoroughly, both
NIL and NBOD are formulated in a nested way, in which the learning is conducted on not just all categories from a full perspective but some hard categories from a partial per-spective. Regarding the learning in the partial perspective, we specifically select the negative categories with high pre-dicted scores as the hard categories by using a proposed
Hard Category Mining (HCM). In the NCL, the learning from two perspectives is nested, highly related and comple-mentary, and helps the network to capture not only global and robust features but also meticulous distinguishing abil-ity. Moreover, self-supervision is further utilized for feature enhancement. Extensive experiments manifest the superi-ority of our method with outperforming the state-of-the-art whether by using a single model or an ensemble. Code is available at https://github.com/Bazinga699/NCL 1.

Introduction
In recent years, deep neural networks have achieved re-sounding success in various visual tasks, i.e., face analy-*The first two authors contributed equally to this work
†Corresponding author
Figure 1. The comparisons of model outputs (logits) and Kull-back–Leibler (KL) distance between two networks that are trained from scratch. Analysis is conducted on CIFAR100-LT dataset with
Imbalanced Factor (IF) of 100. The logits are visualized on the basis of a random selected example, and the KL distance is com-puted based on the whole test set and then the average results of each category are counted and reported. Although the employed two networks have the same network structure and training set-tings, their predictions differ largely from each other especially in tail classes. Bested viewed in color. sis [47, 61], action and gesture recognition [39, 65]. De-spite the advances in deep technologies and computing ca-pability, the huge success also highly depends on large well-designed datasets of having a roughly balanced distribution, such as ImageNet [12], MS COCO [35] and Places [64].
This differs notably from real-world datasets, which usu-ally exhibit long-tailed data distributions [37, 51] where few head classes occupy most of the data while many tail classes have only few samples. In such scenarios, the model is easily dominated by those few head classes, whereas low accuracy rates are usually achieved for many other tail classes. Undoubtedly, the long-tailed characteristics chal-lenges deep visual recognition, and also immensely hinders the practical use of deep models.
In long-tailed visual recognition, several works focus on designing the class re-balancing strategies [11,17,24,44,45, 51] and decoupled learning [4, 27]. More recent efforts aim to improve the long-tailed learning by using multiple ex-perts [2, 33, 50, 53, 58]. The multi-expert algorithms follow a straightforward idea of complementary learning, which means that different experts focus on different aspects and each of them benefits from the specialization in the domi-nating part. For example, LFME [53] formulates a network with three experts and it forces each expert learn samples from one of head, middle and tail classes. Previous multi-expert methods [2, 50, 53], however, only force each expert to learn the knowledge in a specific area, and there is a lack of cooperation among them.
Our motivation is inspired by a simple experiment as shown in Fig. 1, where the different networks vary con-siderably, particularly in tail classes, even if they have the same network structure and the same training settings. This signifies the great uncertainty in the learning process. One reliable solution to alleviate the uncertainty is the collabo-rative learning through multiple experts, namely, that each expert can be a teacher to others and also can be a stu-dent to learn additional knowledge of others. Grounded in this, we propose a Nested Collaborative Learning (NCL) for long-tailed visual recognition. NCL contains two main important components, namely Nested Individual Learning (NIL) and Nested Balanced Online Distillation (NBOD), the former of which aims to enhance the discriminative ca-pability of each network, and the later collaboratively trans-fers the knowledge among any two experts. Both NCL and NBOD are performed in a nested way, where the NCL or NBOD conducts the supervised learning or distillation from a full perspective on all categories, and also imple-ments that from a partial perspective of focusing on some important categories. Moreover, we propose a Hard Cat-egory Mining (HCM) to select the hard categories as the important categories, in which the hard category is defined as the category that is not the ground-truth category but with a high predicted score and easily resulting to misclassifica-tion. The learning manners from different perspectives are nested, related and complementary, which facilitates to the thorough representations learning. Furthermore, inspired by self-supervised learning [18], we further employ an ad-ditional moving average model for each expert to conduct self-supervision, which enhances the feature learning in an unsupervised manner.
In the proposed NCL, each expert is collaboratively learned with others, where the knowledge transferring be-tween any two experts is allowed. NCL promotes each ex-pert model to achieve better and even comparable perfor-mance to an ensemble’s. Thus, even if a single expert is used, it can be competent for prediction. Our contributions can be summarized as follows:
• We propose a Nested Collaborative Learning (NCL) to collaboratively learn multiple experts concurrently, which allows each expert model to learn extra knowl-edge from others.
• We propose a Nested Individual Learning (NIL) and
Nested Balanced Online Distillation (NBOD) to con-duct the learning from both a full perspective on all categories and a partial perspective of focusing on hard categories.
• We propose a Hard Category Mining (HCM) to greatly reduce the confusion with hard negative categories.
• The proposed method gains significant performance over the state-of-the-art on five popular datasets in-cluding CIFAR-10/100-LT, Places-LT, ImageNet-LT and iNaturalist 2018. 2.