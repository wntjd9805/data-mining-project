Abstract
Synthesizing high-quality realistic images from text de-scriptions is a challenging task. Existing text-to-image Gen-erative Adversarial Networks generally employ a stacked architecture as the backbone yet still remain three flaws.
First, the stacked architecture introduces the entanglements between generators of different image scales. Second, ex-isting studies prefer to apply and fix extra networks in adversarial learning for text-image semantic consistency, which limits the supervision capability of these networks.
Third, the cross-modal attention-based text-image fusion that widely adopted by previous works is limited on several special image scales because of the computational cost. To these ends, we propose a simpler but more effective Deep
Fusion Generative Adversarial Networks (DF-GAN). To be specific, we propose: (i) a novel one-stage text-to-image backbone that directly synthesizes high-resolution images without entanglements between different generators, (ii) a novel Target-Aware Discriminator composed of Matching-Aware Gradient Penalty and One-Way Output, which en-hances the text-image semantic consistency without intro-ducing extra networks, (iii) a novel deep text-image fu-sion block, which deepens the fusion process to make a full fusion between text and visual features. Compared with current state-of-the-art methods, our proposed DF-GAN is simpler but more efficient to synthesize realistic and text-matching images and achieves better performance on widely used datasets. Code is available at https:
//github.com/tobran/DF-GAN . 1.

Introduction
The last few years have witnessed the great success of
Generative Adversarial Networks (GANs) [8] for a variety of applications [4, 27, 48]. Among them, text-to-image syn-thesis is one of the most important applications of GANs.
*Corresponding Author
Figure 1. (a) Existing text-to-image models stack multiple gener-ators to generate high-resolution images. (b) Our proposed DF-GAN generates high-quality images directly and fuses the text and image features deeply by our deep text-image fusion blocks.
It aims to generate realistic and text-consistent images from the given natural language descriptions. Due to its prac-tical value, text-to-image synthesis has become an active research area recently [3, 9, 13, 19–21, 32, 33, 35, 51, 53, 60].
Two major challenges for text-to-image synthesis are the authenticity of the generated image, and the semantic con-sistency between the given text and the generated image.
Due to the instability of the GAN model, most recent mod-els adopt the stacked architecture [56,57] as the backbone to generate high-resolution images. They employ cross-modal attention to fuse text and image features [37, 50, 56, 57, 60] and then introduce DAMSM network [50], cycle consis-tency [33], or Siamese network [51] to ensure the text-image semantic consistency by extra networks.
Although impressive results have been presented by pre-vious works [9,19,21,32,33,51,60], there still remain three problems. First, the stacked architecture [56] introduces en-tanglements between different generators, and this makes the final refined images look like a simple combination of fuzzy shape and some details. As shown in Figure 1(a), the final refined image has a fuzzy shape synthesized by
G0, coarse attributes (e.g., eye and beak) synthesized by
G1, and fine-grained details (e.g., eye reflection) added by
G2. The final synthesized image looks like a simple combi-nation of visual features from different image scales. Sec-ond, existing studies usually fix the extra networks [33, 50] during the adversarial training, making these networks eas-ily fooled by the generator to synthesize adversarial fea-tures [30, 52], thereby weakening their supervision power on semantic consistency. Third, cross-modal attention [50] can not make full use of text information. They can only be applied two times on 64×64 and 128×128 image features due to its high computational cost. It limits the effectiveness of the text-image fusion process and makes the model hard to extend to higher-resolution image synthesis.
To address the above issues, we propose a novel text-to-image generation method named Deep Fusion Genera-tive Adversarial Network (DF-GAN). For the first issue, we replace the stacked backbone with a one-stage back-bone. It is composed of hinge loss [54] and residual net-works [11] which stabilizes the GAN training process to synthesize high-resolution images directly. Since there is only one generator in the one-stage backbone, it avoids the entanglements between different generators.
For the second issue, we design a Target-Aware Dis-criminator composed of Matching-Aware Gradient Penalty (MA-GP) and One-Way Output to enhance the text-image semantic consistency. MA-GP is a regularization strategy on the discriminator. It pursues the gradient of discrimina-tor on target data (real and text-matching image) to be zero.
Thereby, the MA-GP constructs a smooth loss surface at real and matching data points which further promotes the generator to synthesize text-matching images. Moreover, considering that the previous Two-Way Output slows down the convergence process of the generator under MA-GP, we replace it with a more effective One-Way Output.
For the third issue, we propose a Deep text-image Fusion
Block (DFBlock) to fuse the text information into image features more effectively. The DFBlock consists of several
Affine Transformations [31]. The Affine Transformation is a lightweight module that manipulates the visual feature maps through channel-wise scaling and shifting operation.
Stacking multiple DFBlocks at all image scales deepens the text-image fusion process and makes a full fusion between text and visual features.
Overall, our contributions can be summarized as follows:
• We propose a novel Deep text-image Fusion Block (DFBlock), which fully fuses text and visual features more effectively and deeply.
• Extensive qualitative and quantitative experiments on two challenging datasets demonstrate that the pro-posed DF-GAN outperforms existing state-of-the-art text-to-image models. 2.