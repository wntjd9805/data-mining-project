Abstract 1.

Introduction
Although achieving significant progress, existing deep generative inpainting methods still show low generalization across different scenes. As a result, the generated images usually contain artifacts or the filled pixels differ greatly from the ground truth, making them far from real-world ap-plications. Image-level predictive filtering is a widely used restoration technique by predicting suitable kernels adap-tively according to different input scenes. Inspired by this inherent advantage, we explore the possibility of addressing image inpainting as a filtering task. To this end, we first study the advantages and challenges of the image-level predictive filtering for inpainting: the method can preserve local struc-tures and avoid artifacts but fails to fill large missing areas.
Then, we propose the semantic filtering by conducting filter-ing on deep feature level, which fills the missing semantic information but fails to recover the details. To address the is-sues while adopting the respective advantages, we propose a novel filtering technique, i.e., Multi-level Interactive Siamese
Filtering (MISF) containing two branches: kernel predic-tion branch (KPB) and semantic & image filtering branch (SIFB). These two branches are interactively linked: SIFB provides multi-level features for KPB while KPB predicts dynamic kernels for SIFB. As a result, the final method takes the advantage of effective semantic & image-level filling for high-fidelity inpainting. Moreover, we discuss the relation-ship between MISF and the naive encoder-decoder-based inpainting, inferring that MISF provides novel dynamic con-volutional operations to enhance the high generalization capability across scenes. We validate our method on three challenging datasets, i.e., Dunhuang, Places2, and CelebA.
Our method outperforms state-of-the-art baselines on four metrics, i.e., L1, PSNR, SSIM, and LPIPS.
*Xiaoguang Li and Qing Guo are co-first authors and contribute equally.
†Wei Feng (wfeng@ieee.org) and Song Wang (songwang@cec.sc.edu) are the corresponding authors. Please try the released code and model in https://github.com/tsingqguo/misf.
Image inpainting is a fundamental problem in computer vision and artificial intelligence applications. The main goal is to fill missing pixels in an image and make it identical to the clean one. Recent works mainly address the task by modeling it as a generation task [17, 19, 24, 30]. As a result, they can employ cutting-edge deep generative techniques (e.g., generative adversarial network [10, 22]) to realize high-quality restoration on challenging datasets. However, the generative network-based inpainting encodes the input image to a latent space and then decodes it to a new image. Such a process neglects the explicit prior, i.e., smoothness across neighboring pixels or features, and the fidelity of inpainting fully relies on the data and training strategy.
Note that, different from the generation task, image in-painting has its specific challenges: First, the image inpaint-ing requires the completed images to respect the clean im-age (i.e., to produce high-fidelity images) and to be natural.
These requirements make image inpainting different from the pure image generation task that mainly focuses on natu-ralness. Second, the missing areas’ shapes may be different and the background scenes are diverse. These facts require the inpainting method to have high generalization capability across missing areas and scenes. Although deep generative networks achieve significant progress on image inpainting, they are far from solving the above challenges. For example, the recent work RFRNet [17] conducts feature reasoning on the encoder-decoder network and achieves state-of-the-art performance on public datasets. Nevertheless, given differ-ent faces with different missing areas, it is hard to produce high-fidelity inpainting results. Moreover, the artifacts ap-pear in the results. As shown in Fig. 1, for the top-left example with small missing areas, RFRNet can generate a natural face. However, when comparing with the ground truth, we see that the local structures around the arrows are distorted. For the bottom-left example with larger missing areas, RFRNet even fails to produce a natural face. When handling other natural scenes (e.g., two examples on the
Figure 1. Four examples of using state-of-the-art methods (i.e., RFRNet [17] and JPGNet [12]) and the proposed method for image inpainting. Our method is able to complete the missing pixels and produce realistic and high-fidelity images. We highlight the main differences via green arrows. right), RFRNet also introduces small artifacts.
Guo et al. [12] have noticed above issues of generative-based inpainting methods [12,17,23,25] and propose JPGNet that uses image-level predictive filtering to alleviate the arti-facts. The image-level predictive filtering reconstructs pixels via their neighboring pixels. The filtering kernels are adap-tively estimated according to the inputs. As a result, JPGNet can recover the local structure while avoiding the artifacts, thus helping RFRNet achieve significant quality improve-ment. Nevertheless, many details are smoothed, while the real structures fail to be recovered (See Fig. 1).
Inspired by the inherent advantages of predictive filter-ing on adaptiveness and restoration, we propose a novel framework to handle the two challenges. Specifically, we make three main efforts: First, we study the advantages and challenges of adopting the existing predictive filtering method for image inpainting, that is, the image-level predic-tive filtering can restore local structures and avoid artifacts but cannot fill large missing areas. Second, we extend the image-level filtering to the deep feature level and propose the semantic filtering, which can complete large missing areas but loses details. Third, to address the issues, we pro-pose a novel filtering technique, i.e., Multi-level Interactive
Siamese Filtering (MISF), which contains two branches: ker-nel prediction branch (KPB) and semantic & image filtering branch (SIFB). These two branches are interactively linked at semantic & pixel levels. SIFB provides multi-level fea-tures for KPB while KPB predicts dynamic kernels for SIFB.
MISF can utilize the smoothness prior across neighbors ex-plicitly and reconstruct clean pixels or features by linearly combining the neighbors. As a result, the final method takes the advantage of effective semantic & pixel-level filling for high-fidelity inpainting. As shown in Fig. 1, our method can generate natural and high-fidelity images under different scenes with different missing areas. In addition, we conduct an insightful discussion about the relationship between our method and the naive generative network, inferring that our method corresponds multi-level dynamic convolutional op-erations that adjusts the convolutional parameters according to different inputs and brings in generalization. We conduct extensive experiments on three challenging datasets (i.e.,
Place2, CelebA, and Dunhuang) and achieve much better scores than the competitive methods on the public datasets in terms of four quality metrics. 2.