Abstract
While today’s video recognition systems parse snapshots or short clips accurately, they cannot connect the dots and reason across a longer range of time yet. Most existing video architectures can only process <5 seconds of a video without hitting the computation or memory bottlenecks.
In this paper, we propose a new strategy to overcome this challenge.
Instead of trying to process more frames at once like most existing methods, we propose to process videos in an online fashion and cache “memory” at each iteration. Through the memory, the model can reference prior context for long-term modeling, with only a marginal cost. Based on this idea, we build MeMViT, a Memory-augmented Multiscale Vision Transformer, that has a tem-poral support 30×longer than existing models with only 4.5% more compute; traditional methods need >3,000% more compute to do the same. On a wide range of settings, the increased temporal support enabled by MeMViT brings large gains in recognition accuracy consistently. MeMViT obtains state-of-the-art results on the AVA, EPIC-Kitchens-100 action classification, and action anticipation datasets.
Code and models will be made publicly available. 1.

Introduction
Our world evolves endlessly over time. The events at dif-ferent points in time influence each other and all together, they tell the story of our visual world. Computer vision promises to understand this story, but today’s systems are still quite limited. They accurately parse visual content in independent snapshots or short time periods (e.g., 5 sec-onds), but not beyond that. So, how can we enable accurate long-term visual understanding? There are certainly many challenges ahead, but having a model that practically runs on long videos is arguably an important first step.
In this paper, we propose a memory-based approach for building efficient long-term models. The central idea is that (a) Traditional long-term models vs. our method, MeMViT. (b) MeMViT
Figure 1. MeMViT is a class of video models that models long videos efficiently. It has a significantly better trade-off than tra-ditional methods, which increase the temporal support of a video model by increasing the number of frames in model input (Fig. 1a).
MeMViT achieves efficient long-term modeling by hierarchically attending the previously cached “memory” of the past (Fig. 1b). instead of aiming to jointly process or train on the whole long video, we simply maintain “memory” as we process a video in an online fashion. At any point of time, the model has access to prior memory for long-term context. Since the memory is ‘reused’ from the past, the model is highly effi-cient. To implement this idea, we build a concrete model called MeMViT, a Memory-augmented Multiscale Vision
Transformer. MeMViT processes 30× longer input dura-tion than existing models, with only 4.5% more compute. In comparison, a long-term model built by increasing the num-ber of frames will require >3,000% more compute. Fig. 1a presents the trade-off comparison in compute/duration. as a concrete instance, but the general idea can be applied to other ViT-based video models.
More concretely, MeMViT uses the “keys” and “values” of a transformer [68] as memory. When the model runs on one clip, the “queries” attend to an extended set of “keys” and “values”, which come from both the current time and the past. When performing this at multiple layers, each layer attends further down into the past, resulting in a sig-nificantly longer receptive field, as illustrated in Fig. 1b.
To further improve the efficiency, we jointly train a mem-ory compression module for reducing the memory footprint.
Intuitively, this allows the model to learn which cues are im-portant for future recognition and keeps only those.
Our design is loosely inspired by how humans parse long-term visual signals. Humans do not process all signals over a long period of time at once. Instead, humans process signals in an online fashion, associate what we see to past memory to make sense of it, and also memorize important information for future use.
Our results demonstrate that augmenting video models with memory and enabling long range attention is simple and very beneficial. On the AVA spatiotemporal action lo-calization [31], the EPIC-Kitchens-1001 action classifica-tion [13,14], and the EPIC-Kitchens-100 action anticipation datasets [13, 14], MeMViT obtains large performance gains over its short-term counterpart and achieves state-of-the-art results. We hope these results are helpful for the community and take us one step closer to understanding the interesting long story told by our visual world. 2.