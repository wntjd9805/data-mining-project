Abstract
Although progress has been made for text-to-image syn-thesis, previous methods fall short of generalizing to unseen or underrepresented attribute compositions in the input text.
Lacking compositionality could have severe implications for robustness and fairness, e.g., inability to synthesize the face images of underrepresented demographic groups. In this paper, we introduce a new framework, StyleT2I, to improve the compositionality of text-to-image synthesis. Specifically, we propose a CLIP-guided Contrastive Loss to better dis-tinguish different compositions among different sentences.
To further improve the compositionality, we design a novel
Semantic Matching Loss and a Spatial Constraint to identify attributes’ latent directions for intended spatial region ma-nipulations, leading to better disentangled latent representa-tions of attributes. Based on the identified latent directions of attributes, we propose Compositional Attribute Adjustment to adjust the latent code, resulting in better compositionality of image synthesis. In addition, we leverage the ℓ2-norm regularization of identified latent directions (norm penalty) to strike a nice balance between image-text alignment and image fidelity. In the experiments, we devise a new dataset split and an evaluation metric to evaluate the compositional-ity of text-to-image synthesis models. The results show that
StyleT2I outperforms previous approaches in terms of the consistency between the input text and synthesized images and achieves higher fidelity. 1.

Introduction
Text-to-image synthesis is a task to synthesize an image conditioned on given input text, which enables many down-stream applications, such as art creation, computer-aided design, and training data generation for augmentation. Al-though progress has been made for this task, the composition-ality aspect is overlooked by many previous methods [39].
As shown in Fig. 1, the input text “he1 is wearing lipstick” describes an intersectional group [3] between two attributes— 1In this work, the gender and gender pronouns denote the visually perceived gender, which does not indicate one’s actual gender identity.
Figure 1. When the text input contains underrepresented com-positions of attributes, e.g., (he, wearing lipstick), in the dataset, previous methods [30,51,64] incorrectly generate the attributes with poor image quality. In contrast, StyleT2I achieves better composi-tionality and high-fidelity text-to-image synthesis results.
“he” and “wearing lipstick,” which is underrepresented in a face dataset [18]. The previous approaches [30,51,64] fail to correctly synthesize the image, which could be caused by overfitting to the overrepresented compositions, e.g., (“she”,
“wearing lipstick”) and (“he”, not “wearing lipstick”), in the dataset. This leads to severe robustness and fairness issues by inheriting biases and stereotypes from the dataset. There-fore, it is imperative to improve the text-to-image synthesis results in the aspect of compositionality.
The crux of the compositionality problem is to prevent models from simply memorizing the compositions in the training data. First, in terms of the objective function, some previous methods [64,65] simply minimize the feature dis-tance between pairwise matched image and text, leading to poor generalizability. In contrast, we propose a CLIP-guided Contrastive Loss to let the network better distinguish different compositions among different sentences, in which
CLIP (Contrastive Language–Image Pre-training) [47] is pre-trained on large-scale matched image-text pairs as a foun-dation model [2]. Second, the compositional text-to-image model needs to be sensitive to each independent attribute described in the text. Most previous methods [30,68,71,75] mainly resort to attention mechanism [60], which focuses more on the correspondence between words and image fea-tures but falls short of separating individual attributes from a composition. Unlike previous approaches, our key idea is to identify disentangled representations [6,14] in the la-tent space of a generative model, where each disentangled representation exclusively corresponds to one attribute in the dataset. By leveraging the disentangled representations of different attributes, we can improve the compositionality by ensuring that each attribute described in the sentence is correctly synthesized.
Motivated by these ideas, we present StyleT2I, a novel framework to improve the compositionality of text-to-image synthesis employing StyleGAN [19]. In specific, we propose a CLIP-guided Contrastive Loss to train a network to find the StyleGAN’s latent code semantically aligned with the input text and better distinguish different compositions in different sentences. To further improve the composition-ality, we propose a Semantic Matching Loss and a Spatial
Constraint for identifying attributes’ latent directions that induce intended spatial region manipulations. This leads to a better disentanglement of latent representations for different attributes. Then we propose Compositional Attribute Adjust-ment to correct the wrong attribute synthesis by adjusting the latent code based on identified attribute directions during the inference stage. However, we empirically found that optimizing the proposed losses above can sometimes lead to degraded image quality. To address this issue, we employ norm penalty to strike a nice balance between image-text alignment and image fidelity.
To better evaluate the compositionality of text-to-image synthesis, we devise a test split for the CelebA-HQ [18] dataset, where the test text only contains unseen composi-tions of attributes. We design a new evaluation metric for the CUB [61] dataset to evaluate if the synthesized image is in the correct bird species. Extensive quantitative results, qualitative results, and user studies manifest the advantages of our method on both image-text alignment and fidelity for compositional text-to-image synthesis.
We summarize our contributions as follows: (1) We propose StyleT2I, a compositional text-to-image synthesis framework with a novel CLIP-guided Contrastive Loss and
Compositional Attribute Adjustment. To the best of our knowledge, this is the first text-to-image synthesis work that focuses on improving the compositionality of different at-tributes. (2) We propose a novel Semantic Matching Loss and a Spatial Constraint for identifying attributes’ latent direc-tions that induce intended variations in the image space, lead-ing to a better disentanglement among different attributes. (3) We devise a new test split and an evaluation metric to bet-ter evaluate the compositionality of text-to-image synthesis. 2.