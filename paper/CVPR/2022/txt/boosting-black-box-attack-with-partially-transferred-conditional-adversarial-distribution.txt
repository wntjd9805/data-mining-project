Abstract
This work studies black-box adversarial attacks against deep neural networks (DNNs), where the attacker can only access the query feedback returned by the attacked DNN model, while other information such as model parameters or the training datasets are unknown. One promising ap-proach to improve attack performance is utilizing the ad-versarial transferability between some white-box surrogate models and the target model (i.e., the attacked model). How-ever, due to the possible differences on model architectures and training datasets between surrogate and target models, dubbed “surrogate biases”, the contribution of adversarial transferability to improving the attack performance may be weakened. To tackle this issue, we innovatively propose a black-box attack method by developing a novel mechanism of adversarial transferability, which is robust to the surrogate biases. The general idea is transferring partial parameters of the conditional adversarial distribution (CAD) of surro-gate models, while learning the untransferred parameters based on queries to the target model, to keep the flexibility to adjust the CAD of the target model on any new benign sample. Extensive experiments on benchmark datasets and attacking against real-world API demonstrate the superior attack performance of the proposed method. The code will be available at https://github.com/Kira0096/CGATTACK. 1.

Introduction
It has been well known [4, 14] that adversarial examples are serious threats to deep neural networks (DNNs). Existing adversarial attacks can be generally partitioned into two main categories. The first category is white-box attack [14], where the attacker can access parameters of the attacked DNN model. The second one is black-box attack [12], where the
* Part of this work was done when he was an intern at Tencent AI Lab.
† Corresponds to Baoyuan Wu (wubaoyuan@cuhk.edu.cn) and Shu-Tao Xia (xiast@sz.tsinghua.edu.cn). attacker can only access the query feedback returned by the attacked model, while model parameters are unknown to the attacker. Since it is difficult to access model parameters in real-world scenarios, black-box attack is more practical, and it is also the main focus of this work.
If only utilizing the query feedback, it is difficult to achieve high attack success rate under limited query budgets.
One promising approach to improve the attack performance, including attack success rate and query efficiency, is utiliz-ing the adversarial transferability [10, 11, 54] between some white-box surrogate models and the target model (i.e., the at-tacked model). Many adversarial transferabilities have been proposed in existing works, such as the gradient [9, 17], or the projection from a low-dimensional space to the original sample space [25], etc. These transferabilities have shown positive contributions to improving the attack performance in some black-box attack scenarios, especially in the closed-set scenario, where the training dataset of the target model is known to the attacker. However, their effects may be signifi-cantly influenced by the differences between surrogate and target models. More precisely, architectures between surro-gate and target models may be different, probably leading to different feedback to the same query. Secondly, under the practical scenario of open-set black-box attack where the training dataset is unknown to the attacker, even using the same architecture, different training sets (including samples and class labels) will also lead to different parameters. We generally summarize the differences, caused by architectures and training datasets between surrogate and target models as surrogate biases. If the biases are too large, the transferred information may mislead the search of adversarial perturba-tion for attacking the target model, causing the degradation of the contribution of adversarial transferability to improving the attack performance (as demonstrated later in Sec. 4.3).
To mitigate the above issue, the transferred term should be not only informative but also robust to surrogate biases. To this end, we focus on the conditional adversarial distribu-tion (CAD) (i.e., the distribution of adversarial perturbations conditioned on benign examples). If the transferred CAD accurately fits the target model, it will be helpful to search successful adversarial perturbations for attacking the target model. Besides, note that CAD is independent with class labels, thus transferring CAD will be robust to the surro-gate bias of training class labels. However, CAD can be influenced by the biases of model architectures and train-ing samples. Thus, we propose a novel transfer mechanism that only partial parameters of CAD are transferred, while the remaining parameters are learned according to the query feedback returned by the target model on the attacked benign sample. Consequently, the CAD of the target model condi-tioned on any new benign sample could be flexibly adjusted, such that the possible negative effect due to surrogate biases of architectures and training samples could be mitigated.
One remaining important issue is how to accurately model the CAD. Here we adopt the conditional generative flow model, called c-Glow [39], whose general idea is invertibly mapping a simple distribution (e.g., Gaussian distribution) to a complex distribution through an invertible network, as shown in Fig. 1(a). c-Glow has shown powerful ability of capturing complex data distributions [39], and we believe that it is capable enough to capture the CAD. To the best of our knowledge, this is the first work to use c-Glow to approx-imate the CAD. Besides, we develop an efficient training algorithm of the c-Glow model based on randomly sam-pled perturbations, rather than costly generated adversarial perturbations, such that the CAD of surrogate models can be efficiently and accurately approximated. Extensive ex-periments are conducted to verify the effectiveness of the proposed attack method, including black-box attack scenar-ios of both closed-set and open-set on benchmark datasets, as well as the attack against real-world API.
In summary, the main contributions of this work are three-fold. 1) We propose an effective and efficient black-box at-tack method by designing a novel adversarial transfer mecha-nism that only partial parameters of the conditional adversar-ial distribution are transferred, which is robust to surrogate biases between surrogate and target models. 2) We are the first to approximate the CAD by the c-Glow model, and de-sign an efficient training algorithm based on randomly sam-pled perturbations. 3) Extensive experiments demonstrate the superiority of the proposed attack method to several state-of-the-art (SOTA) black-box methods by improving attack success rate and query efficiency simultaneously. 2.