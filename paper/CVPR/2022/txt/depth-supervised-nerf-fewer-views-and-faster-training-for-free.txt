Abstract
A commonly observed failure mode of Neural Radiance
Field (NeRF) is fitting incorrect geometries when given an insufficient number of input views. One potential reason is that standard volumetric rendering does not enforce the constraint that most of a scene’s geometry consist of empty space and opaque surfaces. We formalize the above assump-tion through DS-NeRF (Depth-supervised Neural Radiance
Fields), a loss for learning radiance fields that takes ad-vantage of readily-available depth supervision. We leverage the fact that current NeRF pipelines require images with known camera poses that are typically estimated by running structure-from-motion (SFM). Crucially, SFM also produces sparse 3D points that can be used as “free” depth super-vision during training: we add a loss to encourage the dis-tribution of a ray’s terminating depth matches a given 3D keypoint, incorporating depth uncertainty. DS-NeRF can ren-der better images given fewer training views while training 2-3x faster. Further, we show that our loss is compatible with other recently proposed NeRF methods, demonstrating that depth is a cheap and easily digestible supervisory signal.
And finally, we find that DS-NeRF can support other types of depth supervision such as scanned depth sensors and RGB-D reconstruction outputs. 1.

Introduction
Neural rendering with implicit representations has be-come a widely-used technique for solving many vision and graphics tasks ranging from view synthesis [5, 15, 25], to re-lighting [12, 13], to pose and shape estimation [17, 21, 31], to 3D-aware image synthesis and editing [3, 11, 23], to mod-eling dynamic scenes [9,18,19]. The seminal work of Neural
Radiance Fields (NeRF) [15] demonstrated impressive view synthesis results by using implicit functions to encode volu-metric density and color observations.
Figure 2. Few view NeRF. NeRF is susceptible to overfitting when given few training views. As seen by the PSNR gap between train and test renders (left), NeRF has overfit and fails at synthesizing novel views. Further, the depth map (right) and depth error (middle) for
NeRF suggest that its density function has failed to extract the surface geometry and can only reconstruct the training views’ colors. Our depth-supervised NeRF model is able to render plausible geometry with consistently lower depth errors.
In spite of this, NeRF has several limitations. Reconstruct-ing both the scene appearance and geometry can be ill-posed given a small number of input views. Figure 2 shows that
NeRF can learn wildly inaccurate scene geometries that still accurately render train-views. However, such models pro-duce poor renderings of novel test-views, essentially overfit-ting to the train set. Furthermore, even given a large number of input views, NeRF can still be time-consuming to train; it often takes between ten hours to several days to model a single scene at moderate resolutions on a single GPU. The training is slow due to both the expensive ray-casting opera-tions and lengthy optimization process.
In this work, we explore depth as an additional, cheap source of supervision to guide the geometry learned by NeRF.
Typical NeRF pipelines require images and camera poses, where the latter are estimated from structure-from-motion (SFM) solvers such as COLMAP [22]. In addition to return-ing cameras, COLMAP also outputs sparse 3D point clouds as well as their reprojection errors. We impose a loss to en-courage the distribution of a ray’s termination to match the 3D keypoint, incorporating reprojection error as an uncer-tainty measure. This is a significantly stronger signal than reconstructing only RGB. Without depth supervision, NeRF is implicitly solving a 3D correspondence problem between multiple views. However, the sparse version of this exact problem has already been solved by SFM, whose solution is given by the sparse 3D keypoints. Therefore depth super-vision improves NeRF by (softly) anchoring its search over implicit correspondences with sparse explicit ones.
Our experiments show that this simple idea translates to massive improvements in training NeRFs and its variations, regarding both the training speed and the amount of training data needed. We observe that depth-supervised NeRF can accelerate model training by 2-3x while producing results with the same quality. For sparse view settings, experiments show that our method synthesizes better results compared to the original NeRF and recent sparse-views NeRF models [26, 33] on both NeRF Real [15] and Redwood-3dscan [6] We also show that our depth supervision loss works well with depth derived from other sources such as a depth camera.
Our code and more results are available at https://www. cs.cmu.edu/˜dsnerf/. Check the full version of the paper at https://arxiv.org/abs/2107.02791. 2.