Abstract
We introduce the visual acoustic matching task, in which an audio clip is transformed to sound like it was recorded in a target environment. Given an image of the target envi-ronment and a waveform for the source audio, the goal is to re-synthesize the audio to match the target room acoustics as suggested by its visible geometry and materials. To ad-dress this novel task, we propose a cross-modal transformer model that uses audio-visual attention to inject visual prop-erties into the audio and generate realistic audio output. In addition, we devise a self-supervised training objective that can learn acoustic matching from in-the-wild Web videos, despite their lack of acoustically mismatched audio. We demonstrate that our approach successfully translates hu-man speech to a variety of real-world environments depicted in images, outperforming both traditional acoustic match-ing and more heavily supervised baselines. 1.

Introduction
The audio we hear is always transformed by the space we are in, as a function of the physical environment’s geometry, the materials of surfaces and objects in it, and the locations of sound sources around us. This means that we perceive the same sound differently depending on where we hear it. For example, imagine a person singing a song while standing on the hardwood stage in a spacious auditorium versus in a cozy living room with shaggy carpet. The underlying song content would be identical, but we would experience it in two very different ways.
For this reason, it is important to model room acoustics to deliver a realistic and immersive experience for many applications in augmented reality (AR) and virtual reality (VR). Hearing sounds with acoustics inconsistent with the scene is disruptive for human perception. In AR/VR, when the real space and virtually reproduced space have different acoustic properties, it causes a cognitive mismatch and the
“room divergence effect” damages the user experience [63].
Creating audio signals that are consistent with an envi-ronment has a long history in the audio community. If the geometry (often in the form of a 3D mesh) and material
Figure 1. Goal of visual acoustic matching: transform the sound recorded in one space to another space depicted in the target visual scene. For example, given source audio recorded in a studio, re-synthesize that audio to match the room acoustics of a concert hall. properties of the space are known, simulation techniques can be applied to generate a room impulse response (RIR), a transfer function between the sound source and the micro-phone that describes how the sound gets transformed by the space. RIRs can then be convolved with an arbitrary source audio signal to generate the audio signals received by the microphone [8, 9, 17, 50, 51]. In the absence of geometry and material information, the acoustical properties can be estimated blindly from audio captured in that room (e.g., re-verberant speech), then used to auralize a signal [29,42,56].
However, both approaches have practical limitations: the former requires access to the full mesh and material prop-erties of the target space, while the latter gets only limited acoustic information about the target space from the rever-beration in the audio sample. Neither uses imagery of the target scene to perform acoustic matching.
We propose a novel task: visual acoustic matching.
Given an image of the target environment and a source au-dio clip, the goal is to re-synthesize the audio as if it were recorded in the target environment (see Figure 1). The idea is to transform sounds from one space to another space by altering their scene-driven acoustic signatures. Visual
acoustic matching has many potential applications, includ-ing smart video editing where a user can inject sounding objects into new backgrounds, film dubbing to make a dif-ferent actor’s voice sound appropriate for the movie scene, audio enhancement for video conference calls, and audio synthesis for AR/VR to make users feel immersed in the visual space displayed to them.
To address visual acoustic matching, we introduce a cross-modal transformer model together with a novel self-supervised training objective that accommodates in-the-wild Web videos having unknown room acoustics.
Our approach accounts for two key challenges: how to faithfully model the complex cross-modal interactions, and how to achieve scalable training data. Regarding the first challenge, different regions of a room affect the acoustics in different ways. For example, reflective glass leads to longer reverberation in high frequencies while absorptive ceilings reduce the reverberation more quickly. Our model provides fine-grained audio-visual reasoning by attending to regions of the image and how they affect the acoustics. Further-more, to capture the fine details of reverberation effects— which are typically much smaller in magnitude than the direct signal—we use 1D convolutions to generate time-domain signals directly and apply a multi-resolution gen-erative adversarial audio loss.
Regarding the second key challenge, one would ideally have paired training data consisting of a sound sample not recorded in the target space plus its proper acoustic render-ing for the scene shown in the target image, i.e., a source and target audio for each visual scene in the training set.
However, such a strategy requires either physical access to the pictured environments, or knowledge of their room impulse response functions—either of which severely lim-its the source of viable training data. Meanwhile, though a Web video does exhibit strong correspondence between its visual scene and the scene acoustics, it offers only the audio recorded in the target space. Accounting for these tradeoffs, we propose a self-supervised objective that auto-matically creates acoustically mismatched audio for train-ing with Web videos. The key insight is to use dereverbera-tion and acoustic randomization to alter the original audio’s acoustics while preserving its content.
We demonstrate our approach on challenging real-world sounds and environments, as well as controlled experiments with realistic acoustic simulations in scanned scenes. Our quantitative results and subjective evaluations via human studies show that our model generates audio that matches the target environment with high perceptual quality, outper-forming a state-of-the-art model that has heavier supervi-sion requirements [52] as well as traditional acoustic match-ing models. 2.