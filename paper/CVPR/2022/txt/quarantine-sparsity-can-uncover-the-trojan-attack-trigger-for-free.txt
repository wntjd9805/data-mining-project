Abstract
Trojan attacks threaten deep neural networks (DNNs) by poisoning them to behave normally on most samples, yet to produce manipulated results for inputs attached with a par-ticular trigger. Several works attempt to detect whether a given DNN has been injected with a specific trigger dur-In a parallel line of research, the lot-ing the training. tery ticket hypothesis reveals the existence of sparse sub-networks which are capable of reaching competitive per-formance as the dense network after independent training.
Connecting these two dots, we investigate the problem of
Trojan DNN detection from the brand new lens of spar-sity, even when no clean training data is available. Our crucial observation is that the Trojan features are signif-icantly more stable to network pruning than benign fea-tures. Leveraging that, we propose a novel Trojan network detection regime: first locating a “winning Trojan lottery ticket” which preserves nearly full Trojan information yet only chance-level performance on clean inputs; then re-covering the trigger embedded in this already isolated sub-network. Extensive experiments on various datasets, i.e.,
CIFAR-10, CIFAR-100, and ImageNet, with different net-work architectures, i.e., VGG-16, ResNet-18, ResNet-20s, and DenseNet-100 demonstrate the effectiveness of our pro-posal. Codes are available at https://github.com/
VITA-Group/Backdoor-LTH. 1.

Introduction
Data-driven techniques for artificial intelligence (AI), such as deep neural networks (DNNs), have powered a technological revolution in a number of key application ar-eas in computer vision [6, 28, 47, 66]. However, a criti-cal shortcoming of these pure data-driven learning systems is the lack of test-time and/or train-time robustness: They often learn ‘too well’ during training – so much that (1)
*Equal Contribution.
Figure 1. Overview of our proposal: Weight pruning identifies the
‘winning Trojan ticket’, which can be leveraged for Trojan detec-tion and recovery. the learned model is oversensitive to small input pertur-bations at testing time (known as evasion attacks) [1, 48]; (2) toxic artifacts injected in the training dataset can be memorized during model training and then passed on to the decision-making process (known as poisoning attacks)
[26, 43]. Methods to secure DNNs against different kinds of ‘adversaries’ are now a major focus in research, e.g., ad-versarial detection [4, 24, 75, 79, 80, 86] and robust train-ing [58, 83, 91].
In this paper, we focus on the study of
Trojan attacks (also known as backdoor attacks), the most common threat model on data security [27, 69]. In particu-lar, we aim to address the following question: (Q) How does the model sparsity relate to its train-time robustness against Trojan attacks?
Extensive research work on model pruning [2,35–37,39, 44, 49, 49, 56, 59–61, 63, 65, 95] has shown that the weights of an overparameterized model (e.g., DNN) can be pruned (i.e., sparsified) without hampering its generalization abil-In particular, Lottery Ticket Hypothesis (LTH), first ity. developed in [18], unveiled that there exists a subnetwork, when properly pruned and trained, that can even perform better than the original dense neural network. Such a sub-network is called a winning lottery ticket. In the past, the model sparsity (achieved by pruning) was mainly studied in the non-adversarial learning context, and thereby, the gen-eralization ability is the only metric to define the quality of a sparse network (i.e., a ticket) [8–10, 12, 18–22, 57, 90, 92].
Beyond generalization, some recent work started to ex-plore the connection between model sparsity and model robustness [31, 34, 70, 84, 88]. However, nearly all ex-isting works restricted model robustness to the prediction resilience against test-time (prediction-evasion) adversarial attacks [15, 23, 81], hence not addressing our question (Q).
To the best of our knowledge, the most relevant works to ours include [40, 84], which showed a few motivating re-sults about pruning vs. Trojan attack. Nevertheless, their methods are either indirect [40] or need an ideal assump-tion on the access to the clean (i.e., unpoisoned) finetuning dataset [84]. Specifically, the work [40] showed that it is possible to generate a Trojan attack by modifying model weights. However, there was no direct evidence showing that the Trojan attack is influenced by weight pruning. Fur-ther, the work [84] attempted to promote model sparsity to mitigate the Trojan effect of an attacked model. However, the pruning setup used in [84] has a deficiency: It was as-sumed that finetuning the pruned model can be conducted over the clean validation dataset. In practice, such an as-sumption is too ideal for achieving if the user has no access to the benign dataset. This assumption also prevents us from understanding the true cause of Trojan mitigation, since the possible effect of model sparsity is entangled with finetun-ing on clean data.
Different from [40, 84], we aim to tackle the research question (Q) in a more practical backdoor scenario - without any access to clean training samples. Moreover, our work bridges LTH and backdoor model detection by (i) identi-fying a crucial subnetwork (that we call ‘winning Trojan ticket’; see Fig. 1) with almost unimpaired backdoor infor-mation and near-random clean-set performance; (ii) recov-ering the trigger with the subnetwork and then detecting the backdoor model. We summarize our contributions below:
• We establish the connection between model sparsity and Trojan attack by leveraging LTH-oriented iterative magnitude pruning (IMP). Assisted by LTH, we pro-pose the concept of Trojan ticket to uncover the prun-ing dynamics of the Trojan model.
• We reveal the existence of a ‘winning Trojan ticket’, which preserves the same Trojan attack effectiveness as in the unpruned model. We propose a linear mode connectivity (LMC)-based Trojan score to detect such a winning ticket along the pruning path.
• We show that the backdoor feature encoded in the win-ning Trojan ticket can be used for reverse engineering of Trojan attack for ‘free’, i.e., with no access to clean training samples nor threat model information.
• We demonstrate the effectiveness of our proposal in detecting and recovering Trojan attacks with vari-ous poisoned DNNs using diverse Trojan trigger pat-terns (including basic backdoor attack and clean-label attack) across multiple network architectures (VGG,
ResNet, and DenseNet) and datasets (CIFAR-10/100 and ImageNet). For example, our Trojan recovery method achieves 90% attack performance improve-ment over the state-of-the-art Trojan attack estimation approach if the clean-label Trojan attack [94] is used by the ground-truth adversary. 2.