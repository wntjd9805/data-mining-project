Abstract
In this paper, we present TransMVSNet, based on our exploration of feature matching in multi-view stereo (MVS).
We analogize MVS back to its nature of a feature match-ing task and therefore propose a powerful Feature Match-ing Transformer (FMT) to leverage intra- (self-) and inter-(cross-) attention to aggregate long-range context informa-tion within and across images. To facilitate a better adap-tation of the FMT, we leverage an Adaptive Receptive Field (ARF) module to ensure a smooth transit in scopes of fea-tures and bridge different stages with a feature pathway to pass transformed features and gradients across differ-ent scales. In addition, we apply pair-wise feature corre-lation to measure similarity between features, and adopt ambiguity-reducing focal loss to strengthen the supervi-sion. To the best of our knowledge, TransMVSNet is the first attempt to leverage Transformer into the task of MVS.
As a result, our method achieves state-of-the-art perfor-mance on DTU dataset, Tanks and Temples benchmark, and BlendedMVS dataset. Code is available at https:
//github.com/MegviiRobot/TransMVSNet. 1.

Introduction
Multi-view stereo (MVS) aims to recover the dense 3D presentation with a series of calibrated images, which is an important task of computer vision. Learning-based MVS networks [10, 31, 32] have achieved remarkable progress in terms of reconstruction quality and efficiency. Typically, a
MVS network extracts image features by a CNN and con-structs cost volume via plane sweep algorithm [5] in which source images are warped to the reference view. This cost
*Equal Contribution. This work is done by the author as interns at
Megvii Research.
†Project lead.
‡Corresponding author (liuxiao@foxmail.com).
Figure 1. Comparison with state-of-the-art learning-based MVS methods [3,10,19,26,27,34] on DTU dataset [1] (lower is better) and Tanks and Temples benchmark [12] (higher is better). volume is regularized afterwards to estimate the final depth.
The nature of MVS is a one-to-many feature matching task, in which each pixel of the reference image is supposed to search along the epipolar line in all warped source images and find an optimal depth with the lowest matching cost.
Some recent studies [22, 24] have proven the importance of long-range global context in feature matching tasks. How-ever, given the aforementioned MVS pipeline, there are two main problems. (a) Local features are well captured by con-volutions. The locality of convolved features prevents the perception of global context information, which is essential for robust depth estimation at challenging regions in MVS, e.g. poor texture, repetitive patterns, and non-Lambertian surfaces. (b) Besides, when computing matching costs, the features to be compared are simply extracted respectively from each image itself, which is to say, potential inter-image correspondences are not taken into consideration.
Recently, Transformer [25], which is initially proposed for natural language processing, has drawn considerable at-tention from the computer vision community for their great performance on vision tasks. Since Transformer utilizes the mechanism of attention and positional encoding for context aggregation, rather than convolutions, it is capable of per-ceiving global and positionally relevant context information
in the true sense.
To this end, we propose a novel end-to-end deep neu-ral network, namely TransMVSNet, to which a power-ful Feature Matching Transformer (FMT) is leveraged to strengthen long-range global context aggregation within and between images. To better adapt FMT into an end-to-end learning-based MVS pipeline, we introduce an Adap-tive Receptive Field (ARF) module to ensure a smooth tran-sition from locally aggregated features by CNN to features with a global receptive field by FMT. In order to lower run-time memory requirements and train FMT with supervision from high-resolution depth maps, we bridge different scales with a transformed feature pathway. We apply pair-wise feature correlation to measure the similarity between the reference feature map and each of its source feature maps.
Afterwards, we follow the coarse-to-fine volume regular-ization pattern [10] and adopt focal loss [16], which better handles samples with ambiguous prediction, to end-to-end train the network.
Thanks to the global context-aware information within and between views, TransMVSNet achieves significant im-provement in reconstruction accuracy and completeness si-multaneously on DTU dataset [1] (as shown in Fig. 1(a)).
Moreover, the overwhelming performance of TransMVS-Net can be generalized to more complex scenes, e.g. the intermediate and advanced set of Tanks and Temples bench-mark [12] (as shown in Fig. 1(b)). To the best of our knowl-edge, it is the first attempt that takes advantage of Trans-former in the task of MVS. Consequently, extensive experi-ments indicate that our method achieves state-of-the-art per-formance. We also conduct ablation experiments to demon-strate the effectiveness of each proposed module. Our main contributions are three-fold as follows.
- We propose a novel end-to-end deep neural network based on a Feature Matching Transformer (FMT), namely TransMVSNet, for robust long-range global context aggregation within and across images.
- To better adapt FMT into an end-to-end MVS pipeline, we introduce an ARF module to adaptively adjust the receptive fields of convolved features and apply ambiguity-aware focal loss for training.
- Our method achieves state-of-the-art results on DTU dataset, Tanks and Temples benchmark, and Blended-MVS dataset. map estimation task. However, the memory and computa-tion costs are quite expensive due to its 3D U-Net architec-ture for cost volume regularization. To alleviate this prob-lem, several networks have been proposed and can be cate-gorized into RNN-based recurrent methods [27, 29, 32] and coarse-to-fine multi-stage methods [3,10,30,34], according to regularization patterns, Recurrent methods regularize the 3D cost volumes recurrently, and adopt RNNs to pass fea-tures between different depth hypotheses. Since recurrent methods trade time for space, they are capable of handling images with large resolution but slow in terms of inference speed. Multi-stage methods predict a coarse depth map ini-tially and narrow down the target depth range at a larger resolution based on the previous prediction. Coarse-to-fine methods are able to infer quickly while keeping a relatively small memory consumption.
Though learning-based MVS methods have achieved promising results, there are still challenging problems re-maining, e.g. robust estimation at non-Lambertian and low-texture regions or severely occluded areas. 2.2. Transformer for Feature Matching
Transformer [25] has been widely used in natural lan-guage processing due to its effectiveness and efficiency, and has drawn increasing attention from the computer vision community recently [2, 8, 17, 20, 21]. Considering Trans-former’s natural superiority to capture global context infor-mation by leveraging attention, its ideology has been uti-lized in the task of feature matching.
SuperGlue [22] utilizes self- and cross-attention in the task of sparse feature matching, leveraging both spatial re-lationships and visual appearance of the keypoints. Su-perGlue achieves impressive performance and becomes the new state of the art. LoFTR [24] establishes accurate dense matches with Transformers in a coarse-to-fine manner. By interleaving the self- and cross-attention layers multiple times, LoFTR learns densely arranged and globally con-sented matching priors in ground-truth matches. STTR [14] models the task of stereo depth estimation from a sequence-to-sequence matching perspective. Transformers with al-ternating self- and cross-attention along intra- and inter-epipolar line are adopted to capture long-range associations between feature descriptors. 2.