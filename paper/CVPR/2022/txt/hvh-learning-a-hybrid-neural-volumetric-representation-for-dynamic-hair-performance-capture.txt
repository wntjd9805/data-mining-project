Abstract
Capturing and rendering life-like hair is particularly challenging due to its ﬁne geometric structure, the complex physical interaction and its non-trivial visual appearance.
Yet, hair is a critical component for believable avatars. In this paper, we address the aforementioned problems: 1) we use a novel, volumetric hair representation that is com-posed of thousands of primitives. Each primitive can be rendered efﬁciently, yet realistically, by building on the lat-est advances in neural rendering. 2) To have a reliable con-trol signal, we present a novel way of tracking hair on the strand level. To keep the computational effort manageable, we use guide hairs and classic techniques to expand those into a dense hood of hair. 3) To better enforce temporal consistency and generalization ability of our model, we fur-ther optimize the 3D scene ﬂow of our representation with multiview optical ﬂow, using volumetric raymarching. Our method can not only create realistic renders of recorded multi-view sequences, but also create renderings for new hair conﬁgurations by providing new control signals. We compare our method with existing work on viewpoint syn-thesis and drivable animation and achieve state-of-the-art results. https://ziyanw1.github.io/hvh/ 1.

Introduction
Although notable progress has been made towards the realism of human avatars, cephalic hair is still one of the hardest parts of the human body to capture and render: with usually more than a hundred-thousand components, with complex physical interaction among them and with com-plex interaction with light, which is extraordinarily hard to model. However, it is an important part of our appearance and identity: hair styles can convey everything from reli-gious beliefs to mood or activity. Hence, hair is critically important to make virtual avatars believable and universally usable.
Previous work on mesh based representations [3, 20, 25, 52, 57, 58, 68] has shown promising results on modeling the face and skin. However, they suffer when modeling hair, be-cause meshes are not well suited for representing hair geom-etry. Recent volumetric representations [26, 35] have high
DoF which allows modeling of a changing geometric struc-ture. They have achieved impressive results in 3D scene ac-quisition and rendering from multi-view photometric infor-mation. Compared to other geometric representations like multi-plane images [2,5,34,56,76] or point-based represen-tations [1,19,33,49,65], volumetric representations support a larger range of camera motion for view extrapolation and do not suffer from holes when rendering dynamic geome-try like point-based representations. Furthermore, they can be learned from multi-view RGB data using differentiable volumetric ray marching, without additional MVS methods.
However, one major ﬂaw of volumetric representations is their cubic memory complexity. This problem is particu-larly signiﬁcant for hair, where high resolution is a require-ment. NeRF [35] circumvents the O(n3) memory complex-ity problem by parameterizing a volumetric radiance ﬁeld using an MLP. Given the implicit form, the MLP-based im-plicit function is not limited by spatial resolution. A hi-erarchical structure with a coarse and ﬁne level radiance function is used and an importance resampling based on the coarse level radiance ﬁeld is utilized for boosting sample resolution. Although promising empirical results have been shown, they come with at the advance of high rendering time and the quality is still limited by the coarse level sam-pling resolution. Another limitation of NeRFs is that they were initially designed for static scenes. There is some re-cent work [21, 22, 41, 42, 46, 59, 63, 67, 74] that extends the original NeRF concept to modeling dynamic scenes. How-ever, they are still limited to relatively small motions, do not support drivable animation or are not efﬁcient for rendering.
We present a hybrid representation: by using many vol-umetric primitives, we focus the resolution of the model onto the relevant regions of the 3D space. For each of the volumes, we construct a neural representation that cap-tures the local appearance of the hair in great detail, similar to [24, 27, 47, 63] . However, without explicitly modeling
the dynamics and structure of hair, it would be hard for the model to learn these properties solely through the in-direct supervision of the multi-view appearance. Given that the model learns to position primitives in an unsupervised manner, the model is also prone to overﬁtting as a result of not incorporating any temporal consistency during train-ing. We address the problem of spatio-temporal modeling of dynamic upper head and hair by explicitly modeling hair dynamics at the coarse level and by enforcing temporal con-sistency of the model by multi-view optical ﬂow at the ﬁne level.
Procedurally, we ﬁrst perform hair strand tracking at a coarse level by lifting multi-view optical ﬂow to a 3D scene
ﬂow. To constrain the hair geometry and reduce the im-pact of the noise in multi-view optical ﬂow, we also make sure the tracked hair strands preserve geometric properties like shape, length and curvature across time. As a second step, we attach volumes to hair strands to model the dy-namic scene which can be optimized using differentiable volumetric raymarching. The volumes that are attached to the hair strands are regressed using a decoder that takes per-hair-strand features and a global latent code as input and is aware of the hair speciﬁc structure. Additionally, we further enforce ﬁne 3D ﬂow consistency by rendering the 3D scene
ﬂow of our model into 2D and compare it with the corre-sponding ground truth optical ﬂow. This step is essential for making the model generalize better to unseen motions.
To summarize, the contributions of this work are
• A hybrid neural volumetric representation that binds volumes to guide hair strands for hair performance capture.
• A hair tracking algorithm that utilizes multiview op-tical ﬂow and per-frame hair strand reconstruction while preserving speciﬁc geometric properties like hair strand length and curvature.
• A volumetric ray marching algorithm on 3D scene ﬂow which enables optimization of the position and orien-tation of each volumetric primitive through multiview 2D optical ﬂow.
• A hair speciﬁc volumetric decoder for hair volume re-gression and with awareness of hair structure. 2.