Abstract
Natural language offers a highly intuitive interface for image editing. In this paper, we introduce the first solution for performing local (region-based) edits in generic natural images, based on a natural language description along with an ROI mask. We achieve our goal by leveraging and com-bining a pretrained language-image model (CLIP), to steer the edit towards a user-provided text prompt, with a de-noising diffusion probabilistic model (DDPM) to generate natural-looking results. To seamlessly fuse the edited region with the unchanged parts of the image, we spatially blend noised versions of the input image with the local text-guided diffusion latent at a progression of noise levels. In addition, we show that adding augmentations to the diffusion pro-cess mitigates adversarial results. We compare against sev-eral baselines and related methods, both qualitatively and quantitatively, and show that our method outperforms these solutions in terms of overall realism, ability to preserve the background and matching the text. Finally, we show several text-driven editing applications, including adding a new ob-ject to an image, removing/replacing/altering existing ob-jects, background replacement, and image extrapolation. 1.

Introduction
It is said that “a picture is worth a thousand words”, but recent research indicates that only a few words are often sufficient to describe one. Recent works that leverage the tremendous progress in vision-language models and data-driven image generation have demonstrated that text-based interfaces for image creation and manipulation are now fi-nally within reach [12, 23, 29, 30, 38, 39, 41, 47, 51, 57].
The most impressive results in text-driven image manip-ulation leverage the strong generative capabilities of mod-ern GANs [6,19,25–27]. However, GAN-based approaches are typically limited to images from a restricted domain, on which the GAN was trained. Furthermore, in order to manipulate real images, they must be first inverted into the
GAN’s latent space. Although many GAN inversion tech-niques have recently emerged [1–3, 44, 48, 52, 59], it was also shown that there is a trade-off between the reconstruc-input+mask no prompt
“white ball”
“bowl of water” input+mask
“big mountain”
“big wall”
“New York City”
Figure 1. Text-driven object/background replacement: Given an input image and a mask, we modify the masked area according to a guiding text prompt, without affecting the unmasked regions. tion accuracy and the editability of the inverted images [48].
Restricting the image manipulation to a specific region in the image is another challenge for existing approaches [4].
In this work, we present the first approach for region-based editing of generic real-world natural images, using natural language text guidance1. Specifically, we aim at a text-driven method that (1) can operate on real images, rather than generated ones, (2) is not restricted to a spe-cific domain, such as human faces or bedrooms, (3) mod-ifies only a user-specified region, while preserving the rest of the image, (4) yields globally coherent (seamless) editing results, and (5) capable of generating multiple results for the same input, because of the one-to-many nature of the task.
Several examples of such edits are shown in Figure 1.
The demanding image editing scenario described above has not received much attention in the deep-learning era.
In fact, the most closely related works are classical ap-proaches, such as seamless cloning [14,37] and image com-pletion [21], none of which are text-driven. A more re-cent related work is zero-shot semantic image painting [4] in which arbitrary simple textual descriptions can be at-tributed to the desired location within an image. However, this method does not operate on real images (requirement 1), does not preserve the background of the image (require-1Code is available at: blended-diffusion-page/ https : / / omriavrahami . com /
ment 3), and does not generate multiple outputs for the same input (requirement 5).
To achieve our goals, we utilize two off-the-shelf pre-trained models: Denoising Diffusion Probabilistic Models (DDPM) [11, 24, 35] and Contrastive Language-Image Pre-training (CLIP) [40]. DDPM is a class of probabilistic gen-erative models that has recently been shown to surpass the image generation quality of state-of-the-art GANs [11]. We use DPPM as our generative backbone in order to ensure natural-looking results. The CLIP model is contrastively trained on a dataset of 400 million (image, text) pairs col-lected from the internet to learn a rich shared embedding space for images and text. We use CLIP in order to guide the manipulation to match the user-provided text prompt.
We show that a na¨ıve combination of DDPM and CLIP to perform text-driven local editing fails to preserve the im-age background, and in many cases, leads to a less natu-ral result. Instead, we propose a novel way to leverage the diffusion process, which blends the CLIP-guided diffusion latents with suitably noised versions of the input image, at each diffusion step. We show that this scheme produces natural-looking results that are coherent with the unaltered parts of the input. We further show that using extending augmentations at each step of the diffusion process reduces adversarial results. Our method utilizes pretrained DDPM and CLIP models, without requiring additional training.
In summary, our main contributions are: (1) We propose the first solution for general-purpose region-based image editing, using natural language guidance, applicable to real, diverse images. (2) Our background preservation technique guarantees that unaltered regions are perfectly preserved. (3) We demonstrate that a simple augmentation technique significantly reduces the risk of adversarial results, allow-ing us to use gradient-based diffusion guidance. 2.