Abstract
Human pose estimation (HPE) usually requires large-scale training data to reach high performance. However, it is rather time-consuming to collect high-quality and fine-grained annotations for human body. To alleviate this is-sue, we revisit HPE and propose a location-free framework without supervision of keypoint locations. We reformulate the regression-based HPE from the perspective of classifi-cation. Inspired by the CAM-based weakly-supervised ob-ject localization, we observe that the coarse keypoint lo-cations can be acquired through the part-aware CAMs but unsatisfactory due to the gap between the fine-grained HPE and the object-level localization. To this end, we propose a customized transformer framework to mine the fine-grained representation of human context, equipped with the struc-tural relation to capture subtle differences among keypoints.
Concretely, we design a Multi-scale Spatial-guided Context
Encoder to fully capture the global human context while fo-cusing on the part-aware regions and a Relation-encoded
Pose Prototype Generation module to encode the structural relations. All these works together for strengthening the weak supervision from image-level category labels on lo-cations. Our model achieves competitive performance on three datasets when only supervised at a category-level and importantly, it can achieve comparable results with fully-supervised methods with only 25% location labels on MS-COCO and MPII. 1.

Introduction
Human pose estimation (a.k.a., keypoint localization) is a challenging yet fundamental computer vision task, which aims to detect the keypoint locations (e.g., eyes, ankles). In recent years, HPE has witnessed dramatic progress with the development of CNNs. An integral factor of the achieve-ment is the availability of large-scale training data with
*Works done while interning at Tencent Youtu Lab.
†Corresponding author.
Figure 1. Column 1: Location-supervised v.s. Location-free
HPE; Column 2-4: the result of our method. Noted that the correct joints are lankle, lshou and rankle respectively. the result of location-free baseline v.s. precise location annotations. However, it’s rather label-intensive and time-consuming to collect high-quality and fine-grained annotations. Thus, we study the keypoint local-ization when only the image-level category labels are given.
The Class Activation Mapping (CAM) [53] is a simple but effective method to discover object regions from inter-mediate classifier activation with only image-level labels, which has been the cornerstone of weakly-supervised ob-ject localization (WSOL) [53] and weakly-supervised se-mantic segmentation (WSSS) [47]. The CAM tends to fo-cus on the most discriminative parts of object, and many ap-proaches [20,46,48] are proposed to improve CAM to cover the full extend of an object. But these methods cannot local-ize the subtle joints due to the gap between the object-level localization and fine-grained keypoint localization.
Our method is also built upon CAM. There are two main obstacles for applying CAM in HPE. i) For tiny and lo-cal keypoints, it’s hard for the model to capture the precise spatial features for accurate prediction without the explicit location labels. Furthermore, the local appearance fea-tures learned only relying on the image-level labels are not comprehensive enough for understanding the human body.
Therefore, more contextual information and explicit spatial
ii) The inter-class differences prior need to be exploited. among joints are subtle and the adjacent or symmetric joints possess similar semantic context or appearances. It tends to result in the location confusions and incorrect responses as in Fig. 1 (Baseline result in column 2-4). It’s hard for model to capture the fine-grained joint-specific features to elimi-nate the confusions especially without the explicit location supervision. The inherent structural relation between joints plays a critical role in helping distinguish or infer the uncer-tain locations. Thus, how to dig out the intrinsic structural relation prior for the model is vital.
Based on above discussions, we propose a novel cus-tomized transformer-based architecture for LOcation-FRee (LOFR) HPE as in Fig. 2. Firstly, thanks to the self-attention mechanism in Transformer [40], the global con-textual information can be effectively captured in HPE. To better capture the precise spatial information, we propose a Multi-scale Spatial-guided Context Encoder (MSC-En).
In MSC-En, we design a Spatial-aware Position Encoding (SPE) module helps the model focus on body regions while capturing the global human context. We capture the multi-scale feature representations to conduct self-attention learn-ing leading to more comprehensive context from aggregated multi-scale information, which is robust to background clut-ters. For alleviating the location confusions, we equip the model with the structural relations encoded via GCN and propose a Relation-Guided Pose Decoder (RGP-De).
In
RGP-De, a Relation-encoded Pose Prototype Generation (RePPG) module is designed to express keypoint-specific relations to help infer the confused parts during decoding.
Finally, part-aware response maps denoting the spatial dis-tributions of specific keypoints (e.g., ankle or head) are ac-quired by exploring the interactions between human context memory and prototypes. To prompt diversity and fine-grain, a Part Diversity Constraint (PDC) is devised to encourage lower correlation between part features and force them fo-cus on their own parts.
In a nutshell, the contribution of this paper is three-fold:
• To our knowledge, we are the first to develop a location-free HPE only with image-level labels. The effectiveness is extensively validated on three datasets and the performance can even outperform the super-vised one when few location labels are given.
• We employ a multi-scale spatial-guided context en-coder to capture the global context features and mean-while make it attend to the body parts by the aid of the spatial-aware positional encoding.
• We design a relation-encoded pose prototype genera-tion strategy to mine the inherent spatial relation prior between keypoints via GCN. Also, a part diversity constraint makes the part-aware features more distin-guished. 2.