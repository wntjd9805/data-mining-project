Abstract
Video prediction is an extrapolation task that predicts future frames given past frames, and video frame interpo-lation is an interpolation task that estimates intermediate frames between two frames. We have witnessed the tremen-dous advancement of video frame interpolation, but the gen-eral video prediction in the wild is still an open question.
Inspired by the photo-realistic results of video frame in-terpolation, we present a new optimization framework for video prediction via video frame interpolation, in which we solve an extrapolation problem based on an interpolation model. Our video prediction framework is based on opti-mization with a pretrained differentiable video frame inter-polation module without the need for a training dataset, and thus there is no domain gap issue between training and test data. Also, our approach does not need any additional in-formation such as semantic or instance maps, which makes our framework applicable to any video. Extensive exper-iments on the Cityscapes, KITTI, DAVIS, Middlebury, and
Vimeo90K datasets show that our video prediction results are robust in general scenarios, and our approach outper-forms other video prediction methods that require a large amount of training data or extra semantic information. 1.

Introduction
Video prediction is an extrapolation task to predict future video frames given some past frames. Video prediction has broad applications including robotics planning, autonomous driving and video manipulations [6,23,39,41]. For instance, predicted videos can help autonomous robots to better plan future actions with future visual information. Video pre-diction is also a fundamental task for unconditional video synthesis that can be decomposed into image synthesis and future video prediction.
Video prediction is a challenging extrapolation problem.
Figure 2. Overview of our method. We optimize optical flow ˜ft+1→t by a video frame interpolation G [11]. Our optimization objective is image-level distance Limg and a consistency constraint between our predicted flow ˜ft+1→t and the flow f G t→t+1 generated by G.
Several studies [18, 20, 22, 42] only take RGB frames as in-put for video prediction and find that the video prediction problem is difficult to solve, because of the inherent high complexity of video prediction and the uncertainty of future states. Thus, recently, a lot of constraints and assumptions about the modeling scene have been employed to simplify the problem. However, these assumptions reduce the gener-alization ability of these video prediction models. FVS [45] and Lee et al.
[15] require semantic maps to decompose the scene. Bei et al. [4] first predict semantic maps and then synthesize future frames. Qi et al. [35] require depth maps to reconstruct 3D point clouds. However, such addi-tional information is often hard to obtain or estimate cor-rectly in general scenarios. These strong assumptions limit these methods to be only applicable to data when these as-sumptions hold. For example, failing to detect some objects (including unseen objects) will lead to performance degra-dation. When this extra information is unavailable or of poor quality, these methods suffer from performance degra-dation and cannot be even applied in diverse videos in the real world.
Moreover, these external methods usually need to train one model for each specific scenario, making them difficult generalize to other scenarios. For example, it is hard to ap-ply a video prediction model trained on a driving scene to a human moving dataset, as the motion difference is huge.
To address these issues, we propose an optimization-based video prediction method, without the requirement of external training (no external dataset is needed for training), and can produce state-of-the-art results. Our insight is that we can cast the video prediction problem as a video frame interpolation (VFI) based optimization problem. Inspired by the recent success of VFI, we connect these two prob-lems to solve the video prediction problem in a new way.
Our method does not require any assumption such as se-mantic segmentation and can be applied to any video. We evaluate our method on multiple datasets, and our method can outperform the state of the arts. Our contributions can be summarized as follows:
• We present the first optimization framework for video prediction. We cast the extrapolation problem of video prediction as an optimization problem with VFI.
• Our framework is highly flexible as it does not require any semantic or instance maps, prior knowledge about the scene, or external training. Our method is applica-ble to video prediction in any scene at any resolution.
• Our method obtains outstanding performance on var-ious datasets and outperforms state-of-the-art video prediction approaches that require additional informa-tion. Our method surpasses external learning methods taking only RGB frames as input by a large margin. 2.