Abstract
Annotating tens or hundreds of tiny objects in a given image is laborious yet crucial for a multitude of Computer
Vision tasks. Such imagery typically contains objects from various categories, yet the multi-class interactive annota-tion setting for the detection task has thus far been unex-plored. To address these needs, we propose a novel interac-tive annotation method for multiple instances of tiny objects from multiple classes, based on a few point-based user in-puts. Our approach, C3Det, relates the full image context with annotator inputs in a local and global manner via late-fusion and feature-correlation, respectively. We perform ex-periments on the Tiny-DOTA and LCell datasets using both two-stage and one-stage object detection architectures to verify the efficacy of our approach. Our approach outper-forms existing approaches in interactive annotation, achiev-ing higher mAP with fewer clicks. Furthermore, we validate the annotation efficiency of our approach in a user study where it is shown to be 2.85x faster and yield only 0.36x task load (NASA-TLX, lower is better) compared to manual annotation. The code is available at https://github. com/ChungYi347/Interactive-Multi-Class-Tiny-Object-Detection. 1.

Introduction
Large-scale data and annotations are crucial for success-ful deep learning [22]. However in many real-world prob-lems, annotations are very labor-intensive and expensive to acquire [8]. Annotation costs increase even higher when handling numerous tiny objects such as in remote sens-ing [7, 15, 33], extreme weather research [24], and micro-scope image analysis [12, 16]. These settings often require highly-skilled annotators and accordingly high compensa-tion. For instance, cell annotation in Computational Pathol-ogy requires expert physicians (pathologists), whose train-ing involves several years of clinical residency [3, 31]. Re-ducing cost and effort for these annotators would directly enable the collection of new large-scale tiny-object datasets, and contribute to higher model performances.
Several prior works have been proposed to reduce anno-tation cost in other tasks. Interactive segmentation meth-ods [23,35] focus on reducing the number of interactions in the segmentation of a single foreground object, which can be classified as a “many interactions to one instance” ap-proach. However, tiny-objects annotation can benefit from a “many interactions to many instances” approach as one image can contain many instances. Object counting meth-ods [4, 26] count multiple instances from a few user clicks and do follow a “many interactions to many instances” ap-proach. However, these methods highlight only objects of the same class as the one being counted and thus can be classified as a “one class to one class” approach. How-ever, images with tiny-objects are often composed of ob-jects from multiple classes. Thus, tiny-object annotation should implement a “many classes to many classes” ap-proach.
To address the above needs, we propose C3Det, an ef-fective interactive annotation framework for tiny object de-tection. Fig. 1 shows how a user interacts with C3Det to create bounding-boxes of numerous tiny objects from mul-tiple classes. Once a user clicks on a few objects and pro-vides their class information, C3Det takes those as inputs
Figure 2. The global context of user inputs matter. “Late-Fusion” does not consider the global context and can miss far away objects (red dotted lines) from user inputs (marked as circles). C3Det captures the global context well and can detect far away objects. and detects bounding boxes of many objects, even includ-ing object classes that the user did not specify. The user repeats this process until the annotation is complete. By utilizing user inputs in the “many interactions to many in-stances” and “many classes to many classes” way, C3Det can significantly speed-up annotation.
A key aspect of our approach is in making each user click influence objects that are nearby (local context) as well as far away (global context). To encourage the annotator-specified class to be consistent with model predictions, we insert user inputs (in heatmap form) at an intermediate stage in the model (late-fusion) and apply a class-consistency loss between user input and model predictions. This alone can capture local context well, but may miss far away objects.
We therefore introduce the C3 (Class-wise Collated Cor-relation) module, a novel feature-correlation scheme that communicates local information to far away objects (see
Fig. 2), allowing us to learn many-to-many instance-wise relations while retaining class information. Through ex-tensive experiments, we show that these components com-bined, result in significant performance improvements.
To validate whether our performance improvements translate to lower annotation cost in the real-world, we per-form a user study with 10 human annotators. Our approach,
C3Det, when combined with further manual bounding box corrections, is shown to be 2.85× faster and yield only 0.36× task load (NASA-TLX) compared to manual annota-tion, achieving the same or even better annotation quality as measured against the ground-truth. This verifies that C3Det not only shows improvements in simulated experiments, but also reduces annotation cost in the real-world.
In summary, we make the following contributions: (a) we address the problem of multi-class and multi-instance interactive annotation of tiny objects, (b) we in-troduce a training data synthesis and an evaluation proce-dure for this setting, (c) we propose a novel architecture for interactive tiny-object detection that considers both lo-cal and global implications of provided user inputs, and fi-nally (d) our experimental results and user study verify that our method reduces annotation cost while achieving high annotation quality. 2.