Abstract
We present a simple and effective framework, named
Point2Seq, for 3D object detection from point clouds.
In contrast to previous methods that normally predict at-tributes of 3D objects all at once, we expressively model the interdependencies between attributes of 3D objects, which in turn enables a better detection accuracy. Speciﬁcally, we view each 3D object as a sequence of words and re-formulate the 3D object detection task as decoding words from 3D scenes in an auto-regressive manner. We further propose a lightweight scene-to-sequence decoder that can auto-regressively generate words conditioned on features from a 3D scene as well as cues from the preceding words.
The predicted words eventually constitute a set of sequences that completely describe the 3D objects in the scene, and all the predicted sequences are then automatically assigned to the respective ground truths through similarity-based se-quence matching. Our approach is conceptually intuitive and can be readily plugged upon most existing 3D-detection backbones without adding too much computational over-head; the sequential decoding paradigm we proposed, on the other hand, can better exploit information from com-plex 3D scenes with the aid of preceding predicted words.
Without bells and whistles, our method signiﬁcantly out-performs previous anchor- and center-based 3D object de-tection frameworks, yielding the new state of the art on the challenging ONCE dataset as well as the Waymo Open
Dataset. Code is available at https://github.com/ ocNflag/point2seq. 1.

Introduction 3D object detection is a critical component of intelligent perception systems for self-driving, aiming to localize and recognize cars, pedestrians, and other key objects around an autonomous vehicle. With the increasing popularity of
LiDAR sensors, 3D object detection from point clouds has
*Joint ﬁrst authors with equal contribution.
†Corresponding author. Email: xinchao@nus.edu.sg
Figure 1. Point2Seq reformulates the 3D object detection problem as auto-regressively generating sequences of words that can repre-sent the 3D objects. The sequential decoding paradigm attains bet-ter detection performance compared to previous works that predict all attributes of the 3D objects in parallel, thanks to its competence in exploring intrinsic dependencies among object attributes. been receiving much attention owing to the advanced detec-tion accuracy compared to other input modalities.
In driving scenes, most 3D objects are extremely small relative to the detection range, and numerous approaches have been employed to detect those small objects from point clouds accurately. Anchor-based methods [13, 38] place predeﬁned anchors on each pixel center of a Bird-Eye-View (BEV) feature map, while center-based approaches [9, 42] treat the pixels near object centers as positives and predict boxes using those pixel features. These methods rely on complex hand-crafted procedures of label assignments and post-processing, and quantization errors introduced by the
BEV representations lead to severe misalignment between the object locations and the pixel features used to predict those objects. Approaches like [16, 25, 27] rely on a sec-ond reﬁnement stage to mitigate the misalignment issue, yet at the cost of adding too much computational overhead.
Therefore, learning more spatially-aligned features to de-tect the 3D objects accurately while maintaining a high efﬁ-ciency poses an open challenge to the research community.
To address the challenge, in this paper, we introduce
Point2Seq, a ﬂexible and streamlined framework for 3D ob-ject detection from point clouds. Unlike prior methods that typically predict all attributes of a 3D object (e.g., location, class, size) simultaneously, we represent each object as a sequence, in which each word corresponds to an object at-tribute, and we explicitly explore the inherent dependencies among words alongside their relations with the input 3D scene to progressively predict each attribute of 3D objects.
Our motivation is quite intuitive: given the fact that each ob-ject is represented by sequential words, the existing words will provide 3D detectors with cues to better exploit spa-tial features and help detectors predict the following words more accurately. For instance, 3D detectors can leverage more spatially-aligned features for an object if the object lo-cation has been formerly predicted and better recognize the object class if its size information has already been known.
It is therefore desirable to design a detection framework that may sequentially predict words of 3D objects conditioned on the preceding generated words as well as the spatial fea-tures until all the words form a set of sequences describing the 3D objects in the scene.
To achieve this goal, we must address two critical chal-lenges: how to design the sequential object words pre-diction module and make it compatible with the existing 3D detection pipelines and how to optimize the 3D de-tector with the ground truth and predicted sequences. To resolve the ﬁrst problem, we propose a novel scene-to-sequence decoder, which takes the BEV feature map and a set of initial region cues as input and auto-regressively decodes sequences for all objects in parallel. The scene-to-sequence decoder is compatible with most grid-based 3D backbones [13, 19, 35, 38, 44] and can effectively aggregate features from those backbones based on the information of the preceding words. By virtue of the highly-parallel deep learning libraries, the scene-to-sequence decoder can gen-erate the sequences of all 3D objects in one shot, with little added time and memory cost.
To handle the second issue, we adopt the set-to-set loss to match the predicted sequences with the ground truths.
Unlike existing approaches [3,21,36] that utilize the sum of the classiﬁcation and regression loss as the cost function for bipartite matching, in this paper, we propose a novel metric to measure the similarity between two sequences. Then we perform bipartite matching by maximizing the global sim-ilarity of the prediction and the ground truth set using the proposed metric. In this manner, each predicted sequence can be automatically assigned to a respective ground truth without pre-deﬁned anchors or centers. The assignments are globally optimal and result in better performance com-pared to previous methods.
With the lightweight scene-to-sequence decoder, our method can progressively predict the words of 3D ob-jects, yielding reliable predictions that signiﬁcantly outper-form state-of-the-art. In addition, our method is free from the human-designed procedures of label assignments with similarity-based sequence matching. Our key contributions are summarized as follows:
• We present an effective and ﬂexible framework for 3D object detection from point clouds. We represent each 3D object as a sequence of words and model the 3D object de-tection problem as decoding the words from the 3D scenes in an auto-regressive manner.
• We propose a scene-to-sequence decoder that can auto-regressively generate sequences representing the detected 3D objects and introduce the similarity-based sequence matching scheme to enable automatic assignments of the predicted sequences to the respective ground truths for end-to-end training.
• Our method signiﬁcantly outperforms the anchor-based and center-based 3D detectors with the same back-bone, attaining 66.16% mAP on the ONCE dataset and 77.52% vehicle L1 mAP on the Waymo Open Dataset. 2.