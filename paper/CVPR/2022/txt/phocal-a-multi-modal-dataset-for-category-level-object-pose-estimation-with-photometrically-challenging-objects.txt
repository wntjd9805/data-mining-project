Abstract 1.

Introduction
Object pose estimation is crucial for robotic applica-tions and augmented reality. Beyond instance level 6D object pose estimation methods, estimating category-level pose and shape has become a promising trend. As such, a new research field needs to be supported by well-designed datasets. To provide a benchmark with high-quality ground truth annotations to the community, we introduce a mul-timodal dataset for category-level object pose estimation with photometrically challenging objects termed PhoCaL.
PhoCaL comprises 60 high quality 3D models of household objects over 8 categories including highly reflective, trans-parent and symmetric objects. We developed a novel robot-supported multi-modal (RGB, depth, polarisation) data ac-quisition and annotation process. It ensures sub-millimeter accuracy of the pose for opaque textured, shiny and trans-parent objects, no motion blur and perfect camera synchro-nisation.
To set a benchmark for our dataset, state-of-the-art
RGB-D and monocular RGB methods are evaluated on the challenging scenes of PhoCaL.
Vision systems interacting with their environment need to estimate the position and orientation of objects in space, which highlights why 6D object pose estimation is an im-portant task for robotic applications. Even though there have been great advances in the field [6, 42], instance-level 6D pose methods require pre-scanned object models and support limited number of objects. Category-level object pose estimation [40] scales better to the needs of real oper-ating environments. However, photometrically challenging objects such as shiny, e.g. metallic, and transparent, e.g. glass, objects are very common in our daily life and little work has been done to estimate their 6D poses within prac-tical accuracy on a category-level. The difficulty arises from two aspects: first, it is difficult to annotate 6D pose ground truth for photometrically challenging objects since no tex-ture can be used to determine key points; second, commonly used depth sensors fail to return the correct depth infor-mation, as structured light and stereo method often fail to correctly interpret reflection and refraction artefacts. As a consequence, RGB-D methods [25, 40] do not work reli-ably with photometrically challenging objects. We intro-Figure 2. Our dataset comprises 60 household objects among 8 object categories. The training and test split is depicted here. duce PhoCaL, a class-level dataset of photometrically chal-lenging objects with high-quality ground-truth annotations.
The dataset provides multi-modal data such as RGB, depth and polarization which enables investigation into object’s surface reflectance properties.
We obtain highly accurate ground truth poses with a novel method using a collaborative robot arm in gravity compensated mode and a calibrated mechanical tip. In or-der to annotate the 6D pose of transparent and non-textured objects, a specially designed tip is mounted on the robot arm. With the calibrated tip, the positions of pre-defined points on the object surface are acquired on the real ob-ject and matched to a scan thereof. Using this method, the object pose can be determined with an order of magnitude more accuracy than previous methods. For transparent and textureless objects, topographic key points are used instead of textural ones. The points gathered in this way are then matched to the object model in a final ICP [2] step to yield an accurate fit.
The camera to robot end-effector transformation is needed to obtain the object poses in camera coordinates.
Typically, hand-eye calibration approaches solve this by vi-sually estimating the marker position and optimizing for the transformation between camera and end-effector. To minimize the error propagation and obtain highly accurate ground truth labels, we instead used the end-effector tip of the arm in gravity-compensated mode to measure the posi-tion of 12 points on a ChArUco [1] board. This allows us to use the robot’s accurate position system to obtain both object poses and camera poses for image sequences.
Beyond photometrically challenging categories and high-quality annotations, multi-modal input is another high-light of PhoCaL. As the active depth sensors fail on metallic and transparent surfaces, we include an additional passive sensor modality in the form of a polarization camera. It pro-vides valuable information on object surfaces [22]. In our setup, we designed and 3D printed a rig that holds multi-ple cameras, each mounted on it and carefully calibrated.
During recording, a pre-defined trajectory is repeated by the robot arm. The robot arm stops when capturing images from all cameras, which avoids motion blur and diminished effects from imperfect synchronization.
In summary, our main contributions are: 1. We propose PhoCaL, a multi-modal (RGBD +
RGBP) dataset for category-level object pose esti-mation. The dataset comprises 60 high-quality 3D models of household objects including symmetric, transparent and reflective objects in 8 categories with 24 sequences featuring occlusion, partial visibility and clutter. 2. We introduce a new and highly accurate pose anno-tation method using a robotic manipulator that al-lows for sub-millimeter precision 6D pose annotations of photometrically challenging objects even with re-flective or transparent surfaces. 2.