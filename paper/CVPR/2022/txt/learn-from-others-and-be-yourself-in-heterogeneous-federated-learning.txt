Abstract (cid:6)(cid:11)(cid:18)(cid:22)(cid:11)(cid:18) (cid:4)(cid:9)(cid:20)(cid:9)(cid:18)(cid:16)(cid:10)(cid:9)(cid:15)(cid:9)(cid:12)(cid:20)(cid:21)(cid:1)(cid:5)(cid:18)(cid:16)(cid:7)(cid:13)(cid:9)(cid:14)
Federated learning has emerged as an important dis-tributed learning paradigm, which normally involves col-laborative updating with others and local updating on pri-vate data. However, heterogeneity problem and catas-trophic forgetting bring distinctive challenges. First, due to non-i.i.d (identically and independently distributed) data and heterogeneous architectures, models suffer perfor-mance degradation on other domains and communication barrier with participants models. Second, in local updat-ing, model is separately optimized on private data, which is prone to overﬁt current data distribution and forgets pre-viously acquired knowledge, resulting in catastrophic for-getting. In this work, we propose FCCL (Federated Cross-Correlation and Continual Learning). For heterogeneity problem, FCCL leverages unlabeled public data for com-munication and construct cross-correlation matrix to learn a generalizable representation under domain shift. Mean-while, for catastrophic forgetting, FCCL utilizes knowledge distillation in local updating, providing inter and intra do-main information without leaking privacy. Empirical re-sults on various image classiﬁcation tasks demonstrate the effectiveness of our method and the efﬁciency of modules. 1.

Introduction
Deep learning algorithms have achieved remarkable progress, owing to the availability of large-scale data [8, 51, 69]. However, in the real world, data are commonly dis-persed over different participants (e.g., mobile devices, or-ganizations). Due to growing privacy concerns and strict data protection regulations [84], participants cannot inte-grate data together to train a model. Driven by such real-istic issues, federated learning [33, 34, 58, 59, 89] provides a privacy-preserving paradigm, where participants collabo-*Corresponding Author: Mang Ye, Bo Du (cid:13)(cid:9)(cid:13)(cid:17)(cid:7)(cid:15)(cid:20) (cid:1) (cid:5)(cid:7)(cid:18)(cid:20)(cid:13)(cid:9)(cid:13)(cid:17)(cid:7)(cid:15)(cid:20) (cid:1) (cid:5) (cid:20)(cid:13) (cid:13) (cid:5)(cid:7)(cid:18)(cid:20)(cid:13)(cid:9)(cid:13)(cid:17)(cid:7)(cid:15)(cid:20) (cid:2) (cid:3)(cid:11)(cid:20)(cid:11)(cid:18)(cid:16)(cid:12)(cid:11)(cid:15)(cid:11)(cid:16)(cid:21)(cid:19) (cid:2)(cid:11)(cid:10)(cid:11)(cid:18)(cid:7)(cid:20)(cid:11)(cid:10) (cid:4)(cid:11)(cid:7)(cid:18)(cid:15)(cid:13)(cid:15)(cid:12) (cid:1) (cid:2) (cid:4)(cid:16)(cid:8)(cid:9)(cid:13) (cid:2)(cid:9)(cid:18)(cid:9)(cid:17)(cid:16)(cid:11)(cid:9)(cid:15)(cid:9)(cid:12)(cid:18)(cid:19) (cid:1)(cid:6)(cid:18)(cid:6) (cid:2)(cid:9)(cid:18)(cid:9)(cid:17)(cid:16)(cid:11)(cid:9)(cid:15)(cid:9)(cid:12)(cid:18)(cid:19) (cid:2)(cid:5)(cid:3)(cid:1)(cid:1)(cid:16)(cid:14)(cid:14)(cid:7)(cid:8)(cid:16)(cid:18)(cid:7)(cid:20)(cid:13)(cid:22)(cid:11) (cid:4)(cid:11)(cid:7)(cid:5)(cid:12)(cid:9)(cid:10)(cid:8) (cid:2)(cid:6)(cid:20)(cid:6)(cid:19)(cid:20)(cid:18)(cid:16)(cid:17)(cid:11)(cid:12)(cid:8)(cid:1)(cid:3)(cid:16)(cid:18)(cid:10)(cid:9)(cid:20)(cid:20)(cid:12)(cid:15)(cid:10) (cid:3)(cid:15)(cid:18)(cid:9)(cid:17) (cid:1)(cid:16)(cid:14)(cid:6)(cid:12)(cid:15) (cid:5)(cid:9)(cid:17)(cid:10)(cid:16)(cid:17)(cid:14)(cid:6)(cid:15)(cid:7)(cid:9) (cid:3)(cid:15)(cid:18)(cid:17)(cid:6) (cid:1)(cid:16)(cid:14)(cid:6)(cid:12)(cid:15) (cid:5)(cid:9)(cid:17)(cid:10)(cid:16)(cid:17)(cid:14)(cid:6)(cid:15)(cid:7)(cid:9) (cid:2)(cid:6)(cid:3)(cid:1)(cid:4)(cid:16)(cid:9)(cid:7)(cid:14) (cid:4)(cid:11)(cid:7)(cid:5)(cid:12)(cid:9)(cid:10)(cid:8)
Figure 1. Problem illustration of heterogeneous federated learning. (a) In collaborative updating, how to handle communi-cation problem of heterogeneous models and learn a generalizable representation under heterogeneous data (domain shift)? (b) In lo-cal updating, how to alleviate catastrophic forgetting to present sta-ble and satisfactory performance in both inter and intra domains? ratively learn a model without leaking private data. It has been an active and challenging research topic and shows promising results in real-world setting [17, 19, 29, 52, 54].
Along with its pilot progress, researches on federated learning are bafﬂed by some key challenges [30, 42]. An inevitable and practical challenge is heterogeneity problem.
On the one hand, distributed data might be non-i.i.d (iden-tically and independently distributed), leading to data het-erogeneity [30,39,95]. A myriad of methods [43,46,73,77] incorporate extra proximal terms to handle the data in label distribution skew (prior probability shift) [30], neglecting the fact that there exists domain shift (same label, different features) [60, 64, 66].
In particular, private model suffers severe performance degradation on other domains with no-ticeably different distribution. As a result, learning a gen-eralizable representation under domain shift is technically challenging. On the other hand, due to different design cri-teria, distinct hardware capabilities [20, 86] and intellectual property rights [56], participants require to customize mod-els, which poses a practical challenge: model heterogene-ity. Preceding methods are developed under the assump-tion that local models share parameters or gradients, which cannot work on heterogeneous models.
In order to solve this problem, a main stream of subsequent effort leverages knowledge transfer through labeled data [38, 74], shared model [48, 72, 92] or group operation [21, 50]. But these methods have different limitations. Speciﬁcally, labeled data require server to collect data with similar distributions to private data, which causes costly human efforts and needs special domain expertise. For shared model, it raises com-putational cost and necessitates additional model structure in participant side. Group operation leverages unlabeled public data to measure distribution divergence. However, these methods mainly focus on label distribution skew and consider the performance on one domain. Simultaneously considering data and model heterogeneity, an essential issue has long been overlooked: (a) How to learn a generalizable representation in heterogeneous federated learning?
Besides heterogeneity problem, another impediment for federated learning steams from its paradigm. Generally, federated learning could be viewed as a two-step cyclic pro-cess: collaborative updating and local updating [58, 89]. In collaborative updating, participants learn from others.
In local updating, model is optimized on private data, which is prone to overﬁt current knowledge and forget previous knowledge, resulting in catastrophic forgetting [57]. To tackle this challenge, one type of methods typically per-forms ﬁne-tuning for several rounds [38, 50, 58, 74, 88].
However, carefully conﬁguring hyper-parameters to achieve satisfactory performance is time-consuming and cannot tackle this problem systematically. Current popular solu-tions [41,43,73,77] focus on calculating parameter stiffness to regulate models, which can not explicitly depict the de-gree of effect from different participants. Consequently, a natural question arises: (b) How to balance multiple knowl-edge to reduce catastrophic forgetting? We further explain heterogeneity problem and catastrophic forgetting in Fig. 1.
For the heterogeneity problem, we take inspiration from the self-supervised learning [5, 6, 11, 13, 18, 25, 49, 91, 94].
In particular, self-supervised learning aims to learn a gen-eralizable representation through rich and diverse data for downstream tasks and unseen classes. Intuitively, we ex-pect that the models would present similar logits output for the same classes in different domains. This motivates us to leverage unlabeled public data for Federated Cross-Correlation Learning, which is diverse and easy to obtain.
Speciﬁcally, we try to maximize the similarity between log-its output and minimize the redundancy within logits out-put on unlabeled public data. Through correlating same di-mensions and decorrelating different dimensions on logits output, models would learn class invariance and encourage the diversity of different classes. Thus, our method handles the communication problem in heterogeneous models and learns a generalizable representation under domain shift.
To handle catastrophic forgetting, we develop Federated
Continual Learning via knowledge distillation [2, 24] in lo-cal updating to continually learn from inter and intra do-mains. To avoid forgetting inter domain information in local updating stage, we propose to distill the knowledge of intra-domain (local) model learned in previous rounds, where it captured the inter domain information after communication with other participants. In addition, for intra domain for-getting problem, we leverage the initially pretrained local model (without knowledge learned from others) to constrain the later local updating for each participant. Therefore, bal-ancing knowledge through distillation with these two mod-els is reasonable to handle the catastrophic forgetting.
In this work, we propose a novel federated learning method, dubbed FCCL (Federated Cross-Correlation and
Continual Learning). The overview of FCCL is illustrated in Fig. 2. In a nutshell, our contributions are three-fold:
• We formulate a simple and effective method for het-erogeneous federated learning. Through leveraging unlabeled public data and adopting self-supervised learning, heterogeneous models achieve communica-tion and learn a generalizable representation.
• We explore to alleviate catastrophic forgetting in feder-ated learning. Through inter and intra domain knowl-edge distillation with updated and pretrained models, it balances knowledge from others and itself.
• We conduct extensive experiments on two image clas-siﬁcation tasks (e.g., Digits [27, 37, 62, 68] and Ofﬁce-Home [82]) with unlabeled public data [35, 69, 87].
FCCL achieves superior inter and intra domain perfor-mance over related methods. Ablation study on core module validates its efﬁcacy and indispensability. 2.