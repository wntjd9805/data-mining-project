Abstract
The brain-inspired and event-driven Spiking Neural Net-work (SNN) aiming at mimicking the synaptic activity of
It biological neurons has received increasing attention. transmits binary spike signals between network units when the membrane potential exceeds the firing threshold. This bio-mimetic mechanism of SNN appears energy-efficiency with its power sparsity and asynchronous operations on spike events. Unfortunately, with the propagation of binary spikes, the distribution of membrane potential will shift, leading to degeneration, saturation, and gradient mismatch problems, which would be disadvantageous to the network optimization and convergence. Such undesired shifts would prevent the SNN from performing well and going deep. To tackle these problems, we attempt to rectify the membrane potential distribution (MPD) by designing a novel distribu-tion loss, MPD-Loss, which can explicitly penalize the un-desired shifts without introducing any additional operations in the inference phase. Moreover, the proposed method can also mitigate the quantization error in SNNs, which is usu-ally ignored in other works. Experimental results demon-strate that the proposed method can directly train a deeper, larger, and better-performing SNN within fewer timesteps. 1.

Introduction
Artificial Neural Networks (ANNs) have achieved huge success in many application fields, including image clas-sification [16, 44, 45], object detection [14, 31, 39], ma-chine translation [2], gaming [43, 46], etc. However, the increased computing resource required by ANNs poses a burden on latency-sensitive applications and energy-limited devices [28,30,51]. Recently, The Spiking Neural Networks (SNN) has received increasing attention and been regarded as a potential competitor of ANNs, due to their biology-inspired neural behavior and efficient computation [40].
*Equal contribution.
†Corresponding author.
SNNs utilize binary spike activity i.e., 0 for nothing and 1 for spike event, to transmit information. This transmitting mode bridges the gap between real value-based information processing of ANNs and spike-based information process-ing of brains. With the characteristics of binary spike-based and sparse temporal communicating mechanisms, SNNs enjoy power-efficiency for implementation on specific neu-romorphic hardwares [3, 7, 21, 33], and have been increas-ingly promising in neuromorphic computation [7], pattern recognition [20], robotics [18], brain-inspired devices [33], etc.
However, the discontinuous and non-differentiable spike activity poses difficulty to directly train SNNs using gradi-ent descent in back-propagation. The current SNN training algorithms for avoiding the dilemma of non-differentiability can be categorized as (i) the ANN-to-SNN conversionand (ii) the surrogate gradient (SG) descent method. The con-version methods usually convert a pre-trained non-spike
ANN to its SNN counterpart with the same architecture [4, 5,9,11,15,26,38,42]. Although the converted networks can achieve comparable performances to the original ANNs, they come at the cost of numerous inference timesteps, which are much more time and energy-consuming. The
SG descent method adopts SGs (i.e., 1|x−Vth|<0.5 or 0, otherwise, where Vth is the firing threshold and usually is 0.5) to replace the all-or-nothing gradients of the spike activity function [10, 17, 24, 29, 34, 41, 49].
It provides a chance to directly train SNNs with several timesteps. How-ever, it also suffers from the problem of gradient vanish-ing or explosion. These problems will lead to performance degradation and shallow network architectures. Many ef-forts [12,13,23,37,52] have been made to solve these prob-lems and drive the models to achieve state-of-the-art per-formance. Nevertheless, due to the lack of comprehen-sive analysis of the difficulty of training SNNs directly, there still has room for improvement in these works. Here, we provide a novel perspective to understand the dilemma of training an SNN by analyzing the membrane potential shifts. The brief analysis for a single channel is as follows.
As the binary spikes propagating through layers, the dis-Figure 1. The overall framework of the proposed RecDis-SNN. (a) The detailed LIF model. (b) The propagation and update of the neuron spikes in two adjacent layers. (c) The examples of the three undesired membrane potential distribution shifts and the balanced distribution adjusted by the proposed MPD-loss. tribution of membrane potential will shift accumulatively during the training and may fall into an inappropriate range, which causes training difficulties. As illustrated in Fig. 1, the membrane potential shift will appear three imbalances in some extreme cases: (i) Degeneration: if almost all the membrane potential values of the neurons in a channel are beyond or below the firing threshold, the spikes of this chan-nel will be homogeneity (i.e., all 0s or 1s) and the feature in-formation of the channel will be negligible; (ii) Saturation: if almost all the membrane potential values of the neurons in a channel are out of the interval [0, 1], the gradients for these neurons will be 0 and result in the back-propagation inability. (iii) Gradient mismatch: if almost all the mem-brane potential values in a channel fall into the interval
[0, 1], it is equivalent to use SGs for all gradient compu-tation, which will enlarge the approximated errors from the accurate gradients, and therefore lead to more severe gradi-ent mismatches. The detailed analysis is in Sec. 3.2.
The uncontrollable shift of membrane potential distribu-tion will increase the training difficulty for SNNs, and re-strict the network scale. In this paper, the three undesired membrane potential distribution (MPD) shifts will be ana-lyzed in detail, firstly. Based on this analysis, a novel distri-bution loss i.e., MPD-Loss is proposed to explicitly penalize those undesired shifts. By incorporating MPD-Loss into an
SNN, we present RecDis-SNN, which will enjoy Rectified membrane potential Distribution. The overall framework of
RecDis-SNN is illustrated in Fig. 1. The main contributions are as follows:
• We present a new perspective to understand the diffi-culty of training SNNs by analyzing three undesired shifts of membrane potential distribution in forward propagation. Then, the MPD-Loss is proposed to pe-nalize the undesired shifts.
• The MPD-Loss is beneficial for alleviating the gradient vanishing or explosion, adjusting the spike rate, and speeding up the convergence. In this sense, it plays a role of the normalization without extra operations in the inference phase. As far as we know, this is one of the few works that can directly train deep SNNs with-out normalization techniques or any warm-start.
• The MPD-Loss can also mitigate the problem of quan-tization error, which is usually ignored in other works.
To our best knowledge, this is the first work that has noticed the problem in SNNs and provides a solution.
• The RecDis-SNN is evaluated on both standard non-spiking and neuromorphic benchmarks. Experimen-tal results show that by using MPD-Loss, the RecDis-SNNs can achieve state-of-the-art performance. Mean-while, MPD-Loss can be easily combined with other methods and further improve their performance. 2.