Abstract
Vanilla unsupervised domain adaptation methods tend to optimize the model with fixed neural architecture, which is not very practical in real-world scenarios since the target data is usually processed by different resource-limited de-vices. It is therefore of great necessity to facilitate archi-tecture adaptation across various devices. In this paper, we introduce a simple framework, Slimmable Domain Adapta-tion, to improve cross-domain generalization with a weight-sharing model bank, from which models of different capac-ities can be sampled to accommodate different accuracy-efficiency trade-offs. The main challenge in this frame-work lies in simultaneously boosting the adaptation perfor-mance of numerous models in the model bank. To tackle this problem, we develop a Stochastic EnsEmble Distilla-tion method to fully exploit the complementary knowledge in the model bank for inter-model interaction. Nevertheless, considering the optimization conflict between inter-model interaction and intra-model adaptation, we augment the ex-isting bi-classifier domain confusion architecture into an
Optimization-Separated Tri-Classifier counterpart. After optimizing the model bank, architecture adaptation is lever-aged via our proposed Unsupervised Performance Eval-uation Metric. Under various resource constraints, our framework surpasses other competing approaches by a very large margin on multiple benchmarks.
It is also worth emphasizing that our framework can preserve the perfor-mance improvement against the source-only model even when the computing complexity is reduced to 1/64. Code will be available at https://github.com/HIK-LAB/SlimDA. 1.

Introduction
Deep neural networks are usually trained on the offline-collected images (labeled source data) and then embedded
†Corresponding author
Figure 1. SlimDA: We only adapt once on cloud computing center but can flexibly sample models with diverse capacities to distribute to different resource-limited edge devices. in edge devices to test the images sampled from new sce-narios (unlabeled target data). This paradigm, in practice, degrades the network performance due to the domain shift.
Recently, more and more researchers have delved into unsu-pervised domain adaptation (UDA) to address this problem.
Vanilla UDA aims to align source data and target data into a joint representation space so that the model trained on source data can be well generalized to target data
[7,11,20,26,29,38,44]. Unfortunately, there is still a gap be-tween academic studies and industrial needs: most existing
UDA methods only perform weight adaptation with fixed neural architecture yet cannot fit the requirements of vari-ous devices in the real-world applications efficiently. Tak-ing the example of a widely-used application scenario as shown in Fig.1, a domain adaptive model trained on a pow-erful cloud computing center is urged to be distributed to different resource-limited edge devices like laptops, smart mobile phones, and smartwatches, for real-time processing.
In this scenario, vanilla UDA methods have to train a se-ries of models with different capacities and architectures time-and-again to fit the requirements of devices with dif-Methods
Data Type
Teacher
Student
Model
CKD
Labeled
Single
Single
Fixed
SEED
Unlabeled
Multiple
Multiple
Stochastic
Table 1. Conventional Knowledge Distillation (CKD) vs. Stochas-tic EnsEmble Distillation (SEED). ferent computation budgets, which is expensive and time-consuming. To remedy the aforementioned issue, we pro-pose Slimmable Domain Adaptation (SlimDA), in which we only train our model once so that the customized mod-els with different capacities and architectures can be sam-pled flexibly from it to supply the demand of devices with different computation budgets.
Although slimmable neural networks [54–56] had been studied in the supervised tasks, in which models with differ-ent layer widths (i.e., channel number) can be coupled into a weight-sharing model bank for optimization, there remain two challenges when slimmable neural networks meet un-supervised domain adaptation: 1) Weight adaptation: How to simultaneously boost the adaptation performance of all models in the model bank? 2) Architecture adaptation:
Given a specific computational budget, how to search an appropriate model on the unlabeled target data?
For the first challenge, there is a straightforward baseline in which UDA methods are directly applied to each model sampled from the model bank. However, this paradigm neglects to exploit the complementary knowledge among tremendous neural architectures in the model bank. To rem-edy this issue, we propose Stochastic EnsEmble Distillation (SEED) to interact the models in the model bank so as to suppress the uncertainty of intra-model adaptation on the unlabeled target data. SEED is a curriculum mutual learn-ing framework in which the expectation of the predictions from stochastically-sampled models are exploited to assist domain adaptation of the model bank. The differences be-tween SEED and the conventional knowledge distillation are shown in Table 1. As for intra-model adaptation, we borrow the solution from the state-of-the-art bi-classifier-based domain confusion method (such as SymNet [57] and
MCD [38]). Nevertheless, we analyze that there exists an optimization conflict between inter-model interaction and intra-model adaptation, which motivates us to augment an
Optimization-Separated Tri-Classifier (OSTC) to modulate the optimization between them.
For the second challenge, it is intuitive to search models with optimal adaptation performance under different com-putational budgets after training the model bank. However, unlike performance evaluation in the supervised tasks, none of the labeled target data is available. To be compatible with the unlabeled target data, we exploit the model with the largest capacity as an anchor to guide performance rank-ing in the model bank, since the larger models tend to be more accurate as empirically proven in [52]. In this way, we propose an Unsupervised Performance Evaluation Met-ric which is eased into the output discrepancy between the candidate model and the anchor model. The smaller the metric is, the better the performance is assumed to be.
Extensive ablation studies and experiments are carried out on three popular UDA benchmarks, i.e., ImageCLEF-DA [27], Office-31 [36], and Office-Home [45], which demonstrate the effectiveness of the proposed framework.
Our method can achieve state-of-the-art results compared with other competing methods.
It is worth emphasizing that our method can preserve the performance improvement against the source-only model even when the computing complexity is reduced to 1/64×. To summarize, our main contributions are listed as follows:
• We propose SlimDA, a “once-for-all” framework to jointly accommodate the adaptation performance and the computation budgets for resource-limited devices.
• We propose SEED to simultaneously boost the adap-tation performance of all models in the model bank.
In particular, we design an Optimization-Separated Tri-Classifier to modulate the optimization between intra-model adaptation and inter-model interaction.
• We propose an Unsupervised Performance Evaluation
Metric to facilitate architecture adaptation.
• Extensive experiments verify the effectiveness of our proposed SlimDA framework, which can surpass other state-of-the-art methods by a large margin. 2.