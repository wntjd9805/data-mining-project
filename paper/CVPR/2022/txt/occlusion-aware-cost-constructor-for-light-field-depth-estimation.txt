Abstract
Matching cost construction is a key step in light ﬁeld (LF) depth estimation, but was rarely studied in the deep learning era. Recent deep learning-based LF depth es-timation methods construct matching cost by sequentially shifting each sub-aperture image (SAI) with a series of pre-deﬁned offsets, which is complex and time-consuming. In this paper, we propose a simple and fast cost constructor to construct matching cost for LF depth estimation. Our cost constructor is composed by a series of convolutions with speciﬁcally designed dilation rates. By applying our cost constructor to SAI arrays, pixels under predeﬁned dispari-ties can be integrated and matching cost can be constructed without using any shifting operation. More importantly, the proposed cost constructor is occlusion-aware and can han-dle occlusions by dynamically modulating pixels from dif-ferent views. Based on the proposed cost constructor, we develop a deep network for LF depth estimation. Our net-work ranks ﬁrst on the commonly used 4D LF benchmark in terms of the mean square error (MSE), and achieves a faster running time than other state-of-the-art methods. 1.

Introduction
Light ﬁeld (LF) cameras can encode 3D scenes into 4D
LF images. By using the abundant spatial and angular infor-mation in the LF images, the scene depth can be obtained by performing LF depth estimation. As a fundamental task in
LF image processing, depth estimation beneﬁts many sub-sequent applications such as refocusing [33], view synthe-sis [3, 13], 3D reconstruction [14], and virtual reality [37].
With the advances of deep neural networks, many deep learning-based methods [2, 6, 7, 9, 17, 19, 26, 29] have been proposed and boosted the performance of LF depth estima-tion. Recent deep learning-based methods achieve LF depth estimation in a four-step pipeline including feature extrac-tion, cost construction, cost aggregation, and depth regres-sion. To achieve higher accuracy, these methods designed different modules for feature extraction [29] and cost aggre-gation [2,9]. However, as a key step in LF depth estimation, non-occluded pixel occluded pixel 1
= u 2
= u 3
= u v = 1 v = 2 v = 3 v = 1 v = 2 v = 3 (a) an example 3×3 LF (b) our OACC
Figure 1. An illustration of the proposed occlusion-aware cost 3 LF, in which constructor (OACC). (a) A toy example of a 3 the yellow star is partially occluded by the green triangle. (b) Our
OACC constructs matching cost via convolutions and can handle occlusions by assigning smaller weights to occluded pixels.
× matching cost construction was rarely studied.
To construct matching costs for LF depth estimation, ex-isting methods [2, 29] shift each sub-aperture image (SAI) with a series of predeﬁned offsets, and then concatenate the shifted SAIs to form a cost volume. Although this shift-and-concat scheme is easy to implement, the large number of shifting operation1 reduces the efﬁciency of these meth-ods. Moreover, during matching cost construction, pixels at different spatial locations are processed equally, which cannot handle the spatially-varying occlusions where some views are less informative and can even deteriorate the esti-mation results.
To handle the aforementioned challenges, in this paper, we propose an occlusion-aware cost constructor (OACC) for LF depth estimation. Our cost constructor is composed by a series of convolutions with speciﬁcally designed di-lation rates. By applying our OACC to SAI arrays, pixels under predeﬁned disparities can be integrated without per-forming shifting operation. More importantly, our OACC can handle occlusions by dynamically modulating pixels from different views, as shown in Fig. 1. Based on the proposed OACC, we develop a deep network for LF depth estimation. Our network achieves state-of-the-art depth es-timation accuracy with a signiﬁcant acceleration.
The contributions of this paper can be summarized as: 1For example, in LFAttNet [29], totally 80 views are shifted by 8 dis-parity levels, resulting in 640 sequential shifting operations.            
• We propose a cost constructor to replace the shift-and-concat approach for matching cost construction.
• We make our cost constructor to be occlusion-aware by modulating pixels from different views in a ﬁne-grained manner.
• We develope an OACC-Net for LF depth estimation.
Our method achieves top accuracy with signiﬁcant ac-celeration as compared to other state-of-the-art meth-ods on the 4D LF benchmark [8]. 2.