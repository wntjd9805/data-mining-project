Abstract
We study the problem of developing autonomous agents that can follow human instructions to infer and perform a sequence of actions to complete the underlying task. Sig-nificant progress has been made in recent years, especially for tasks with short horizons. However, when it comes to long-horizon tasks with extended sequences of actions, an agent can easily ignore some instructions or get stuck in the middle of the long instructions and eventually fail the task. To address this challenge, we propose a model-agnostic milestone-based task tracker (M-TRACK) to guide the agent and monitor its progress. Specifically, we pro-pose a milestone builder that tags the instructions with nav-igation and interaction milestones which the agent needs to complete step by step, and a milestone checker that system-ically checks the agent’s progress in its current milestone and determines when to proceed to the next. On the chal-lenging ALFRED dataset, our M-TRACK leads to a notable 33% and 52% relative improvement in unseen success rate over two competitive base models. 1.

Introduction
As autonomous agents (e.g., robots) become more inte-grated into our daily life, it is increasingly important to de-velop autonomous agents that can understand natural lan-guage commands and carry out the corresponding tasks. To facilitate such a goal, various benchmarks have been pro-posed in the realm of robot instruction following such as vision-and-language navigation (VLN) [1, 3, 4, 9, 13, 28, 29, 38, 44], together with a number of novel algorithms that consistently push forward the state of the art [20,21,34,37].
Specifically, to succeed in VLN, an agent must compre-hend the language instruction, ground it into the partially-observable environment with only visual perception, and plan and perform navigation and interaction actions in the
Figure 1. Illustration of our M-TRACK approach. We show an ALFRED task [30], which consists of an overall goal (text on the top) and six subtasks (text below each image). The blue/red text box within each image is our extracted navigation/interaction milestones from the subtask instructions. An agent needs to reach the milestone of the current subtask (e.g., reaching proximity to the target object for navigation milestones, or having interacted with the target objects for interaction milestone; green masks for target objects) before it can proceed to the next subtask. environment to complete the task.
One critical challenge in VLN arises when the task hori-zon becomes substantially longer [30]. That is, a task is so complex that it essentially consists of multiple “subtasks” that need to be completed sequentially to fulfill the whole task. For example, in Figure 1 the task “put a hot potato on the counter to the right of the sink” can be decomposed into six subtasks. Moreover, the subtask “heat the potato” must be carried out before the subtask “put the potato on the counter”; otherwise, the final task is doomed to fail no matter how accurate the subsequent planning is. Such a se-quential dependency requires the agent to closely monitor its progress and ensure it is staying on the right track when carrying out a long-horizon task.
At first glance, this challenge may seem trivial if the lan-guage instruction is detailed enough (like in Figure 1), such that it already defines the subtasks and their order. How-ever, as shown in the literature [2, 14, 20, 31, 37, 42] and our experiments, an agent fed with detailed instructions still frequently skips subtasks, or wanders around within a sub-task even when it is already completed. In essence, what an agent truly struggles with is the lack of awareness of where it currently is in the long subtask sequence and how much progress it has made within a subtask.
To address this issue, we propose to equip VLN agents with an explicit task tracker, which keeps track of the agent’s progress within a subtask and guides it for when to move on to the next. Concretely, we propose the concept of milestone, which renders the necessary condition of com-pleting a subtask. Namely, for a subtask to be considered as completed, the milestone must be reached. Take the subtask
“take a cold potato out of the fridge” in Figure 1 as exam-ple. To complete it, the necessary condition is that the agent must see the potato and the fridge, be close enough to them, and perform an interaction action with the potato. We argue that by explicitly extracting such milestones from the in-structions and grounding them to the environment state, we can systematically determine if the agent should continue working on the current subtask or proceed to the next.
To this end, we propose the milestone-based task tracker (M-TRACK), which consists of two components: milestone builder and milestone checker. The milestone builder ex-tracts the milestone (i.e., the necessary completion condi-tion) of each subtask from the corresponding language in-struction. We model it as a named entity recognition prob-lem and train a BERT-CRF tagger [6, 32] to accurately ex-tract both the target objects and their action type (i.e., nav-igation or interaction). The milestone checker then tries to ground (i.e., identify and localize) the extracted target ob-jects in the perceived environment using an object detection model [10] and checks if the agent is close enough to them and/or is about to interact with them — to decide if the agent is completing the current subtask and ready to move on. It is worth noting that our M-TRACK only needs to access the language instructions, the visual input to the agent, and the agent’s action, not any internal states of the agent. Thus, it is model-agnostic and can be easily integrated with any agent model with minimal changes.
How can M-TRACK interacts with the agent to affect its action (e.g., to not skip a subtask)? We propose two simple yet effective ways. First, at any time step, we feed the agent with only the part of the instructions that corre-sponds to the current subtask determined by the milestone tracker. This explicitly guides the agent to focus on the cur-rent subtask. Second, and more importantly, we apply the milestone checker proactively — before the agent executes its predicted action — to reject actions that will lead to sub-task failures. For instance, we reject the action of taking a
“sponge” if the milestone object is “fork” (Figure 2).
We validate M-TRACK on ALFRED [30], a recently released large-scale VLN dataset for common household tasks. The tasks in ALFRED are considered long-horizon because on average each task needs 50 actions to com-plete. In contrast, another popular dataset R2R [1] needs only 5. We integrate M-TRACK into two baseline VLN models LSTM [30] and VLN⟳BERT [12], and demonstrate notable and consistent performance gains. When tested in seen environments, M-TRACK leads to 16%–57% relative improvement in success rate. In more challenging unseen environments, the relative gain increases to 33%–52%. Our ablation studies and qualitative results further verify that the improvement indeed comes from agents able to better fol-low the sequence of subtasks and stay on the right track. 2.