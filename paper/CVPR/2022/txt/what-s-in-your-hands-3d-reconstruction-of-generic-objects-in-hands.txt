Abstract
Our work aims to reconstruct hand-held objects given a single RGB image. In contrast to prior works that typi-cally assume known 3D templates and reduce the problem to 3D pose estimation, our work reconstructs generic hand-held object without knowing their 3D templates. Our key insight is that hand articulation is highly predictive of the object shape, and we propose an approach that condition-ally reconstructs the object based on the articulation and the visual input. Given an image depicting a hand-held ob-ject, we first use off-the-shelf systems to estimate the under-lying hand pose and then infer the object shape in a nor-malized hand-centric coordinate frame. We parameterized the object by signed distance which are inferred by an im-plicit network which leverages the information from both visual feature and articulation-aware coordinates to pro-cess a query point. We perform experiments across three datasets and show that our method consistently outperforms baselines and is able to reconstruct a diverse set of objects.
We analyze the benefits and robustness of explicit articu-lation conditioning and also show that this allows the hand pose estimation to further improve in test-time optimization. 1.

Introduction
Humans interact with their surrounding world with their hands. Not only do we understand interaction as abstract concepts such as touching screens, squeezing balls, hold-ing pens, etc., we also perceive their underlying 3D shape.
Holding a pen means a stick lying on the purlicue and gripped by thumb, index, and middle fingers; holding a bowl is placing it on top of an up-facing palm. We aim to build a recognition system that can perceive and reason about the geometric information of hand-object interactions (HOI) for generic objects.
Over the past decade, we have made significant advances in inferring the 3D shape of both hands and objects in iso-lation. Hand poses can be recovered in the form of 2D keypoints, 3D skeletons [34, 45â€“47, 74], or even full 3D meshes [2, 56] via either fitting templates or direct regres-sion. On the other hand, recent works have also pursued scaling object reconstruction from estimating the 6D pose of one specific known object [51] to more generic objects, such as various instances within one category [23,35,39], or even pursuing a joint model for cross-category reconstruc-tion [11,22]. But one area where the progress has been quite limited is understanding human-object interactions (HOI) specifically for manipulable objects [14, 58].
Reconstruction of objects in hand in the wild is highly challenging and ill-posed due to lack of data, presence of mutual and self-occlusion. Current works [4, 17, 30, 41, 62] typically focus on reconstructing a handful of known ob-ject (3D model is given) which reduces reconstruction to a 6D pose estimation problem. We argue that knowing the 3D template of the object as a priori during inference is a strong assumption and prevents these systems from recon-structing unknown objects. Furthermore, they struggle to handle various object shapes in the wild as these templates are rigid and instance-specific. In contrast, our work studies hand-object reconstruction without object templates and in-stead focuses on reconstructing HOI for novel objects from images.
Our key observation is that hand articulation is driven by the local geometry of the object. Thus, hand articulation provides strong cues for the object in interaction. Fingers curled like fists indicate thin handles in between while open palms are likely to interact with flat surfaces.
Instead of treating the hand occlusion as noise to marginalize over, we explicitly consider hand pose as informative cues for the object it interacts with. We operationalize this idea by con-ditionally predicting the object shape based on hand articu-lation and the input image. Instead of estimating both hand pose and object shape jointly, we leverage advances in hand pose reconstruction to estimate hand pose first. Given the inferred articulated hand along with the input image, our approach then reconstructs the object in a normalized hand-centric coordinate frame.
We evaluate our method across three datasets includ-ing synthetic and real-world benchmarks and compare ours with prior explicit and implicit HOI reconstruction methods that infer the shape of unknown objects independent of hand pose. Our articulation-conditioned object shape prediction consistently outperforms prior works by large margins and can reconstruct various objects in a wide range of shapes.
We also analyze how our model benefits from articulation-aware coordinates. Lastly, we show that the initial hand pose estimation could be further improved by encouraging interaction between the predicted hand and the object. 2.