Abstract 1.

Introduction
Prior work has studied different visual modalities in iso-lation and developed separate architectures for recogni-tion of images, videos, and 3D data. Instead, in this pa-per, we propose a single model which excels at classifying images, videos, and single-view 3D data using exactly the same model parameters. Our ‘OMNIVORE’ model lever-ages the flexibility of transformer-based architectures and is trained jointly on classification tasks from different modal-ities. OMNIVORE is simple to train, uses off-the-shelf stan-dard datasets, and performs at-par or better than modality-specific models of the same size. A single OMNIVORE model obtains 86.0% on ImageNet, 84.1% on Kinetics, and 67.1% on SUN RGB-D. After finetuning, our models outperform prior work on a variety of vision tasks and generalize across modalities. OMNIVORE’s shared visual representation nat-urally enables cross-modal recognition without access to correspondences between modalities. We hope our results motivate researchers to model visual modalities together.
∗Equal technical contribution.
Computer vision research spans multiple modalities re-lated to our perception of the visual world, such as images, videos, and depth. In general, we study each of these modal-ities in isolation, and tailor our computer vision models to learn the best features from their specificities. While these modality-specific models achieve impressive performance, sometimes even surpassing humans on their specific tasks, they do not possess the flexibility that a human-like vision system doesÐthe ability to work across modalities. We ar-gue that the first step towards a truly all-purpose vision sys-tem is to build models that work seamlessly across modali-ties, instead of being over-optimized for each modality.
Beyond their flexibility, such modality-agnostic models have several advantages over their traditional, modality-specific counterparts. First, a modality-agnostic model can perform cross-modal generalization: it can use what it has learned from one modality to perform recognition in other modalities. For example, it can recognize pumpkins in 3D images even if it has only seen labeled videos of pumpkins.
In turn, this allows existing labeled datasets to be used more
effectively: it becomes possible to train models on the union of vision datasets with different input modalities. Second, it saves the research and engineering effort spent on opti-mizing models for a specific modality. For example, image and video models have followed a similar trajectory of evo-lution, from hand-crafted descriptors [47, 55] to convolu-tional networks [34, 91] and, eventually, vision transform-ers [5, 21]; however, each had to be developed and tuned individually. A common architecture would make scientific progress readily available to users of any visual modality.
Finally, a model that operates on many visual modalities is naturally multi-modal and can easily leverage new visual sensors as they becomes available. For instance, a modality-agnostic recognition model running on a robot can readily exploit a new depth sensor when it is installed on that robot.
Despite such clear advantages, modality-agnostic models have rarely been studied and their performance compared to their modality-specific counterparts has been disappoint-ing. There are many reasons that explain this situation, such as the need for a flexible architecture with enough capacity to learn modality-specific cues from the different modali-ties; and enough compute to train it on video, images, and single-view 3D simultaneously.
This paper develops a modality-agnostic vision model that leverages recent advances in vision architectures [21, 51]. The model we develop is ªomnivorousº in that it works on three different visual modalities: images, videos, and single-view 3D. Our OMNIVORE model does not use a custom architecture for each visual modality.
It per-forms recognition on all three modalities using the same, shared model parameters. It works by converting each in-put modality into embeddings of spatio-temporal patches, which are processed by exactly the same Transformer [92] to produce a representation of the input. We train OMNI-VORE on a collection of standard, off-the-shelf classifica-tion datasets that have different input modalities. Unlike prior work [33, 77], our training does not use explicit corre-spondences between different input modalities.
Our experiments demonstrate the advantages of our OM-NIVORE models. Surprisingly, we find that OMNIVORE representations generalize well across visual modalities (see Figure 1) even though OMNIVORE was not explicitly trained to model cross-modal correspondences. These ca-pabilities emerge without explicit cross-modal supervision simply due to the parameter sharing between models for different modalities. On standard image, video, and single-view 3D benchmarks, OMNIVORE performs at par with or better than modality-specific vision models with the same number of parameters. The same OMNIVORE model ob-tains 85.6% top-1 accuracy on ImageNet-1K, 83.4% top-1 on Kinetics-400, and 67.4% top-1 accuracy on SUN RGB-D. OMNIVORE’s strong generalization capabilities also ex-tend to transfer learning experiments. OMNIVORE performs
Input
Patches
Omnivore Model
Image
Video
Linear
Linear
Linear
+
Linear
T r a n s f o r m e r
Single-view 3D
Embeddings
Figure 2. Multiple visual modalities in the OMNIVORE model.
We convert image, video, and single-view 3D modalities into em-beddings that are fed into a Transformer model. The images are converted into patches, videos into spatio-temporal tubes, and the single-view 3D images are converted into RGB patches and depth patches. The patches are projected into embeddings using linear layers. We use the same linear layer for (image or video) RGB patches and a separate one for depth patches. at par with recent large transformers on ImageNet-1K, sets a new state-of-the-art on action recognition benchmarks such as EPIC-Kitchens-100, Something Something-v2, and on single-view 3D classification and segmentation bench-marks. We believe our work presents a compelling argu-ment for shifting towards the development of vision models that can operate on any visual modality. 2.