Abstract
In this paper, we propose a novel method to realize multi-modal image registration and fusion in a mutually reinforcing framework, termed as RFNet. We handle the registration in a coarse-to-ﬁne fashion. For the ﬁrst time, we exploit the feedback of image fusion to promote the registration accuracy rather than treating them as two separate issues. The ﬁne-registered results also improve the fusion performance. Speciﬁcally, for image registration, we solve the bottlenecks of deﬁning registration metrics applicable for multi-modal images and facilitating the network convergence. The metrics are deﬁned based on image translation and image fusion respectively in the coarse and ﬁne stages. The convergence is facilitated by the designed metrics and a deformable convolution-based network. For image fusion, we focus on texture preser-vation, which not only increases the information amount and quality of fusion results but also improves the feedback of fusion results. The proposed method is evaluated on multi-modal images with large global parallaxes, images with local misalignments and aligned images to validate the performances of registration and fusion. The results in these cases demonstrate the effectiveness of our method. 1.

Introduction
Multi-modal image fusion aims to merge the informa-tion from different imaging modalities to generate a sin-gle image with rich information and high quality. Be-cause the fused images can describe scenes comprehen-sively by merging the complementary information, image fusion serves as a powerful tool for wide applications, such as security, remote sensing, clinical treatment, etc.
As multi-modal images are taken from different de-vices/sensors, it inevitably leads to parallaxes due to bi-ased positions, angles, etc. However, almost all the fusion methods fail to consider parallaxes. They require an ac-∗Corresponding author (a) Separate issues (b) Proposed method
Figure 1. Treating registration and fusion as separate issues in ex-isting methods and the proposed mutually reinforcing framework. curate registration before fusion can take place, as shown in Fig. 1(a). Unfortunately, the diversity between differ-ent modalities poses a great challenge to improve the regis-tration accuracy, still resulting in mitigated misalignments in the pre-registered images. When registration and fusion are separate issues, existing fusion methods have to “toler-ate” rather than “ﬁght” the pre-registration misalignments.
Thus, multi-modal image registration and fusion become an urgent issue for the practical application of image fusion.
Meanwhile, in existing separate branches, image fusion is a downstream task of registration, and fails to provide feedbacks to improve registration accuracy. Nevertheless, considering the characteristics of fused images, it is possi-ble for image fusion to inversely eliminate misalignments.
First, the fused images integrate the information from both modalities. When the fused image is registered with either source image, the alleviated modal diversity reduces the dif-ﬁculty of registration. Second, the misalignments in fused images undoubtedly lead to more but repeated salient struc-tures, i.e., dense gradients. By comparison, an accurate reg-istration encourages the sparseness of gradients. Thus, the gradient sparsity of fusion results can act as a criterion to improve registration accuracy in a feedback fashion with-out losing the scene information in source images. Third, the fused images retain the obvious salient structures in a single image and discard some superﬂuous and useless in-formation during the fusion process. It reduces the nega-tive impact of superﬂuous information on image registra-tion. When image fusion helps to eliminate misalignments, the more precisely aligned data further promotes fusion re-sults. As a result, these two tasks can be mutually reinforced in this way, as illustrated in Fig. 1(b).
Speciﬁcally to the individual solution of each task, ei-ther image registration or fusion has its own bottlenecks.
For image registration, it is a difﬁcult problem to develop appropriate registration metrics or evaluation ways adap-tive for multi-modal data. The other important issue is to ensure that the designed registration constraints should be practical for deep network optimization though gradient de-scent. For image fusion, a general purpose is to enable fused images present the most amount of information, partly rep-resented by gradients. Moreover, as stated above, the gradi-ents of fused images play a critical role in eliminating mis-alignments. Combining these two aspects, fusion methods should be dedicated to the retention of texture information, which is consistent with both the fusion target and the feed-back function of image fusion to image registration.
To address the limitations of prior works and unexplored issues, we explore multi-modal image registration and fu-sion in a mutually reinforcing framework. We propose an unsupervised network to realize it, termed as RFNet. The proposed framework is summarized as Fig. 1(b). The regis-tration is handled in a coarse-to-ﬁne approach. The coarse stage corrects the global parallaxes through an evaluation metric based on image translation. The coarse-registered re-sults help generate meaningful but rough fused images. Im-age fusion and ﬁne registration are integrated in a single net-work. Then, to correct local misalignments, we rely on the characteristics of fused images to optimize the deformation-related parts in this network. Finally, the network generates the ﬁne-registered and fused image.
The main contributions of RFNet are summarized as fol-i) The problems of multi-modal image registration lows: and fusion are mutually reinforced in our work. It is the ﬁrst time that image fusion is exploited to promote multi-modal image registration accuracy through a deep neural network. ii) We focus on designing constraints to optimize the multi-modal registration performance.
In the coarse stage, we apply image translation to build an image-level evaluation metric. An improved network architecture is proposed to help facilitate the network convergence. In the ﬁne stage, the metric is designed based on the fusion results. iii) Con-sidering the texture retention in image fusion, we adapt a gradient channel attention mechanism to adaptively adjust the channel-wise contributions of features. Besides, we de-sign a gradient loss with bias. The network architecture and loss function are both based on the texture richness. 2.