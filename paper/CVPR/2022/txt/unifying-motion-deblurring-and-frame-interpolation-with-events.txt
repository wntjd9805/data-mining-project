Abstract
Slow shutter speed and long exposure time of frame-based cameras often cause visual blur and loss of inter-frame information, degenerating the overall quality of cap-tured videos. To this end, we present a unified framework of event-based motion deblurring and frame interpolation for blurry video enhancement, where the extremely low latency of events is leveraged to alleviate motion blur and facilitate intermediate frame prediction. Specifically, the mapping re-lation between blurry frames and sharp latent images is first predicted by a learnable double integral network, and a fu-sion network is then proposed to refine the coarse results via utilizing the information from consecutive blurry inputs and the concurrent events. By exploring the mutual constraints among blurry frames, latent images, and event streams, we further propose a self-supervised learning framework to enable network training with real-world blurry videos and events. Extensive experiments demonstrate that our method compares favorably against the state-of-the-art ap-proaches and achieves remarkable performance on both synthetic and real-world datasets. Codes are available at https://github.com/XiangZ-0/EVDI. 1.

Introduction
Highly dynamic scenes, e.g., fast-moving targets or non-linear motions, pose challenges for high-quality video gen-eration as the captured frame is often blurred and target in-formation is missing between consecutive frames [29]. Ex-isting frame-based methods attempt to tackle these prob-lems by developing motion deblurring [11], frame interpo-lation [1] or blurry video enhancement techniques [10, 25].
However, it is difficult for frame-based deblurring methods to predict sharp latent frames from severely blurred videos
†Corresponding author
The research was partially supported by the National Natural Science
Foundation of China under Grants 61871297, the Natural Science Founda-tion of Hubei Province, China under Grant 2021CFB467, the Fundamental
Research Funds for the Central University under Grant 2042020kf0019, and the National Natural Science Foundation of China Enterprise Innova-tion Development Key Project under Grant U19B2004.
Figure 1. Illustrative examples of video deblurring and interpola-tion via the state-of-the-art deblurring approach LEVS [11], inter-polation approach Time Lens [30] and our EVDI method. because of motion ambiguities and the erasure of intensity textures [11]. Besides, current frame-based interpolation approaches generally assume the motion between neigh-boring frames to be linear [1], which is not always valid in real-world scenarios especially when encountering non-linear motions, thus often leading to incorrect predictions.
Recent works have revealed the advantages of event cameras [5] in motion deblurring and frame interpolation.
On one hand, the output of event camera inherently embeds precise motions and sharp edges [2] since it reports asyn-chronous event data with extremely low latency (in the or-der of µs) [5, 13], which is effective in alleviating motion blur [14, 21, 22, 31, 34]. On the other hand, event camera is able to record almost continuous brightness changes to compensate the missing information between consecutive frames, making it feasible to recover accurate intermedi-ate frames even under non-linear motions [14, 30]. How-ever, existing works generally treat motion deblurring and frame interpolation as separate tasks, while the problems of
motion blur and missing information between frames have strong co-occurrence in real scenes and thus need to be con-sidered simultaneously. In real-world scenarios, the afore-mentioned methods face two main challenges as follows.
• Limitations of Separate Tasks: The performance of interpolation methods [30] is often highly dependent on the quality of reference frames, and it is difficult to interpolate clear results when the reference frames are degraded by motion blur. For deblurring task, most methods [31, 34] focus on recovering sharp images inside the exposure time of blurry inputs, neglecting these latent images between blurry frames (see Fig. 1).
• Data Inconsistency: Most previous works employ well-labeled synthetic datasets for supervised [30, 31] or semi-supervised learning [34], which often causes performance drop in real scenes due to the inconsis-tency between synthetic and real-world data [34].
In this paper, we present a unified framework of Event-based Video Deblurring and Interpolation (EVDI) for blurry video enhancement. The proposed method consists of two main modules: a learnable double integral (LDI) network and a fusion network. The LDI network is designed to automatically predict the mapping relation between blurry frames and sharp latent images from the corresponding events, where the timestamp of the latent image can be cho-sen arbitrarily inside the exposure time of blurry frames (de-blurring task) or between consecutive blurry frames (inter-polation task). The fusion network receives the coarse re-construction of latent images and generates a fine result by utilizing all the information from consecutive blurry frames and event streams. For training, we take advantage of the mutual constraints among blurry frames, sharp latent im-ages and event streams, and propose a fully self-supervised learning framework to help the network fit the distribution of real-world data without the need of ground truth images.
The main contributions of this paper are three-fold:
• We present a unified framework of event-based video deblurring and interpolation that generates arbitrarily high frame-rate sharp videos from blurry inputs.
• We propose a fully self-supervised framework to en-able network training in real-world scenarios without any labeled data.
• Experiments on both synthetic and real-world datasets show that our method achieves state-of-the-art results while maintaining an efficient network design. 2.