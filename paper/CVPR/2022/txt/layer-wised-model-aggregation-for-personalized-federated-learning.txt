Abstract
Personalized Federated Learning (pFL) not only can cap-ture the common priors from broad range of distributed data, but also support customized models for heteroge-neous clients. Researches over the past few years have applied the weighted aggregation manner to produce per-sonalized models, where the weights are determined by cal-ibrating the distance of the entire model parameters or loss values, and have yet to consider the layer-level im-pacts to the aggregation process, leading to lagged model convergence and inadequate personalization over non-IID datasets.
In this paper, we propose a novel pFL train-ing framework dubbed Layer-wised Personalized Federated learning (pFedLA) that can discern the importance of each layer from different clients, and thus is able to optimize the personalized model aggregation for clients with heteroge-neous data. Specifically, we employ a dedicated hyper-network per client on the server side, which is trained to identify the mutual contribution factors at layer granularity.
Meanwhile, a parameterized mechanism is introduced to update the layer-wised aggregation weights to progressively exploit the inter-user similarity and realize accurate model personalization. Extensive experiments are conducted over different models and learning tasks, and we show that the proposed methods achieve significantly higher performance than state-of-the-art pFL methods. 1.

Introduction
Federated learning (FL) has emerged as a prominent col-laborative machine learning framework to exploit inter-user similarities without sharing the private data [33, 43, 52].
When users’ datasets are non-IID (independent and iden-tically distributed), i.e., the inter-user distances are large
[23, 53], sharing a global model for all clients may lead
†Equal contribution
∗Corresponding author (a) (b)
Figure 1. A toy example: Layer-wised vs. Model-wised aggre-gation method. (a) Model performance of client 1. Both of two methods perform similarity-based personalized aggregation. i.e., layer-wised: perform personalized aggregation by calculating the similarity between layers; model-wised: perform personalized ag-gregation by calculating the similarity between models. (b) The weight of each layer for client 1 in the last communication round. to slow convergence or poor inference performance as the model may significantly deviate from their local data
[14, 56].
To deal with such statistical diversity, personalized feder-ated learning (pFL) mechanisms are proposed to allow each client to train a customized model to adapt to their own data distribution [9, 12, 15, 22]. Literature status quo to achieve i.e., smoothing pFL include the data-based approaches, the statistical heterogeneity among clients’ datasets [8, 16], the single-model approaches, e.g., regularization [22, 41], meta-learning [9], parameter decoupling [5, 24, 26], and the multiple-model ways, i.e., train personalized models for each client [15, 54], which can produce personalized mod-els for each client via weighted combinations of clients’ models. Existing pFL methods apply a distance metric among the whole model parameters or loss values of differ-ent clients, which is insufficient to exploit their heterogene-ity since the overall distance metric cannot always reflect the importance of each local model and can lead to inac-curate combining weights or unbalance contribution from non-IID distributed datasets, and thus prevent further per-sonalization for clients at scale. The main reason is that different layers of a neural network can have different util-ities, e.g., the shallow layers focus more on local feature extraction, while the deeper layers are for extracting global features [6, 20, 21, 47, 49]. Measuring the model distances would ignore such layer-level differences, and cause inaccu-rate personalization that hinders the pFL training efficiency.
In this paper, we propose a band-new pFL framework that can realize the layer-level aggregation for FL personal-ization, which can accurately recognize the utility of each layer from clients’ model for adequate personalization, and thus can improve the training performance over non-IID datasets. A toy example is presented to illustrate that tra-ditional model-level aggregation based pFL method fails in reflecting the inner relationship among all local models, which motivates us to exploit an effective way to discern the layer-level impacts during the pFL training procedure.
Observation of Layer-wised Personalized Aggregation.
In the toy example, we consider six clients to collabora-tively learn their personalized models for a nine-class clas-sification task. The average model accuracy is obtained via both the layer-wised and model-wised aggregation ap-proaches, which utilize the inter-layer and inter-model sim-ilarities respectively. Figure 1 shows that higher model ac-curacy can be achieved by the layer-wised approach com-paring with the model-wised one for a certain client. The weights of layers for this client after the last communication round are also plotted, and we show that applying different weights for different layers, e.g., the first and second fully-connected layer (i.e., FC1, FC2) on client 1 have larger weights, while the second convolution layer, i.e., Conv1 layer has smaller weights, can produce significant perfor-mance gain for the personalized model accuracy.
The toy example demonstrates the potential of the layer-wised aggregation to achieve higher performance than tra-ditional model based pFL methods, since the layer-level similarities can reflect more accurate correlation among clients. By exploiting such layer-wised similarity and iden-tifying the layer-level inter-user contribution, it is promis-ing to produce efficient and effective personalized models for all clients. Motivated by such observation, we propose a novel federated training framework, namely, pFedLA, which adaptively facilitates the underlying collaboration be-tween clients in a layer-wised manner. Specifically, at the server side, we introduce a dedicated hypernetwork for each client to learn the weights of cross-clients’ layers during the pFL training procedure, which is shown to effectively boost the personalization over non-IID datasets. Extensive experi-ments are conducted, and we demonstrate that the proposed pFedLA can achieve higher performance than the state-of-the-art baselines over widely used models and datasets, i.e.,
EMNIST, FashionMNIST, CIFAR10 and CIFAR100. The contributions of the paper are summarized as follows:
• To the best of our knowledge, this paper is the first to explicitly reveal the benefits of layer-wised aggrega-tion comparing with model-wised approaches in pFL among heterogeneous FL clients;
• We propose a layer-wised personalized federated learning (pFedLA) training framework that can effec-tively exploit the inter-user similarities among clients with non-IID data and produce accurate personalized models;
• We conduct extensive experiments on four typical im-age classification tasks, which demonstrated the supe-rior performance of pFedLA over the state-of-the-art approaches. 2.