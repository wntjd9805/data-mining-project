Abstract
Mean Average Precision (mAP) is the primary evalua-tion measure for object detection. Although object detec-tion has a broad range of applications, mAP evaluates de-tectors in terms of the performance of ranked instance re-trieval. Such the assumption for the evaluation task does not suit some downstream tasks. To alleviate the gap between downstream tasks and the evaluation scenario, we propose
Optimal Correction Cost (OC-cost), which assesses detec-tion accuracy at image level. OC-cost computes the cost of correcting detections to ground truths as a measure of accuracy. The cost is obtained by solving an optimal trans-portation problem between the detections and the ground truths. Unlike mAP, OC-cost is designed to penalize false positive and false negative detections properly, and every image in a dataset is treated equally. Our experimental re-sult validates that OC-cost has better agreement with hu-man preference than a ranking-based measure, i.e., mAP for a single image. We also show that detectors’ rankings by OC-cost are more consistent on different data splits than mAP. Our goal is not to replace mAP with OC-cost but pro-vide an additional tool to evaluate detectors from another aspect. To help future researchers and developers choose a target measure, we provide a series of experiments to clarify how mAP and OC-cost differ. 1.

Introduction
Evaluation measure is an important factor determining the direction of the algorithm development. Most object detection benchmarks adopt the mean average precision (mAP) as their primary evaluation metric, and, therefore, great efforts are made to achieve higher mAP scores. While much research relies on mAP, we may not be completely aware of the consequences of optimising mAP.
Mean average precision is a ranking measure used in the
Figure 1. Toy example of three detectors and corresponding mAP and OC-cost. Higher is better in mAP, and lower is better in OC-cost. Top and middle: mAP does not treat each image equally.
Detector A and B get the same mAP even though detector B does not produces any detections in the two images. Bottom: mAP does not penalizes incorrect detections ranked lower than correct ones. information retrieval community [20]. Originally, the VOC challenge adopted mAP and included it into the evaluation protocol for object detection [7]. In this evaluation protocol, all detected instances are ranked in the order of their con-fidence scores. Then, the average precision (AP) for each object category is calculated from the precision/recall curve of the ranked instances. Mean average precision summa-rizes the individual APs by averaging them across all cate-gories. Evaluation using mAPs views object detection as a task to rank detected instances for each category. However, the range of real-world applications of detection algorithms is broad, and the ranking measures may not always be the appropriate objective to be optimized.
Figure 1 illustrates the characteristic behaviors of the mAP measure. Suppose we have three detectors A, B, and
C. Each detector attempts to find ducks and donkeys from a database consisting of three images. Detectors A and
B result in the identical mAP scores, although detector B completely ignores two of the three images. This exam-ple demonstrates that mAP does not treat images equally, and a detector does not get penalized for not producing any detections in some images in the dataset. Consistent op-eration on variety of images is a critical property in many real-world applications. For example, in autonomous driv-ing, neglecting the performance in rare scenes may lead to serious risks. However, mAP does not capture such local performance drop. One remedy to mitigate this problem is to compute mAP for individual images by considering each image as a dataset, which has only one sample.
There is another issue in ranking measure-based eval-uation. The example of detector C illustrates how mAP ignores some types of incorrect detections, which is non-intuitive. Detector C is an extreme example that only detects donkeys. Detector C produces many incorrect donkey de-tections, however, mAP does not penalize the incorrect de-tections as the detections are ranked lower than the correct one. Although the mAP’s behavior is reasonable for eval-uation of a ranking problem such as content-based image retrieval, some applications need different type of evalua-tion. For example, services like an on-demand visual recog-nition API, where various users independently upload their images, have to provide consistent performance for a wide range of images. For such services, the detection accuracy in the image level is more important than the per-class rank-ing performance over the entire dataset.
There are also several problems in the mAP’s implemen-tation. In mAP, each prediction is assigned to a ground truth to determine if the prediction is successful or not. However, the assignment is obtained in a greedy fashion, which the obtained solution may not be optimal. Non-optimal assign-ment may underrate detection results. Furthermore, mAP commonly uses thresholding on the intersection over union (IoU) scores to determine the success or failure of each de-tection. Prior research has shown that due to this threshold-ing, mAP does not reflect how well the predicted bound-ing box localizes the ground-truth instance [15]. Lastly, in mAP, classification is more critical than the localiza-tion quality. All detections are first grouped based on the predicted category, and misclassifications are considered as complete failures regardless of their localization quality.
In this paper, we propose a new evaluation measure called Optimal Correction Cost (OC-cost) that aims to eval-uate the detection accuracy at the image level. Different from mAP that evaluates ranked instances detected in a whole dataset, OC-cost evaluates detection result for a sin-gle image, and the score is independent from other images.
Specifically, we evaluate the detection performance using the cost of correcting detections to the ground truths. We expect that our evaluation measure better suits applications where image-level detection accuracy is critical.
To address the aforementioned problems, we formulate the computation of OC-cost as an optimal transportation problem. For every detection and ground-truth pair, we de-fine a unit correction cost that consists of a classification and localization cost. Given the pair-wise correction costs, we find the globally optimal assignment that minimizes the to-tal cost by solving an optimal transportation problem. This approach, inspired by the previous work [8], avoids non-optimal assignments of detections to ground truths and IoU thresholding. The approach also allow users to balance clas-sification and localization assessment. We explore the po-tential of the optimal transportation cost as an alternative evaluation measure for detection tasks and re-define it as an evaluation measure.
Our contributions are summarized as follows:
• We develop an alternative evaluation measure for ob-ject detection tasks. Unlike mAP, which evaluates the performance of instance ranking, ours evaluates image-level detection accuracy.
Image-level evalua-tion is suitable for some applications where a detector is expected to work consistently over various images.
• We conduct a series of experiments to illustrate the behavior of OC-cost and how OC-cost differs from a ranking measure-based evaluation, i.e., mAP. We also demonstrate that our measure is useful for developing detectors by tuning post-processing parameters. 2.