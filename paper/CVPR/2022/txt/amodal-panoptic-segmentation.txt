Abstract
Humans have the remarkable ability to perceive objects as a whole, even when parts of them are occluded. This abil-ity of amodal perception forms the basis of our perceptual and cognitive understanding of our world. To enable robots to reason with this capability, we formulate and propose a novel task that we name amodal panoptic segmentation. The goal of this task is to simultaneously predict the pixel-wise semantic segmentation labels of the visible regions of stuff classes and the instance segmentation labels of both the visible and occluded regions of thing classes. To facilitate research on this new task, we extend two established bench-mark datasets with pixel-level amodal panoptic segmentation labels that we make publicly available as KITTI-360-APS and BDD100K-APS. We present several strong baselines, along with the amodal panoptic quality (APQ) and amodal parsing coverage (APC) metrics to quantify the performance in an interpretable manner. Furthermore, we propose the novel amodal panoptic segmentation network (APSNet), as a first step towards addressing this task by explicitly mod-eling the complex relationships between the occluders and occludes. Extensive experimental evaluations demonstrate that APSNet achieves state-of-the-art performance on both benchmarks and more importantly exemplifies the utility of amodal recognition. The datasets are available at http:
//amodal-panoptic.cs.uni-freiburg.de. 1.

Introduction
Humans rely on their ability to perceive complete physical structures of objects even when they are only partially visible, to navigate through their daily lives [19]. This ability, known as amodal perception, serves as the link that connects our per-ception of the world to its cognitive understanding. However, unlike humans, robots are limited to modal perception [17, 27, 36], which restricts their ability to emulate the visual experience that humans have. In this work, we bridge this gap by proposing the amodal panoptic segmentation task.
Any given scene can broadly be categorized into two components: stuff and thing. Regions that are amorphous or (a) Panoptic Segmentation (b) Amodal Panoptic Segmentation
Figure 1. Illustration of (a) panoptic segmentation and (b) amodal panoptic segmentation that encompasses visible regions of stuff classes, and both visible and occluded regions of thing classes as amodal masks. uncountable belong to stuff classes (e.g., sky, road, sidewalk, etc.), and the countable objects of the scene belong to thing classes (e.g., cars, trucks, pedestrians, etc.). The amodal panoptic segmentation task illustrated in Fig. 1 (b) aims to concurrently predict the pixel-wise semantic segmentation labels of visible regions of stuff classes, and instance segmen-tation labels of both the visible and occluded regions of thing classes. We believe this task is the ultimate frontier of visual recognition and will immensely benefit the robotics com-munity. For example, in automated driving, perceiving the whole structure of traffic participants at all times, irrespective of partial occlusions [28], will minimize the risk of accidents.
Moreover, by inferring the relative depth ordering of objects in a scene, robots can make complex decisions such as in which direction to move relative to the object of interest [9] to obtain a clearer view without additional sensor feedback.
Amodal panoptic segmentation is substantially more chal-lenging as it entails all the challenges of its modal counter-part (scale variations, illumination changes, cluttered back-ground, etc.) while simultaneously requiring more complex
occlusion reasoning. This becomes even more complex for non-rigid classes such as pedestrians. These aspects also re-flect on the groundtruth annotation effort that it necessitates.
In essence, this task requires an approach to fully grasp the structure of objects and how they interact with other objects in the scene to be able to segment occluded regions even for cases that seem ambiguous.
Our contributions in this paper are twofold. First, we propose the novel task of amodal panoptic segmentation, a comprehensive scene recognition problem. To fully establish the task as well as to encourage future research, we extend two challenging urban driving datasets with amodal panop-tic segmentation labels to create the KITTI-360-APS and
BDD100K-APS benchmarks. We present several baselines for this task by combining state-of-the-art amodal instance segmentation methods with top-down panoptic segmenta-tion networks. Further, we introduce two evaluation metrics referred to as amodal panoptic quality (APQ) and amodal parsing coverage (APC), to coherently quantify the perfor-mance of segmentation of stuff classes in visible regions and thing classes in both visible and occluded object regions.
The APQ metric measures the performance independent of the size of instances and the APC metric considers the size of instances while giving more importance to the segmentation quality of larger objects than smaller objects. We introduce the size-dependent metric since a variety of applications seek high-quality segmentation of objects closer to the camera than far away objects, such as in autonomous driving.
Second, we propose the novel APSNet architecture that consists of a shared backbone and task-specific semantic and amodal instance segmentation heads followed by a parameter-free fusion module that yields the amodal panop-tic segmentation output. In our approach, we split the amodal bounding box contents into the visible region mask of the target object, the occluded region mask of the target object referred to as the occlusion mask, and the object masks that occludes the target object referred to as the occluder. The oc-cluder and occlusion features enable the amodal mask head to identify occlusion regions, while the visual and occlusion features enable the network to predict the amodal mask of the object. Furthermore, we refine the visible mask with amodal features in conjunction with visible features to impart oc-clusion awareness. To prevent the loss of localization of features in favor of semantic features, we increase the recep-tive field for context aggregation with dilated convolutions instead of downsampling in the semantic head. We make our code and models publicly available at http://amodal-panoptic.cs.uni-freiburg.de. 2.