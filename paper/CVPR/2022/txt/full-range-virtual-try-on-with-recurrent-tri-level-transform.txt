Abstract
Virtual try-on aims to transfer a target clothing image onto a reference person. Though great progress has been achieved, the functioning zone of existing works is still lim-ited to standard clothes (e.g., plain shirt without complex laces or ripped effect), while the vast complexity and vari-ety of non-standard clothes (e.g., off-shoulder shirt, word-shoulder dress) are largely ignored.
In this work, we propose a principled framework, Re-current Tri-Level Transform (RT-VTON), that performs full-range virtual try-on on both standard and non-standard clothes. We have two key insights towards the framework design: 1) Semantics transfer requires a gradual feature transform on three different levels of clothing representa-tions, namely clothes code, pose code and parsing code. 2) (cid:0) Corresponding author
Geometry transfer requires a regularized image deforma-tion between rigidity and flexibility. Firstly, we predict the semantics of the “after-try-on” person by recurrently refin-ing the tri-level feature codes using local gated attention and non-local correspondence learning. Next, we design a semi-rigid deformation to align the clothing image and the predicted semantics, which preserves local warping simi-larity. Finally, a canonical try-on synthesizer fuses all the processed information to generate the clothed person im-age. Extensive experiments on conventional benchmarks along with user studies demonstrate that our framework achieves state-of-the-art performance both quantitatively and qualitatively. Notably, RT-VTON shows compelling re-sults on a wide range of non-standard clothes. Project page: https://lzqhardworker.github.io/RT-VTON/. 1.

Introduction
Virtual try-on is a rapidly advancing topic in both academia and industry with the increasing power of gener-ative models. Various pipelines [4, 10, 14, 37, 42] are pro-posed to build the system, but it remains challenging to perform full-range try-on with different clothing types in real-world scenarios. Standard clothes such as T-shirts and long-sleeve jackets show clear relationship with the refer-ence person, while non-standard clothes can involve irreg-ular patterns and design, thus resulting in more ambiguous corresponding relations. Two typical non-standard types are the off-shoulder clothes (normal collar with shoulder ex-posed) and the word-shoulder clothes (a horizontal collar line towards shoulder). The results on those kinds of non-standard clothes are scarcely reported in any of the try-on papers [4, 10, 14, 29, 37, 42].
Earlier works [14, 37] utilize the coarse shape and pose map to synthesize try-on results with Thin-plate Spline (TPS) warping. Pioneering methods [10, 17, 42] amelio-rate the blurry artifacts caused by coarse shape [14, 37] by firstly predicting the semantic layout with the target cloth-ing image and then warping the clothing image by regu-larized TPS, producing better results with sharper bound-aries. However, these methods [10, 42] still struggle to pre-cisely depict the “after-try-on” semantics, where the func-tioning zone is restricted to standard clothes. Another bar-rier preventing full-range virtual try-on is the misalignment of the clothing image with the reference person. TPS is a usual practice as used in [2, 4, 14, 37, 42] to spatially trans-form the clothing image while preserving the characteris-tics. However, over-distortion of the clothing image hinders the TPS-based methods, instigating the increasing prefer-ence for affine-based algorithms [9,20]. As opposed to TPS, affine-based methods [9, 20] demonstrate large potential in generating undistorted results, but the non-rigid part of de-formation is not involved which fails to mimic the natural interaction between the clothes and the person. Flow-based methods [5, 11, 12, 41] embed the maximized capacity in the deformation modeling which densely predict the pixel-wise offset field. However, without the ground-truth flow, optimizing the flow network is only possible with strong regularization priors such as affine prior, total variance con-straint, or second-order Laplacian penalty.
To achieve full-range virtual try-on, we propose a prin-cipled framework, Recurrent Tri-Level Transform (RT-VTON) which deeply mines the “after-try-on” semantics by accurately predicting the semantic layout of the refer-ence person given the target clothing image, and then co-herently deform the clothing image with our semi-rigid deformation to balance rigidity and flexibility. Specif-ically, RT-VTON follows a conventional split-transform-merge scheme (Fig. 2) as in [4, 5, 22, 41, 42]. The first mod-ule is Semantic Generation Module (SGM), which gradu-ally transforms the tri-level feature codes to predict the se-mantic segmentation of the body parts and the clothing re-gion. As opposed to prior works, our SGM can accurately capture the correlation between the target clothing image with the human body, and thus perform full-range try-on especially for non-standard clothes (see Fig. 1). The sec-ond module is Clothes Deformation Module (CDM) which applies a novel semi-rigid deformation to align the target clothing image according to the semantic output of SGM.
We borrow the widely-used geometric editing technique from graphics [16, 34] and, for the first time, integrate it within a differentiable learning-based framework. Finally, a Try-on Synthesizer Module (TOM) similar to [4,42] fuses the semantic segmentation, the warped clothes as well as the non-target body image to synthesize the final try-on output, where an auxiliary clothes reconstruction loss is used to en-hance texture preserving.
We summarize our contributions as follows. 1) We pro-pose a new image-based virtual try-on framework, i.e. RT-VTON, which accurately depicts the “after-try-on” seman-tics and thus greatly improve try-on quality and adaptabil-ity for full-range garment types. 2) A novel Recurrent Tri-Level Transform is proposed to improve the semantic layout prediction, which gradually updates three different levels of clothing representations, namely clothes code, pose code, and parsing code, by local gated attention mechanism with non-local correspondence learning. 3) To perform undis-torted clothes warping, we design a semi-rigid deforma-tion to align the clothing image with the predicted seman-tics, which preserves local warping similarity. 4) Exten-sive experiments demonstrate that the proposed method can perform realistic virtual try-on for both standard and non-standard clothes, outperforming the state-of-the-art meth-ods qualitatively and quantitatively. 2.