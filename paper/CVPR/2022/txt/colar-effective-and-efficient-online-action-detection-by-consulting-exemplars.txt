Abstract
Online action detection has attracted increasing re-search interests in recent years. Current works model his-torical dependencies and anticipate the future to perceive the action evolution within a video segment and improve the detection accuracy. However, the existing paradigm ig-nores category-level modeling and does not pay sufficient attention to efficiency. Considering a category, its repre-sentative frames exhibit various characteristics. Thus, the category-level modeling can provide complimentary guid-ance to the temporal dependencies modeling. This pa-per develops an effective exemplar-consultation mechanism that first measures the similarity between a frame and exem-plary frames, and then aggregates exemplary features based on the similarity weights. This is also an efficient mecha-nism, as both similarity measurement and feature aggrega-tion require limited computations. Based on the exemplar-consultation mechanism, the long-term dependencies can be captured by regarding historical frames as exemplars, while the category-level modeling can be achieved by re-garding representative frames from a category as exem-plars. Due to the complementarity from the category-level modeling, our method employs a lightweight archi-tecture but achieves new high performance on three bench-marks.
In addition, using a spatio-temporal network to tackle video frames, our method makes a good trade-off between effectiveness and efficiency. Code is available at https://github.com/VividLe/Online-Action-Detection. 1.

Introduction
With the development of mobile communications, video has become a powerful medium to record life and trans-form information. As a result, video understanding tech-nologies have aroused increasing research interests. Among these technologies, temporal action detection [36, 39, 62] can discover action instances from untrimmed videos and extract valuable information. Well-performed action detec-*Corresponding author.
Figure 1. Comparison between existing state-of-the-art method
OadTR [42] and our proposed Colar. Unlike OadTR, Colar con-sults historical exemplars to model long-term dependencies and consults category exemplars to capture category-level particular-ity, forming an effective and efficient method. tion algorithms can benefit smart surveillance [32], anomaly detection [4] etc. In recent years, along with action detec-tion technologies becoming mature, a more challenging but more practical task, namely online action detection, has been proposed [8]. The online action detection algorithm tackles a streaming video, reports the occurrence of an ac-tion instance, and keeps alarming until the action ends [8].
In inference, the algorithm only employs historical frames that have been observed, but has no access to future frames.
As an early exploration, Geest et al. [8] discovered the importance of modeling long-term dependencies. Later, Xu et al. [45] revealed the value of anticipating future status to enhance the long-term dependencies modeling. OadTR
[42] recently utilized the multi-head self-attention module to jointly model historical dependencies and anticipate the future, which achieved promising online action detection results.
As an under-explored domain, there are three core chal-lenges for online action detection: How to model long-term dependencies? How to associate a frame with representa-tive frames from the same category? How to conduct de-tection efficiently? Existing works [8, 42, 45] primarily fo-cus on the long-term dependencies modeling, but ignore the
other two challenges. However, as shown in Figure 1 (a), both analyzing historical frames and anticipating future sta-tus only model relationships within a video segment, leav-ing the category-level modeling under-explored. Because an action category contains multiple instances and each in-stance exhibits special appearance and motion characteris-tic, the guidance of exemplary frames can make the online detection algorithm more robust to resist noises within a video segment.
In addition, a practical online action de-tection algorithm should always consider the computational efficiency, including both the efficiency to perform online detection and the efficiency to extract video features.
This paper develops an exemplar-consultation mecha-nism to tackle above three challenges in a unified frame-work. The exemplar-consultation mechanism first jointly transforms a frame and its exemplary frames to the key space and value space. Then, it measures the similarity in the key space and employs the similarity to aggregate information in the value space. As both feature transfor-mation and similarity measurement require limited compu-tations, the proposed exemplar-consultation mechanism is efficient. Considering a video segment, we can effectively model long-term dependencies by using historical frames as exemplars based on the exemplar-consultation mecha-nism. As we only compare one frame with its historical frames, rather than performing self-attention on all frames, the computational burden is alleviated. Similarly, we can also regard representative frames of each category as ex-emplars and conduct category-level modeling based on the exemplar-consultation mechanism. Compared with a video segment, category exemplars can provide complementary guidances and make the algorithm more robust.
By consulting exemplars, we build a unified framework, namely Colar, to perform online action detection, as shown in Figure 2. Colar maintains the dynamic branch and the static branch in parallel, where the former models long-term dependencies within a video segment and the latter models category-level characteristics. In the dynamic branch, Colar consults previous frames and aggregates historical features.
In the static branch, Colar first obtains category exemplars via clustering, then consults exemplars and aggregate cate-gory features. Finally, two classification scores are fused to detect actions. Moreover, we analyze the running time bot-tleneck of existing works and discover the expensive costs to extract flow features. Thus, we employ a spatio-temporal network to only dispose of video frames and perform end-to-end online action detection, which only takes 9.8 seconds to tackle a one-minute video. To sum up, this paper makes the following contributions:
• We make an early attempt to conduct category-level modeling for the online action detection task, which provides holistic guidance and makes the detection al-gorithm more robust.
• We propose the exemplar-consultation mechanism to compare similarities and aggregate information, which can efficiently model long-term dependencies and cat-egory particularities.
• Due to the effectiveness of the exemplar-consultation mechanism and the complimentary guidance from category-level modeling, our method employs a lightweight architecture. Still, it achieves superior per-formance and builds new state-of-the-art performance on three benchmarks. 2.