Abstract
Effectively utilizing the vast amounts of ego-centric nav-igation data that is freely available on the internet can ad-vance generalized intelligent systems, i.e., to robustly scale across perspectives, platforms, environmental conditions, scenarios, and geographical locations. However, it is dif-ﬁcult to directly leverage such large amounts of unlabeled and highly diverse data for complex 3D reasoning and plan-ning tasks. Consequently, researchers have primarily fo-cused on its use for various auxiliary pixel- and image-level computer vision tasks that do not consider an ulti-mate navigational objective.
In this work, we introduce
SelfD, a framework for learning scalable driving by utiliz-ing large amounts of online monocular images. Our key idea is to leverage iterative semi-supervised training when learning imitative agents from unlabeled data. To handle unconstrained viewpoints, scenes, and camera parameters, we train an image-based model that directly learns to plan in the Bird’s Eye View (BEV) space. Next, we use unla-beled data to augment the decision-making knowledge and robustness of an initially trained model via self-training.
In particular, we propose a pseudo-labeling step which en-ables making full use of highly diverse demonstration data through “hypothetical” planning-based data augmentation.
We employ a large dataset of publicly available YouTube videos to train SelfD and comprehensively analyze its gen-eralization beneﬁts across challenging navigation scenar-ios. Without requiring any additional data collection or annotation efforts, SelfD demonstrates consistent improve-ments (by up to 24%) in driving performance evaluation on nuScenes, Argoverse, Waymo, and CARLA. 1.

Introduction
How can we learn generalized models for robust vision-based navigation in complex and dynamic settings? While humans can effortlessly transfer general navigation knowl-edge across settings and platforms [36, 46, 52, 53], cur-rent real-world development of navigation agents gener-ally deploys within a ﬁxed pre-assumed setting (e.g., ge-ographical location, use-case) and carefully calibrated sen-sor conﬁgurations. Consequently, each autonomy use-case generally requires its own prohibitive data collection and platform-speciﬁc annotation efforts [4, 6, 19, 56, 79]. Due to such development bottlenecks, brittle navigation models trained in-house by various developers (e.g., Tesla’s Au-topilot [23], Waymo’s Driver [4], Amazon’s Astro [69],
Fedex’s Roxo [25], etc.) are easily confounded by the im-mense complexity of the real-world navigation task, e.g., rare scenarios, novel social settings, geographical locations, and camera mount perturbations. Yet, every minute, a vast amount of highly diverse and freely available ego-centric navigation data containing such scenarios is uploaded to the web. In this paper, we work towards effectively utiliz-ing such freely available demonstration data to improve the efﬁciency, safety, and scalability of generalized real-world navigation agents.
There are two key challenges to employing the large amounts of unconstrained and unlabeled online data for training robust vision-based navigation policies.
First, while online images may be collected in various layouts and camera settings, existing monocular-based prediction and decision-making methods tend to rely on restrictive as-sumptions of planar scenes [22, 70] and known camera pa-rameters [13, 28, 41, 55, 59, 71, 77]. Towards a dataset and platform-agnostic navigation agent, our proposed architec-ture does not explicitly rely on such assumptions. Second, due to safety-critical requirements, methods for learning decision-making in complex navigation settings generally also assume access to highly curated benchmarks with clean annotations [4,7,10,13,17,20,26,33,64,74]. Consequently, such methods must be revisited when learning from unla-beled and diverse internet videos, e.g., with various qual-ity demonstrations [74]. To effectively utilize such data, we propose to leverage recent advances in iterative semi-supervised learning [8, 38, 60]. Yet, as such techniques are studied within pixel and image-level tasks [14, 29, 41], their utility for learning complex and safety-critical 3D reasoning and planning tasks is not well-understood.
Contributions: Motivated by how humans are able to ef-fortlessly learn and adapt various skills using online videos, our SelfD approach introduces three main contributions.
First, to facilitate learning from unconstrained imagery, we develop a model for mapping monocular observations di-rectly to a Bird’s Eye View (BEV) planning space (i.e., without requiring camera calibration). Second, we intro-duce a novel semi-supervised learning approach incorporat-ing self-training with “hypothetical” data augmentation. We demonstrate the proposed sampling step to be essential for making use of highly diverse demonstration data. Third, we perform a set of novel cross-dataset experiments to analyze the ability of an initially trained decision-making model to self-improve its generalization capabilities using minimal assumptions regarding the underlying data. We evaluate across various datasets with complex navigation and harsh conditions to demonstrate state-of-the-art model generaliza-tion performance. 2.