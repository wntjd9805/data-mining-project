Abstract 1.

Introduction
Many video understanding tasks require analyzing multi-shot videos, but existing datasets for video object segmen-tation (VOS) only consider single-shot videos. To address this challenge, we collected a new dataset—YouMVOS—of 200 popular YouTube videos spanning ten genres, where each video is on average five minutes long and with 75 shots. We selected recurring actors and annotated 431K segmentation masks at a frame rate of six, exceeding pre-vious datasets in average video duration, object variation, and narrative structure complexity. We incorporated good practices of model architecture design, memory manage-ment, and multi-shot tracking into an existing video seg-mentation method to build competitive baseline methods.
Through error analysis, we found that these baselines still fail to cope with cross-shot appearance variation on our
YouMVOS dataset. Thus, our dataset poses new chal-lenges in multi-shot segmentation towards better video anal-ysis. Data, code, and pre-trained models are available at https://donglaiw.github.io/proj/youMVOS
†Equally contributed.
∗Research completed during internships at Harvard University.
The broad computer vision goal of video understanding must include the analysis of multi-shot videos with com-plex narratives [25, 32], including depictions of people and objects that vary in their visual appearance and space-time relationships across shot transitions. Video object segmen-tation (VOS) [37, 56] plays an essential role in video under-standing. In multi-shot videos, this task requires accurately tracking and masking the same object across cuts despite appearance changes (Fig. 1). If multi-shot VOS is achieved, then it can ease video applications like editing [6] for privacy blur, relighting, or semantic color grading, and help in the analysis of poses and actions of specific objects. In the long term, multi-shot VOS data and methods can build towards a more sophisticated high-level video understanding.
Methods for VOS and the related video instance seg-mentation (VIS) problem are developed on datasets with single-shot videos that span only several seconds—a limita-tion highlighted in a recent survey [52]. Consequently, VOS methods often lose track of object instances in multi-shot videos, especially in videos with fast cuts and many object appearance changes, such as in music videos. As there are no existing multi-shot VOS datasets, it is hard to characterize these errors and reliably improve performance.
To foster research in multi-shot VOS, we collect a new dataset—YouMVOS—consisting of 200 multi-shot
YouTube videos at full length that are on average five min-utes long and with 75 shots (Tab. 1). The average duration of videos in YouMVOS is at least 7.7× longer than existing
VOS datasets. To choose representative online videos, we first pick ten popular video genres, including sports, cook-ing, and music videos, and then pick 20 popular videos in each genre with diverse content and temporal structures. Our multi-shot VOS dataset focuses on actors—human, animal, or virtual characters—that vary their positions, poses, and appearances across edited shots. This is similar in intent to object-specific VOS datasets, e.g., cars and pedestrians for autonomous driving [50]. To annotate our dataset efficiently, we build a semi-automatic system with manual proofreading using modern keyframe selection, mask initialization, and mask propagation. This produced 431K annotated instance masks—46% more masks than the latest VIS dataset [38].
To understand the new challenges in YouMVOS, we first benchmark existing unsupervised VOS methods [58] and video instance segmentation (VIS) methods [57, 8, 10], which were developed using short-term single-shot video datasets. To improve single-shot models, we examine model architectures and memory management practices to handle the change of actor position and appearance across shots.
Then, we add multi-shot tracking to further improve base-line suitability to YouMVOS. The adapted model defines the baseline performance on YouMVOS. Finally, we perform error analysis using oracle data to identify where improve-ments can be made in the future, finding that cross-shot appearance variation is still a challenge.
Our contributions are 1) the YouMVOS dataset with 200 multi-shot YouTube videos and 431K annotated instance masks for the main actors, 2) an improved baseline seg-mentation model on YouMVOS to better handle long-term multi-shot videos, and 3) an error analysis of the improved baseline methods. Together, this provides a new challenge for the computer vision community and another step towards a more general understanding of complex videos. We also publicly release our data, code, and models. 2.