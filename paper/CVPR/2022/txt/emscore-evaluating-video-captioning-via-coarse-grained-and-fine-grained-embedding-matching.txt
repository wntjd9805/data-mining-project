Abstract
Current metrics for video captioning are mostly based on the text-level comparison between reference and can-didate captions. However, they have some insuperable drawbacks, e.g., they cannot handle videos without refer-ences, and they may result in biased evaluation due to the one-to-many nature of video-to-text and the neglect of vi-sual relevance. From the human evaluator’s viewpoint, a high-quality caption should be consistent with the pro-vided video, but not necessarily be similar to the refer-ence in literal or semantics. Inspired by human evaluation, we propose EMScore (Embedding Matching-based score), a novel reference-free metric for video captioning, which directly measures similarity between video and candidate captions. Benefiting from the recent development of large-scale pre-training models, we exploit a well pre-trained vision-language model to extract visual and linguistic em-beddings for computing EMScore. Specifically, EMScore combines matching scores of both coarse-grained (video and caption) and fine-grained (frames and words) levels, which takes the overall understanding and detailed char-acteristics of the video into account. Furthermore, con-sidering the potential information gain, EMScore can be flexibly extended to the conditions where human-labeled references are available. Last but not least, we collect
VATEX-EVAL and ActivityNet-FOIl datasets to systemati-cally evaluate the existing metrics. VATEX-EVAL experi-ments demonstrate that EMScore has higher human corre-lation and lower reference dependency. ActivityNet-FOIL experiment verifies that EMScore can effectively identify
“hallucinating” captions. Code and datasets are available at https://github.com/shiyaya/emscore.
*Corresponding author
Figure 1. Two examples of caption evaluation. All the metric scores are scaled to [0, 1], including human scores. For example (a), reference-based metrics over-penalize for this correct candi-date caption due to “a rock” is not contained in the references. Our reference-free metric EMScore gives a reasonable high score with the help of using video as ground truth. For example (b), some reference-based metrics (e.g., ROUGE L and METEOR) under-penalize the hallucination (e.g., “different games”) which is not related to the video, and give an unreasonable higher score for
“hallucinating” caption B than correct caption A.
1.

Introduction
Video Captioning [4] aims to generate a text describing the visual content of a given video. Driven by the neu-ral encoder-decoder paradigm, research in video caption-ing has made significant progress [29, 35]. To make further advances in video captioning, it is essential to accurately evaluate generated captions. The most ideal metric is hu-man evaluation while carrying human judgments is time-consuming and labor-intensive. Thus, various automatic metrics are applied for video caption evaluation.
However, most of the widely applied video caption metrics like BLEU [19], ROUGE [12], CIDEr [28], and
BERTScore [34] come from the other tasks, such as ma-chine translation, text summarization and image captioning, which may neglect the special characteristic of video cap-tioning and then limit the development of video caption-ing. Furthermore, these automatic metrics require human-labeled references — and thus they are called reference-based metrics — and such requirements cause three in-trinsic drawbacks: (1) They can not be used when pro-vided videos have no human-labeled references, which is not uncommon in this age that millions of reference-free videos are produced online every day. (2) They may over-penalize the correct captions since references hardly de-scribe all details of videos due to the one-to-many na-ture [32] of captioning task, especially when the number of references is limited. Fig.1 (a) shows one such example where a candidate caption correctly describes the “a rock” while reference-based metrics punish this word since ref-(3) As pointed by [23], these erences do not contain it. reference-based metrics may under-penalize the captions with “hallucinating” descriptions since these metrics only measure similarity to references, and the visual relevance cannot be fully captured. For example, as shown in Fig.1 (b), due to the word “games” appearing in the references, some reference-metrics return higher scores for caption B than caption A, even though “different games” is a “hallu-cinating” phrase which is not related to the video.
These drawbacks inspire us to develop a reference-free metric. From the human evaluator’s viewpoint, if a caption is consistent with the source video, i.e., the visual contents in the video are comprehensively and accurately described by the caption, this caption is a high-quality one, and not necessarily be similar to the reference in literal or seman-tics. A promising evaluation metric should imitate the hu-man evaluation process, and introduce video content into the evaluation. Nowadays, due to the boom of the large-scale vision-language pre-training models [11, 17, 21], the gaps between the visual and linguistic embeddings have been further narrowed, enabling us to judge whether a cap-tion is consistent with a video.
Motivated by these research progresses, we propose a reference-free metric EMScore (Embedding Matching-based score) for evaluating video captions, which exploits a pre-trained large-scale vision-language model to extract visual and linguistic embeddings. Specifically, to obtain a comprehensive comparison between the video and cap-tion, EMScore averages the matching scores of both coarse-grained (video and caption) and fine-grained (frames and words) levels. For the coarse-grained one, we compute the similarity between the global embeddings of the video and the candidate caption, which take the overall understanding of the video into account and evaluate candidates from a global perspective. For the fine-grained embedding match-ing, we compute the sum of cosine similarities between the frame and word embeddings, which takes the detailed char-acteristic of the video (visual elements change over time) into account. Also, it provides more interpretability for EM-Score. Furthermore, considering the potential information gain, such as syntactic structure in references, and doing embedding matching in the same language domain is easier than cross-modal domains, we extend EMScore to the con-ditions where human-labeled references are available and name the extended metric EMScore ref.
Currently, there is no available video caption quality dataset that can be used to evaluate metrics. To facili-tate the development of video captioning evaluation met-rics, we are the first to collect a video caption quality dataset VATEX-EVAL which contains 54,000 human rat-ings for video-caption pairs. Experiments on VATEX-EVAL show the following advantages of our EMScore by introducing the video in evaluating. First, EMScore has a higher human correlation compared with some popular au-tomatic metrics like BLEU, ROUGE, or CIDEr. Second,
EMScore has low reference dependency, e.g., EMScore’s 0-reference Kendall’s correlation with humans is similar to BLEU 1’s 4-reference correlation or EMScore ref’s 1-reference is similar to CIDEr’s 9-reference correlations.
Therefore, EMScore can significantly reduce the cost of manually annotating references. Third, EMScore is more robust to quality drift that it achieves higher correlations compared with the other automatic metrics when evaluat-ing captions of different qualities. Furthermore, we collect another dataset ActivityNet-FOIL which contains “halluci-nating” captions to verify the sensitivity of EMScore. Ex-periment results show that EMScore is more effective to identify “hallucinating” captions than the other metrics.
Our contributions are summarized as follows:
• We propose a reference-free video captioning met-ric EMScore that directly measures consistency with video contents in both coarse-grained and fine-grained levels, and extend it to reference-available condition.
• We two collect datasets VATEX-EVAL and to study the
ActivityNet-FOIL for metrics’ correlation with human judgments and sensitivity in the “hallucinating” case, respectively. researchers
• Exhaustive experimental results verify that EMScore has a higher human correlation and is able to effec-tively identify the “hallucinating” captions. 2.