Abstract
Given weak supervision from image- or video-caption pairs, we address the problem of grounding (localizing) each object word of a ground-truth or generated sen-tence describing a visual input. Recent weakly-supervised approaches leverage region proposals and ground words based on the region attention coefﬁcients of captioning models. To predict each next word in the sentence they at-tend over regions using a summary of the previous words as a query, and then ground the word by selecting the most attended regions. However, this leads to sub-optimal grounding, since attention coefﬁcients are computed with-out taking into account the word that needs to be local-ized. To address this shortcoming, we propose a novel
Grounded Visual Description Conditional Variational Au-toencoder (GVD-CVAE) and leverage its latent variables for grounding. In particular, we introduce a discrete ran-dom variable that models each word-to-region alignment, and learn its approximate posterior distribution given the full sentence. Experiments on challenging image and video datasets (Flickr30k Entities, YouCook2, ActivityNet Enti-ties) validate the effectiveness of our conditional generative model, showing that it can substantially outperform soft-attention-based baselines in grounding. 1.

Introduction
Linking words to visual regions provides a ﬁne-grained bridge between vision and language modalities and is a fun-damental block of many applications, such as human-robot interaction [57, 60], visual question answering [27, 61], and even unsupervised neural machine translation [58]. Thus, visual grounding has become a prominent research area at the intersection of vision and language [12, 16, 29, 51].
Training visual grounding systems typically requires an-notations of textual descriptions combined with bounding boxes for each groundable word (e.g., object nouns). Since constructing datasets with such ﬁne-grained bounding box annotations is rather time-consuming and costly, we focus
Figure 1. Our proposed framework jointly models visual de-scriptions and word-to-region alignments conditioned on an in-put image (or video) and region proposals. Without using any bounding box annotations during training, it can tackle two tasks:
Visual Object Grounding and Grounded Visual Description. Un-like prior work [74] that leverages soft attention for grounding and always predicts the same region for two words given the same vi-sual input and partial caption context, our model can ground words by taking into account the full ground-truth or generated sentence. on weakly-supervised training of visual grounding systems, which require only image-caption pairs for training. In par-ticular, we consider two tasks, as illustrated in Fig. 1: (1)
Weakly-Supervised Visual Object Grounding (WS-VOG), where given an input image (or video) and its visual descrip-tion, the goal is to localize the referred semantic entities in the visual input, and (2) Weakly-Supervised Grounded Vi-sual Description (WS-GVD), where given an input image (or video), we must jointly generate a natural language de-scription and localize the generated words.
Most prior work has focused on learning how to align words with regions by learning how to correctly match images and videos to sentences [8, 26, 56, 65]. How-ever, these matching-based approaches can only tackle the
ﬁrst task (WS-VOG), and cannot generate grounded vi-sual descriptions. On the other hand, captioning-based ap-proaches [40, 74] aim to learn how to ground words by learning how to generate captions based on region propos-als, thus they can tackle both tasks. For example, the GVD captioning-based model [74] grounds words by using the region attention mechanism of a discriminative, encoder-decoder captioning model to select regions with maximum attention coefﬁcients. Nonetheless, exploiting soft attention as a grounding mechanism suffers from two major limita-tions. First, despite being an effective, end-to-end learnable mechanism for summarizing relevant context, attention is not explicitly encouraged to capture meaningful alignments and can result in poor grounding [36], unless it is super-vised. Second, each word is generated using attention coef-ﬁcients computed from a query that summarizes the previ-ously generated words. Hence, the coefﬁcients do not take into account the word to be grounded. For example, con-sider grounding the words ‘hat’ and ‘jacket’ given the sen-tences “A man is wearing a hat” and “A man is wearing a jacket”, respectively. As shown in Fig. 1, existing attention-based grounding approaches wrongly predict the same box for ‘hat’ and ‘jacket’, since the partial caption is the same.
To overcome these limitations, we propose a conditional generative model for the joint probability distribution of sentences and latent word-to-region alignments given an in-put image (or video) and a set of region proposals. That is, we account for the lack of grounding annotations by intro-ducing discrete latent variables that model word-to-region alignments. We parameterize our model with state-of-the-art visual encoders, language decoders and attention mod-ules, and leverage Amortized Variational Inference [30, 59] to learn its parameters. The resulting Grounded Visual
Description Conditional Variational Autoencoder (GVD-CVAE) allows us to both generate sentences and also in-fer the latent word-to-region alignments based on the whole sentence, including the word to be grounded. Hence, it can correctly ground the hat in the motivating example.
In summary, this work makes three key contributions.
First, we introduce the GVD-CVAE, a novel conditional generative model of visual descriptions with a sequential discrete latent space and attention-based parameterization of the prior and approximate posterior alignment distribu-tions. Second, we propose a training objective that encour-ages our model to learn latent variables that capture mean-ingful word-to-region alignments. Third, we evaluate our method on three challenging image and video datasets and demonstrate that both our “prior” and “approximate poste-rior” alignment distributions improve upon soft attention.
This leads to a 12% absolute improvement in WS-VOG on
Flickr30k Entities. Our model also achieves state-of-the-art or competitive grounding and captioning performance com-pared with a diverse family of state-of-the-art methods that are tailored to WS-VOG or WS-GVD. 2.