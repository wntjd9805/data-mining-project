Abstract
Semantic image synthesis is a challenging task with many practical applications. Albeit remarkable progress has been made in semantic image synthesis with spatially-adaptive normalization, existing methods usually normal-ize the feature activations under the coarse-level guidance (e.g., semantic class). However, different parts of a seman-tic object (e.g., wheel and window of car) are quite differ-ent in structures and textures, making blurry synthesis re-sults usually inevitable due to the missing of fine-grained guidance.
In this paper, we propose a novel normaliza-tion module, termed as REtrieval-based Spatially Adap-tIve normaLization (RESAIL), for introducing pixel level fine-grained guidance to the normalization architecture.
Specifically, we first present a retrieval paradigm by find-ing a content patch of the same semantic class from train-ing set with the most similar shape to each test seman-tic mask. Then, the retrieved patches are composited into retrieval-based guidance, which can be used by RESAIL for pixel level fine-grained modulation on feature activations, thereby greatly mitigating blurry synthesis results. More-over, distorted ground-truth images are also utilized as al-ternatives of retrieval-based guidance for feature normal-ization, further benefiting model training and improving vi-sual quality of generated images. Experiments on several challenging datasets show that our RESAIL performs favor-ably against state-of-the-arts in terms of quantitative met-rics, visual quality, and subjective evaluation. The source code is available at https://github.com/Shi-Yupeng/RESAIL-For-SIS. 1.

Introduction
Semantic image synthesis aims to generate photo-realistic image from the given semantic map. It is an im-portant problem in computer vision that can be adopted in a variety of downstream tasks such as virtual idol, special effect, robotics [13] and image manipulation [9].
Humans have a remarkable ability to produce new cre-ation from past experiences as references.
In their early ages, children can paint a picture including flowers, sky and buildings by referring to templates of representative objects and backgrounds. Thus, producing something from refer-ences is a natural way for image generation because edit-ing the references and stitching them is relatively easy than creating the entire image out of thin air. Inspired by this spirit, early works have well studied reference-based image synthesis, where proper references are searched from exter-nal memories [4, 7, 11, 14, 16]. Nonetheless, the retrieval, editing and stitching are conducted in separated and hand-crafted manners, which are optimized in a sub-optimal way.
SIMS [23] leverages deep network for further improving the quality of reference-based synthesized results, but it simply takes the retrieved image as network input, which is limited in synthesizing complex real-world scenes.
With the recent advance of deep generative networks, some recent studies [21,22,24,25,34] tackle semantic image synthesis using a spatially-adaptive normalization archi-tecture, achieving significant performance improvements.
However, with the coarse-level guidance (e.g., semantic class), these methods modulate the activations inside each semantic object in spatially uniform manner, regardless of the huge internal variation of the objects. This inevitably leads to blurry results, especially for large semantic object with complex parts. We take two representative spatially-adaptive normalization architectures as examples in Fig. 1.
SPADE [22] leverages the semantic layout as input and learns the modulation parameters through several convo-lution layers, being limited in generating high-quality ob-ject parts and leading to blurry synthesis results (Fig. 1(a)).
SEAN [34] improves SPADE by extracting style codes from selected regions, leading to flexible style control. However, the style map is generated by broadcasting the style codes to the corresponding semantic regions, which also prefers spatially uniform synthesis result (Fig. 1(b)). Most recent methods, e.g., CLADE [25] and OASIS [24], intrinsically are also based on coarse-level guidance.
In this paper, we tackle the above issues by presenting a novel feature normalization method, termed as REtrieval-based Spatially AdaptIve normaLization (RESAIL). Our in-tuition is two-fold. On the one hand, the object segment mask of the input semantic map can not only provide the semantic class but also the object shape. On the other hand, the training dataset contains rich shape and texture infor-mation of objects which cannot be entirely captured by the learned deep generative networks. Taking these intuitions into account, given a object segment mask, we present a retrieval paradigm for retrieving a segment image with the most similar shape from the training dataset. The retrieved segment images are then composited into a retrieval-based guidance, which naturally is spatially variant in pixel level.
We further propose a retrieval-based spatially adaptive nor-malization, where retrieval-based guidance and semantic map collaborate to provide pixel level fine-grained modu-lation on feature activations. As shown in Fig. 1(c), bene-fited from pixel level fine-grained guidance, our RESAIL is effective in generating visually plausible image with clear
In contrast to SIMS [23], our method leverages details. retrieval-based guidance for spatially adaptive normaliza-tion, which is more effective in synthesizing photo-realistic images. In comparison to SPADE [22] and SEAN [22], our
RESAIL can effectively leverage pixel level fine-grained guidance for improving synthesized results.
When retrieval-based guidance is used for feature nor-malization, it is difficult to exploit perceptual supervision for training, due to that the ground-truth image correspond-ing to retrieval-based guidance is missing. On the contrary, the ground-truth image of a semantic map can be naturally treated as a retrieval-based guidance, while the ground-truth image itself can also be used to facilitate perceptual super-vision. However, ground-truth image is quite different from real retrieval-based guidance, and using it as guidance can-not make the learned model generate better synthesis results in the testing stage.
Instead, we introduce a data distor-tion mechanism on ground-truth images to mimic the qual-ity of retrieval-based guidance. During training, the dis-torted ground-truth images are also used as alternatives of retrieval-based guidance, making it feasible to leverage per-ceptual supervision for improving model training and visual quality. Experiments on several challenging datasets show that our RESAIL performs favorably against state-of-the-arts. The contributions of this work are summarized as:
• A novel retrieval-based synthesis model is proposed by leveraging the retrieval-based guidance as pixel level fine-grained modulation, i.e., Retrieval-based Spatially
Adaptive Normalization (RESAIL), for semantic im-age synthesis.
• During training, a data distortion mechanism on the ground-truth images is introduced to facilitate model training and improves visual quality of synthesized re-sults.
• Extensive experiments show the effectiveness of our proposed method in synthesizing photo-realistic image from given semantic map. 2.