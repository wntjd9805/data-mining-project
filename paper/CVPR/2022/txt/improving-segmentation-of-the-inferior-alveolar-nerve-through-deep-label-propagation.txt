Abstract
Many recent works in dentistry and maxillofacial im-agery focused on the Inferior Alveolar Nerve (IAN) canal detection. Unfortunately, the small extent of available 3D maxillofacial datasets has strongly limited the performance of deep learning-based techniques. On the other hand, a huge amount of sparsely annotated data is produced every day from the regular procedures in the maxillofacial prac-tice. Despite the amount of sparsely labeled images being signiﬁcant, the adoption of those data still raises an open problem.
Indeed, the deep learning approach frames the presence of dense annotations as a crucial factor. Recent efforts in literature have hence focused on developing label propagation techniques to expand sparse annotations into dense labels. However, the proposed methods proved only marginally effective for the purpose of segmenting the alve-olar nerve in CBCT scans. This paper exploits and pub-licly releases a new 3D densely annotated dataset, through which we are able to train a deep label propagation model which obtains better results than those available in litera-ture. By combining a segmentation model trained on the 3D annotated data and label propagation, we signiﬁcantly improve the state of the art in the Inferior Alveolar Nerve segmentation. 1.

Introduction
Dental implant placement within the jawbone is a rou-tinely executed surgical procedure, which can become com-plex due to the local presence of the Inferior Alveolar Nerve (IAN). In particular, the nerve is oftentimes in close relation to the roots of molars, and its position must thus be carefully detailed before the surgical removal. As avoiding contact with the IAN is a primary concern during these operations, segmentation plays a key role in surgical preparations.
Given the exceptionally large amount of time required for 3D manual segmentation, perfect anatomical annotation accuracy is usually overlooked in favour of a fast execution time. Therefore, the de facto standard in radiology medi-cal centers for dentistry and maxillofacial purposes is based on sparse annotations, which can be obtained from 2D im-ages in a relatively small amount of time. Nevertheless, 2D annotations fail to identify a considerable amount of inner information about the IAN position and the bone structure.
The incomplete detection of the nerve positioning is often sufﬁcient to facilitate a positive outcome of surgical inter-vention, but it is not an accurate anatomical representation.
Convolutional Neural Networks (CNNs) have provided amazing results for both 2D and 3D segmentation, along-side several more computer vision tasks [2, 5, 9, 11, 13, 26].
As a matter of fact, a segmentation CNN would be able to correctly portray the 3D structures of an IAN with-out any need for manual adjustment. Unfortunately, the great capabilities of CNNs in this ﬁeld are limited by the lack of carefully annotated data, which is indispensable for training deep learning models. Indeed, despite the signiﬁ-cant amount of raw data available, the supervised learning paradigm requires dense 3D annotations to reach its full po-tential, which are extremely expensive to acquire.
In this work, we propose a novel label propagation method, based on deep learning, that can translate sparse 2D labels into 3D voxel-level annotations. This method can ﬁll the gap between the most modern and sophisticated meth-ods for 3D segmentation and the lack of viable annotated data in the maxillofacial ﬁeld. Moreover, with the goal of pushing the state of the art in 3D IAN segmentation, we de-sign a novel 3D segmentation CNN that exploits positional information to generate the ﬁnal 3D prediction.
In order to correctly evaluate both of the proposed meth-ods and any future competitor, a new dataset has been col-lected, annotated by medical experts at voxel-level (Fig. 1e), and publicly released along with this paper1. We address this as 3D annotation in contrast to the traditional one, which is performed on a 2D “panoramic view” obtained from the volume (Fig. 1d).
The main contributions introduced by this paper can be 1https://ditto.ing.unimore.it/maxillo/
(a) (b) (c) (d) (e)
Figure 1. Samples from the proposed dataset. Each line of the image contain a different patient, from left to right you can see (a) left-side, (b) frontal and (c) right-side views of the CBCT volume. (d) and (e) depicts sparse and dense annotations of the inferior alveolar nerve respectively. Best viewed in color. summarized as follows:
• We design a novel deep label propagation technique to enhance sparse 2D annotations and yield dense voxel-level annotations;
• A novel deep learning architecture for 3D segmenta-tion is proposed, which improves the state-of-the-art segmentation accuracy for the mandibular canal;
• We collect and publicly release the ﬁrst CBCT (Cone
Beam Computed Tomography) 3D dataset with profes-sionally produced 3D annotations;
• The source code which allows to exactly reproduce all the reported experiments is publicly released.
The rest of the paper is organized as follows. Sec. 2 presents state-of-the-art approaches for both the automatic detection of the inferior alveolar nerve and label propagation. Sec. 3 and Sec. 4 describe respectively the collected dataset and the proposed methods, both for label propagation and 3D
IAN segmentation. Finally, experiments are detailed in
Sec. 5, and conclusions are drawn in Sec. 6. 2.