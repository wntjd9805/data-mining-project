Abstract
Modeling in computer vision has long been dominated by convolutional neural networks (CNNs). Recently, in light of the excellent performances of self-attention mech-anism in the language field, transformers tailored for vi-sual data have drawn numerous attention and triumphed
CNNs in various vision tasks. These vision transformers heavily rely on large-scale pre-training to achieve competi-tive accuracy, which not only hinders the freedom of archi-tectural design in downstream tasks like object detection, but also causes learning bias and domain mismatch in the fine-tuning stages. To this end, we aim to get rid of the
“pre-train & fine-tune” paradigm of vision transformer and train transformer based object detector from scratch. Some earlier work in the CNNs era have successfully trained
CNNs based detectors without pre-training, unfortunately, their findings do not generalize well when the backbone is switched from CNNs to vision transformer.
Instead of proposing a specific vision transformer based detector, in this work, our goal is to reveal the insights of training vision transformer based detectors from scratch.
In particular, we expect those insights can help other re-searchers and practitioners, and inspire more interesting research in other fields, such as semantic segmentation, visual-linguistic pre-training, etc. One of the key findings is that both architectural changes and more epochs play criti-cal roles in training vision transformer based detectors from scratch. Experiments on MS COCO datasets demonstrate that vision transformer based detectors trained from scratch can also achieve similar performances to their counterparts with ImageNet pre-training. 1.

Introduction
The extraordinary performance of AlexNet [25] on the
ImageNet image classification challenge has sparked the passion in convolutional neural networks (CNNs), and led to a variety of powerful CNN backbones through greater (a) Swin-T based FCOS [43]. (b) Swin-T based Faster R-CNN [36].
Figure 1. We train and evaluate Swin-T [30] based detectors (FCOS [43] and Faster R-CNN [36]) on the COCO dataset. We observe that: 1). Swin-T based detectors trained from scratch do not achieve comparable mAP to their ImageNet pre-trained coun-terpart, even if more epochs of training are conducted following
He et al. [14]. 2). The results of Swin-T based FCOS will in-crease if its architecture is modified following DSOD [39], which is originally proposed to boost the proposal-free CNNs based de-tector when pre-training is unavailable. However, the performance of “Swin-T + FCOS +DSOD” detector trained from scratch is still not as good as the ImageNet pre-trained one. 3). With suitable ar-chitectural changes and sufficient training epochs, the proposed vi-sion transformer based detectors without pre-training demonstrate competitive mAP to their ImageNet pre-trained counterparts. scale [17], more extensive connections [24], and more so-phisticated forms of convolution [6]. Consequently, model-ing in computer vision has long been dominated by CNNs, until the Transformer architecture [8] is recently adapted from natural language processing (NLP) to vision commu-nity. A group of transformers tailored for visual data have triumphed numerous CNN-based methods in many vision tasks (e.g.
, image classification [9], object detection [2], semantic segmentation [5], etc). Among them, object de-tection is one of the fastest moving areas due to its wide applications in surveillance, autonomous driving, etc.
Most of the advanced object detectors require initializa-tion from large-scale pre-training to achieve good perfor-mances, no matter whether their backbones are CNNs or vision transformers [30, 36]. Typically, these methods first pre-train the backbone model on ImageNet [38] dataset, then fine-tune the pre-trained weights on the specific object detection task. Fine-tuning from pre-trained models has at
Figure 2. Qualitative comparisons between naively trained-from-scratch Faster R-CNN, and ours. least two advantages. First, it is convenient to reuse vari-ous state-of-the-art deep models that are publicly available.
Second, fine-tuning can quickly generate the final model and requires much fewer annotated training samples than the classification task. The fine-tuning process can also be viewed as an instance of transfer learning [33].
However, there are also critical limitations when adopt-ing the pre-trained networks in object detection: 1). Lim-ited structure design space [39]. The pre-trained mod-els are usually cumbersome (containing a huge number of parameters) for performing well on the ImageNet classi-fication task. Existing object detectors directly adopt the pre-trained networks, resulting in little flexibility to con-trol/adjust the network structures. The requirement of com-puting resources is also boosted by the complex network structures. 2). Learning bias [48]. Both the loss functions and category distributions differ between classification and detection tasks, leading to different searching/optimization spaces. Thus, learning may be biased towards a local min-imum for detection tasks. 3). Domain mismatch [12].
Though fine-tuning can mitigate the gap of different target category distributions, it is still a severe problem when the source domain (ImageNet) has a huge mismatch with the target domain such as depth images, medical images, etc.
Some earlier work have studied on training CNNs based object detection networks from scratch [14, 39]. Specifi-cally, DSOD [39] argues that only proposal-free detectors can be trained from scratch, though proposal-based methods like Faster R-CNN [36] often have superior performances than proposal-free ones. In detail, DSOD [39] augments the original detector by deep supervision, stem block and dense prediction, etc., to achieve ideal detection performances.
In contrast, He et al. [14] points out that no architectural change is required for training from scratch. As long as sufficient training iterations are executed, detectors trained from scratch can converge to similar accuracy to their Ima-geNet pre-training counterparts.
Given the fact that vision transformers have outper-formed CNNs in numerous computer vision tasks, we are motivated to raise the following two questions: 1). Do the findings [14, 39] obtained on CNNs based detectors remain effective, in the era of vision transformer? 2). If not, is it still possible to train vision transformer based object detec-tors from scratch?
In this work, we experimentally answer the two ques-tions above in Section 3 and Section 4. Specifically, we first show that naively applying the experiences from [14, 39] to vision transformer is not enough. As illustrated in Figure 1, if either architectural changes or more training epochs are solely applied, vision transformer based detectors that are trained from scratch will achieve inferior results compared to their pre-trained counterparts. Then, instead of propos-ing a specific vision transformer based detector, we aim to reveal the insights of training vision transformer based detector from scratch. In particular, we find that both ar-chitectural changes and more epochs are important in train vision transformer based detectors from scratch. Together with several other techniques, we manage to train trans-former based detectors from scratch and achieve compet-itive results to the ImageNet pre-trained counterpart. We expect those insights can help other researchers and practi-tioners, and inspire more interesting research in other fields, such as semantic segmentation [21], visual-linguistic pre-training [19], etc.
Our main findings are summarized as follows: 1. From RoIPooling to RoIAlign. We observe that proposal-based and proposal-free detectors exhibit dis-is, tinct behavior when trained from scratch, proposal-free detectors degrade less than proposal-based ones compared to their pre-trained counter-parts. We find out this phenomenon is essentially i.e., it hinders the gradient caused by RoIPooling, from being smoothly back-propagated to backbone layers. We address this problem by replacing RoIPool-that
ing with RoIAlign, and achieve consistencies between proposal-based and proposal-free detectors. 2. From T-T-T-T to C-C-T-T. Recent studies have re-vealed that large-scale pre-training essentially makes lower attention layers to learn inductive bias and “act like convolutions” [35]. Thus, we replace the first two stages of vision transformers with convolution blocks, namely, from T-T-T-T to C-C-T-T, where T and C stand for transformer and convolution block, respectively.
Such a replacement directly introduces the inductive prior of convolution into the backbone model, making it less dependent on ImageNet pre-training. 3. Gradient Calibration.
In C-C-T-T architecture, we observe that the convolution and self-attention layers exhibit significant differences in terms of the scale of gradient. Since it is better to adjust all of the layers a little rather than to adjust just a few layers a large amount [49], we propose to calibrate the gradients of our model, and achieve better convergence property. 4. More Training Epochs. As argued by He et al. [14], it is unrealistic and unfair to expect models trained from random initialization to converge as fast as those initialized from ImageNet pre-training. Typical Ima-geNet pre-training can learn not only semantic infor-mation, but also low-level features (e.g., edges, tex-tures) that do not need to be re-learned during fine-tuning. Therefore, models trained from scratch must be trained for more epochs than typical fine-tuning schedules. 2.