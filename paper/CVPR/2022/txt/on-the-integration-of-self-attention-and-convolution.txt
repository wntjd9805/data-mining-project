Abstract
Convolution and self-attention are two powerful tech-niques for representation learning, and they are usually considered as two peer approaches that are distinct from each other. In this paper, we show that there exists a strong underlying relation between them, in the sense that the bulk of computations of these two paradigms are in fact done with the same operation. Specifically, we first show that a traditional convolution with kernel size k × k can be decomposed into k2 individual 1 × 1 convolutions, fol-lowed by shift and summation operations. Then, we in-terpret the projections of queries, keys, and values in self-attention module as multiple 1 × 1 convolutions, followed by the computation of attention weights and aggregation of the values. Therefore, the first stage of both two mod-ules comprises the similar operation. More importantly, the first stage contributes a dominant computation complex-ity (square of the channel size) comparing to the second stage. This observation naturally leads to an elegant in-tegration of these two seemingly distinct paradigms, i.e., a mixed model that enjoys the benefit of both self-Attention and Convolution (ACmix), while having minimum compu-tational overhead compared to the pure convolution or self-attention counterpart. Extensive experiments show that our model achieves consistently improved results over com-petitive baselines on image recognition and downstream tasks. Code and pre-trained models will be released at https://github.com/LeapLabTHU/ACmix and https://gitee.com/mindspore/models. 1.

Introduction
Recent years have witnessed the vast development of convolution and self-attention in computer vision. Convolu-tion neural networks (CNNs) are widely adopted on image
*Corresponding author.
Figure 1. A sketch of ACmix. We explore a closer relationship between convolution and self-attention in the sense of sharing the same computation overhead (1 × 1 convolutions), and combining with the remaining lightweight aggregation operations. We show the computation complexity of each block w.r.t the feature channel. recognition [19, 23], semantic segmentation [9] and object detection [38], and achieve state-of-the-art performances on various benchmarks. On the other hand, self-attention is first introduced in natural language processing [1, 42], and also shows great potential in the fields of image generation and super-resolution [10, 34]. More recently, with the ad-vent of vision transformers [7,15,37], attention-based mod-ules have achieved comparable or even better performances than their CNN counterparts on many vision tasks.
Despite the great success that both approaches have achieved, convolution and self-attention modules usually follow different design paradigms. Traditional convolution leverages an aggregation function over a localized receptive field according to the convolution filter weights, which are shared in the whole feature map. The intrinsic characteris-tics impose crucial inductive biases for image processing.
Comparably, the self-attention module applies a weighted average operation based on the context of input features, where the attention weights are computed dynamically via a similarity function between related pixel pairs. The flexi-bility enables the attention module to focus on different re-gions adaptively and capture more informative features.
Considering the different and complementary properties of convolution and self-attention, there exists a potential
possibility to benefit from both paradigms by integrating these modules. Previous work has explored the combina-tion of self-attention and convolution from several differ-ent perspectives. Researches from early stages, e.g., SENet
[22], CBAM [46], show that self-attention mechanism can serve as an augmentation for convolution modules. More recently, self-attention modules are proposed as individual blocks to substitute traditional convolutions in CNN mod-els, e.g., SAN [53], BoTNet [40]. Another line of research focuses on combining self-attention and convolution in a single block, e.g., AA-ResNet [3], Container [16], while the architecture is limited in designing independent paths for each module. Therefore, existing approaches still treat self-attention and convolution as distinct parts, and the underly-ing relations between them have not been fully exploited.
In this paper, we seek to unearth a closer relationship be-tween self-attention and convolution. By decomposing the operations of these two modules, we show that they heavily rely on the same 1×1 convolution operations. Based on this observation, we develop a mixed model, named ACmix, and integrate self-attention and convolution elegantly with mini-mum computational overhead. Specifically, we first project the input feature maps with 1 × 1 convolutions and obtain a rich set of intermediate features. Then, the intermedi-ate features are reused and aggregated following different paradigms, i.e, in self-attention and convolution manners respectively. In this way, ACmix enjoys the benefit of both modules, and effectively avoids conducting expensive pro-jection operations twice.
To summarize, our contributions are two folds: (1) A strong underlying relation between self-attention and convolution is revealed, providing new perspectives on understanding the connections between two modules and inspirations for designing new learning paradigms. (2) An elegant integration of the self-attention and con-volution module, which enjoys the benefits of both worlds, is presented. Empirical evidence demonstrates that the hybrid model outperforms its pure convolution or self-attention counterpart consistently. 2.