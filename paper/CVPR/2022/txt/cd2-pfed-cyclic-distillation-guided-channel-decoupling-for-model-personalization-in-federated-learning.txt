Abstract
Federated learning (FL) is a distributed learning paradigm that enables multiple clients to collaboratively learn a shared global model. Despite the recent progress, it remains challenging to deal with heterogeneous data clients, as the discrepant data distributions usually pre-vent the global model from delivering good generalization ability on each participating client. In this paper, we pro-pose CD2-pFed, a novel Cyclic Distillation-guided Channel
Decoupling framework, to personalize the global model in
FL, under various settings of data heterogeneity. Differ-ent from previous works which establish layer-wise per-sonalization to overcome the non-IID data across different clients, we make the first attempt at channel-wise assign-ment for model personalization, referred to as channel de-coupling. To further facilitate the collaboration between private and shared weights, we propose a novel cyclic dis-tillation scheme to impose a consistent regularization be-tween the local and global model representations during the federation. Guided by the cyclical distillation, our chan-nel decoupling framework can deliver more accurate and generalized results for different kinds of heterogeneity, such as feature skew, label distribution skew, and concept shift.
Comprehensive experiments on four benchmarks, including natural image and medical image analysis tasks, demon-strate the consistent effectiveness of our method on both lo-cal and external validations. 1.

Introduction
Deep learning techniques have received notable attention in various vision tasks, such as image classification [7], ob-ject detection [31], and semantic segmentation [25]. Yet, the success of deep neural networks heavily relies on a tremendous volume of valuable training images. One possi-ble solution is to collaboratively curate numerous data sam-ples from different parties (e.g., different mobile devices
*Corresponding Author.
Figure 1. Illustration of different parameter decoupling manners for model personalization in Federated Learning. The previous approaches combine local and global parameters in a layer-wise mechanism, including LG-Fed [22] in low-level input layers (a) and FedPer [2] in high-level output layers (b). Instead, we achieve model personalization via channel-wise decoupling (c). and companies). However, collecting distributed data into a centralized storage facility is costly and time-consuming.
Additionally, in real practice, decentralized image data should not be directly shared, due to privacy concerns or legal restrictions [1, 39]. In this case, conventional central-ized machine learning frameworks fail to satisfy the data privacy protection constraint. the
Therefore, distributed training data-private paradigms, especially Federated Learning (FL), have received an increasing popularity [3–5,19,24,28,36,47,50].
To be more specific, in FL, a shared model is globally trained with an orchestration of local updates within data stored at each client. A pioneering FL algorithm named
Federated Average (FedAVG), aggregates parameters at the central server by communication across clients once per global epoch, without explicit data sharing [28]. Compared with local training, the federation on a larger scale of training data has demonstrated its superiority to boost the generalization ability on unseen data, with the orchestration of distributed private data [6, 28].
However, data heterogeneity is one of the most funda-mental challenges faced by FL. The concept of independent and identically distributed (IID) is clear, while data can be non-IID in many ways, e.g., feature skew, label distribution skew, or concept shift [11]. Previously, sharp performance degradation was observed on FedAVG with unbalanced and non-IID data. This ill-effect is attributed to the weight di-vergence, which can be quantified by the earth mover’s dis-tance between distributions over classes [49]. Although each client can train a private model locally by optimizing the objective with no information change among each other, it would inevitably result in overfitting and a poor general-ization ability on new samples. As suggested in [28], simply sharing a small subset of data globally greatly enhances the generalization of FedAVG. However, this scheme cannot be directly applied to real-world tasks due to the violation of privacy concerns.
Consequently, researchers have sought to train a collec-tion of models that is stylized for each local distribution to enable stronger performance for each participating client without requiring any data sharing [49], which is known as personalized federated learning PFL [37]. Various ap-proaches have been proposed to accomplish the model per-sonalization in FL [6, 10, 35, 37]. Among these different paradigms, one popular solution is to directly assign person-alized parameters for each local client. For this line of meth-ods, the private personalized parameters are trained locally and not shared with the central server. Existing works have made attempts to achieve personalization by assigning per-sonalized parameters in either top layers [2] or bottom lay-ers [22]. However, these approaches usually require prior knowledge for the determination of which layers to be per-sonalized. More critically, we observe performance degra-dation that existing PFL approaches fail to achieve a con-sistent generalization over comprehensive settings of data heterogeneity [29]. Additionally, existing layer-wise per-sonalization approaches cannot effectively handle the dis-crepancy between the learned local and global model rep-resentations due to the weight divergence [49]. The infe-rior performance of some local clients motivates us to seek a more generic yet efficient combination between the local and global information.
In light of these challenges, we propose CD2-pFed, a novel Cyclic Distillation-guided Channel Decoupling framework for model personalization in FL. As shown in
Figure 1, different from previous layer-wise personalization approaches, e.g., FedPer [2] and LG-Fed [22], the proposed novel channel decoupling paradigm dynamically decouples the parameters at the channel dimension for personalization instead. By employing learnable personalized weights at all layers, our channel decoupling paradigm no longer re-quires heuristics for designing specific personalization lay-ers. More importantly, our method achieves model per-sonalization for both low-level and high-level layers, which facilitates tackling feature heterogeneity, distribution skew, and concept shift.
To bridge the semantic gap between the learned visual representation from the decoupled channels, we further pro-pose a novel cyclic distillation scheme by mutually distill-ing the local and global model representation (i.e., soft pre-dictions by the private and shared weights) from each other.
Benefiting from the distilled knowledge, our channel decou-pling framework enables synergistic information exchange between the global and local model training, therefore pre-venting biased local model training on non-IID data. Ex-tensive experimental results on both heterogeneous data and exterior unseen samples [22] demonstrate that our method largely improves the generalization of FedAVG with negli-gible additional computation overhead. Below, we summa-rize the major contributions of this work.
• We propose a novel channel decoupling paradigm to decouple the global model at the channel dimension for personalization.
Instead of using personalization layers for tackling either feature or label distribution skew, our approach provides a unified solution to ad-dress a broad range of data heterogeneity.
• To further enhance the collaboration between private and shared weights in channel decoupling, we design a novel cyclic distillation scheme to narrow the diver-gence between them.
• We compare our method with previous state-of-the-art
PFL approaches on four benchmark datasets, including synthesized and real-world image classification tasks, with different kinds of heterogeneity. Results demon-strate the superiority of our method over state-of-the-art PFL approaches. 2.