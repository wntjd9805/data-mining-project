Abstract
Scene Graph Generation (SGG) remains a challenging visual understanding task due to its compositional property.
Most previous works adopt a bottom-up two-stage or a point-based one-stage approach, which often suffers from high time complexity or sub-optimal designs. In this work, we propose a novel SGG method to address the aforementioned issues, formulating the task as a bipartite graph construction problem. To solve the problem, we develop a transformer-based end-to-end framework that first generates the entity and predicate proposal set, followed by inferring directed edges to form the relation triplets.
In particular, we de-velop a new entity-aware predicate representation based on a structural predicate generator that leverages the compo-sitional property of relationships. Moreover, we design a graph assembling module to infer the connectivity of the bipartite scene graph based on our entity-aware structure, enabling us to generate the scene graph in an end-to-end manner. Extensive experimental results show that our design is able to achieve the state-of-the-art or comparable perfor-mance on two challenging benchmarks, surpassing most of the existing approaches and enjoying higher efficiency in inference. We hope our model can serve as a strong baseline for the Transformer-based scene graph generation. 1 1.

Introduction
Inferring structural properties of a scene, such as the relationship between entities, is a fundamental visual under-standing task. The visual relationship between two entities can be typically represented by a triple <subject entity, pred-icate, object entity>. Based on the visual relationships, a scene can be modeled as a graph structure, with entities as nodes and predicates as edges, referred to as scene graph.
The scene graph provides a compact structural representation 1This work was supported by Shanghai Science and Technology Pro-gram 21010502700. Code is available: https://github.com/
Scarecrow0/SGTR
Figure 1. The illustration of SGTR pipeline paradigm. We formulate SGG as a bipartite graph construction process. First, the entity and predicate nodes are generated, respectively. Then we assemble the bipartite scene graph from two types of nodes. for a visual scene, which has potential applications in many vision tasks such as visual question answering [8, 25, 31], image captioning [42, 43] and image retrieval [9].
Different from the traditional vision tasks (e.g., object detection) that focus on entity instances, the main challenge of scene graph generation (SGG) lies in building an effective and efficient model for the relations between the entities. The compositional property of visual relationships induces high complexity in terms of their constituents, which makes it difficult to learn a compact representation of the relationship concept for localization and/or classification.
Most previous works attempt to tackle this problem using two distinct design patterns: bottom-up two-stage [1, 4, 5, 7, 14, 18, 40, 44] and point-based one-stage design [6, 23]. The former typically first detects N entity proposals, followed by predicting the predicate categories of those entity combina-tions. While this strategy achieves high recalls in discovering relation instances, its O(N 2) predicate proposals not only incur considerable computation cost but also produce sub-stantial noise in context modeling. In the one-stage methods, entities and predicates are often extracted separately from
the image in order to reduce the size of relation proposal set.
Nonetheless, they rely on a strong assumption of the non-overlapping property of interaction regions, which severely restricts their application in modeling complex scenes2.
In this work, we aim to tackle the aforementioned lim-itation by leveraging the compositional property of scene graphs. To this end, as illustrated in Fig. 1, we first formulate the SGG task as a bipartite graph construction problem, in which each relationship triplet is represented as two types of nodes (entity and predicate) linked by directed edges. Such a bipartite graph allows us to jointly generate entity/predicate proposals and their potential associations, yielding a rich hypothesis space for inferring visual relations. More impor-tantly, we propose a novel entity-aware predicate represen-tation that incorporates relevant entity proposal information into each predicate node. This enriches the predicate repre-sentations and therefore enables us to produce a relatively small number of high-quality predicate proposals. More-over, such a representation encodes potential associations between each predicate and its subject/object entities, which can facilitate predicting the graph edges and lead to efficient generation of the visual relation triplets.
Specifically, we develop a new transformer-based end-to-end SGG model, dubbed Scene graph Generation TRans-former (SGTR), for constructing the bipartite graph. Our model consists of three main modules, including an entity node generator, a predicate node generator and a graph assembling module. Given an image, we first introduce two
CNN+Transformer sub-networks as the entity and predicate generator to produce a set of entity and predicate nodes, respectively. To compute the entity-aware predicate repre-sentations, we design a structural predicate generator con-sisting of three parallel transformer decoders, which fuses the predicate feature with an entity indicator representation.
After generating entity and predicate node representations, we then devise a differentiable graph assembling module to infer the directed edges of the bipartite graph, which ex-ploits the entity indicator to predict the best grouping of the entity and predicate nodes. With end-to-end training, our
SGTR learns to infer a sparse set of relation proposals from both input images and entity proposals adaptively, which can mitigate the impact of noisy object detection.
We validate our method by extensive experiments on two SGG benchmarks: We validate our method by exten-sive experiments on two SGG benchmarks: Visual Genome and OpenImages-V6 datasets, with comparisons to previous state-of-the-art methods. The results show that our method outperforms or achieves comparable performance on both benchmarks and with high efficiency during inference.
The main contribution of our work has three-folds:
• We propose a novel transformer-based end-to-end scene 2e.g., two different relationships cannot have largely overlapped area – a phenomenon also discussed in the recent works on (HOI) [3, 28] graph generation method with a bipartite graph con-struction process that inherits the advantages of both two-stage and one-stage methods.
• We develop an entity-aware structure for exploiting the compositional properties of visual relationships.
• Our method achieves the state-of-the-art or comparable performance on all metrics w.r.t the prior SGG methods and with more efficient inference. 2.