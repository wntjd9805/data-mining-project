Abstract
We introduce Programmatic Motion Concepts, a hierar-chical motion representation for human actions that cap-tures both low-level motion and high-level description as motion concepts. This representation enables human motion description, interactive editing, and controlled synthesis of novel video sequences within a single framework. We present an architecture that learns this concept representation from paired video and action sequences in a semi-supervised man-ner. The compactness of our representation also allows us to present a low-resource training recipe for data-efficient learning. By outperforming established baselines, especially in the small data regime, we demonstrate the efficiency and effectiveness of our framework for multiple applications. 1.

Introduction
The advent of new datasets and progress in machine learn-ing has pushed the boundaries of several video analysis tasks.
In particular, there has been tremendous progress on descrip-tion tasks such as action recognition [20, 22, 36, 46, 47] and localization [4,11,12,44,53,57], and on synthesis tasks such as human motion synthesis [1, 14–16, 27, 37, 43] and video synthesis [5, 40, 45, 50]. However, most of these models focus solely on their respective tasks of video description or synthesis. We posit that these tasks are better learned jointly.
We present a single framework for human motion description (recognizing and temporally localizing individual actions in the video), synthesis (generating videos from abstract de-scriptions) and editing (adding or removing actions and other fine-grained manipulations).
Our key insight is a hierarchical motion representation,
Programmatic Motion Concepts (PMC). Beginning with standard low-level input (sequences of keypoints), we learn a representation of the distribution of high-level motion con-cepts (such as jumping jacks) first from a few examples and then from longer sequences of repetitions of the concept that
* and † indicate equal contribution. Project page: https : / / sumith1896.github.io/motion-concepts we automatically segment. More specifically, for each mo-tion concept and each body keypoint, we learn a distribution of time-space trajectories expressed as cubic splines. The fact that we explicitly learn a distribution of motions at the level of the entire motion concept is what makes it possi-ble to perform recognition and localization well, since our methods leverage information about the motion of individual keypoints over the entire span of the motion concept. Simi-larly, sampling from these distributions allows us to directly synthesize natural-looking instances of the motion concept.
PMC is based on prior work on hierarchical motion un-derstanding with programmatic primitive-based represen-tations [23].
Illustrated in Figure 1(I), a human motion sequence is represented as a sequence of motion primitives, which are compact, and human-interpretable, parametric curves. We further represent motion concepts as group-ings of consecutive motion primitives that are named by hu-mans, such as jumping jack and squat in workout videos, as shown in Figure 1(I). This representation choice has access to both low-level pose sequences and high-level description sequences as motion concepts.
We want to enable easy and efficient learning of our rep-resentation to quickly adapt this framework to different do-mains. Hence, we propose to learn this representation from weakly-annotated data. Our input data contains each concept annotated with only a few mouse clicks by the human an-notator. For example, the annotation of each concept in our dataset consists of weak labels from < 15 workout videos.
Illustrated in Figure 1(II), in each video, the human annota-tor provides a start and end point for a video segment that contains repetitions of jumping jacks as well as the start and end points of three individual jumping jack instances.
Our method learns two models from this data, a recognition model that can be applied on any human motion sequences to detect occurrences of jumping jacks and a generative model that can synthesize jumping jack motions.
The key ideas behind our data efficient learning algorithm are three-fold. First, instead of using human skeleton se-quences as input, our model operates on a representation of motion as time-space curves, which is a strong but generi-cally applicable inductive bias for videos of human motion.
Figure 1. (I) We present a hierarchical human motion description framework. Each video is represented as a sequence of motion concepts and each motion concept is further grounded as a sequence of motion primitives. (II) Motion concepts can be learned from very small amount of human annotation: human annotators label a repetition range of a motion concept and just three instance ranges with in this group of repetitions. (III) Motion concept supports interactive editing and video synthesis. Human editors can flexibly edit a human motion video at both the concept level or the primitive level. We use neural generative models to render the output video.
Second, our model exploits the repetition of motion concepts in a single video. By annotating the beginning and the end of the repetition and a few occurrences of the action, our model automatically extrapolates to all instances in the repe-tition span. Third, the recognition and generation models are trained jointly, aiming at localizing individual occurrences of actions in the training data and learning the correspond-ing distribution of motion sequences, which significantly improves the quality of our motion synthesis model.
Programmatic Motion Concepts supports multiple human video analysis tasks, including recognition, localization, syn-thesis, and editing. Illustrated in Figure 1(III), given the input motion sequence, our model produces a human-interpretable label sequence of motion concepts. Each label is localized to a time span of the input. Users can tune the parameters for different motion primitives and even edit the label sequence itself to synthesize new motion sequences. Combined with techniques for human skeleton detection and skeleton-to-video synthesis, our data-efficient methods can be used to construct a full, practical pipeline for video-to-concept and concept-to-video workflows.
To summarize, our contributions are:
• We present a novel hierarchical representation of human motion that jointly supports description, synthesis, and editing of human motion videos.
• We present an data-efficient learning algorithm that leverages a motion primitive-based representation and repetitive structures in videos.
• Finally, we demonstrate the efficiency and effectiveness of our concept learning framework on three downstream tasks: motion description, action-conditioned motion synthesis, and controlled motion and video synthesis.
We also present qualitative results on interactive editing. 2.