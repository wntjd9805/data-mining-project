Abstract
Human speech is often accompanied by body gestures including arm and hand gestures. We present a method that reenacts a high-quality video with gestures matching a target speech audio. The key idea of our method is to split and re-assemble clips from a reference video through a novel video motion graph encoding valid transitions between clips. To seamlessly connect different clips in the reenactment, we propose a pose-aware video blending network which synthesizes video frames around the stitched frames between two clips. Moreover, we developed an audio-based gesture searching algorithm to find the optimal order of the reenacted frames. Our system generates reen-actments that are consistent with both the audio rhythms and the speech content. We evaluate our synthesized video quality quantitatively, qualitatively, and with user studies, demonstrating that our method produces videos of much higher quality and consistency with the target audio compared to previous work and baselines. Our project page https://github.com/yzhou359/vid-reenact includes code and data. 1.

Introduction
Gesture is a key visual component for human speech communication [31]. It enhances the expressiveness of hu-man performance and helps the audience to better compre-hend the speech content [19]. Given the progress in talk-ing head generation [17, 65, 83, 85], synthesizing plausible gesture videos becomes increasingly important for applica-tions such as digital voice assistants [47] and photo-realistic virtual avatars [24, 83]. In this paper, we propose an audio-driven gesture reenactment system that synthesizes speaker-specific human speech video from a target audio clip and a single reference speech video (Figure 1).
Unlike lip motions with specific phoneme-to-viseme mappings [20, 63, 86] or facial expressions mostly corre-sponding to low-frequency sentimental signals [67], ges-tures exhibit complex relationships with not only acoustics but also semantics of the audio [45]. Therefore, it is nontriv-ial to find a direct cross-modal mapping from audio wave-form to gesture videos, even for the same speaker. To bridge the gap between audio and video, previous methods [24,40] predict body pose (i.e., a jointed skeleton) as an intermedi-ate low dimensional representation to drive the video syn-thesis. However, they dissect the problem into two indepen-dent modules (audio-to-pose, and pose-to-video) and pro-duce results suffering from noticeable artifacts, e.g. dis-torted body parts and blurred appearance.
Our method introduces a video reenactment method that is able to synthesize high-resolution, high-quality speech gesture videos directly in the video domain by cutting, re-assembling, and blending clips from a single input refer-ence video. The process is driven by a novel video motion graph, inspired by 3D motion graphs used in character an-imation [4, 34]. The graph nodes represent frames in the reference video, and edges encode possible transitions be-tween them. We discover possible valid transitions between
frames, and also discover paths in the graph leading to the generation of a new video such that the re-enacted gestures are coherent and consistent with both the audio rhythms and speech content of the target audio.
Direct playback on the discovered paths for an output video can cause temporal inconsistency at the boundary of two disjoint raw frames. Existing frame blending methods cannot easily solve this problem, especially with fast mov-ing and highly deformed human poses. Therefore, we also propose a novel human pose-aware video blending network to smoothly blend frames around the temporally inconsis-tent boundaries to produce naturally-looking video transi-tions. By doing so, we successfully transform the problem of audio-driven gesture reenactment into the search for valid paths that best match the given audio.
Our path discovery algorithm is motivated by psycho-logical studies on co-speech gesture analysis. The studies show co-speech gestures can be categorized into rhythmic gestures and referential gestures [45]. While rhythmic ges-tures are well synchronized with audio onsets [9, 78], ref-erential gestures mostly co-occur with certain phrases, e.g. a greeting gesture of hand-waving appears when a speaker says ‘hello‘ or ‘hi‘ [8,15]. We analyze the speech of the ref-erence video and detect the audio onset peaks [18] as well as a set of keywords from its transcript [74] as audio fea-tures added to the corresponding nodes on the video motion graph. Given the extracted audio onset peaks and keywords from a new audio clip, the optimal paths that best match audio features are used to drive our video synthesis.
Our contributions are summarized as follows:
• a new system that creates high-quality human speech videos with realistic gestures driven by audio only,
• a novel video motion graph that preserves the video realism and gesture subtleties,
• a pose-aware video blending neural network that syn-thesizes smooth transitions of two disjoint reference video clips along graph paths, and
• an audio-based search algorithm that drives the video synthesis to match the synthesized gesture frames with both the audio rhythms and the speech content. 2.