Abstract
Deep neural network (DNN) suffers from catastrophic forgetting when learning incrementally, which greatly limits its applications. Although maintaining a handful of samples (called “exemplars”) of each task could alleviate forget-ting to some extent, existing methods are still limited by the small number of exemplars since these exemplars are too few to carry enough task-specific knowledge, and therefore the forgetting remains. To overcome this problem, we pro-pose to “imagine” diverse counterparts of given exemplars referring to the abundant semantic-irrelevant information from unlabeled data. Specifically, we develop a learnable feature generator to diversify exemplars by adaptively gen-erating diverse counterparts of exemplars based on seman-tic information from exemplars and semantically-irrelevant information from unlabeled data. We introduce semantic contrastive learning to enforce the generated samples to be semantic consistent with exemplars and perform semantic-decoupling contrastive learning to encourage diversity of generated samples. The diverse generated samples could effectively prevent DNN from forgetting when learning new tasks. Our method does not bring any extra inference cost and outperforms state-of-the-art methods on two bench-marks CIFAR-100 and ImageNet-Subset by a clear margin. 1.

Introduction
Recent years have witnessed the rapid development of deep neural networks (DNNs) in various tasks [10, 15, 26].
However, when a pretrained deep model learns a new task, it tends to forget the knowledge learned from previous tasks in the absence of the corresponding training data [1, 7, 9, 17, 19]. Such a catastrophic forgetting phenomenon greatly limits the real-world application of deep models because it
*Both authors contributed equally
†Corresponding author (a) Intuitive understanding of our method. (b) Illustration of our method and TSNE visualization.
Figure 1. (a). Our motivation. We propose to ‘imagine’ diverse counterparts of the limited exemplars referring to the semantically-(b). We propose a irrelevant information from unlabeled data. feature generator to ‘imagine’ diverse counterparts by adaptively mixing semantic information from exemplars with semantically-irrelevant information from unlabeled data. In the TSNE visual-ization, blue/green/red dots are the features of one class’s gener-ated samples/exemplars/real data. We observe that the exemplars and generated samples cover real data. Best viewed in color. is impractical to maintain the training data of each task due to privacy concerns and so forth [4, 11, 20, 22, 29, 30, 32].
To overcome catastrophic forgetting, incremental learn-ing methods are developed. Previous works widely adopt rehearsal strategy [3, 6, 16, 22, 32]: storing a limited quan-tity of samples called exemplars from the original training dataset and reusing them against forgetting when the model
learning new tasks. For instance, RM [3] selects hard sam-ples as exemplars according to the classification uncertainty.
GD [16] distills the knowledge from the old network to the new one based on stored exemplars. BiC [32] optimizes the classification bias referring to a subset of exemplars.
However, only a handful of exemplars conveying limited variations could be stored due to the reasons such as pri-vacy concerns, which hinders the development of existing methods. When learning a new task with abundant train-ing data, the exemplars are too few and the model capacity tends to be dominated by the training data of the new task.
Although one could emphasize the exemplars during learn-ing, the deep model may overfit the exemplars as shown in recent work [29], leading to unsatisfactory performance.
In this work, we propose a plug-and-play learnable fea-ture generator to adaptively diversify the exemplars by ex-ploiting unlabeled data. When a deep model learns new tasks, it is easy to collect massive unlabeled data in the real world [16, 37]. Referring to the abundant semantic-irrelevant information within the unlabeled data, we could learn a feature generator to ‘imagine’ various counterparts for the given exemplars and consequently diversify the ex-emplars for tackling the forgetting problem (See Fig. 1).
Our method adopts a two-stage training schedule.
Specifically, we sample a handful of exemplars from the current dataset when a task ends. Before dropping the orig-inal dataset, we train the feature generator to generate di-verse counterparts of the exemplars based on exemplars and massive unlabeled data. We perform semantic contrastive learning between the generated samples and the original dataset so that (i) the generator could learn to keep the gen-erated samples semantically consistent with the exemplars and (ii) the generated samples are encouraged to be as di-verse as possible. To further facilitate the exploration of semantically-irrelevant information within unlabeled data and generate more diverse samples, we further introduce semantic-decoupling contrastive learning between the gen-erated samples and the unlabeled data. When a new task starts, the feature generator is frozen and used to generate diverse samples to prevent the deep model from forgetting knowledge of previous tasks. At this time, the feature gen-erator does not require any gradient and serves as a static non-linear mapping function. Our method does not bring extra inference costs. The feature generator is discarded and only the vanilla deep model is needed during inference.
Our main contributions are as follows. Firstly, we pro-posed a learnable feature generator to adaptively generate diverse counterparts of limited exemplars by exploiting the semantically irreverent information in a messy unlabeled dataset. With the diverse generated samples, the model could better overcome forgetting. Our method does not bring extra inference cost and is insensitive to unlabeled data. Secondly, we introduce semantic contrastive learning and semantic-decoupling contrastive learning to ensure the generated samples are diverse and semantically consistent with given exemplars. Finally, experimental results show that our method is effective and outperforms existing meth-ods by a clear margin with arbitrary unlabeled data. 2.