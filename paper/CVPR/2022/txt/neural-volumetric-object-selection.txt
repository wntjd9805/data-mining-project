Abstract
We introduce an approach for selecting objects in neural volumetric 3D representations, such as multi-plane images (MPI) and neural radiance fields (NeRF). Our approach takes a set of foreground and background 2D user scribbles in one view and automatically estimates a 3D segmenta-tion of the desired object, which can be rendered into novel views. To achieve this result, we propose a novel voxel fea-ture embedding that incorporates the neural volumetric 3D representation and multi-view image features from all input views. To evaluate our approach, we introduce a new dataset of human-provided segmentation masks for depicted objects in real-world multi-view scene captures. We show that our approach out-performs strong baselines, including 2D seg-mentation and 3D segmentation approaches adapted to our task. 1.

Introduction
Object selection is an important task in an artist’s work-flow. With the growth of 3D photography, there is a need to support a suite of editing operations, such as object se-lection, that are readily available for 2D photographs. In
∗Work partly done during an internship at Adobe Research.
†Alphabetic order. this work, we consider object selection in neural volumetric 3D representations. Neural volumetric 3D representations— explicitly via Multi-plane Images (MPI) [43] or implic-itly via Neural Radiance Fields (NeRF) [33]—recover a remarkably accurate 3D representation of a scene from a given set of multi-view images. These representations have been shown to be particularly useful for novel-view synthe-sis [6,24,33,48,52,55], as this is the task that they are trained for. However, beyond just visualizing the same scene from a novel viewpoint, often we want to create a 3D reconstruc-tion of an object so that we can extract it, and place it in a different context. There is, of course, substantial research in automatic object segmentation. However, user-driven meth-ods, e.g., Photoshop, remain the most common means, as which object to select is fundamentally a high-level decision.
Similar user-driven methods for 3D object segmentation are particularly important for augmented and virtual reality (AR/VR) applications, e.g., if one wants to composite a se-lected object into a new scene, apply a filter to a selected object, remove a selected object, or share a real-world object with friends in an AR/VR environment.
While user-driven segmentation for 2D images has been studied for decades [1, 11, 34, 40], very little is known about how segmentation would work in those novel 3D scene rep-resentations. Specifically, for image segmentation, early works focus on energy minimization with graph cuts [4], different forms of user interactions [4, 17, 35, 39], and ways
to obtain better object priors [14, 45]. More recently, re-search has focused on encoding object priors with deep nets [23, 26–28, 49].
Naively applying these techniques to the set of images that are used to capture neural volumetric 3D representa-tions is sub-optimal. For example, simply transferring user interactions like scribbles from one image to another using a known camera transformation may fail to cover the in-tended object because of occlusions. Similarly, transferring an appearance model learned on one image to all remaining images is challenging because of appearance and lighting changes. Asking a user to interact with all images requires an interface that may not be intuitive or require excessive work, and furthermore may produce a segmentation that is not view-consistent.
For those reasons, novel user-driven 3D segmentation techniques are warranted. We propose a novel voxel feature embedding that incorporates discretized features from the neural volumetric 3D representation and image features from all input views. Formally, we first project user interactions in the form of 2D scribbles from a reference image to sparse 3D locations using the reconstructed scene. We then learn a 3D object representation model that incorporates image features from all views via a developed multi-view feature embedding. We use these features to directly segment the object in the volumetric 3D scene representation and apply a post-processing step to remove outliers. The extracted 3D object can subsequently be viewed from different directions, as visualized in Fig. 1.
We evaluate the proposed method on real world samples from the LLFF [32], Shiny [48], and NeRF-real360 [33] data. As shown in Fig. 1, despite very few scribbles on a single reference image, the proposed method recovers an accurate 3D model of the object of interest and retains fine details (e.g., the ribs of the dinosaur in row 1). To study quantitatively, we obtained annotations using a professional service. Our method out-performs 2D and 3D interactive segmentation baselines by a margin on all benchmarks.
In summary, we present the first method for user-driven 3D object selection targeting recent neural volumetric re-construction. We show that a pre-trained network to embed multi-view features produces a more robust selection method than applying existing interactive 3D segmentation methods.
For evaluation, we contribute a set of high-quality ground-truth annotations on three real-world datasets. 2.