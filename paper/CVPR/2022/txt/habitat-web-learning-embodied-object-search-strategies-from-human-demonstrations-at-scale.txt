Abstract 1.

Introduction
We present a large-scale study of imitating human demon-strations on tasks that require a virtual robot to search for objects in new environments – (1) ObjectGoal Navigation (e.g. ‘ﬁnd & go to a chair’) and (2) PICK&PLACE (e.g. ‘ﬁnd mug, pick mug, ﬁnd counter, place mug on counter’). First, we develop a virtual teleoperation data-collection infras-tructure – connecting Habitat simulator running in a web browser to Amazon Mechanical Turk, allowing remote users to teleoperate virtual robots, safely and at scale. We collect 80k demonstrations for OBJECTNAV and 12k demonstra-tions for PICK&PLACE, which is an order of magnitude larger than existing human demonstration datasets in sim-ulation or on real robots. Our virtual teleoperation data contains 29.3M actions, and is equivalent to 22.6k hours of real-world teleoperation time, and illustrates rich, diverse strategies for solving the tasks. Second, we use this data to answer the question – how does large-scale imitation learn-ing (IL) (which has not been hitherto possible) compare to reinforcement learning (RL) (which is the status quo)? On
OBJECTNAV, we ﬁnd that IL (with no bells or whistles) us-ing 70k human demonstrations outperforms RL using 240k agent-gathered trajectories. This effectively establishes an
‘exchange rate’ – a single human demonstration appears to be worth ∼4 agent-gathered ones. More importantly, we ﬁnd the IL-trained agent learns efﬁcient object-search behavior from humans – it peeks into rooms, checks corners for small objects, turns in place to get a panoramic view – none of these are exhibited as prominently by the RL agent, and to in-duce these behaviors via contemporary RL techniques would require tedious reward engineering. Finally, accuracy vs. training data size plots show promising scaling behavior, suggesting that simply collecting more demonstrations is likely to advance the state of art further. On PICK&PLACE, the comparison is starker – IL agents achieve ∼18% suc-cess on episodes with new object-receptacle locations when trained with 9.5k human demonstrations, while RL agents fail to get beyond 0%. Overall, our work provides compelling evidence for investing in large-scale imitation learning.
Project page: ram81.github.io/projects/habitat-web.
General-purpose robots that can perform a diverse set of embodied tasks in a diverse set of environments have to be good at visual exploration. Consider the canonical example of asking a household robot, ‘Where are my keys?’. To answer this (assuming the robot does not remember the answer from memory), the robot would have to search the house, often guided by intelligent priors – e.g. peeking into the washroom or kitchen might be sufﬁcient to be reasonably sure the keys are not there, while exhaustively searching the living room might be much more important since keys are more likely to be there. While doing so, the robot has to internally keep track of where all it has been to avoid redundant search, and it might also have to interact with objects, e.g. check drawers and cabinets in the living room (but not those in the washroom or kitchen!).
This example illustrates fairly sophisticated exploration, in-volving a careful interplay of various implicit objectives (semantic priors, exhaustive search, efﬁcient navigation, in-teraction, etc.). Many recent tasks of interest in the embodied
AI community – e.g. ObjectGoal Navigation [1, 2], rear-rangement [3,4], language-guided navigation [5,6] and inter-action [7], question answering [8–12] – involve some ﬂavor of this visual exploration. With careful reward engineering, reinforcement learning (RL) approaches to these tasks have achieved commendable success [13–17]. However, engi-neering the ‘right’ reward function so that the learned policy exhibits desired behavior is unintuitive and frustrating (even for domain experts), expensive (requiring multiple rounds of retraining under different rewards), and not scalable to new tasks or behaviors. For complex tasks (e.g. object rearrange-ment or tasks speciﬁed in open-ended natural language), RL from scratch may not even get off the ground.
In this work, we advance the alternative research agenda of imitation learning [18] – i.e. collecting a large dataset of human demonstrations (that implicitly capture intelligent be-havior we wish to impart to our agents) and learning policies directly from these human demonstrations.
First, we develop a safe scalable virtual teleoperation data-collection infrastructure – connecting the Habitat simulator
Figure 1. a) Example OBJECTNAV 1) human demonstration, 2) agent trained on human demonstrations, and 3) shortest path. Notice how humans demonstrate sophisticated exploration behavior to succeed at this task in unseen environments, which is hard to engineer into the right reward for an RL agent and is unlikely to be captured in shortest path demonstrations. An agent trained on human demonstrations learns this exploration and object-search behavior. b) Success on the OBJECTNAV MP3D-VAL split vs. no. of human demonstrations for training. running in a browser to Amazon Mechanical Turk (AMT).
We develop this in way that enables collecting human demon-strations for a variety of tasks being studied within the Habi-tat [19, 20] ecosystem (e.g. PointNav [2], OBJECTNAV [1, 2],
ImageNav [21], VLN-CE [6], MultiON [22], etc.).
We use this infrastructure to collect human demonstra-tion datasets for 2 tasks requiring visual search – 1) Ob-jectGoal Navigation (e.g. ‘ﬁnd & go to a chair’) and 2)
PICK&PLACE (e.g. ‘ﬁnd mug, pick mug, ﬁnd counter, place on counter’).
In total we collect 92k human demonstra-tions, 80k demonstrations for OBJECTNAV and 12k demon-strations for PICK&PLACE. In contrast, the largest exist-ing datasets have 3-10k human demonstrations in simula-tion [23–25] or on real robots [26,27], an order of magnitude smaller. This virtual teleoperation data contains 29.3M ac-tions, which is equivalent to 22, 600 hours of real-world tele-operation time assuming a LoCoBot motion model from [28] (details in appendix (Sec. A.3)). The ﬁrst thing this data pro-vides is a ‘human baseline’ with sufﬁciently tight error-bars to be taken seriously. On the OBJECTNAV validation split, humans achieve 93.7±0.1% success and 42.5±0.5% Success
Weighted by Path Length (SPL) [2] (vs. 34.6% success and 7.9% SPL for the 2021 Habitat ObjectNav Challenge win-ner [15]). The success rate (93.7%) suggests that this task is largely doable for humans (but not 100%). The SPL (42.5%) suggests that even humans need to explore signiﬁcantly.
Beyond scale, the data is also rich and diverse in the strate-gies that humans use to solve the tasks. Fig. 1 shows an example trajectory of an AMT user controlling a LoCoBot looking for a ‘plant’ in a new house – notice the peeking into rooms, looping around the dining table – all of which is (understandably) absent from the shortest path to the goal.
We use this data to answer the question – how does large-scale imitation learning (IL) (which has not been hitherto possible) compare to large-scale reinforcement learning (RL) (which is the status quo)? On OBJECTNAV, we ﬁnd that IL (with no bells or whistles) using only 70k human demon-strations outperforms RL using 240k agent-gathered trajec-tories. This effectively establishes an ‘exchange rate’ – a single human demonstration appears to be worth ∼4 agent-gathered ones. More importantly, we ﬁnd the IL-trained agent learns efﬁcient object-search behavior – as shown in
Fig. 1 and Sec. 7. The IL agent learns to mimic human behavior of peeking into rooms, checking corners for small objects, turning in place to get a panoramic view – none of these are exhibited as prominently by the RL agent. Fi-nally, the accuracy vs. training-data-size plot (Fig. 1b) shows promising scaling behavior, suggesting that simply collect-ing more demonstrations is likely to advance the state of art further. On PICK&PLACE, the comparison is even starker – IL-agents achieve ∼18% success on episodes with new object-receptacle locations when trained with 9.5k human demonstrations, while RL agents fail to get beyond 0%.
On both tasks, we ﬁnd that demonstrations from humans are essential; imitating shortest paths from an oracle pro-duces neither accuracy nor the strategic search behavior. In hindsight, this is perfectly understandable – shortest paths (e.g. Fig. 1(a3)) do not contain any exploration but the task requires the agent to explore. Essentially, a shortest path is inimitable, but imitation learning is invaluable. Overall, our work provides compelling evidence for investing in large-scale imitation learning of human demonstrations. 2.