Abstract
User-level differential privacy (DP) provides certifiable privacy guarantees to the information that is specific to any user’s data in federated learning. Existing methods that ensure user-level DP come at the cost of severe accu-racy decrease. In this paper, we study the cause of model performance degradation in federated learning with user-level DP guarantee. We find the key to solving this issue is to naturally restrict the norm of local updates before ex-ecuting operations that guarantee DP. To this end, we pro-pose two techniques, Bounded Local Update Regularization and Local Update Sparsification, to increase model quality without sacrificing privacy. We provide theoretical analy-sis on the convergence of our framework and give rigorous privacy guarantees. Extensive experiments show that our framework significantly improves the privacy-utility trade-off over the state-of-the-arts for federated learning with user-level DP guarantee. 1.

Introduction
Federated learning (FL) [17] is a promising paradigm of distributed machine learning with a wide range of applica-tions [5, 13, 15]. FL enables distributed agents to collabora-tively learn a centralized model under the orchestration of the cloud without sharing their local data. By keeping data usage local, FL sidesteps the ethical and legal concerns and is advantageous in privacy compared with the traditional centralized learning paradigm.
However, FL alone does not protect the agents or users from inference attacks that use the output information. Ex-tensive inference attacks demonstrate that it is feasible to infer the subgroup of people with a specific property [19], identify individuals [24], or even infer completion of social security numbers [4], with high confidence from a trained model.
*Corresponding Author.
To solve these issues, differential privacy (DP) [6] has been applied to FL in order to protect either each instance in the dataset of any agent (instance-level DP) [11, 25, 26], or the whole data of any agent (user-level DP) [7, 12, 18].
These two DP definitions on different levels are suitable for different situations. For example, when several banks aim to train a fraud detection model via FL, instance-level DP is more suitable to protect any individual records of any bank from being identified. In another situation, when a smart-phone app attempts to learn a face recognition model from users’ face images, it is more appropriate to apply user-level
DP to protect each user as a unit.
Existing methods that ensure user-level DP [7, 12, 18] are predominantly built upon Gaussian mechanism which is a Gaussian noise perturbation-based technique. Unfortu-nately, directly applying the Gaussian mechanism to ensure strong user-level DP in FL drastically degrades the utility of the resulted models. Specifically, the Gaussian mecha-nism requires to clip the l2 magnitude of local updates to a sensitivity threshold S and adding noise proportional to
S to the high dimensional local updates. These two steps lead to either large bias (when S is small) or large variance (when S is large), which slows down the convergence and damages the performance of the global model [30]. How-ever, existing methods [7, 12, 18] do not explicitly involve interaction between the operations for ensuring DP and the learning process of FL, which makes the learning process hard to adapt to the clipping and noise perturbation opera-tions, thereby leading to utility degradation of the learned models.
To address the above issues, in this paper, we propose two techniques to improve the model utility in FL with user-level DP guarantees. Our motivation is to naturally reduce the l2 norm of local updates before clipping, thereby mak-ing the local updates more adaptive to the clipping oper-ation. First, we propose Bounded Local Update Regular-ization (BLUR). It introduces a regularization term to the agent’s local objective function and explicitly regularizes the l2 norm of local updates to be bounded. As a result, the
l2 norm of local updates could be naturally smaller than S, thereby decreasing the impact of clipping operation. Then we propose Local Update Sparsification (LUS) to further reduce the magnitude of local updates. Before clipping, it zeros out some update values that have little effect on the performance of the local model, thereby reducing the norm of local updates without damaging the accuracy of the local model.
Our contributions can be summarized as follows:
• We propose two techniques to improve the model util-ity with user-level DP guarantee in federated learning.
• We provide theoretical analysis on the convergence of our framework and give rigorous privacy guarantees.
• Extensive experiments validate the effectiveness and advantages of the proposed methods. 2.