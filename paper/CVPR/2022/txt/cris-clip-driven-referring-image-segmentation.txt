Abstract
Referring image segmentation aims to segment a referent via a natural linguistic expression. Due to the distinct data properties between text and image, it is challenging for a network to well align text and pixel-level features. Exist-ing approaches use pretrained models to facilitate learning, yet separately transfer the language/vision knowledge from pretrained models, ignoring the multi-modal corresponding information. Inspired by the recent advance in Contrastive
Language-Image Pretraining (CLIP), in this paper, we pro-pose an end-to-end CLIP-Driven Referring Image Segmen-tation framework (CRIS). To transfer the multi-modal knowl-edge effectively, CRIS resorts to vision-language decoding and contrastive learning for achieving the text-to-pixel align-ment. More specifically, we design a vision-language de-coder to propagate fine-grained semantic information from textual representations to each pixel-level activation, which promotes consistency between the two modalities. In addi-tion, we present text-to-pixel contrastive learning to explic-itly enforce the text feature similar to the related pixel-level features and dissimilar to the irrelevances. The experimental results on three benchmark datasets demonstrate that our proposed framework significantly outperforms the state-of-the-art performance without any post-processing. 1.

Introduction
Figure 1. An illustration of our main idea. (a) CLIP [39] jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of image I and text T , which can capture the multi-modal corresponding information. (b) To transfer this knowl-edge of the CLIP model from image level to pixel level, we propose a CLIP-Driven Referring Image Segmentation (CRIS) framework.
Firstly, we design a vision-language decoder to propagate fine-grained semantic information from textual features to pixel-level visual features. Secondly, we combine all pixel-level visual features
V with the global textual feature T and adopt contrastive learning to pull text and related pixel-wise features closer and push other irrelevances away.
Referring image segmentation [14, 49, 50] is a fundamen-tal and challenging task at the intersection of vision and language understanding, which can be potentially used in a wide range of applications, including interactive image editing and human-object interaction. Unlike semantic and instance segmentation [8,10,12,47], which requires segment-ing the visual entities belonging to a pre-determined set of categories, referring image segmentation is not limited to
*Equal contribution indicating specific categories but finding a particular region according to the input language expression.
Since the image and language modality maintain dif-ferent properties, it is difficult to explicitly align textual features with pixel-level activations. Benefiting from the powerful capacity of the deep neural network, early ap-proaches [14, 22, 25, 33] concatenate textual features with each visual activation directly, and use these combined fea-tures to generate the segmentation mask. Subsequently, to
Figure 2. Comparison between the direct fine-tuning and our proposed methods. “Naive” denotes the direct fine-tuning mentioned in section 4. Compared with the direct fine-tuning, our method can not only leverage the powerful cross-modal matching capability of the
CLIP, but also learn fine-grained visual representations. address the lack of adequate interaction between two modali-ties, a series of methods [4,16,17,42,49] adopt the language-vision attention mechanism to better learn cross-modal fea-tures.
Existing methods [4, 16, 17, 42, 49] leverage external knowledge to facilitate learning in common, while they mainly utilize a single-modal pretraining (e.g., the pretrained image or text encoder), which is short of multi-modal corre-spondence information. By resorting to language supervision from large-scale unlabeled data, vision-language pretrain-ing [34, 39, 46] is able to learn ample multi-modal represen-tations. Recently, the remarkable success of the CLIP [39] has shown its capability of learning SOTA image-level vi-sual concepts from 400 million image-text pairs, which as-sists many multi-modal tasks achieve significant improve-ments, including image-text retrieval [39], video-text re-trieval [6, 31]. However, as shown in Figure 2, the direct usage of the CLIP can be sub-optimal for pixel-level pre-diction tasks, e.g., referring image segmentation, duo to the discrepancy between image-level and pixel-level prediction.
The former focuses on the global information of an input image, while the latter needs to learn fine-grained visual representations for each spatial activation.
In this paper, we explore leveraging the powerful knowl-edge of the CLIP model for referring image segmentation, in order to enhance the ability of cross-modal matching.
Considering the characteristics of referring image segmenta-tion, we propose an effective and flexible framework named
CLIP-Driven Referring Image Segmentation (CRIS), which can transfer ample multi-modal corresponding knowledge of the CLIP for achieving text-to-pixel alignment. Firstly, we propose a visual-language decoder that captures long-range dependencies of pixel-level features through the self-attention operation and adaptively propagate fine-structured textual features into pixel-level features through the cross-attention operation. Secondly, we introduce the text-to-pixel contrastive learning, which can align linguistic features and the corresponding pixel-level features, meanwhile distin-guishing irrelevant pixel-level features in the multi-modal embedding space. Based on this scheme, the model can explicitly learn fine-grained visual concepts by interwinding the linguistic and pixel-level visual features.
Our main contributions are summarized as follow:
• We propose a CLIP-Driven Referring Image Segmen-tation framework (CRIS) to transfer the knowledge of the CLIP model for achieving text-to-pixel alignment.
• We take fully advantage of this multi-modal knowledge with two innovative designs, i.e., the vision-language decoder and text-to-pixel contrastive learning.
• The experimental results on three challenging bench-marks significantly outperform previous state-of-the-art methods by large margins (e.g., + 4.89 IoU on Ref-COCO, + 8.88 IoU on RefCOCO+, + 5.47 IoU on G-Ref). 2.