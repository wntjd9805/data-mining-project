Abstract
Cross language-image modality retrieval in E-commerce is a fundamental problem for product search, recommenda-tion, and marketing services. Extensive efforts have been made to conquer the cross-modal retrieval problem in the general domain. When it comes to E-commerce, a com-mon practice is to adopt the pretrained model and finetune on E-commerce data. Despite its simplicity, the perfor-mance is sub-optimal due to overlooking the uniqueness of
E-commerce multimodal data. A few recent efforts [10, 72] have shown significant improvements over generic methods with customized designs for handling product images. Un-fortunately, to the best of our knowledge, no existing method has addressed the unique challenges in the e-commerce language. This work studies the outstanding one, where it has a large collection of special meaning entities, e.g.,
“Dissel (brand)”,“Top (category)”, “relaxed (fit)” in the fashion clothing business. By formulating such out-of-distribution finetuning process in the Causal Inference paradigm, we view the erroneous semantics of these spe-cial entities as confounders to cause the retrieval failure.
To rectify these semantics for aligning with e-commerce do-main knowledge, we propose an intervention-based entity-aware contrastive learning framework with two modules, i.e., the Confounding Entity Selection Module and Entity-Aware Learning Module. Our method achieves competitive performance on the E-commerce benchmark Fashion-Gen.
Particularly, in top-1 accuracy (R@1), we observe 10.3% and 10.5% relative improvements over the closest baseline in image-to-text and text-to-image retrievals, respectively. 1.

Introduction
Cross visual and linguistic retrieval, as a fundamental component in the multimodal searching system, has been extensively studied [13, 18, 24, 27, 32, 38, 41, 43, 69, 70]. It
*Work done during the author’s internship at Adobe Research.
†Corresponding author.
Figure 1.
Illustration of domain shift between general domain and e-commerce domain. In e-commerce domain, a collection of tag entities with strong domain semantics are associated with a title/description and image. takes linguistic data as the query and retrieves the corre-sponding visual data, or verse vice. One key challenge in this area is how to align the visual and textual data semanti-cally.
In the cross-modal retrieval of e-commerce products, there are many unique characteristics in both e-commerce image and language. As shown in Fig. 1, an e-commerce product image usually only contains a simple scene with one or two foreground objects and a plain background.
Meanwhile, an e-commerce language is usually composed of a set of metadata (tag entities) [15, 39], including prod-uct title/description, brand, category, composition, etc. Pre-vious works such as FashionBERT [10] and KaleidoBERT
[72] suggest that cross-modal retrieval in fashion domains requires more fine-grained features (e.g. short sleeve and crewneck). However, the popular Region of Interest (RoI)
[11] based methods detect unsatisfactory region propos-als with either repeated object regions or irrelevant sub-regions to the product. To this end, these works focus on fine-grained representation learning of images through the patch-based method. Despite the great successes, they only focus on the challenges of images, while the language part
still follows the vanilla BERT [5].
In this work, we improve cross-modality product re-trieval from the language part. Specifically, we design our model with the following two motivations about the unique language in e-commerce. Motivation-1: the word tokens often come up with special meanings in e-commerce, while the pretrained language model part in [10, 38, 72] is biased despite of the large-scale pretraining corpus. For instance, entity “diesel” in pretrained CLIP model is strongly as-sociated with the concept “fuel”, while in e-commerce fash-ion domain, “diesel” is tagged as a brand entity. Other examples include “canada goose (brand)”, “golden goose (brand)”, “top (category)”, to name a few. Such out-of-distribution problem in multimodal finetuning is re-cently studied from the causal inference viewpoint [67].
Zhang et al. formulate this undesirable spurious correla-tions between image and language as “confounders” learned from the pretrained dataset. By modeling with structural causal model (SCM) graph [36], the authors perform hard intervention to remove the dataset bias via backdoor inter-vention [36]. However, when modeling the confounding variables, Zhang et al. follow the traditional BERT token vocabulary, treating each entity as a group of (sub)word to-kens as others [10, 72]. This overlooks a large collection of special meaning entities in e-commerce, such as “Dissel (brand)”,“top (category)”, “relexed (fit)”. Moreover, this will inevitably intertwine different entities with the shared confounding (sub)word tokens, such as “Canada
Goose” and “Golden Goose”. To this end, the language part should be entity-aware [31, 47, 71] and disentangled from the conventional meanings of special entities encoded in the pretrained language model.
Figure 2. Empirical analysis of image-to-text and text-to-image tasks on Fashion-Gen. We finetune the pretrained CLIP model by concatenating different textual meta data. Results on top-1 accu-racy are reported.
Meanwhile, the varieties of meta data leads to our
Motivation-2: meta data contribute unevenly to the cross-modality retrieval. Specifically, previous methods usually concatenate all the metadata together to form a long sen-tence [10, 24, 38, 41, 43, 72]. However, this straightforward solution treats each meta information equally. In practice, for different image/text pairs, metadata (tag entity) may contribute differently. Some metadata can even be harmful to retrieval. To support the claim, we conduct an empirical study on Fashion-Gen dataset using a simple yet effective
CLIP model [38]. We finetune the pretrained CLIP model given different meta entity concatenations on Fashion-Gen dataset. From Fig. 2, it is observed that given the product description (dark blue), “brand” (orange) is the only help-ful metadata. Adding “category” (yellow), “season” (grey), or “composition” (light blue) can contribute little or even harm the performance. More importantly, if we concatenate all the meta data (green), both performances are dropped compared to only appending “brand” in text-to-image and image-to-text tasks. To this end, it is thus important to iden-tify the beneficial metadata while discarding the others.
As motivated, we propose an Entity-aware Intervention-based contrastive learning framework, termed EI-CLIP, for e-commerce product retrieval problem with two specific module designs in the causal learning paradigm, i.e., Entity-Aware Learning Module (EA-learner) for motivation-1 and Confounding Entity Selection Module (CE-selector) for motivation-2.
It is worth clarifying that we do not propose a new causality method, but rather formulate the entity-aware e-commerce cross-modal retrieval problem in the casual view. Specifically, the EA-learner learns an in-dividual representation for each informative confounding entity for better mitigating the out-of-distribution problem.
Then the CE-selector aims to automatically select the most informative group of meta data (e.g., “brand” in Fig. 2) from the abundant textual meta data.
We summarize our main contributions as follows:
• To the best of our knowledge, this is the pioneer-ing work to tackle the challenges introduced by e-commerce special entities in language modality. Previ-ous cross-modal retrieval works only focus on images.
• We are the first to formulate the entity-aware retrieval task in causal view. We argue that the erroneous se-mantics of e-commerce special entities learned in the general domain are the confounders causing the re-trieval failures.
• Equipped with backdoor adjustment [36] in causal inference, we propose an Entity-aware Intervention-based contrastive learning framework (EI-CLIP), with two new components, i.e., CE-selector and EA-learner.
• EI-CLIP achieves competitive performance on e-commerce benchmark dataset Fashion-Gen. In partic-ular, in top-1 accuracy (R@1), we observe 10.3% and 10.5% relative improvements over the closest baseline in image-to-text and text-to-image, respectively. 2.