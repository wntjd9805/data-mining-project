Abstract
Unsupervised learning has been popular in various com-puter vision tasks, including visual object tracking. How-ever, prior unsupervised tracking approaches rely heavily on spatial supervision from template-search pairs and are still unable to track objects with strong variation over a long time span. As unlimited self-supervision signals can be ob-tained by tracking a video along a cycle in time, we investi-gate evolving a Siamese tracker by tracking videos forward-backward. We present a novel unsupervised tracking frame-work, in which we can learn temporal correspondence both on the classification branch and regression branch. Specif-ically, to propagate reliable template feature in the forward propagation process so that the tracker can be trained in the cycle, we first propose a consistency propagation trans-formation. We then identify an ill-posed penalty problem in conventional cycle training in backward propagation pro-cess. Thus, a differentiable region mask is proposed to se-lect features as well as to implicitly penalize tracking errors on intermediate frames. Moreover, since noisy labels may degrade training, we propose a mask-guided loss reweight-ing strategy to assign dynamic weights based on the qual-ity of pseudo labels. In extensive experiments, our tracker outperforms preceding unsupervised methods by a substan-tial margin, performing on par with supervised methods on large-scale datasets such as TrackingNet and LaSOT. Code is available at https://github.com/FlorinShum/ULAST. 1.

Introduction
Visual tracking has become an integral part of various video applications such as autonomous driving and video recognition.
In the mainstream of visual object tracking, deep learning-based trackers are dominant [28], requiring
†Work performed when Qiuhong Shen is an intern at SenseTime. a large number of labeled videos. Since the labeled data occupy a relatively small portion of practical scenes, the trained tracker cannot reliably track previously unseen ob-jects. Thus, learning from unlabeled videos becomes a promising approach. Prior works on unsupervised tracking fall into two categories: exploiting self-supervision signals in videos from either the spatial or temporal dimensions.
For the first category [24, 33], the focus is on how to con-struct a template-search pair using a still frame. Since these methods are limited by the inability to learn temporal cor-respondence over long periods of time, trained trackers can no longer track objects with strong variation. To cope with the appearance variations that occur during online tracking, we center our attention on methods in the latter category, exploiting the temporal self-supervision signal in videos.
In supervised tracking methods, box-regression branch has been demonstrated to be effective to capture the objects with large scale variation along temporal dimension [4, 22].
However, in existing unsupervised methods, this branch is always absent [33, 34, 38]. Recently, USOT [47] introduced a box-regression head, but the tracker is initially trained with template-search pairs from single frames, followed by cycle memory training to enhance the robustness of classifi-cation branch, whereas the box-regression branch is trained with spatial supervision alone. In this paper, we aim to train a better tracker by learning temporal correspondence both on the classification branch and on the regression branch.
However, we identify there are three critical challenges.
First, despite unlimited self-supervision can be obtained by tracking a video along a cycle in time, how to explore the self-supervision signal in temporal dimension of videos for training a tracker equipped with a box-estimation branch is not well explored in existing methods. In cycle training, as illustrated in Fig. 1, a tracker is assumed to be capable of tracking back to the initial location. The tracker is evolved in the cycle by utilizing the inconsistency between start and end location in initial frames. However, when training from
Figure 1. Three challenges when learning a better tracker with temporal correspondence on both classification and regression branches. First, in forward propagation process, tracker often loses target objects in intermediate frames, breaking down the train-ing pipeline. Second, in backward propagation process, gradient cannot flow through the whole framework due to the RoI-Align.
Third, the pseudo labels are often noisy, degrading the perfor-mance. In this figure, T , S, R, B, and L denote template kernel, search region, candidate boxes, pseudo labels, and loss value, re-spectively. The subscript indexes the temporal order of frames. scratch, it is hard for the tracker to find the target object.
The template kernel generated on the intermediate frames is likely to not contain any features of the target object, which means the tracker cannot return to the initial location. The training pipeline will break down after several iterations.
Second, we identify misalignment in cycle training: when the tracking result R2 is inaccurate, the generated template kernel T2 is likely to embrace many distractor fea-tures, but the imposed loss still forces the tracker to pre-dict accurate target box by using such noisy template, which causes an ill-posed penalty. From the gradient flow of cycle training, we can observe the select operation like RoI-Align, since the coordinate is quantized, is not differentiable on boxes coordinate. Therefore the gradient cannot back prop-agate to the node before R2. In other words, the tracking errors on intermediate frames cannot be penalized in this pipeline.
Third, as unsupervised tracking framework still relies on the pseudo labels in initial frames and box-regression branch training requires objects that have clear edges, pseudo labels in initial frames are crucial. However, we observe these labels are likely to be noisy, degrading the tracking performance.
To address the aforementioned three challenges, we propose a novel unsupervised tracking framework called
ULAST, which aims to learn temporal correspondence both on the classification branch and regression branch. It con-sists of three newly proposed components: consistency propagation transformation, region mask operation, and mask-guided loss re-weighting. Specifically, the consis-tency propagation transformation aims to generate reliable template kernel for tracking next frame, which uses both long-term and short-term information from template ker-nels and search regions of previous frames. As a result, it enables our framework to exploit temporal self-supervision signal and avoiding the training pipeline breaking down. In-stead of using RoI-Align, our region mask operation selects features with all candidates in R2 based on the search re-gion feature and predicted bounding boxes from previous frame, and makes regression and classification heads differ-entiable to implicitly penalize tracking errors on intermedi-ate frames. The mask-guided loss re-weighting strategy dy-namically assigns weights to samples based on the quality of their pseudo labels, which avoid using the noisy pseudo labels.
We evaluate the trained tracker on five diverse bench-mark datasets, and the favourable performance against state-of-the-art methods demonstrate the effectiveness of our proposed framework. The main contribution of this work are summarized as follows:
• We propose a novel unsupervised learning framework called ULAST, which can lean temporal correspon-dence both on classification and regression branches.
• A consistency propagation transformation is proposed to generate reliable template kernel, avoiding the train-ing process of our ULAST framework breaking down.
• A differentiable region mask operation is proposed to select features as well as implicitly penalize the track-ing errors of intermediate frames in backward propa-gation process.
• A mask-guided loss re-weighting strategy is proposed to mitigate the negative impact of noise on training. 2.