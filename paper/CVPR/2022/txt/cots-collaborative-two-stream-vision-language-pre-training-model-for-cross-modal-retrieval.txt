Abstract
Large-scale single-stream pre-training has shown dra-matic performance in image-text retrieval. Regrettably, it faces low inference efﬁciency due to heavy attention layers.
Recently, two-stream methods like CLIP and ALIGN with high inference efﬁciency have also shown promising per-formance, however, they only consider instance-level align-ment between the two streams (thus there is still room for improvement). To overcome these limitations, we propose a novel COllaborative Two-Stream vision-language pre-training model termed COTS for image-text retrieval by en-hancing cross-modal interaction. In addition to instance-level alignment via momentum contrastive learning, we leverage two extra levels of cross-modal interactions in our COTS: (1) Token-level interaction – a masked vision-language modeling (MVLM) learning objective is devised without using a cross-stream network module, where varia-tional autoencoder is imposed on the visual encoder to gen-erate visual tokens for each image. (2) Task-level interac-tion – a KL-alignment learning objective is devised between text-to-image and image-to-text retrieval tasks, where the probability distribution per task is computed with the nega-tive queues in momentum contrastive learning. Under a fair comparison setting, our COTS achieves the highest perfor-mance among all two-stream methods and comparable per-formance (but with 10,800× faster in inference) w.r.t. the latest single-stream methods. Importantly, our COTS is also applicable to text-to-video retrieval, yielding new state-of-the-art on the widely-used MSR-VTT dataset. 1.

Introduction
The pretrain-then-ﬁnetune paradigm has achieved great success in the ﬁeld of natural language processing (NLP), where models are ﬁrst pre-trained with large-scale data (e.g., BERT [10], RoBERTa [30], and GPT3 [5]) and then
ﬁnetuned for each downstream task. Recently, this prac-*The corresponding author. tice has also shown its effectiveness in the vision-language (VL) domain [9, 17–19, 28, 37, 52], where the performance on various VL tasks (e.g., image-text retrieval, video-text retrieval, and visual question answering) has been signif-icantly improved by vision-language pre-training (VLP).
VLP models typically take huge image-text pairs as in-put and aim to learn joint image-text representations with single- and cross-modal pre-training objectives, such as masked token prediction and image-text matching.
Existing VLP models can be divided into two groups: single-stream models and two-stream ones. Single-stream
VLP models (see Figure 1(a)) often utilize cross-modal fusion modules (e.g., Transformer [43] layers) to model
ﬁne-grained interactions between image regions and text words. Although these models achieve promising perfor-(1) During inference, mance, they have two limitations: all possible query-candidate pairs need to be fed into the fusion modules to calculate similarity scores, resulting in huge computational cost. (2) To obtain meaningful im-age regions, single-stream models typically adopt object detectors, which are expensive in both computation and data annotation. For example, extracting object regions from a 800×1,333 image takes about 900ms for Faster R-CNN [39], while ViT-base [11] only needs 15ms (i.e., 60× faster).
In contrast, two-stream VLP models [22, 44] ap-ply separate image and text encoders and match image-text pairs on the ﬁnal embedding level. Although two-stream models (see Figure 1(b)–(c)) are much more efﬁcient than single-stream ones, they only achieve sub-optimal results due to the lack of closer image-text interactions. Therefore, a few works [42, 46] (see Figure 1(b)) reconsider object de-tectors, and most recent ones (e.g., CLIP [37], ALIGN [18], and WenLan [17]) resort to extra large pre-training data crawled from the Internet. However, they still fail to model
ﬁne-grained interactions between the two modalities.
To address the inefﬁciency of single-stream VLP mod-els and the lack of closer vision-language interactions of two-stream ones, we propose a novel COllaborative Two-Stream vision-language pre-training model termed COTS
Figure 1. Four categories of vision-language pre-training (VLP) models. (a) Single-stream models (e.g., Oscar [28] and VinVL [52]). (b) Two-stream models with the object detector (e.g., LigntingDot [42]). (c) Two-stream models with instance-level interaction (e.g.,
CLIP [37] and ALIGN [18]). (d) COTS: our two-stream model with multi-level interactions. The inference time and time complexity of each module are also reported, and more details can be found in Section 4.2. for cross-modal retrieval, which retains the advantage of real-time inference speed and also enhances the interac-tions between the two modalities (see Figure 1(d)). Con-cretely, we consider three levels of cross-modal interactions in our COTS: (1) Instance-level interaction – an image-text matching learning objective at the ﬁnal embedding level (typically adopted by two-stream VLP models) is devised via momentum contrastive learning [15], where we main-tain two sample queues (one per modality) to have large size of negative samples. (2) Token-level interaction – a novel masked vision-language modeling (MVLM) learning objective is considered without using any cross-stream net-work module. To this end, we ﬁrst tokenize both the im-age and the text for each input image-text pair, where vari-ational autoencoder [21] is imposed on the visual encoder (e.g., ViT [11]) to generate visual tokens and BERT [10] is adopted for the text encoder. We then perform masked visual token prediction based on the unmasked visual to-kens and the feature of each image’s paired text, and per-form masked language token prediction similarly. (3) Task-level interaction – a novel KL-alignment learning objective is devised between text-to-image and image-to-text retrieval tasks by minimizing the Kullback-Leibler (KL) Divergence between probability distributions of the two retrieval tasks.
For each image-text pair, the probability distribution of the text-to-image retrieval task is obtained with the similarities of the chosen text and its unpaired images in the negative image queue maintained in momentum contrastive learning, and we can obtain the other distribution similarly.
As the scale of pre-training data becomes large (e.g., tens of millions or even billions of image-text pairs crawled from the Internet), it is impossible to perform human-annotation and thus there inevitably exist noises in the large-scale data.
Noisy data such as mis-matched image-text pairs and to-tally meaningless ones could bring negative effect for pre-training. In this paper, we thus propose an adaptive momen-tum ﬁlter (AMF) module for our COTS, which can make full use of the momentum mechanism in our contrastive learning-based training algorithm. Speciﬁcally, we ﬁrst cal-culate the similarity scores of all image-text pairs from the dynamically maintained image and text queues to obtain an extra queue. Further, we model this queue of similar-ity scores as a normal distribution and ﬁlter out the noisy data with the distribution mean and variance on the ﬂy.
Our contributions are summarized as follows: (1) We propose a novel COllaborative Two-Stream (COTS) VLP model to improve the performance of two-stream mod-els and retain their efﬁciency advantage at the same time.
We achieve this by leveraging two extra levels of cross-modal interactions in addition to the typical instance-level alignment: a masked vision-language modeling (MVLM) learning objective for token-level interaction, and a KL-alignment interaction. learning objective for task-level (2) To alleviate the negative effect caused by the noises in large-scale pre-training data, we propose an adaptive mo-mentum ﬁlter (AMF) module. AMF makes full use of the momentum mechanism in our instance-level alignment and adaptively ﬁlters noisy image-text pairs during pre-training. (3) Under a fair comparison setting, our COTS achieves the highest performance among all two-stream methods and performs comparably (but 10,800× faster in inference) with the latest single-stream ones. Importantly, our COTS is also applicable to text-to-video retrieval, yielding new state-of-the-art on the widely-used MSR-VTT dataset. 2.