Abstract 1.

Introduction
This paper presents a novel Region-Aware Face Swap-ping (RAFSwap) network to achieve identity-consistent har-monious high-resolution face generation in a local-global manner: 1) Local Facial Region-Aware (FRA) branch augments local identity-relevant features by introducing the Transformer to effectively model misaligned cross-2) Global Source Feature-scale semantic interaction.
Adaptive (SFA) branch further complements global identity-relevant cues for generating identity-consistent swapped faces. Besides, we propose a Face Mask Predictor (FMP) module incorporated with StyleGAN2 to predict identity-relevant soft facial masks in an unsupervised manner that is more practical for generating harmonious high-resolution faces. Abundant experiments qualitatively and quantita-tively demonstrate the superiority of our method for gen-erating more identity-consistent high-resolution swapped faces over SOTA methods, e.g., obtaining 96.70 ID retrieval that outperforms SOTA MegaFS by 5.87↑.
∗ Work done during an internship at Bytedance.
† Equal contribution.
‡ Corresponding Author.
Face swapping aims at transferring the identity of the source identity to the target identity while keeping the identity-irrelevant attributes of the target face unchanged, which has attracted widespread attention in the film in-dustry and computer games. Recently, many researchers have achieved significant progress in face swapping, espe-cially designing inversion-based methods to generate high-resolution face images. However, there are two continu-ously critical issues: 1) How to maintain identity consis-tency with the source identity, including local and global facial details. Almost all current methods [7, 19] per-form feature interaction only on global feature representa-tion without modeling identity-relevant local regions, e.g., lips, nose, brows, and eyes, which will limit the model’s ability to express identity consistency. 2) How to generate high-resolution swapped faces while keeping the identity-irrelevant details consistent with the target face under the
GAN inversion framework, e.g., background and occlu-sions. Recent works [38, 46] exploit the StyleGAN2 [17] as the powerful decoder but fail to maintain the consistency of the identity-irrelevant attributes of the target face. In this paper, we are dedicated to solving both the above problems.
Recent works [7, 18, 19, 34, 37] regard face swapping as a style transfer task that employs global AdaIN [14] to transfer the identity information of the source face into the target face. However, the identity vector produced by the face recognition network is naturally not well-disentangled, which inevitably includes some identity-irrelevant informa-tion of the source face, e.g., background, light distribution, and hairstyle. This wrong information will be further in-jected into the target feature in a global manner via AdaIN, resulting in low-quality generation results. As shown in the left part of Fig. 1, recent AdaIN-based methods cannot preserve the source identity well, in which generated faces contain excessive information of the source face in some challenging situations, e.g., bangs and white hair. To bet-ter preserve the identity consistency of the generated face, we explicitly model the local facial features besides global representation to perform feature interaction more finely, which also excludes the influence of the identity-irrelevant area of the source face at the same time. In this way, our method is well competent for the above challenges, as de-picted in the fourth column of Fig. 1. Specifically, we de-sign two parallel branches to process different fine-grained information: 1) local Facial Region-Aware (FRA) branch to model identity-relevant feature interaction between source and target faces, which employs a Region-Aware Identity
Tokenizer (RAT), transformer layers [31], and a Region-Aware Identity Projector (RAP) to realize misaligned cross-scale semantic interaction, i.e., lips, nose, brows, and eyes. 2) global Source Feature-Adaptive (SFA) branch to com-plement global identity-relevant cues, e.g., skin wrinkle, for more identity-consistent results. Details can be found in following Sec. 3.1 and 3.2.
To achieve high-resolution face generation for more practical application, we adopt GAN inversion frame-work [26,30] similar to recent face swapping works [38,46].
But these methods introduce a fatal problem of failing to preserve the background and occlusions, because vector-conditioned progressive generation will inevitably change identity-irrelevant regions. Recent MegaFS [46] blends the high-resolution result to the target face by the pre-existing face mask in a post-processing way, while HifiFace [34] learns to predict face masks in a supervised manner that restricts applications. These methods must rely on ground truth face masks and usually produce artifacts around facial contours, as shown in the right part of Fig 1. Differently, considering that pre-trained StyleGAN2 [17] encapsulate rich facial semantic prior, we design a Face Mask Predictor (FMP) to predict identity-relevant soft facial mask in an un-supervised manner, i.e., without using specific mask super-vision. In this way, our model achieves harmonious high-resolution face generation that keeps identity-irrelevant at-tributes consistent with the target face.
In summary, we make the following three contributions:
• We propose a novel Region-Aware Face Swapping (RAFSwap) network, which consists of a novel
FRA branch to augment local identity-relevant fea-tures by introducing the Transformer to effectively model misaligned cross-scale semantic interaction, and a novel SFA branch to further complement global identity-relevant cues for generating identity-consistent swapped faces.
• We propose a FMP module incorporated with Style-GAN2 to predict identity-relevant soft facial masks in an unsupervised manner that is more practical.
• Abundant experiments qualitatively and quantitatively demonstrate the superiority of our method for generat-ing more identity-consistent high-resolution swapped faces over SOTA methods. 2.