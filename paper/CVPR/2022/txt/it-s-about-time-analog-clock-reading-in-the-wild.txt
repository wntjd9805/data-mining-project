Abstract
In this paper, we present a framework for reading ana-log clocks in natural images or videos. Speciﬁcally, we make the following contributions: First, we create a scal-able pipeline for generating synthetic clocks, signiﬁcantly reducing the requirements for the labour-intensive annota-tions; Second, we introduce a clock recognition architec-ture based on spatial transformer networks (STN), which is trained end-to-end for clock alignment and recognition.
We show that the model trained on the proposed synthetic dataset generalises towards real clocks with good accuracy, advocating a Sim2Real training regime; Third, to further reduce the gap between simulation and real data, we lever-age the special property of “time”, i.e. uniformity, to gener-ate reliable pseudo-labels on real unlabelled clock videos, and show that training on these videos offers further im-provements while still requiring zero manual annotations.
Lastly, we introduce three benchmark datasets based on
COCO, Open Images, and The Clock movie, with full anno-tations for time, accurate to the minute. 1.

Introduction
Humans are able to sense the time to some level of gran-ularity given environmental cues, such as luminance or the extent of shadows. However, in order to know the exact time we read from a time-keeping instrument such as a clock or watch. Clocks come in different shapes, forms and styles, and humans are able to read them despite not having seen the particular clock before. In this paper our objective is to enable a machine to perform the same task of telling the time from clocks in the wild.
Nowadays, clocks come in two main types – digital and analog. Digital clocks can be handled by text spotting meth-ods [25, 27–29, 32] with relative ease, which we show in the Appendix, but reading analog clocks is a different and challenging problem: there are signiﬁcant appearance vari-ations between clock faces (see Figures 2 and 5), their im-aged shape and the position of the numbering is severely affected by camera viewpoint, and the presence of shadows and specular reﬂections add confusion with the clock hands.
While this problem has existed for a long time [1, 9, 35], no previous solutions are able to robustly read the time from clocks, apart from under extremely limited situations.
And, somewhat surprisingly, reading analog clocks in un-constrained images has been largely overlooked in the com-puter vision literature. Additionally, there are no reliable benchmarks for evaluation, hindering the research commu-nity from tackling this task.
However, there are similarities between analog clock reading and text spotting in natural scenes – since in both cases the design (of the clock face or text font) is chosen to be readable, and in both cases there is a detection stage and then a reading stage. Clock reading has the additional challenges outlined above, but it also has an additional re-dundancy cue in that the position of the hour hand gives some information about the position of the minute hand.
Given the similarities between the two tasks, we start with an approach that has been successful for text spotting: us-ing synthetic datasets [12, 18] and spatial transformer net-works [34], and ask if these ideas transfer to our task. We
ﬁnd that they do to an extent, and provide further contribu-tions to bridge the Sim2Real generalisation gap.
While being able to carry out a new task is a sufﬁcient reward in itself, there are a number of applications that are opened up, once we are able to automate time reading in images in the wild: ﬁrst, it will now be possible to offer corrections where the image’s EXIF metadata differs from the time read in the image; second, in video forensics, it will now be possible to spot if the video has been tampered with if the temporal ordering does not progress monotonically or if there is manipulation of the speed [15]; third, it provides a new method of searching, retrieving and grouping images and videos; and, ﬁnally, clocks are just a (rather difﬁcult) instance of an analog scale, and the methods we have pro-posed can be applied with a simple adaptation to other type of scales – from scientiﬁc instruments to industrial gauges.
In this paper, we provide the ﬁrst working solution to
these issues. We make the following contributions:
First, we propose a synthetic dataset generator, Syn-Clock, that is designed to generalize to real clocks. Syn-Clock has several controllable features that enables it to generate clocks with a wide range of designs. Moreover, we mimic difﬁculties faced in recognising real clocks into the generator’s data augmentation process, e.g. homography transformation, artefacts, shadows.
Second, we design a two-stage framework involving de-tection and recognition stages. The detection can simply be an off-the-shelf object detection model. The recogni-tion stage involves an alignment network, which is a spa-tial transformer network that regresses homography trans-formation parameters in order to make the clock fronto-parallel, and a classiﬁcation network, which determines the time accurate to the minute. We show that the model is able to generalise towards real clocks with good accuracy.
Third, we leverage the uniformity of time – that it ﬂows at a constant rate, in order to generate pseudo-labels on un-labelled clock videos. Speciﬁcally, we can be reasonably conﬁdent that the time labels in a video are correct if the rate of change of predicted time is constant throughout the video. To achieve this, we use a bundle adjustment algo-rithm to ﬁlter eligible videos and train on those with the pseudo labels. We also propose a dataset of 3,443 unla-belled clock time-lapse videos, and show that learning from pseudo-labelled real data improves the performance. We will release the raw videos, as well as reliable automatic annotations for 1.5M frames across 2,511 videos.
Fourth, we propose three new benchmark datasets. The
ﬁrst two are based on existing datasets for object detec-tion, namely COCO and OpenImages. We also introduce the Clock Movies dataset, based on the ﬁlm The Clock (2010), which is a 24-hour montage of different movies featuring clocks1. Our model achieves 80.4%, 77.3% and 79.0% top-1 accuracy on each dataset respectively, marking the ﬁrst time that analog clocks can be read successfully in unconstrained images. 2.