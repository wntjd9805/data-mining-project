Abstract
We propose a new method for reconstructing control-lable implicit 3D human models from sparse multi-view
RGB videos. Our method defines the neural scene repre-sentation on the mesh surface points and signed distances from the surface of a human body mesh. We identify an indistinguishability issue that arises when a point in 3D space is mapped to its nearest surface point on a mesh for learning surface-aligned neural scene representation. To address this issue, we propose projecting a point onto a mesh surface using a barycentric interpolation with modi-fied vertex normals. Experiments with the ZJU-MoCap and
Human3.6M datasets show that our approach achieves a higher quality in a novel-view and novel-pose synthesis than existing methods. We also demonstrate that our method eas-ily supports the control of body shape and clothes. Project page: https://pfnet- research.github.io/ surface-aligned-nerf/. 1.

Introduction
Human body modeling is a long-studied topic for its wide range of real-world applications.
In visual applica-tions such as movies or games, which often require free-viewpoint rendering, it is common to expect 3D human models to have controllable properties such as pose, shape, and clothes. Because manually designing high-quality 3D human models is usually labor-intensive, increasing stud-ies [1–3, 24, 27, 30, 35, 36] have proposed the reconstruction of 3D human models using only 2D observations. In this paper, we focus on the free-viewpoint 3D human synthesis with the above controllable properties from sparse multi-view RGB videos.
Early approaches [6, 42] deformed the pre-scanned tem-plate meshes with a skeleton for modeling a human shape and/or texture. Parametric 3D human models [4, 26, 34] have been proposed to reconstruct rough human meshes with pose and body shape estimation. Subsequent stud-ies [1–3, 27] introduced more features, such as per-vertex deformation or texture, to express richer details. Such
*Work done while the first author interned at Preferred Networks, Inc. parametric model-based approaches, despite having good controllable properties, show limitations in representing clothed humans, particularly when the actual shape differs significantly from the base parametric mesh estimation.
Neural radiance fields (NeRF) [28], a new form of 3D scene representation, has recently become the new base-line method in 3D reconstruction for its photorealistic ren-dering results of novel camera view. NeRF represents the scene as a continuous volumetric representation us-ing a neural network to regress the color and density at a given query point from a given view direction. Sev-eral approaches [24, 30, 35, 36] have been proposed to in-corporate knowledge from a statistical 3D human model and its pose estimation with NeRF. They differ in how a query point is transformed and represented. Deformation-based approaches [24, 35] use a deformation field to trans-form the query point from the observation space to a pose-independent canonical space and then build NeRF in the canonical space. Other approaches transform the query points into local coordinate systems [30] or to a latent code representation [36], with the help of human pose estimation.
In this paper, we propose a new approach for achieving a dynamic human reconstruction by combining parametric 3D body models such as SMPL [26] with NeRF. The ba-sic idea of our approach is straightforward and simple: we propose building a NeRF on the mesh surface. We devise an algorithm to map a query point to a mesh surface point with a signed height, which can represent the local position of the point with respect to the mesh. Using the informa-tion of the surface point position and the signed height of a query point as input, we build a surface-aligned NeRF that is aligned with the mesh surface; thus, it can be easily de-formed or controlled according to the base SMPL model.
Our approach has the following advantages: First, with the help of the devised mapping algorithm, our method does not rely on a learned deformation field, saving the number of learned parameters. Second, the models reconstructed using our method can be controlled directly by the SMPL parameters, that is, both poses and body shapes. Third, due to the surface-aligned property, our approach shows a better generalization ability for a novel human pose synthesis.
In summary, our contributions are as follows:
• We propose an algorithm that can injectively map a spatial point to a novel surface-aligned representation that consists of a projected surface point and a signed height to the mesh surface.
• We propose novel surface-aligned neural radiance fields using the proposed mapping, which can be eas-ily controlled using the SMPL parameters. Compared to existing methods, our approach shows a better gen-eralization performance on a novel view and novel pose synthesis while supporting manipulations such as changes to the body shape and clothes. 2.