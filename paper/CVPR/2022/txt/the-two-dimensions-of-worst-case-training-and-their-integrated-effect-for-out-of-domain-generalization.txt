Abstract
Training with an emphasis on “hard-to-learn” compo-nents of the data has been proven as an effective method to improve the generalization of machine learning models, especially in the settings where robustness (e.g., generaliza-tion across distributions) is valued. Existing literature dis-cussing this “hard-to-learn” concept are mainly expanded either along the dimension of the samples or the dimension of the features. In this paper, we aim to introduce a simple view merging these two dimensions, leading to a new, sim-ple yet effective, heuristic to train machine learning mod-els by emphasizing the worst-cases on both the sample and the feature dimensions. We name our method W2D follow-ing the concept of “Worst-case along Two Dimensions”.
We validate the idea and demonstrate its empirical strength over standard benchmarks. 1.

Introduction
The remarkable empirical performance of deep learning over i.i.d data, sometimes paralleling the human visual sys-tem [23, 37], has encouraged the community to challenge potentially more demanding scenarios where the models are trained with data from one or more distributions but tested with data from other distributions. We refer to this scenario as the out-of-distribution (OOD) generalization testing set-ting following the terminology used in [74].
In this OOD test scenario, deep learning techniques often underdeliver the promising results made with i.i.d data, as observed by multiple preceding works with different strate-gies to generate the test data, such as with salient patterns added to the data [17, 27], with carefully constructed im-perceptible noise perturbing the data (adversarial attacks)
[20, 60], or with additionally collected datasets that humans can nonetheless generalize to despite potentially significant disparities between the training and test distributions (e.g., domain adaptation/generalization) [6, 48].
⋆,† equal contribution
Figure 1. The conceptual illustration of our main idea W2D for a simple example of canary vs. goldfish image classification. For a regular model trained on standard images (left-bottom block), there are two dimensions of hard samples: following the catego-rization in [74], the vertical dimension corresponds to the “diver-sity shift” of the images (e.g., photos of the animals vs. cartoons of the animals) and the horizontal dimension corresponds to the
“correlation shift” of the images (e.g., birds in cage and fish in water vs. fish in cage and birds in water). The W2D algorithm conceptually selects the images that are hard at the sample dimen-sion and augment these samples toward being harder at the feature dimension.
Despite the variation in multiple OOD settings, the un-derlying reason leading to the performance drop may have a shared theme: the models’ incapability to learn what hu-mans will consider important in the data, as discussed pre-viously in empirical [70] and statistical [68] perspectives.
Thus, we conjecture that a key to training models that can perform consistently well in the OOD setting is to de-sign a new training heuristic that can better imitate the hu-In addition, we also hope the man’s learning behaviors. new heuristic is simple and general so that it can be directly plugged into and benefit existing methods across different architectures, optimizers, losses, or regularizations.
A psychological prior: In seeking the answer of how a human can learn most efficiently, we notice that a world-renowned psychologist (Dr. K. Anders Ericsson) has de-voted his life-long career decoding the habits of people with expert-level performances. His main conclusion [14] is that the high-end performances are the result of extensive prac-tices beyond one’s comfort zone.
Back to the discussion of machine learning, we analo-gize the beyond comfort zone elements of one’s daily life to the elements of the training data that are particularly hard to learn for a model. We notice this “element” can be in-terpreted with two perspectives: one interpretation is that a certain pattern across many images is hard to learn; and the other is that some specific images in the dataset are hard to learn.
Previous discussions devoted exclusively to either one of these two elements has been expanded extensively. For example, a line of methods have been invented to counter the model’s tendency to learn some simple patterns [4, 50, 65, 66], and another line of methods have been introduced to push the model to learn patterns represented by a small set of samples [38, 57]. However, there seems to be no discussion aiming to train the model to overcome the lim-itations raised from both of these perspectives, while do-ing so would intuitively improve the model’s performances, as well as align well with the psychological findings men-tioned above.
In this paper, inspired by the psychological prior above, we aim to introduce a simple training heuristic that will push the model to learn the hard-to-learn concept on both the feature dimension and the sample dimension. As our method is intuitively a combination of worst-case training at the feature level and worst-case training at the sample level, the new technique can serve as a simple heuristic to replace the existing training procedure of deep learning models re-gardless of model architecture, optimizer, loss, or regular-ization etc, as long as the optimization is within the gradient descent family. We name our method W2D following the concept of “Worst-case along Two Dimensions”.
The remainder of this paper is organized as follows. In
Section 2, we first introduce the background of this paper, with an emphasis on the “worst-case training” along the two dimensions, and their corresponding effects on OOD gen-eralization, which inspired us to investigate the integrated effect of these two worst-case training dimensions. In Sec-tion 3, we introduce our new heuristic that combines these two directions, and we demonstrate the method’s empirical strength in Sections 4 and 5. We offer several related dis-cussion in Section 6 before we conclude with Section 7. 2.