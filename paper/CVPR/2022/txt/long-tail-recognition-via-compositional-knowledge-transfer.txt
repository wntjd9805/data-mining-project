Abstract
In this work, we introduce a novel strategy for long-tail recognition that addresses the tail classes’ few-shot prob-lem via training-free knowledge transfer. Our objective is to transfer knowledge acquired from information-rich com-mon classes to semantically similar, and yet data-hungry, rare classes in order to obtain stronger tail class repre-sentations. We leverage the fact that class prototypes and learned cosine classifiers provide two different, comple-mentary representations of class cluster centres in feature space, and use an attention mechanism to select and re-compose learned classifier features from common classes to obtain higher quality rare class representations. Our knowledge transfer process is training free, reducing over-fitting risks, and can afford continual extension of classi-fiers to new classes. Experiments show that our approach can achieve significant performance boosts on rare classes while maintaining robust common class performance, out-performing directly comparable state-of-the-art models. 1.

Introduction
Standard classification models rely on the assumption that all classes of interest are equally represented in train-ing datasets. This strong assumption is rarely valid in prac-tice: most real life datasets exhibit long-tail distributions where a subset of classes comprise a large amount of train-ing data (the so-called common or head classes) and re-maining tail (rare) classes only possess a handful of training samples [17]. These distributions typically reflect what is observed in the wild, with data corresponding to tail classes being more challenging to learn due to sample rarity, ac-quisition and collation costs or combinations of these fac-tors. This results in trained models that display highly im-balanced accuracies, achieving strong performance on com-mon classes, and poor performance on rare classes.
Long-tail distributions present two main challenges for recognition models. Firstly, the imbalanced distribution of class specific data leads to models exceedingly biased to-Figure 1. Our approach for long-tail recognition seeks to improve representations for rare classes via knowledge transfer from com-mon classes. We identify semantically similar head classes using class prototypes and transfer reliable learned characteristics from these classes to obtain higher quality rare class representations. wards common classes. Numerous strategies have been developed to address this imbalance challenge. Popular strategies aim to rebalance the training process via bi-ased resampling [14, 17, 21, 33], rebalancing loss func-tions [2, 7, 12, 21, 25], or ensembling and routing [6, 29, 31].
While the class imbalance problem has been extensively studied, a second important challenge associated with long-tail distributions has received only limited attention. Rare classes on the distribution tail typically have very limited amounts of data available (e.g. 1 to 20 training samples per class), which can easily lead to overfitting or memorisa-tion when attempting to learn class specific decision bound-aries. This issue is exacerbated by rebalancing strategies, in particular those relying on oversampling data from the tail. However approaches dedicated to long-tail recogni-tion very rarely include specific mechanisms to account for the limited tail data. A synthetic sample generation pro-cess was proposed in [27] to improve rare class distribu-tions. A knowledge transfer mechanism was proposed in
[17], aiming to learn better feature representations by trans-ferring knowledge from learnable memory vectors, under the assumption that the model can automatically recognise which images require such transfer. The process, however, failed to consider a transfer mechanism for classifier learn-ing, which is where lack of data has the largest impact [14].
In contrast, few-shot learning (FSL) has been extensively studied in settings that rely on strong assumptions, mak-ing direct application of existing solutions to the long-tail
Indeed, FSL approaches are typi-problem challenging. cally developed so as to perform well on so-called episode benchmarks [20, 26], defined as a small group of sampled classes with a fixed, consistent number of training samples per class. These assumptions typically lead to poor gener-alisation both with regards to common classes and when a variable number of training samples are available [17].
Class prototypes are a popular concept in FSL. They are defined as class representations computed directly from the data, which is advantageous in limited data regimes due to the difficulty of learning reliable few-shot classifiers.
In-stead of seeking appropriate decision boundaries, prototyp-ical models seek to learn compact and well separated repre-sentations by minimising samples’ distance with respect to their class prototypes [24]. This can be achieved by explicit prototypes computation via artificially constructed episode batches [24], or implicitly, using cosine-distance based clas-sifiers [10, 19]. Learning distance based feature representa-tions encourages classes with similar semantic meanings to be close in feature space. This provides an advantageous solution to leverage class similarities for our long-tail task.
In this work, we introduce a novel classification strat-egy for long-tail recognition that addresses the few-shot problem via a training free knowledge transfer mechanism.
Our objective is to transfer knowledge, acquired from data-rich common classes, to semantically similar and yet data-hungry rare classes in order to obtain richer tail class repre-sentations. We provide an overview schematic of this idea in Figure 1. We leverage the fact that class prototypes and learned cosine classifiers provide two different representa-tions of class cluster centres in feature spaces, and use an attention mechanism to transfer and recompose learned fea-tures from classifiers to prototypes. Importantly, informa-tion is only transferred from common to rare classes, ad-dressing the limited reliability of classifiers trained in few shot settings and affording easy introduction of new, unseen classes in the classification process.
Our proposed approach is not tied to a specific model ar-chitecture or training strategy as it leverages a pre-trained model. As a result, we additionally provide a detailed anal-ysis of the impact of cosine classifiers on pre-trained back-bones, as well as the impact of sampling strategies on pro-totype quality. Based on this analysis, we propose train-ing recommendations to optimise the feature and classifier learning stage for better knowledge transfer.
Our contributions can be summarised as follows:
• A novel knowledge transfer mechanism that composes semantically similar, learned, common-class represen-tations to obtain richer and more discriminative tail-class classifiers. Performance gains of up to 5% on rare classes, while maintaining common class perfor-mance, without any additional training requirements.
• A flexible solution that affords continual adaptation to new, unseen classes; as well as the ability to construct classifiers tailored for specific class groups.
• An analysis of popular training strategies (sampling methods, classifier type) with recommendations to-wards optimisation of our transfer learning task. 2.