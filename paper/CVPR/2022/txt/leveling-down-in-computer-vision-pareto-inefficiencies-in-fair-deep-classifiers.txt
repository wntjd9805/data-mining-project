Abstract
Algorithmic fairness is frequently motivated in terms of a trade-off in which overall performance is decreased so as to improve performance on disadvantaged groups where the algorithm would otherwise be less accurate. Contrary to this, we find that applying existing fairness approaches to computer vision improve fairness by degrading the per-formance of classifiers across all groups (with increased degradation on the best performing groups).
Extending the bias-variance decomposition for classifi-cation to fairness, we theoretically explain why the major-ity of fairness methods designed for low capacity models should not be used in settings involving high-capacity mod-els, a scenario common to computer vision. We corrobo-rate this analysis with extensive experimental support that shows that many of the fairness heuristics used in computer vision also degrade performance on the most disadvantaged groups. Building on these insights, we propose an adaptive augmentation strategy that, uniquely, of all methods tested, improves performance for the disadvantaged groups. 1.

Introduction
High-capacity neural classifiers achieve state-of-the-art performance on most computer vision tasks when assessed by overall test set accuracy. However, researchers have be-gun examining the unfairness of these models. Here, we use ‘unfairness’ to refer to systematic accuracy differences across protected subgroups defined by human-sensitive at-tributes like gender and race [7, 8, 36].1These differences in accuracy can harm certain population groups, and as a re-sult, numerous strategies for training models that match var-ious measures of accuracy across subgroups have been de-veloped [2, 5, 33, 85]. Typically, such methods quantify un-fairness by comparing accuracy-related rates between vari-ous groups, for example the Difference of Equal Opportu-† Work done during an internship at AWS
∗ Corresponding author zietld@amazon.de nity (DEO) [36] compares groupwise true positive rates.
Many recent computer vision fairness studies are moti-vated by a fairness-accuracy trade-off originally targetting low-capacity models [1, 2, 15, 22, 47, 56, 85] where high ac-curacy on better performing (and often larger) groups comes at the cost of lower accuracy on the worse performing (and often smaller) groups. In this case, it is possible to increase fairness by reducing the accuracy on the best-performing group and increasing the accuracy on the worst-performing group, see models A and B in Figure 1.
We revisit this trade-off and show that it does not hold when using high-capacity neural classifiers prevalent in computer vision (see Figure 2).
Instead, many fairness methods degrade the accuracy of networks on all groups, with a greater degradation occurring for the better perform-ing groups. This increases fairness, but at the cost of pro-ducing a worse performing classifier (Figure 1 model C).
The phenomenon of balancing fairness by degrading the performance on the better off groups is referred to as lev-eling down in law and philosophy, where it has received substantial criticism [12, 19, 26, 41, 57]. The behavior we are concerned with is even more extreme than the typical levelling down – rather than just lower the performance on the best performing groups, every group is worse off.
If fairness methods decrease performance for all groups, they are Pareto Inefficient with respect to group accuracy and should not be used in contexts where the accuracy of any group is a primary concern.2 For example, we find that for classifiers on CelebA [52] regularized by a DEO fair-ness measure, the increased fairness comes at the cost of degraded performance for every group including the worst performing group (see Figure 2). We attribute this problem to two issues: 1In this work, we focus on fairness measures that compare accuracy across groups. These are in contrast to measures such as demographic par-ity [14, 28], which matches the proportion of positive decisions per group. 2Various works in fairness have made use of Pareto Efficiency. It has been used both to refer to trading-off global accuracy against notions of fairness [76], and for the notion of trading-off per group accuracies against each other [56]. We only refer to the second case.
Figure 1. Pareto curve. We depict the typical trade-off assumed by most computer vision fairness studies. The accuracies of a fam-ily of classifiers on its best- and worst-performing groups form a
Pareto curve (dotted gray line). Points A and B are maximally efficient configurations that lie on the curve; B is fairer and has a lower accuracy difference between the groups (see bar plot, right).
Point C is as fair as B, but is inefficient because it reduces the accu-racies of both groups. Applying accuracy-based fairness methods to deep networks tend to result in inefficient configurations like C.
Figure 2. Accuracy-fairness trade-off. We trained multiple fair models on CelebA. The protected attribute is “Male” (as anno-tated in the dataset).Different trade-offs are achieved by vary-ing the strength of a fairness regularizer added to the overall loss [61, 83]. In (I), we plot overall accuracy against the fairness measure DEO (low is fair). As expected, improvements in fairness come with a loss in accuracy. However, this comes at an additional cost; as we see in (II), the worst group accuracy also decreases as
DEO improves.
High-capacity classifiers fit training data nearly per-fectly: Most methods within the fairness community are designed for low-dimensional data where a classifier can-not fit data from multiple distinct distributions well, even on training data [1, 2, 15, 22, 36, 47, 56, 85]. This is not the case in computer vision, where high-dimensional data and high-capacity models mean that near-zero training error is common [87]. Fairness notions that are aligned with the perfect classifier are therefore trivially satisfied on training data. Unfortunately, most existing methods do not take this into account and enforce fairness constraints on the train-ing set [2, 10, 22, 24, 25, 27, 44, 47, 53, 56, 61, 82, 85].
Inappropriate evaluation of fairness methods: Most papers presenting fairness methods report a combination of accuracy and a fairness measure such as DEO, and take a decrease in accuracy and an improved fairness measure as an indication that the method is successfully trading off fair-ness against accuracy [2,65,66,81,82,85]. The crucial ques-tion of whether the learned models work better for more dis-advantaged groups remains unanswered. Consequently, the choice of metrics may mask a systematic deterioration of classifiers, where accuracy decreases across all groups, and not just in the high-accuracy groups.
To address these limitations, we make three contributions: 1. We revisit the bias-variance trade-off in a simple de-composition of fairness on test data into training er-ror (bias) and generalization error (variance), and observe that in situations where the training error goes to zero, any measure of fairness must be dominated by generalization error. As such, methods not using held-out data for fairness constraints cannot work for the high-capacity classifiers common to computer vision. 2. We perform an extensive evaluation of existing fair-ness methods and show that their reported improve-ment in fairness metrics is accompanied by worse per-formance across all groups. 3. To confirm our theoretical analysis that better gener-alization is key to improving performance on the most disadvantaged groups, we explore the use of data aug-mentation combined with adaptive sampling. We propose a novel GAN-based augmentation and show that it improves fairness by improving accuracy on the most disadvantaged groups on the CelebA dataset. 2.