Abstract
Recently vision transformer has achieved tremendous success on image-level visual recognition tasks. To effec-tively and efficiently model the crucial temporal informa-tion within a video clip, we propose a Temporally Efficient
Vision Transformer (TeViT) for video instance segmenta-tion (VIS). Different from previous transformer-based VIS methods, TeViT is nearly convolution-free, which contains a transformer backbone and a query-based video instance segmentation head. In the backbone stage, we propose a nearly parameter-free messenger shift mechanism for early temporal context fusion. In the head stages, we propose a parameter-shared spatiotemporal query interaction mecha-nism to build the one-to-one correspondence between video instances and queries. Thus, TeViT fully utilizes both frame-level and instance-level temporal context information and obtains strong temporal modeling capacity with negligible extra computational cost. On three widely adopted VIS benchmarks, i.e., YouTube-VIS-2019, YouTube-VIS-2021, and OVIS, TeViT obtains state-of-the-art results and main-tains high inference speed, e.g., 46.6 AP with 68.9 FPS on YouTube-VIS-2019. Code is available at https:// github.com/hustvl/TeViT. 1.

Introduction
Video Instance Segmentation (VIS) [62] is a representa-tive and challenging video understanding task that requires detecting, segmenting and tracking video instances across frames simultaneously.
Similar to other instance-level video recognition tasks, making full use of temporal con-text information is critical for building high-performance
VIS systems. Vision transformer (ViT) [13], which is based on self-attention [51], has shown strong long-range con-text modeling ability and obtained great successes on im-*This work was done while Shusheng Yang and Yu Li were at Applied
Research Center (ARC), Tencent PCG.
†Corresponding author, E-mail: xgwang@hust.edu.cn. age classification [13, 15, 21, 31, 47, 50, 55], object detec-tion [7, 33, 44, 68], semantic segmentation [11, 43, 59], in-stance segmentation [12, 17, 66], and video recognition [1, 4, 14, 32, 36, 65, 67].
Recently, how to design ViTs for instance-level video understanding, especially VIS, becomes an emerging prob-lem. Different from the detection transformers [7,16,33,44, 68], semantic segmentation transformers [11,43,59], and in-stance segmentation transformers [12, 17, 66], which focus on 2D contextual information modeling, VIS transformers additionally require to perform temporal context modeling.
To this end, VisTR [57] firstly proposes a transformer en-coder to fuse patch features from a sequence of frames using a CNN backbone and leverages a query-based decoder to predict video instances, IFC [22] introduces memory tokens to store frame-level features and performs cross-frame fea-ture interaction by computing self-attention among memory tokens, and then decodes instance-level results using a con-ditional mask head.
In this paper, we focus on the efficiency of model-ing temporal information for ViT-based VIS. This is a very important problem since (1) computing self-attention among all video patches has extremely high time and space complexity [57], (2) additional multi-head self attention (MHSA) layers for temporal modeling have extra param-eters and are sensitive to pre-training [22], (3) the CNN or transformer backbones in these methods [22,37,57,64] only support single frame feature extraction and fail to capture temporal information in the backbone stage. To remedy the above issues, we present Temporally Efficient ViT (TeViT) to fully utilize temporal contextual information for efficient and effective video instance segmentation.
TeViT contains a transformer backbone and a series of query-based VIS heads.
In the backbone stage, we use messenger tokens [15] to extract intra-frame information via self-attention and propose a messenger shift mechanism for frame-level context modeling, in which messenger to-kens are divided into several groups to perform temporal shift with various of time steps. Different from previous
VIS methods, the messenger shift transformer enables early temporal feature fusion.
In the head stages, we convert the QueryInst [17] instance segmentation head into our VIS head by reusing the multi-head self-attention (MHSA) [51] parameters for instance-level temporal information interac-tion. The instance-level MHSA fuses the features for a sin-gle video instance among input frames, thus it realizes the concept of a video instance as a query.
Experiments are conducted on three large-scale VIS i.e., YouTube-VIS-2019 [62], YouTube-VIS-datasets, 2021 [61], and OVIS [39]. New state-of-the-art (SoTA) per-formance has been obtained, e.g., TeViT obtains 46.6 AP with 68.9 FPS on YouTube-VIS-2019. Our main contribu-tions are summarized as follows.
• TeViT is the first video instance segmentation trans-former that can efficiently capture temporal contextual information at both frame level and instance level.
• Benefiting from the flexibility of self-attention, the proposed temporal modeling modules, i.e., messenger token shift and spatiotemporal query interaction, both are friendly to the image-level pre-trained models, cost marginal extra computation overhead and parameters.
• TeViT is a nearly convolution-free framework and ob-tains SoTA VIS results.
In TeViT, the concepts of
“early temporal feature fusion” and “a video instance as a query” shield lights on how to build effective video transformers for instance-level recognition tasks. 2.