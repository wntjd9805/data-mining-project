Abstract
+45(cid:28733)
-45(cid:28733) 0(cid:28733)
In-the-wild 3D face modelling is a challenging problem as the predicted facial geometry and texture suffer from a lack of reliable clues or priors, when the input images are degraded. To address such a problem, in this paper we pro-pose a novel Learning to Restore (L2R) 3D face framework for unsupervised high-quality face reconstruction from low-resolution images. Rather than directly reﬁning 2D image appearance, L2R learns to recover ﬁne-grained 3D details on the proxy against degradation via extracting generative facial priors. Concretely, L2R proposes a novel albedo restoration network to model high-quality 3D facial texture, in which the diverse guidance from the pre-trained Gener-ative Adversarial Networks (GANs) is leveraged to comple-ment the lack of input facial clues. With the ﬁner details of the restored 3D texture, L2R then learns displacement maps from scratch to enhance the signiﬁcant facial structure and geometry. Both of the procedures are mutually optimized with a novel 3D-aware adversarial loss, which further im-proves the modelling performance and suppresses the po-tential uncertainty. Extensive experiments on benchmarks show that L2R outperforms state-of-the-art methods under the condition of low-quality inputs, and obtains superior performances than 2D pre-processed modelling approaches with limited 3D proxy. 1.

Introduction 3D human face reconstruction has been rapidly advanced during these two decades with applications including hu-man digitalization, animation, and biometrics. The ﬁrst groundbreaking effort should be the 3D Morphable Model (3DMM) [8], which provides reasonable geometry assump-tions for modelling. Based on this, the reconstruction can
*Chengjie Wang and Zhifeng Xie are corresponding authors
GT
Input
Ours
Unsup3D
Unsup3D + DFDNet
Unsup3D
Unsup3D+DFDNet
LAP
LAP+DFDNet
Ours
LAP
LAP + DFDNet 0.7 0.75 0.8 0.85 0.9 0.95 (a) Visual Results of Different Methods (b) Cross-view Geometry Modelling Accuracy
Figure 1. Experimental analyses among non-parametric methods. (a) Visual comparison under degraded input. (b) Cross-view scale-invariant depth error on MICC [5] dataset, where we use the mod-els to predict geometry from the image of the current pose, and perform testing on images of other two poses. We use DFD-Net [37] to preprocess the degraded images. Unsup3D [58] and
LAP [67] suffer from the input degradation, and receive limited beneﬁt from the 2D appearance enhancing method. In contrast, our method models more detailed predictions, and shows more ro-bust performance in cross-view validation. Please refer to supple-mental materials for more details. be achieved through optimization on low-dimensional pa-rameters [46, 47]. With the development of deep learning, recent works utilize neural networks to regress 3DMM pa-rameters from 2D images [45, 70]. Although 3DMM based approaches are further improved with non-linearity [19, 25, 53–55,60,69] and multi-view consistency [6,10,52,57,61], they still suffer from several drawbacks: limited amount of subjects (e.g., BFM [44] with 200 subjects) with controlled conditions, difﬁculties on building skin details, anatomic grounded muscles [16] and large variations of identity [71].
As a result, efforts are made on non-parametric mod-elling for potential ﬂexibility, which regress face normal or depth directly from an input image without 3DMM as-sumption [3, 49]. More recent works [42, 58] disentan-gle a face into intrinsic factors and accomplishes canoni-cal reconstruction in an unsupervised manner via render-ing loss [34]. Although the non-parametric methods capture more detailed and distinct facial structures, they usually suf-fer from degradation of appearance as facial clues are only provided by input images without 3DMM prior. As shown in Fig. 1, degraded inputs signiﬁcantly reduce the recon-struction accuracy.
In practice, in-the-wild facial images often have low resolution and quality due to unsatisfactory equipment or a low proportion of the whole scene. On top of these, we argue that high-quality face modelling against degraded images is practical and crucial for non-parametric methods. To tackle this problem, a direct way is using pre-trained super-resolution models [37, 40] to process the de-graded images. However, these models only tackle 2D ap-pearance but fail to enhance inherent 3D clues. As illus-trated in Fig. 1, 2D pre-processing cannot well improve 3D reconstruction accuracy, showing unsatisfactory visual results and fragile performances on pose variation. While 3D texture completion methods [22, 68] inpaint the missing facial region, they cannot enhance the geometry.
In this paper, we propose a novel Learning to Restore (L2R) 3D face framework to improve 3D face modelling against limited image quality. L2R achieves such a goal by mining 2D facial priors from pretrained GANs for the propagation of 3D texture/geometry clues. The framework is conducted in a mutual paradigm to iteratively boost 3D texture and geometry modelling from a simple proxy. Con-cretely, to constrain the generated texture with suitable con-tent and 3D UV-position, L2R encodes input images and albedo proxy to StyleGAN [33] generator, providing style codes and spatial prior, respectively. In this way, L2R urges
StyleGAN to predict diverse clues on modelling realistic 3D albedo beyond degraded input. Further, beneﬁted from the 3D textures, L2R learns high-resolution facial shapes and displacement maps to enhance facial details without pre-deﬁned topology. As 3D texture and geometry modelling complement each other via rendering, we mutually opti-mize these two procedures with a novel 3D-aware adver-sarial loss, which enhances the consistency of prediction.
Extensive experiments demonstrate that L2R models supe-rior texture and geometry from low-resolution images than state-of-the-art and 2D pre-processed methods, and obtains competitive results to models without degradation.
In summary, this paper has contributions in followings: i) A novel Learning to Restore (L2R) 3D face framework is proposed to model high-quality 3D faces from degraded images in an unsupervised manner. In contrast to 2D ap-pearance processing methods, L2R is able to enhance in-herent 3D clues on texture and geometry reconstruction. ii) With a novel albedo restoration network, L2R mines 2D generative facial priors to complement the lack of facial clues and models 3D ﬁner textures. iii) Based on the restored 3D texture, L2R uses a novel geometry reﬁning network to model detailed facial depth
MOFA [54], DECA [19]
RingNet [48], MVF [57]
D3DFR [14], GANﬁt [25]
Cross-modal [3], DF2Net [63]
Unsup3D [58], LAP [67]
Ours
Non-parametric
×
×
× (cid:2) (cid:2) (cid:2)
Supervision
I, keypoint
I, keypoint
I, keypoint
I, 3D scan
I
I 3D Texture Degraded Input (cid:2)
× (cid:2)
× (cid:2) (cid:2)
×
× (cid:2)
×
× (cid:2)
Table 1. Comparison with selected recent methods on settings. I means 2D image. Most methods do not tackle the condition of degraded input. with a displacement map and enhances the 3D proxy. 2.