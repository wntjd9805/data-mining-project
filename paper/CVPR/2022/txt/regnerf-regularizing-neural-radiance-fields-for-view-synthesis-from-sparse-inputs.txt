Abstract
Neural Radiance Fields (NeRF) have emerged as a pow-erful representation for the task of novel view synthesis due to their simplicity and state-of-the-art performance.
Though NeRF can produce photorealistic renderings of un-seen viewpoints when many input views are available, its performance drops significantly when this number is re-duced. We observe that the majority of artifacts in sparse input scenarios are caused by errors in the estimated scene geometry, and by divergent behavior at the start of training.
We address this by regularizing the geometry and appear-ance of patches rendered from unobserved viewpoints, and annealing the ray sampling space during training. We ad-ditionally use a normalizing flow model to regularize the color of unobserved viewpoints. Our model outperforms not only other methods that optimize over a single scene, but in many cases also conditional models that are exten-sively pre-trained on large multi-view datasets. 1.

Introduction
Coordinate-based neural representations [7, 34, 35, 44] have gained increasing popularity in the field of 3D vi-sion.
In particular, Neural Radiance Fields (NeRF) [37] have emerged as a powerful representation for the task of novel view synthesis, where the goal is to render unseen viewpoints of a scene from a given set of input images.
Though NeRF achieves state-of-the-art performance, it requires dense coverage of the scene. However, in real-world applications such as AR/VR, autonomous driving, and robotics, the input is typically much sparser, with only few views of any particular object or region available per
*The work was primarily done during an internship at Google. (a) Sparse Set of 3 Input Images (b) Novel Views Synthesized by mip-NeRF [2] (c) Same Novel Views Synthesized by Our Method
Figure 1. View Synthesis from Sparse Inputs. While Neural
Radiance Fields (NeRF) allow for state-of-the-art view synthe-sis if many input images are provided, results degrade when only few views are available (1b). In contrast, even with sparse inputs our novel regularization and optimization strategy leads to 3D-consistent representations that render realistic novel views (1c). scene. In this sparse setting, the quality of NeRF’s rendered novel views drops significantly (see Fig. 1).
Several works have proposed conditional models to over-come these limitations [6, 8, 30, 56, 58, 62]. These models require expensive pre-training, i.e. training the model on large-scale datasets of many scenes with multi-view images and camera pose annotations, as opposed to test-time opti-mization which is done from scratch for a given test scene.
At test time, novel views can be generated from only a few input images through amortized inference, optionally com-bined with per scene test time fine-tuning. Though these models achieve promising results, obtaining the necessary
Figure 2. Overview. NeRF optimizes the reconstruction loss for a given set of input images (blue cameras). For sparse inputs, however, this leads to degenerate solutions. In this work, we propose to sample unobserved views (red cameras) and regularize the geometry and appearance of patches rendered from those views. More specifically, we cast rays through the scene and render patches from unobserved viewpoints for a given radiance field fθ. We then regularize appearance by feeding the predicted RGB patches through a trained normalizing flow model ϕ and maximizing predicted log-likelihood. We regularize geometry by enforcing a smoothness loss on the rendered depth patches. Our approach leads to 3D-consistent representations even for sparse inputs from which realistic novel views can be rendered. pre-training data by capturing or rendering many differ-ent scenes can be prohibitively expensive. Moreover, these techniques may not generalize well to novel domains at test time, and may exhibit blurry artifacts as a result of the in-herent ambiguity of sparse input data.
One alternate approach is to optimize the network weights from scratch for every new scene and introduce reg-ularization to improve the performance for sparse inputs, e.g., by adding extra supervision [24] or learning embed-dings representative of the input views [19]. However, ex-isting methods either heavily rely on external supervisory signals that might not always be available, or operate on low-resolution renderings of the scene that provide only high-level information.
In this paper, we present RegNeRF, a
Contribution: novel method for regularizing NeRF models for sparse in-put scenarios. Our main contributions are the following:
• A patch-based regularizer for depth maps rendered from unobserved viewpoints, which reduces floating artifacts and improves scene geometry.
• A normalizing flow model to regularize the colors pre-dicted at unseen viewpoints by maximizing the log-likelihood of the rendered patches and thereby avoid color shifts between different views.
• An annealing strategy for sampling points along the ray, where we first sample scene content within a small range before expanding to the full scene bounds which prevents divergence early during training. 2.