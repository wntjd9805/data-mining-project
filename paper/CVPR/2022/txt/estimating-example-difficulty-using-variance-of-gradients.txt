Abstract
In machine learning, a question of great interest is un-derstanding what examples are challenging for a model to classify. Identifying atypical examples ensures the safe de-ployment of models, isolates samples that require further human inspection and provides interpretability into model behavior. In this work, we propose Variance of Gradients (VoG) as a valuable and efficient metric to rank data by difficulty and to surface a tractable subset of the most chal-lenging examples for human-in-the-loop auditing. We show that data points with high VoG scores are far more difficult for the model to learn and over-index on corrupted or mem-orized examples. Further, restricting the evaluation to the test set instances with the lowest VoG improves the model’s generalization performance. Finally, we show that VoG is a valuable and efficient ranking for out-of-distribution detec-tion. 1.

Introduction
Over the past decade, machine learning models are increasingly deployed to high-stake decision applications such as healthcare [4, 20, 52, 70], self-driving cars [51] and finance [53]. For gaining trust from stakeholders and model practitioners, it is important for deep neural networks (DNNs) to make decisions that are interpretable to both re-searchers and end-users. To this end, for sensitive domains, there is an urgent need for auditing tools which are scalable and help domain experts audit models.
Reasoning about model behavior is often easier when presented with a subset of data points that are relatively more difficult for a model to learn. Besides aiding inter-pretability through case-based reasoning [11, 30, 39], it can also be used to surface a tractable subset of atypical exam-ples for further human auditing [46, 73], for active learning to inform model improvements, and to choose not to clas-sify some instances when the model is uncertain [7, 14, 21].
One of the biggest bottlenecks for human auditing is the large scale of modern datasets and the cost of annotating in-dividual features [3, 38, 68]. Methods which automatically surface a subset of relatively more challenging examples for human inspection help prioritize limited human annotation and auditing time. Despite the urgency of this use-case, ranking examples by difficulty has had limited treatment in the context of deep neural networks due to the computa-tional cost of ranking a high dimensional feature space.
Present work. A popular interpretability tool is saliency maps, where each of the features of the input data are scored based on their contribution to the final output [64]. How-ever, these explanations are typically for a single predic-tion and generated after the model is trained. Our goal is to leverage these explanations to automatically surface a subset of relatively more challenging examples for human inspection to help prioritize limited human annotation and auditing time. To this end, we propose a ranking method across all examples that instead measures the per-example change in explanations over training. Examples that are dif-ficult for a model to learn will exhibit higher variance in gradient updates throughout training. On the other hand, the backpropagated gradients of the samples that are relatively easier will exhibit lower variance because the loss from these examples does not consistently dominate the model training.
We term this class normalized ranking mechanism Vari-ance of Gradients (VoG) and demonstrate that VoG is a meaningful way for ranking data by difficulty and surfac-ing a tractable subset of the most challenging examples for human-in-the-loop auditing across a variety of large-scale datasets. VoG assigns higher scores to test set examples that are more challenging for the model to classify and proves to be an efficient tool for detecting out-of-distribution (OoD) samples. VoG is model and domain-agnostic as all that is required is the backpropagated gradients from the model.
Contributions. We demonstrate consistent results across two architectures and three datasets – Cifar-10, Cifar-100
[43] and ImageNet [61]. Our contributions can be enumer-ated as follows: 1. We present Variance of Gradients (VoG) – a class-normalized gradient variance score for determining the relative ease of learning data samples within a given
class (Sec. 2). VoG identifies clusters of images with clearly distinct semantic properties, where images with low VoG scores feature far less cluttered backgrounds and more prototypical vantage points of the object
In contrast, images with high VoG scores (Fig. 4). over-index on images with cluttered backgrounds and atypical vantage points of the object of interest. 2. VoG effectively surfaces memorized examples, i.e. it allocates higher scores to images that require memo-rization (Sec. 4). Further, VoG aids in understanding the model behavior at different training stages and pro-vides insight into the learning cycle of the model. 3. We show the reliability of VoG as an OoD detection technique and compare its performance to 9 existing
OoD methods, where it outperforms several methods, such as PCA [24] and KDE [15, 54]. VoG presents an overall improvement of 9.26% in precision compared to all other methods. 2. VoG Framework
We consider a supervised classification problem where a DNN is trained to approximate the function F that maps an input variable X to an output variable Y, formally F :
X (cid:55)→ Y, where Y is a discrete label vector associated with each input X and y ∈ Y corresponds to one of C categories or classes in the dataset.
A given input image X can be decomposed into a set of pixels xi, where i = {1, . . . , N } and N is the total num-ber of pixels in the image. For a given image, we compute the gradient of the activation Al p with respect to each pixel xi, where l designates the pre-softmax layer of the network and p is the index of either the true or predicted class prob-ability. We would like to note that the pre-softmax layer is responsible for connecting activations from previous layers in the network to individual class scores. Hence, comput-ing the gradients w.r.t. this class indexed score measures the contribution of features to the final class prediction [64].
Note our goal is to rank examples, so for each example, we compute the pre-softmax activation gradient indexed at predicted/true label with respect to the input. This is far more computationally efficient than computing the full Ja-cobian matrix with individual layers.
Let S be a matrix that represents the gradient of Al p with respect to individual pixels xi, i.e. for an image of size 3 × 32 × 32, the gradient matrix S will be of dimensions 3 × 32 × 32.
S =
∂Al p
∂xi (1)
This formulation may feel familiar as it is often computed based upon the weights of a trained model and visualized as a image heatmap for interpretability purposes [5, 31, 63, 64, 64–66]. In contrast to saliency maps which are inherently local explanation tools, we are leveraging relative changes in gradients across training to rank all examples globally.
Following several seminal papers in explainability lit-erature [31, 63–66], we take the average over the color channels to arrive at a gradient matrix [63–66] where S ∈
R32×32. For a given set of K checkpoints, we generate the above gradient matrix S for all individual checkpoints, i.e.,
{S1, . . . , SK}. We then calculate the mean gradient µ by taking the average of the K gradient matrices. Note, µ is the mean across different checkpoints and is of the same size as the gradient matrix S. We then calculate the vari-ance of gradients across each pixel as:
µ = 1
K
K (cid:88) t=1
St.
VoGp = (cid:114) 1
K
K (cid:88) t=1 (St − µ)2. (2) (3)
We average the pixel-wise variance of gradients to compute a scalar VoG score for the given input image:
VoG = 1
N
N (cid:88) (VoGp), t=1 (4) where N is the total number of pixels in a given image. First calculating the pixel-wise variance (Eqn. 3) and then aver-age over the pixels (Eqn. 4) is consistent with previous XAI works where the gradients of an input image are computed independently for each pixel in an image [64–66].
In order to account for inherent differences in variance between classes, we normalize the absolute VoG score by class-level VoG mean and standard deviation. This amounts to asking: What is the variance of gradients for a given image with respect to all other exemplars of this class category? 2.1. Validating the behavior of VoG on synthetic data
In Fig. 1a, we illustrate the principle and effectiveness of VoG in a controlled toy example setting. The data was generated using two separate isotropic Gaussian clusters.
In such a simple low dimensional problem, the most chal-lenging examples for the model to classify can be quanti-fied by distance to the decision boundary. In Fig. 1a, we visualize the trained decision boundary of a multiple layer perceptron (MLP) with a single hidden layer trained for 15 epochs. We compute VoG for each training data point and plot final VoG score for each point against the distance to the trained boundary. In Fig. 1b, we can see that VoG suc-cessfully ranks highest the examples closest to the decision
(a) Toy dataset trained decision boundary (b) Distance vs. VoG score
Figure 1. Left: Variance of Gradients (VoG) for each testing data point in the two-dimensional toy problem. Right: VoG accords higher scores to the most challenging examples closest to the decision boundary (as measured by the perpendicular distance).
LOWEST VOG
HIGHEST VOG
LOWEST VOG
HIGHEST VOG e l p p a
; 0 0 1
-R
A
F
I
C e n a p l
; 0 1
-R
A
F
I
C (a) Early-stage training (b) Late-stage training
Figure 2. The 5×5 grid shows the top-25 Cifar-10 and Cifar-100 training-set images with the lowest and highest VoG scores in the Early (a) and Late (b) training stage respectively of two randomly chosen classes. Lower VoG images evidence uncluttered backgrounds (for both apple and plane) in the Late training stage. VoG also appears to capture a color bias present during the Early training stage for both apple (red). The VoG images in Late training stage present unusual vantage points, with images where the frame is zoomed in on the object of interest. boundary. The most challenging examples exhibit the great-est variance in gradient updates over the course of the train-ing process. In the following sections, we will scale this toy problem and show consistent results across multiple archi-tectures and datasets. 2.2. Experimental Setup
Datasets. We evaluate our methodology on Cifar-10 and
Cifar-100 [43], and ImageNet [61] datasets. For all datasets, we compute VoG for both training and test sets.
Cifar Training. We use a ResNet-18 network [25] for both Cifar-10 and Cifar-100. For each dataset, we train the model for 350 epochs using stochastic gradient descent (SGD) and compute the input gradients for each sample ev-ery 10 epochs. We implemented standard data augmenta-tion by applying cropping and horizontal flips of input im-ages. We use a base learning rate schedule of 0.1 and adap-tively change to 0.01 at 150th and 0.001 at 250th training epochs. The top-1 test set accuracy for Cifar-10 and Cifar-100 were 89.57% and 66.86% respectively.
ImageNet Training. We use a ResNet-50 [25] model for training on ImageNet. The network was trained with batch normalization [35], weight decay, decreasing learn-ing rate schedules, and augmented training data. We train for 32, 000 steps (approximately 90 epochs) on ImageNet with a batch size of 1024. We store 32 checkpoints over the course of training, but in practice observe that VoG rank-ing is very stable computed with as few as 3 checkpoints.
Our model achieves a top-1 accuracy of 76.68% and top-5 accuracy of 93.29%.
Number of checkpoints. The number of checkpoints used to compute VoG balances efficiency for practitioners to use with the robustness of ranking. This can be set by the prac-titioner, and we note that in practice the last 3 checkpoints are sufficient for a robust VoG ranking (minimal difference when restricting to the last 3 in Figs. 5b,8b,11b vs. eval-uating on all checkpoints in Fig. 4). For all experiments,
VoG(early-stage) is computed using checkpoints from the first 3 epochs and VoG(late-stage) is computed using check-points from the last 3 epochs. The test set accuracy at the early-stage is 44.65%, 14.16%, and 51.87% for Cifar-10,
Cifar-100, and ImageNet, respectively. In the late-stage it is 89.57%, 66.86%, and 76.68% for Cifar-10, Cifar-100, and
ImageNet, respectively. 3. Utility of VoG as an Auditing Tool
In this section, we evaluate the merits of VoG as an au-diting tool. Specifically, we (1) present the qualitative prop-erties of images at both ends of the VoG spectrum, (2) mea-sure how discriminative VoG is at separating easy examples from difficult, (3) quantify the stability of the VoG ranking, (4) use VoG as an auditing tool for test dataset, and (5) lever-age VoG to understand the training dynamics of a DNN. 1) Qualitative inspection of ranking. A qualitative inspec-tion of examples with high and low VoG scores shows that there are distinct semantic properties to the images at either end of the ranking. We visualize 25 images ranked lowest and highest according to VoG for both the entire dataset (vi-sualized for ImageNet in Fig. 7) and for specific classes (vi-sualized for ImageNet in Fig. 3 and for Cifar-10 and Cifar-Images with low VoG score tend to have 100 in Fig. 2). uncluttered and often white backgrounds with the object of interest centered clearly in the frame. Images with the high
VoG scores have cluttered backgrounds and the object of in-terest is not easily distinguishable from the background. We also note that images with high VoG scores tend to feature atypical vantage points of the objects such as highly zoomed frames, side profiles of the object or shots taken from above.
Often, the object of interest is partially occluded or there are image corruptions present such as heavy blur. 2) Test set error and VoG. A valuable property of an au-diting tool is to effectively discriminate between easy and challenging examples. In Fig. 4, we plot the test set error of examples bucketed by VoG decile. Note that we plot error, so lower is better. We show that examples at the low-est percentiles of VoG have low error rates, and misclassi-fication increases with an increase in VoG scores. Our re-sults are consistent across all datasets, yet the trend is more pronounced for more complex datasets such as Cifar-100 and ImageNet. We ascribe this to differences in underlying model complexity. Furthermore, in Fig. 10, we observe that test set error on the lowest VoG scored images are lower than the baseline test set performance. 3) Stability of VoG ranking. To build trust with an end-user, a key desirable property of any auditing tool is consis-tency in performance. We would expect a consistent method to produce a ranking with a closely bounded distribution of scores across independently trained runs for a given model and dataset. To measure the consistency of the VoG ranking, we train five Cifar-10 networks from random initialization following the training methodology described in Sec. 2.2.
Empirically, Fig. 6 shows that VoG rankings evidence a con-sistent distribution of test-error at each percentile given the same model and dataset. For completeness, we also mea-sure instance-wise VoG stability by computing the standard deviation of VoG scores for 50k Cifar-10 samples across 10 independent initializations. The standard deviation of the
VoG scores is negligible with a mean deviation of 3.81e−9 across all samples. In addition, we find similar results for
Cifar-100 dataset where the output VoG scores are stable (mean std of 9.6e−6) across different model initializations.
Finally, we extend our stability experiments to understand the effect of different training hyperparameter settings (e.g., batch size) on the VoG scores. Here, we train 5 Cifar-10 models using different batch sizes, i.e., {128, 256, 384, 512, 640}, and find that the mean VoG standard deviation across 50k Cifar-10 samples was 1.9e−5. 4) VoG as an unsupervised auditing tool. Many auditing tools used to evaluate and understand possible model bias require the presence of labels for protected attributes and underlying variables. However, this is highly infeasible in real-world settings [68]. For image and language datasets, the high dimensionality of the problem makes it hard to
LOWEST VOG
HIGHEST VOG
LOWEST VOG
HIGHEST VOG
M A G P I E
P O P B O T T L E
Figure 3. Each 5×5 grid shows the top-25 ImageNet training-set images with the lowest and highest VoG scores for the class magpie and pop bottle. Training set images with higher VoG scores tend to feature zoomed-in images with atypical color schemes and vantage points. (a) Cifar-10 (b) Cifar-100 (c) ImageNet
Figure 4. The mean top-1 test set error (y-axis) for the examples thresholded by VoG score percentile (x-axis). Across Cifar-10, Cifar-100 and ImageNet, mis-classification increases with an increase in VoG scores. Across all datasets the group of samples in the top-10 percentile VoG scores have the highest error rate, i.e. contains most number of misclassified samples. identify a priori what underlying variables one needs to be aware of. Even acquiring the labels for a limited number of attributes protected by law (gender, race) is expensive and/or may be perceived as intrusive, leading to noisy or in-complete labels [2, 29]. This means that ranking techniques which do not require labels at test time are very valuable.
One key advantage of VoG is that we show it continues to produce a reliable ranking even when the gradients are com-puted w.r.t. the predicted label. In Fig. 7, we include the top and bottom 25 VoG ImageNet test images using predicted labels from the model. Finally, we also computed the mean test-error for the predicted VoG distribution, and find that it also effectively discriminates between top-10 and bottom-10 examples, respectively (Fig. 12a). 5) VoG understands early and late training dynamics.
Recent works have shown that there are distinct stages to training in deep neural networks [1, 17, 36, 49]. To this end, we investigate whether VoG rankings are sensitive to the stage of the training process. We compute VoG sepa-rately for two different stages of the training process: (i) the Early-stage (first three epochs) and (ii) the Late-stage (last three epochs). We plot VoG scores against the test set error at each decile in early- and late-stage and find a flipping behavior across all datasets and networks (Fig. 5 for ImageNet, Fig. 8 for Cifar-100, and Fig. 11 for Cifar-10).
In the early training stage, samples having higher
VoG scores have a lower average error rate as the gradi-ent updates hinge on easy examples. This phenomenon re-verses during the late-stage of the training, where, across all datasets, high VoG scores in the late-stage have the highest error rates as updates to the challenging examples dominate the computation of variance. Further, we note a noticeable visual difference between the image ranking computed for early- and late-stages of training. As seen in Fig. 2, for some classes such as apple, it appears that VoG scores also
(a) Early-stage training (b) Late-stage training
Figure 5. The mean top-1 test set error (y-axis) for the examples thresholded by VoG score percentile (x-axis) in ImageNet validation set. The Early (a) and Late (b) stage VoG analysis shows inverse behavior where the role of VoG flips as the training progresses. the model and (2) detecting out-of-distribution examples. 4.1. Surfacing examples that require memorization
Overparameterized networks have been shown to achieve zero training error by memorizing examples [19,32, 72]. We explore whether VoG can distinguish between ex-amples that require memorization and the rest of the dataset.
To do this, we replicate the general experiment setup of
Zhang et al. [72] and replace 20% of all labels in the train-ing set with randomly shuffled labels. We re-train the model from random initialization and compute VoG scores across training for all examples in the training set. Our network achieves 0% training error which would only be possible given successful memorization of the noisy examples with shuffled labels. We now answer the question: Is VoG able to discriminate between these memorized examples and the rest of the dataset?
We perform a two-sample t-test with unequal variances
[69] and show that this difference is statistically significant at a p-value of 0.001, i.e. shuffled labels have a different
VoG distribution than the non-shuffled dataset. Intuitively, the two-sample t-test produces a p-value that can be used to decide whether there is evidence of a significant differ-ence between the two distributions of VoG scores. The p-value represents the probability that the difference between the sample means is large, i.e. the smaller the p-value, the stronger is the evidence that the two populations have dif-ferent means. For both Cifar-10 and Cifar-100, we find a statistically significant difference in VoG scores for each population (p-value is < 0.001), which shows that VoG is discriminative at distinguishing between memorized and non-memorized examples. We include more details about the statistical testing in Sec. C.
Figure 6. The VoG top-1 test set error for five ResNet-18 networks independently trained on Cifar-10 from random initialization. The plot shows that VoG produces a stable ranking with a similar distribution of error in each percentile across all images capture the network’s color bias during the early training stage, where images with the lowest VoG scores over-index on red-colored apples. 4. Relationship between VoG Scores and Mem-orized/OoD Examples
Recent works have highlighted that DNNs produce un-calibrated output probabilities that cannot be interpreted as a measure of certainty [22, 26, 37, 44]. To this end, we ar-gue that if VoG is a reliable auditing tool, it should cap-ture model uncertainty even when it’s not reflected in the output probabilities. We consider VoG rankings on a task where the network produces highly confident predictions for incorrect/out-of-distribution inputs and evaluate VoG on two separate tasks: (1) identifying examples memorized by
(a) Lowest VoG (b) Highest VoG
Figure 7. Each 5×5 grid shows the top-25 ImageNet test set images with the lowest and highest VoG scores for the top-1 predicted class. Test set images with higher VoG scores tend to feature zoomed-in images and are misclassified more as compared to the lower VoG images which tend to feature more prototypical vantage points of objects. (a) Early-stage training (b) Late-stage training
Figure 8. The mean top-1 test set error (y-axis) for the exemplars thresholded by VoG score percentile (x-axis) in Cifar-100 testing set. The early (a) and late (b) stage VoG analysis shows inverse behavior where the role of VoG flips as the training progresses. Results for Cifar-10 are shown in Appendix Fig. 11. 4.2. Out-of-Distribution detection
We have already established that VoG is very effective at distinguishing between easy and challenging examples (Fig. 10). Here, we ask whether this makes VoG an effective out of distribution (OoD) detection tool. It also gives us a setting in which to compare VoG as a ranking mechanism to other methods
Ruff et al. [59] benchmark a variety of OoD detection techniques on MNIST-C [50]. For completeness, we repli-cate this precise setup by using a trained LeNet model and evaluate VoG on MNIST-C against 9 other methods
[12, 41, 45, 56–58, 60, 62, 67].
Evaluation metrics. We evaluate OoD detection perfor-mance using the following metrics: i) AUROC. The Area Under the Receiver Operator Charac-teristic (AUROC) curve can be interpreted as the probability that a positive example is assigned a higher detection score than a negative example [18]. ii) AUPR (In). The Area Under the Precision Recall (AUPR) curve computes the precision-recall pairs for differ-ent probability thresholds by considering the in-distribution examples as the positive class. iii) AUPR (Out). AUPR (Out) is AUPR as described above, but calculated considering the OoD examples as the positive class. We treat this outlier class as positive by multiplying the VoG scores by −1 and labelling them positive when cal-culating AUPR (Out).
Table 1. Comparison of VoG to 9 existing OoD detection methods. Shown are average values of metrics and standard deviations across 15 corruptions in the MNIST-C datasets.
Arrows (↑) indicate the direction of better metric perfor-mance. VoG outperforms most baselines by a large margin.
OoD methods AUROC (↑) AUPR OUT (↑)
KDE [57]
MVE [58]
DOCC [60] kPCA [12]
SVDD [67]
PCA [56]
Gaussian [45]
VoG
AE [41]
AEGAN [62] 57.46±32.09 62.84±21.92 69.16±28.35 72.12±31.00. 74.01±21.39 77.71±30.90 80.57±29.71 85.42±10.28. 89.89±18.52 95.93±7.90. 62.56±24.16 61.42±19.1 70.37±23.25 75.39±26.37 73.33±21.98 80.86±25.2 84.51±22.62 84.96±9.61 89.99±18.19 95.40±9.46
Findings.
In Table 1, we observe that VoG outperforms all methods except AutoEncoders (AE) and AutoEncoder
GAN (AEGAN). In stark contrast to VoG, AE and AEGAN require complex training of auxiliary models and do not fea-sibly scale beyond small-scale datasets like MNIST. Given these limitations, VoG remains a valuable and scalable OoD detection method as it can be used for large-scale datasets (e.g. ImageNet) and networks (e.g. ResNet-50). Unlike gen-erative models, VoG does not require an uncorrupted train-ing dataset for learning image distributions. Further, VoG only leverages data from training itself, is computed from checkpoints already stored over the course of training, and does not require the true label to rank. 5.