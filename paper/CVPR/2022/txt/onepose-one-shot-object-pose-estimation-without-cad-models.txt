Abstract
We propose a new method named OnePose for object pose estimation. Unlike existing instance-level or category-level methods, OnePose does not rely on CAD models and can handle objects in arbitrary categories without instance-or category-specific network training. OnePose draws the idea from visual localization and only requires a simple
RGB video scan of the object to build a sparse SfM model of the object. Then, this model is registered to new query images with a generic feature matching network.
To mitigate the slow runtime of existing visual localization methods, we propose a new graph attention network that directly matches 2D interest points in the query image with the 3D points in the SfM model, resulting in efficient and robust pose estimation. Combined with a feature-based pose tracker, OnePose is able to stably detect and track 6D poses of everyday household objects in real-time. We also collected a large-scale dataset that consists of 450 se-quences of 150 objects. Code and data are available at the project page: https://zju3dv.github.io/onepose/. 1.

Introduction
Object pose estimation plays an important role in aug-mented reality (AR). The ultimate goal of object pose es-timation in AR is to use arbitrary objects as “virtual an-chors” of AR effects, which demands the ability to estimate poses of surrounding objects in our daily life. Most estab-lished works in object pose estimation [16, 26, 46] assume that the CAD model of the object is known a priori. Since high-quality CAD models of everyday objects are often in-accessible, the research on object pose estimation for AR scenarios necessitates new problem settings.
To not rely on instance-level CAD models, many recent methods have been working on category-level pose estima-tion [4, 43]. By training a network on different instances in
∗Equal contribution. The authors from Zhejiang University are affil-iated with the State Key Lab of CAD&CG and the ZJU-SenseTime Joint
Lab of 3D Vision. †Corresponding author: Xiaowei Zhou.
Comparison of different problem settings of
Figure 1. instance/category-level object pose estimation and the one-shot pose estimation proposed in this work. Unlike previous works that rely on instance- or category-specific network training, the pro-posed approach only requires a simple video scan of the object to build a sparse SfM model of the object and uses a generic 3D-2D feature matching network (GATs) to estimate its pose, without
CAD models or additional network training. the same category, the network can learn a category-level representation of object appearances and shapes and thus be able to generalize to new instances in the same cate-gory. However, such approaches require a large number of training samples in the same category, which can be hard to obtain and annotate. Furthermore, the generalization capa-bilities of category-level methods are not guaranteed when a new instance has a significantly different appearance or shape. More importantly, training and deploying a network for each category are unaffordable in many real world ap-plications, e.g., mobile AR, when the number of object cat-egories to be handled is huge.
To alleviate the demand for CAD models or category-specific training, we go back to an “old” problem setting for object pose estimation, but renovate the entire pipeline with a new learning-based approach. Similar to the task of visual localization, which estimates the unknown cam-era pose given an SfM map of a scene, object pose esti-mation has long been formulated in the localization-based setting [21, 35]. Different from instance- or category-level
methods, this setting assumes that a video sequence of the object is given, and a sparse point cloud model can be re-constructed from the sequence. Estimating the object pose is then equivalent to localizing the camera pose with re-spect to the reconstructed point cloud model. At test time, 2D local features are extracted from the query image and matched with the points in the SfM model to obtain 2D-3D correspondences, from which the object pose can be solved by PnP. Instead of learning instance- or category-specific representations by neural networks, this traditional pipeline leverages an explicit 3D model of the object that can be built on-the-fly for a new instance, which brings better gen-eralization capabilities to arbitrary objects while making the system more explainable.
In this paper, we refer to this problem setting as one-shot object pose estimation, where the objective is being able to estimate 6D pose of an object in arbitrary category, given only a few pose-annotated images of the object for training. While this problem is similar to visual localiza-tion, directly migrating existing visual localization methods does not solve this problem. The modern visual localization pipeline [31] produces 2D-3D correspondences by first per-forming a 2D-2D matching between the query image and the retrieved database images. To ensure a high success rate of localization, matching to multiple image retrieval candi-dates is necessary, so that the 2D-2D matching can be ex-pensive especially for learning-based matchers [32, 36]. As a result, the runtime of existing visual localization methods is often seconds and cannot satisfy the requirement to track moving objects in real-time.
For the reasons above, we propose to directly perform 2D-3D matching between the query image and the SfM point clouds. Our key idea is to use graph attention net-works (GATs) [40] to aggregate the 2D features that cor-respond to the same 3D SfM point (i.e., a feature track) to form a 3D feature. The aggregated 3D features are later matched with 2D features in the query images with self- and cross-attention layers. Together with the self- and cross-attention layers, the GATs can capture the globally-consented and context-dependent matching priors exhibited in ground-truth 2D-3D correspondences, making the match-ing more accurate and robust.
To evaluate the proposed method, we collected a large-scale dataset for the one-shot pose estimation setting, which contains 450 sequences of 150 objects. Compared with pre-vious instance-level method PVNet [27] and category-level method Objectron [4], OnePose achieves better precision without training for any object instances or categories in the validation set, while taking only 58 ms to process one frame on GPU. To the best of our knowledge, when combined with a feature-based pose tracker, OnePose is the first learning-based method that can stably detect and track poses of ev-eryday household objects in real-time (refer to the project page).
Contributions.
• Renovating the visual localization pipeline for object pose estimation that can handle novel objects without CAD models or additional network training.
• A new architecture of graph attention networks for robust 2D-3D feature matching.
• A large-scale object dataset for one-shot object pose esti-mation with pose annotations. 2.