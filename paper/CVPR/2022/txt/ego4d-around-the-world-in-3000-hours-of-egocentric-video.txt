Abstract
We introduce Ego4D, a massive-scale egocentric video dataset and benchmark suite. It offers 3,670 hours of daily-life activity video spanning hundreds of scenarios (house-hold, outdoor, workplace, leisure, etc.) captured by 931 unique camera wearers from 74 worldwide locations and 9 different countries. The approach to collection is designed to uphold rigorous privacy and ethics standards, with con-senting participants and robust de-identification procedures where relevant. Ego4D dramatically expands the volume of diverse egocentric video footage publicly available to the research community. Portions of the video are accompanied by audio, 3D meshes of the environment, eye gaze, stereo, and/or synchronized videos from multiple egocentric cam-eras at the same event. Furthermore, we present a host of new benchmark challenges centered around understanding the first-person visual experience in the past (querying an episodic memory), present (analyzing hand-object manipu-lation, audio-visual conversation, and social interactions), and future (forecasting activities). By publicly sharing this massive annotated dataset and benchmark suite, we aim to push the frontier of first-person perception. Project page: https://ego4d-data.org/ 1.

Introduction
Today’s computer vision systems excel at naming objects and activities in Internet photos or video clips. Their tremen-dous progress over the last decade has been fueled by major dataset and benchmark efforts, which provide the annota-tions needed to train and evaluate algorithms on well-defined tasks [49, 60, 61, 92, 108, 143].
While this progress is exciting, current datasets and mod-els represent only a limited definition of visual perception.
First, today’s influential Internet datasets capture brief, iso-lated moments in time from a third-person “spectactor” view.
Figure 1. Ego4D is a massive-scale egocentric video dataset of daily life activity spanning 74 locations worldwide. Here we see a snapshot of the dataset (5% of the clips, randomly sampled) highlighting its diversity in geographic location, activities, and modalities. The data includes social videos where participants consented to remain unblurred. See https://ego4d-data.org/fig1.html for interactive figure.
However, in both robotics and augmented reality, the input is a long, fluid video stream from the first-person or “ego-centric” point of view—where we see the world through the eyes of an agent actively engaged with its environment.
Second, whereas Internet photos are intentionally captured by a human photographer, images from an always-on wear-able egocentric camera lack this active curation. Finally, first-person perception requires a persistent 3D understand-ing of the camera wearer’s physical surroundings, and must interpret objects and actions in a human context—attentive to human-object interactions and high-level social behaviors.
Motivated by these critical contrasts, we present the
Ego4D dataset and benchmark suite. Ego4D aims to cat-alyze the next era of research in first-person visual percep-tion. Ego is for egocentric, and 4D is for 3D spatial plus temporal information.
Our first contribution is the dataset: a massive ego-video collection of unprecedented scale and diversity that captures daily life activity around the world. See Figure 1. It consists of 3,670 hours of video collected by 931 unique participants from 74 worldwide locations in 9 different countries. The vast majority of the footage is unscripted and “in the wild”, representing the natural interactions of the camera wearers as they go about daily activities in the home, workplace, leisure, social settings, and commuting. Based on self-identified characteristics, the camera wearers are of varying back-grounds, occupations, gender, and ages—not solely graduate students! The video’s rich geographic diversity supports the inclusion of objects, activities, and people frequently absent from existing datasets. Since each participant wore a camera for 1 to 10 hours at at time, the dataset offers long-form video content that displays the full arc of a person’s complex interactions with the environment, objects, and other people.
In addition to RGB video, portions of the data also provide audio, 3D meshes, gaze, stereo, and/or synchronized multi-camera views that allow seeing one event from multiple perspectives. Our dataset draws inspiration from prior ego-centric video data efforts [43,44,129,138,179,201,205,210], but makes significant advances in terms of scale, diversity, and realism.
Equally important to having the right data is to have the right research problems. Our second contribution is a suite of five benchmark tasks spanning the essential components of egocentric perception—indexing past experiences, ana-lyzing present interactions, and anticipating future activity.
To enable research on these fronts, we provide millions of rich annotations that resulted from over 250,000 hours of annotator effort and range from temporal, spatial, and seman-tic labels, to dense textual narrations of activities, natural language queries, and speech transcriptions.
Ego4D is the culmination of an intensive two-year effort by 14 institutions around the world who came together for the common goal of spurring new research in egocentric per-ception. We are kickstarting that work with a formal bench-mark challenge to be held at CVPR 2022. In the coming years, we believe our contribution can catalyze new research not only in vision, but also robotics, augmented reality, 3D sensing, multimodal learning, speech, and language. These directions will stem not only from the benchmark tasks we propose, but also alternative ones that the community will develop leveraging our massive, publicly available dataset.
2.