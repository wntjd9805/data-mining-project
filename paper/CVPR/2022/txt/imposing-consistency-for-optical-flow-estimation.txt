Abstract
Imposing consistency through proxy tasks has been shown to enhance data-driven learning and enable self-supervision in various tasks. This paper introduces novel and effective consistency strategies for optical ﬂow esti-mation, a problem where labels from real-world data are very challenging to derive. More speciﬁcally, we propose occlusion consistency and zero forcing in the forms of self-supervised learning and transformation consistency in the form of semi-supervised learning. We apply these consis-tency techniques in a way that the network model learns to describe pixel-level motions better while requiring no ad-ditional annotations. We demonstrate that our consistency strategies applied to a strong baseline network model using the original datasets and labels provide further improve-ments, attaining the state-of-the-art results on the KITTI-2015 scene ﬂow benchmark in the non-stereo category. Our method achieves the best foreground accuracy (4.33% in
Fl-all) over both the stereo and non-stereo categories, even though using only monocular image inputs. 1.

Introduction
Optical ﬂow characterizes dense displacements between corresponding pixels across images, e.g. between two con-secutive frames in a video [9, 19, 40, 43]. It is widely em-ployed in video analysis applications including video com-pression [32, 46], action recognition [6, 29], video denois-ing [3, 8], and object tracking [25, 52], to point out a few.
As important as its, optical ﬂow estimation comes with signiﬁcant challenges. Occlusions due to camera and ob-ject motions present one inherent difﬁculty, where a part of the scene is visible in one but not in the other image of the pair. Several methods addressed this problem by explic-itly estimating regions to be excluded [34, 51], by applying
* Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc.
† This work was done while at Qualcomm AI Research.
‡ Nojun Kwak was supported by the National Research Foundation of Ko-rea (NRF) grant funded by the Korea government (2021R1A2C3006659).
Figure 1. During training, we enforce occlusion consistency with self-supervision by applying random occlusion patterns and im-posing the network to detect the regions under occlusion be-tween consecutive images (It, It+1). We also employ transforma-tion consistency (equivariance to geometric transformations) in a semi-supervised manner for an image pair (It, It+k) and the trans-formed pair (T (It), T (It+k)) with k ≥ 1. self-supervision [31], or by incorporating contextual infor-mation [43]. These methods, however, had limited reception since they rely on multiple forward-backward iterations for predicting occlusion areas [34, 41] or fail for larger occlu-sions.
Obtaining precise annotations for optical ﬂow is another challenge that directly impacts the learning performance.
Since pixel-level motion annotation requires specialized and costly data acquisition systems, and in many cases, such annotations do not support high precision and spatial reso-lution, optical ﬂow datasets are limited in number, variety, and degree of realism [9, 19]. The need for large-scale real-world datasets, therefore, becomes a bottleneck.
To mitigate the annotation issues, unsupervised learn-ing [20, 24, 34, 45] and semi-supervised learning [27, 47] methods have been proposed in the past. Unsupervised learning schemes, however, typically result in degraded lagging behind fully supervised learning performance,
counterparts [24, 30, 45]. In comparison, semi-supervised learning [27] may offer potential performance gains with data augmentation along with generative adversarial net-works [14].
In this paper, we introduce two consistency strategies for optical ﬂow estimation to address these challenges as de-picted in Fig. 1. First, we propose occlusion consistency that generates a random occlusion mask, which is used to create additional image pairs, and constrains the network to predict the mask and a zero-forced ﬂow ﬁeld in a self-supervised manner. Unlike other approaches, our occlusion consistency allows generating occlusion ground truth without forward-backward iterations. Although this intuitive strategy is sim-ple, it enables the network not to confuse occlusion patterns as motion indicators without losing its representative capac-ity for the unoccluded image regions. It also helps the net-work to derive more informative features for the partially occluded regions within local receptive ﬁelds of the kernels without requiring additional labeling.
We also incorporate a transformation-based consis-tency regularization that has been shown useful in semi-supervised image classiﬁcation and object detection tasks [21, 22, 28, 36, 42]. This strategy helps the model im-pose equivariance through such consistency regularization.
We apply whole-image geometric transformations includ-ing ﬂippings, translations, and rotations. Then we restore the transformation before evaluating the overall transfor-mation consistency losses. While our transformation con-sistency is derived with two passes of forward ﬂow estima-tion, the cycle consistency [44] is computed with one pass of forward and the other pass of backward ﬂow estimation.
To the best of our knowledge, this is the ﬁrst attempt to impose equivariance through consistency regularization for optical ﬂow estimation. Note that our approach is different from conventional data augmentation schemes, which ex-pand training samples without imposition of sophisticated consistency losses during training.
Our proposed self- and semi-supervised consistency learning strategies not only complement the previous state-of-the-art RAFT [43] baseline, but enable signiﬁcant im-provement in the model accuracy performance as evidenced in our experiment results. Our proposed method achieves the new state-of-the-art accuracies and has ranked at the top of the KITTI-2015 scene ﬂow non-stereo leaderboard (Ours: 4.33%, 6.01%, 3.99% vs. RAFT: 5.10%, 6.87%, 4.74% in Fl-all, Fl-fg, and Fl-bg, respectively). Our train-ing with consistency strategies can potentially be adapted to other dense prediction tasks.
In summary, our main contributions are as follows:
• We propose a novel occlusion consistency strategy, which facilitates learning occlusion-robust representa-tions efﬁciently in a self-supervised manner.
• We incorporate transformation consistency equivari-ance enabling learning from a more diverse set of im-age pairs without additional labeling.
• Applying these two consistency strategies jointly in training and integrating an occlusion estimation chan-nel in the architecture, our model generates superior results over its baseline achieving state-of-the-art per-formance in the KITTI-2015 scene ﬂow non-stereo monocular dataset. 2.