Abstract
Humans have the ability to accumulate knowledge of new tasks in varying conditions, but deep neural networks of-ten suffer from catastrophic forgetting of previously learned knowledge after learning a new task. Many recent meth-ods focus on preventing catastrophic forgetting under the assumption of train and test data following similar distribu-tions. In this work, we consider a more realistic scenario of continual learning under domain shifts where the model must generalize its inference to an unseen domain. To this end, we encourage learning semantically meaningful fea-tures by equipping the classiﬁer with class similarity metrics as learning parameters which are obtained through Maha-lanobis similarity computations. Learning of the backbone representation along with these extra parameters is done seamlessly in an end-to-end manner. In addition, we propose an approach based on the exponential moving average of the parameters for better knowledge distillation. We demon-strate that, to a great extent, existing continual learning algorithms fail to handle the forgetting issue under multi-ple distributions, while our proposed approach learns new tasks under domain shift with accuracy boosts up to 10% on challenging datasets such as DomainNet and OfﬁceHome. 1.

Introduction
Figure 1. Top: Existing settings on 1) continually learning new visual categories from single domain (left), and 2) continually learn-ing from new domains with evaluation on the same domains (right).
Bottom: Our setting which has a sequence of visual categories coming from various domains, with evaluation on an unseen do-main. The proposed approach utilizes a continual domain alignment strategy dubbed Mahalanobis Similarity Learning (MSL). Colors indicate domains, and shapes indicate categories.
Humans possess the extraordinary capability of acquir-ing new knowledge in dynamically changing environments, while preserving knowledge learned in the past. The ob-tained knowledge can be further generalized to unseen situa-tions without the need of re-educating. On the other hand, there has been a surge of efforts to devise machine learning based algorithms to build more intelligent models and miti-gate the aforementioned challenges from two perspectives, namely continual learning [2, 4, 26, 32] and domain general-ization [12, 13, 20, 21]. This is particularly more important when deployed in the real world under a life-long learning setup [18, 25, 28]. For instance, consider warehouse robots that might perceive new inventory or unseen room layouts that they require to adapt to function properly. The observa-tions are captured at different time frames (e.g., day or night) and different locations (e.g., aisles) such that the observed domains come with an unpredictable sequence. In these situations, the key to success is to have certain embedded
adaptability in the robots to handle the challenges without costly re-training or entirely replacing them.
To put the discussion into perspective, on one hand, con-tinual learning based methods mainly try to deal with catas-trophic forgetting, which refers to the performance degrada-tion of previously acquired knowledge when new concepts are learned. On the other side, domain generalization is to
ﬁnd a good feature representation that goes well beyond the training distributions, while at the same time being dis-criminative for the task at hand. While effective, there has been comparatively little efforts in research to provide an-swers for the two aforementioned challenges simultaneously.
One effort is the work of Volpi et al. [35] which proposes continual domain adaptation, i.e., where different domains arrive in a continual fashion (top-right in Fig. 1). Other similar effort includes the work of Kundu et al. [19], which suggests class-continual learning with source-target domain adaptation as in the open-set setting. However, in both works the main aspect of generalization beyond seen domains is largely missing, limiting applicability of them in real-world scenarios. Moreover, the notion of incrementally adding training tasks is constrained to source and target domains only (i.e., two tasks).
In this work, we propose an approach for cross-domain continual learning, which also has the capability of general-ization to unseen domains. Our setup considers a sequence of tasks (i.e. different visual categories), where each task’s data is originated from various domains (Fig. 1 bottom-left).
Note that, our setup does not have any prior assumptions about the domains (e.g., availability of domain identiﬁers or speciﬁc orderings) associated to the given training samples in each task. This is a realistic scenario where the model is deemed to be agnostic about the origin of training sam-ples, e.g., when preserving privacy is important. We deem the domain alignment be done in a discriminative manner by equipping our classiﬁer with class-speciﬁc Mahalanobis similarity metrics, as shown in Fig. 1 (bottom-right). Here, the classiﬁer network also takes into account the underly-ing distribution of the class samples when generating the predictions. This is to encourage learning semantically mean-ingful features across training domains. We then learn the backbone representation parameters along with these extra parameters in an end-to-end fashion. In addition, we propose an approach based on the exponential moving average of the parameters for better knowledge distillation, preventing ex-cessive divergence from the previously learned parameters.
To evaluate our method, we deﬁne highly dynamic en-vironments with data coming from various domains and expanding visual categories. We perform extensive experi-ments on four different datasets – DomainNet [27], Ofﬁce-Home [34], PACS [21], and NICO [14]. The results show that our method consistently leads to an improvement of up to 10% compared to baselines [16, 23, 30, 38, 40] on 10-task and 5-task protocols. Furthermore, our proposed method also prevents catastrophic forgetting, achieving the lowest backward transfer rate [25] on average, e.g., ∼10% and ∼8% on DomainNet and OfﬁceHome, respectively.
To summarize, our contributions include 1. We provide a uniﬁed testbed for cross-domain continual learning with comparison to continual learning methods and techniques for domain generalization. 2. We propose a projection technique in an end-to-end scheme for domain generalization. In particular, we make use of learnable Mahalanobis similarity metrics for robust classiﬁers against unseen domains. 3. We devise an exponential moving average framework for knowledge distillation. The proposed module is integrated with our learnable projection technique to alleviate the degrading impact of catastrophic forgetting and distributional shifts by adaption to a history of the old parameters. 2.