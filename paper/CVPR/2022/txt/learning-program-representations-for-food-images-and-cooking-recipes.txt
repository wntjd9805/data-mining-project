Abstract
In this paper, we are interested in modeling a how-to instructional procedure, such as a cooking recipe, with a meaningful and rich high-level representation. Specifically, we propose to represent cooking recipes and food images as cooking programs. Programs provide a structured repre-sentation of the task, capturing cooking semantics and se-quential relationships of actions in the form of a graph. This allows them to be easily manipulated by users and executed by agents. To this end, we build a model that is trained to learn a joint embedding between recipes and food images via self-supervision and jointly generate a program from this embedding as a sequence. To validate our idea, we crowdsource programs for cooking recipes and show that: (a) projecting the image-recipe embeddings into programs leads to better cross-modal retrieval results; (b) generat-ing programs from images leads to better recognition re-sults compared to predicting raw cooking instructions; and (c) we can generate food images by manipulating programs via optimizing the latent code of a GAN. Code, data, and models are available online1. 1.

Introduction
Food is an important part of our lives. Imagine an AI agent that can look at a dish and recognize ingredients and reliably reconstruct the exact recipe of the dish, or another agent that can read, interpret and execute a cook-ing recipe to produce our favorite meal. Computer vision community has long studied image-level food classifica-tion [4, 10, 21, 26, 27, 34], and only recently focused on un-derstanding the mapping between recipes and images using multi-modal representations [14, 30, 48, 49, 67]. However, retrieval systems are limited to the existing database and usually fail for queries outside of this database while gener-ating the full recipe from an image remains a challenge [47]. 1http://cookingprograms.csail.mit.edu
Figure 1. Cooking programs. We learn cooking programs from food images and recipes. Programs provide a structured represen-tation of the cooking procedures which can also be represented as graphs (for brevity, we only show action and ingredient nodes).
Cooking recipes are step-by-step instructional proce-dures which we propose to represent as programs, captur-ing all the cooking semantics and relationships. A program contains a sequence of actions that can be written as func-tions (e.g., Cook(), Add()). Each action operates on specific ingredients under certain conditions, such as time or tool (e.g., Cook(pasta, time=‘10 minutes’, tool=‘pot’)). A program also captures the sequential dependency of the actions by maintaining their input-output connections. Note that even though cooking actions are of-ten performed sequentially in time, their connections are not necessarily sequential. The program can also be represented as a graph where each function and parameter is a node, while the edges are the function connections or the connec-tions between the parameters and the actions (Fig. 1).
Our goal is to generate cooking programs conditioned on food images or cooking recipes. We build a model that leverages the natural pairing of food images and recipes by
Figure 2. Annotation of cooking programs. (Top) We obtain a graph from an input recipe via named-entity, split-and-merge parsing, taxonomy dictionaries and connection annotation. (Bottom) We obtain all valid program sequences from the graph and the final program. learning a joint embedding using a vision and a text encoder.
The visual and text embeddings are then used in a program decoder to generate cooking programs. Our model is trained end-to-end by jointly optimizing a ranking loss between the visual and text representations and two losses on the pro-gram sequence predictions. Because the sequence of some actions can be permuted without violating the input-output connections between the functions, we generate the set of all valid program sequences for each recipe (Fig. 2) and de-sign a loss that operates on this set. At test time, the model can not only perform image-to-recipe retrieval tasks but can also predict the cooking program from an image or a recipe.
To validate our idea, we first crowdsource programs for cooking recipes selected from the Recipe1M dataset [49] using carefully designed tasks that can be easily performed by naive annotators. Experimental results show that our model leads to better cross-modal retrieval when it is jointly trained to generate programs. Moreover, generating pro-grams leads to better food recognition results compared to predicting the raw cooking instructions. Finally, we show how to generate food images by manipulating cooking pro-grams via optimizing the latent code of a GAN. 2.