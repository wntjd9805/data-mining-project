Abstract
This work aims to tackle Model Inversion (MI) attack on
Split Federated Learning (SFL). SFL is a recent distributed training scheme where multiple clients send intermediate activations (i.e., feature map), instead of raw data, to a cen-tral server. While such a scheme helps reduce the compu-tational load at the client end, it opens itself to reconstruc-tion of raw data from intermediate activation by the server.
Existing works on protecting SFL only consider inference and do not handle attacks during training. So we pro-pose ResSFL, a Split Federated Learning Framework that is designed to be MI-resistant during training. It is based on deriving a resistant feature extractor via attacker-aware training, and using this extractor to initialize the client-side model prior to standard SFL training. Such a method helps in reducing the computational complexity due to use of strong inversion model in client-side adversarial training as well as vulnerability of attacks launched in early training epochs. On CIFAR-100 dataset, our proposed framework successfully mitigates MI attack on a VGG-11 model with a high reconstruction Mean-Square-Error of 0.050 com-pared to 0.005 obtained by the baseline system. The frame-work achieves 67.5% accuracy (only 1% accuracy drop) with very low computation overhead. Code is released at: https://github.com/zlijingtao/ResSFL. 1.

Introduction
Collaborative training schemes have become popular in applications where preserving data privacy is very impor-tant. A representative example is federated learning [13], which has been used in a broad range of computer vision tasks involving private data, such as in human face recog-nition. Split Federated Learning (SFL) [18] is a recent col-laborative training scheme that combines the merits of Split
Learning (SL) [6] and Federated Learning (FL). It has sig-niﬁcant advantages on computation reduction and memory
Private
Inputs
Activation (cid:2202) (cid:2157)(cid:2191) grad
Loss
Client-side Model
Server-side Model (a) Training process of SFL
Inputs
Outputs
Query Client-side Model
Train
Activation (cid:2202) (cid:2157)(cid:2191)
Inversion 
Model
Reconstructed
Input (b) Server perform MI attack
Step-2: Transfer
SFL scheme
Source 
Dataset
MI-Resistant 
Clients
Step-1: Pre-training
Server (c) Our proposed ResSFL two-step framework
Figure 1. ResSFL overview. (a) Multi-client SFL training pro-cess. (b) MI attack performed by server. The client-side model is queried to build inversion model and the clients’ intermediate ac-tivations are used to reconstruct private data. (c) High-level view of the ResSFL framework, consisting of pre-training step and re-sistance transfer step. usage compared to FL [7, 13] and signiﬁcantly faster com-pared to the original SL scheme.
In SFL, a neural network model is split into a client-side model and a server-side model. Multiple clients oper-ate on their private inputs using their client-side model and pass the intermediate activations to the server. The server then computes on the more expensive server-side model and passes gradients back to clients. After an epoch, lo-cal copies of client-side model are averaged similar to FL.
Such a scheme is described in Figure 1(a). Thus SFL avoids clients’ raw data being sent to the server and also reduces client’s computation overhead.
Unfortunately, SFL is reported to be vulnerable to
Model Inversion (MI) attack [9, 21]. Here the honest-but-curious [17] server behaves as the attacker. As shown in Figure 1(b), by training an inversion model of the client-side model, the server can reconstruct clients’ raw data from intermediate activations it received during SFL training.
Prior works provide MI resistance for SFL inference by protecting intermediate activations [9, 19–21] or conﬁ-dence score (intermediate activations of last softmax layer)
[22–24]. However, MI resistance at training time is sig-niﬁcantly more difﬁcult. While inference-time defense only needs to make the ﬁnal model resistant to MI attack, training-time defense must be resistant to MI attack anytime during the SFL training process. This is because the honest-but-curious server has access to all intermediate activations and can launch an attack anytime.
To defend against MI attack during training and achieve low computational capability of the clients, we present
ResSFL, a two-step Split Learning framework that is resis-tant to MI attacks. The ﬁrst step uses attacker-aware train-ing to develop MI-resistant feature extractor and the second step uses this extractor to initialize client-side training prior to standard SFL-based training. An overview of our pro-posed framework is depicted in Figure 1(c).
The attacker-aware training emulates a strong attacker using a strong inversion model and adds bottleneck lay-ers [2] to the inversion model to reduce the feature space.
To reduce the computational complexity of such a process at the client-side and address vulnerability issues in early training epochs, the attacker-aware training scheme is im-plemented on an expert device and the MI resistant model is used to initialize the client side model. The clients then implement a lite-version attacker-aware training to achieve good accuracy on the new task while maintaining the resis-tance of the expert. Our contributions can be summarized as follows:
• Two-step resistant SFL framework. We present a novel two-step resistant SFL framework based on the proposed attacker-aware training and resistance trans-fer. To the best of our knowledge, this is the ﬁrst work that successfully mitigates training-time MI attack while achieving good accuracy in SFL.
• Attacker-aware Training Method. We use a combi-nation of (i) strong inversion model to mimic the MI attack behavior and (ii) bottleneck layers to shrink the large feature space resulting in good MI resistance and accuracy.
• Protect Training by Resistance Transfer. We use transfer learning to protect early-epoch vulnerability of attacker-aware training as well as reduce high com-putation cost of client-end training with complex in-version models. 2.