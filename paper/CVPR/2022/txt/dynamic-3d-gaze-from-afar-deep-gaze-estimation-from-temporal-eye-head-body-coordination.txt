Abstract
We introduce a novel method and dataset for 3D gaze estimation of a freely moving person from a distance, typi-cally in surveillance views. Eyes cannot be clearly seen in such cases due to occlusion and lacking resolution. Exist-ing gaze estimation methods suffer or fall back to approxi-mating gaze with head pose as they primarily rely on clear, close-up views of the eyes. Our key idea is to instead lever-age the intrinsic gaze, head, and body coordination of peo-ple. Our method formulates gaze estimation as Bayesian prediction given temporal estimates of head and body ori-entations which can be reliably estimated from a far. We model the head and body orientation likelihoods and the conditional prior of gaze direction on those with separate neural networks which are then cascaded to output the 3D gaze direction. We introduce an extensive new dataset that consists of surveillance videos annotated with 3D gaze di-rections captured in 5 indoor and outdoor scenes. Experi-mental results on this and other datasets validate the accu-racy of our method and demonstrate that gaze can be accu-rately estimated from a typical surveillance distance even when the person’s face is not visible to the camera. 1.

Introduction
Figure 1. We introduce a novel method for estimating the gaze di-rection of people (orange arrows) in videos captured from afar by leveraging the temporal coordination of the gaze, head, and body orientations in a Bayesian framework. Our method does not rely on the appearance of the eyes, and can tell the gaze direction even when the person is facing away from the camera. We introduce a new dataset for gaze estimation in the wild with ground truth anno-tation. Note that the markers, eye tracker, and body worn cameras are only used for ground truth annotation.
What if we could continuously trace the gaze direction of a person from a distance, for instance, with cameras fixed to room and street corners? If we can, the practicality of gaze estimation will be significantly increased and its utility will be greatly expanded. It will allow us to use already installed surveillance cameras or those monitoring elderlies to follow the dynamically changing gaze of a person, which will let us gauge much deeper into the person’s internal state not just her whereabouts.
Despite the large advances in gaze estimation research, especially by leveraging deep neural networks [9,16,32,34, 35], most appearance-based methods cannot be applied to videos taken from a distance. This is because they inher-ently require a clear and close-up view of the eyes. For in-stance, most leading methods assume a person sufficiently close to the camera (ranging from 10cm to 1m), or they are only applicable to the frontal view (up to 90°) of a person.
We target typical surveillance and monitoring views which may range from a few meters to 10m.
The few methods that demonstrate gaze estimation from surveillance images approximate the gaze with the head or body orientation, which is too crude for most downstream tasks [25, 26]. A recent method [6] does estimate gaze di-rection from surveillance cameras by regressing it from hu-man body keypoints detected with OpenPose. The method, however, only estimates gaze in 2D (i.e., in the image plane)
and it is validated only on a limited number of surveillance videos (1 scene with 2 cameras).
In this paper, we introduce a 3D gaze estimation method for videos of freely moving people taken from a distance, for typical room-sized surveillance scenarios. Our key idea is to fully leverage the temporal coordination of the gaze, head, and body orientations of a person to estimate the per-son’s dynamically changing 3D gaze direction just from the head and body orientations which can be estimated reliably from afar. We show that we can estimate the gaze direc-tion from a temporal sequence of head and body orientation estimates even without seeing the eyes at all.
We formulate gaze estimation as Bayesian prediction that capitalizes on a learned angular-temporal prior of the gaze direction conditioned on the head and body orienta-tions. The gaze, head, and body angular orientations have strong but complex temporal dependencies. For example, when we look for something in the room, our eyes first move into the desired direction, and then our head moves to follow the eyes. When our head is following the saccade, our eyes move in the opposite direction to stabilize the im-age on the retina during head movement. These seemingly simple angular-temporal relationships are cascaded one af-ter another and temporally blended together resulting in a complex dependency that can no longer be captured with a simple analytical model. We model this complex gaze-head-body coordination with a cascade of two learned deep networks that encode the head and body orientation like-lihoods and gaze orientation conditioned on them, respec-tively, to leverage their rich dependency to infer the gaze direction just from the head and body orientations.
Given a sequence of video frames of a person captured at a distance, we first estimate the head and body orienta-tions by devising a network that uses both the body appear-ance and 2D trajectory. These orientations are estimated as von Mises-Fisher distributions to canonically represent their uncertainties. These head and body orientation likeli-hoods are then multiplied with a learned conditional prior of the gaze direction given the head and body orientations that encode their natural temporal coordination. We model this conditional prior with a network that encodes the tempo-ral dependency of gaze direction in each frame on past and future head and body orientations. Optionally, we extend our method to opportunistically leverage the eye appear-ance when they happen to be visible and multiview head and body appearance when we have access to multiple cameras.
We introduce a new dataset of annotated surveillance videos of freely moving people taken from a distance in both indoor and outdoor scenes. The videos are captured with multiple cameras placed in eight different daily envi-ronments. People in the videos undergo large pose varia-tions and are frequently occluded by various environmental factors. Most important, their eyes are mostly not clearly visible as is often the case in surveillance videos. We intro-duce the first rigorously annotated dataset of 3D gaze direc-tions of freely moving people captured from afar. Through extensive evaluation using this new dataset, we show that our method enables accurate 3D gaze estimation from afar.
We also demonstrate that our method generalizes well to different scenes and camera poses. All data and code can be downloaded from our project web page. 2.