Abstract
Multi-task learning commonly encounters competition for resources among tasks, specifically when model capac-ity is limited. This challenge motivates models which al-low control over the relative importance of tasks and total compute cost during inference time. In this work, we pro-pose such a controllable multi-task network that dynami-cally adjusts its architecture and weights to match the de-sired task preference as well as the resource constraints.
In contrast to the existing dynamic multi-task approaches that adjust only the weights within a fixed architecture, our approach affords the flexibility to dynamically control the total computational cost and match the user-preferred task importance better. We propose a disentangled train-ing of two hypernetworks, by exploiting task affinity and a novel branching regularized loss, to take input prefer-ences and accordingly predict tree-structured models with adapted weights. Experiments on three multi-task bench-marks, namely PASCAL-Context, NYU-v2, and CIFAR-100, show the efficacy of our approach. Project page is available at https://www.nec-labs.com/˜mas/DYMU . 1.

Introduction
Multi-task learning [7, 40] (MTL) solves multiple tasks using a single model, with potential advantages of fast infer-ence and improved generalization by sharing representations across related tasks. However, in practical scenarios, simul-taneously optimizing all tasks is difficult due to task conflicts and limited model capacity [54]. Consequently, a trade-off between the competing tasks has to be found, necessitating precise balancing of the different task losses during optimiza-tion. In many applications, the desired trade-off can change over time, requiring a new model to be retrained from scratch.
To overcome this lack of flexibility, recent methods propose dynamic networks for multi-task learning [26, 36]. These frameworks enable a single multi-task model to learn the entire trade-off curve, and allow users to control the desired trade-off during inference via task preferences denoting the relative task importance.
Figure 1. Problem setup. Our goal is to enable users to control resource allocation dynamically among multiple tasks at inference time. Conventional dynamic networks (PHN [36]) for MTL achieve this in terms of weight changes within a fixed model (color gra-dients indicate proportion of weights allocated for each task). In contrast, we perform resource allocation in terms of both architec-ture and weights. This enables us to control total compute cost in addition to task preference. Dashed circle represents maximum compute budget, while filled circle represents the desired budget.
Portion of colors represents the user-defined task importance.
Conventional dynamic approaches for MTL assume a fixed model architecture, with all but the last prediction layers shared, and control trade-offs by changing the weights of this model. While such hard-parameter sharing is helpful in saving resources, the performance is inevitably lower than single task baselines when task conflicts exist due to over-sharing of parameters between tasks [40] . Furthermore, the fixed architecture suffers from a lack of flexibility, leading to a constant compute cost irrespective of the given task preference or compute budget changes. In many applications where the budget can change over time, these approaches may fail to take advantage of the increased resources in order to improve performance or accordingly lower the compute cost in order to satisfy stricter budget requirements.
To address the aforementioned issue and strike a balance between flexibility and performance, we propose a more expressive tree-structured [14] dynamic multi-task network which can adapt its architecture in addition to its weights at test-time, as illustrated in Figure 1. Specifically, we design a controller using two hypernetworks [16] that predict archi-tectures and weights, respectively, given a user preference that specifies test-time trade-offs of relative task importance and resource availability. This increases flexibility by chang-ing branching locations to re-allocate resources over tasks to match user-preferred task importance, and enhance or compromise task accuracy given computation budget re-quirements at any given moment. However, this comes at the cost of increase in complexity: 1) generalizing architecture prediction to unseen preferences, and 2) performing dynamic weight changes on potentially thousands of different models.
To tackle these challenges, we develop a two-stage train-ing scheme that starts from an N -stream network, termed the anchor net, which is initialized using weights from N pre-trained single-task models. This guides the architecture search as a prior that is preference-agnostic yet captures inter-task relations. In the first stage, we exploit inter-task relations derived from the anchor net to train the first hy-pernetwork that predicts connections between the different streams. We introduce a branching regularized loss that en-courages more resource allocation for dominant tasks while reducing the network cost from the less preferred ones. The predicted architectures contain edges that have not been ob-served during the anchor net initialization. These are denoted as cross-task edges since they connect nodes that belong to different streams. In the second stage, to improve the perfor-mance of the predicted architectures with cross-task edges, we train a secondary hypernetwork for cross-task adaptation via modulation of the normalization parameters.
Our framework is evaluated on three MTL datasets (PASCAL-Context, NYU-v2 and CIFAR-100) in terms of task performance, computational cost, and controllability (for both task importance and computational cost). Achiev-ing performance comparable to state-of-the-art MTL archi-tecture search methods under uniform task preference, our controller can further approximate efficient architectures for non-uniform preferences with provisions for reducing network size depending on computational constraints.
The primary contributions of our work are as follows:
• A controllable multi-task framework which allows users to assign task preference and the trade-off between task per-formance and network capacity via architectural changes.
• A controller, composed of two hypernetworks, to provide dynamic network structure and adapted network weights.
• A new joint learning objective including task-related losses and network complexity regularization to achieve the user defined trade-offs.
• Experiments on several MTL benchmarks (PASCAL-Context [35], NYU-v2 [45], CIFAR-100 [23]) demonstrate the efficacy of our framework. 2.