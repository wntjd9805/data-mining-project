Abstract
We present a super-fast convergence approach to recon-structing the per-scene radiance ﬁeld from a set of images that capture the scene with known poses. This task, which is often applied to novel view synthesis, is recently revolution-ized by Neural Radiance Field (NeRF) for its state-of-the-art quality and ﬂexibility. However, NeRF and its variants re-quire a lengthy training time ranging from hours to days for a single scene. In contrast, our approach achieves NeRF-comparable quality and converges rapidly from scratch in less than 15 minutes with a single GPU. We adopt a represen-tation consisting of a density voxel grid for scene geometry and a feature voxel grid with a shallow network for com-plex view-dependent appearance. Modeling with explicit and discretized volume representations is not new, but we propose two simple yet non-trivial techniques that contribute to fast convergence speed and high-quality output. First, we introduce the post-activation interpolation on voxel den-sity, which is capable of producing sharp surfaces in lower grid resolution. Second, direct voxel density optimization is prone to suboptimal geometry solutions, so we robustify the optimization process by imposing several priors. Finally, evaluation on ﬁve inward-facing benchmarks shows that our method matches, if not surpasses, NeRF’s quality, yet it only takes about 15 minutes to train from scratch for a new scene.
Code: https:// github.com/ sunset1995/ DirectVoxGO. 1.

Introduction
Achieving free-viewpoint navigation of 3D objects or scenes from only a set of calibrated images as input is a de-manding task. For instance, it enables online product show-case to provide an immersive user experience comparing to static image demonstration. Recently, Neural Radiance
Fields (NeRFs) [37] have emerged as powerful representa-tions yielding state-of-the-art quality on this task. 1National Tsing Hua University 2ASUS AICS Department 3Joint Research Center for AI Technology and All Vista Healthcare 4Aeolus Robotics 23.64 PSNR. 32.72 PSNR. 34.22 PSNR.
Ours at 13.72 mins.
Ours at 5.07 mins.
Ours at 2.33 mins. (a) The synthesized novel view by our method at three training checkpoints. (b) The training curves of different methods on Lego scene. The training time of each method is measured on our machine with a single NVIDIA RTX 2080 Ti GPU.
Figure 1. Super-fast convergence by our method. The key to our speedup is to optimize the volume density modeled in a dense voxel grid directly. Note that our method needs neither a conversion step from any trained implicit model (e.g., NeRF) nor a cross-scene pretraining, i.e., our voxel grid representation is directly and efﬁciently trained from scratch for each scene.
Despite its effectiveness in representing scenes, NeRF is known to be hampered by the need of lengthy training time and the inefﬁciency in rendering new views. This makes
NeRF infeasible for many application scenarios. Several follow-up methods [15, 18, 29, 30, 42, 43, 66] have shown sig-niﬁcant speedup of FPS in testing phase, some of which even achieve real-time rendering. However, only few methods show training times speedup, and the improvements are not comparable to ours [1, 10, 31] or lead to worse quality [6, 59].
On a single GPU machine, several hours of per scene opti-mization or a day of pretraining is typically required.
To reconstruct a volumetric scene representation from a set of images, NeRF uses multilayer perceptron (MLP) to implicitly learn the mapping from a queried 3D point (with a viewing direction) to its colors and densities. The queried properties along a camera ray can then be accumulated into a pixel color by volume rendering techniques. Our work takes inspiration from the recent success [15, 18, 66] that uses classic voxel grid to explicitly store the scene properties,
which enables real-time rendering and shows good quality.
However, their methods can not train from scratch and need a conversion step from the trained implicit model, which causes a bottleneck to the training time.
The key to our speedup is to use a dense voxel grid to directly model the 3D geometry (volume density). Develop-ing an elaborate strategy for view-dependent colors is not in the main scope of this paper, and we simply use a hybrid representation (feature grid with shallow MLP) for colors.
Directly optimizing the density voxel grid leads to super-fast converges but is prone to suboptimal solutions, where our method allocates “cloud” at free space and tries to ﬁt the photometric loss with the cloud instead of searching a geometry with better multi-view consistency. Our solution to this problem is simple and effective. First, we initialize the density voxel grid to yield opacities very close to zero everywhere to avoid the geometry solutions being biased toward the cameras’ near planes. Second, we give a lower learning rate to voxels visible to fewer views, which can avoid redundant voxels that are allocated just for explaining the observations from a small number of views. We show that the proposed solutions can successfully avoid the suboptimal geometry and work well on the ﬁve datasets.
Using the voxel grid to model volume density still faces a challenge in scalability. For parsimony, our approach au-tomatically ﬁnds a BBox tightly encloses the volume of interest to allocate the voxel grids. Besides, we propose post-activation—applying all the activation functions after trilinearly interpolating the density voxel grid. Previous work either interpolates the voxel grid for the activated opac-ity or uses nearest-neighbor interpolation, which results in a smooth surface in each grid cell. Conversely, we prove math-ematically and empirically that the proposed post-activation can model (beyond) a sharp linear surface within a single grid cell. As a result, we can use fewer voxels to achieve better qualities—our method with 1603 dense voxels already outperforms NeRF in most cases.
In summary, we have two main technical contributions.
First, we implement two priors to avoid suboptimal geometry in direct voxel density optimization. Second, we propose the post-activated voxel-grid interpolation, which enables sharp boundary modeling in lower grid resolution. The resulting key merits of this work are highlighted as follows:
• Our convergence speed is about two orders of magni-tude faster than NeRF—reducing training time from 10−20 hours to 15 minutes on our machine with a sin-gle NVIDIA RTX 2080 Ti GPU, as shown in Fig. 1.
• We achieve visual quality comparable to NeRF at a rendering speed that is about 45× faster.
• Our method does not need cross-scene pretraining.
• Our grid resolution is about 1603, while the grid reso-lution in previous work [15, 18, 66] ranges from 5123 to 13003 to achieve NeRF-comparable quality. 2.