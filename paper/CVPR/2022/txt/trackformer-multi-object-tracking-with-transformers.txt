Abstract
The challenging task of multi-object tracking (MOT) re-quires simultaneous reasoning about track initialization, identity, and spatio-temporal trajectories. We formulate this task as a frame-to-frame set prediction problem and introduce TrackFormer, an end-to-end trainable MOT ap-proach based on an encoder-decoder Transformer archi-tecture. Our model achieves data association between frames via attention by evolving a set of track predictions through a video sequence. The Transformer decoder ini-tializes new tracks from static object queries and autore-gressively follows existing tracks in space and time with the conceptually new and identity preserving track queries.
Both query types benefit from self- and encoder-decoder attention on global frame-level features, thereby omitting any additional graph optimization or modeling of mo-tion and/or appearance. TrackFormer introduces a new tracking-by-attention paradigm and while simple in its de-sign is able to achieve state-of-the-art performance on the task of multi-object tracking (MOT17) and segmentation (MOTS20). The code is available at https://github. com/timmeinhardt/trackformer 1.

Introduction
Humans need to focus their attention to track objects in space and time, for example, when playing a game of ten-nis, golf, or pong. This challenge is only increased when tracking not one, but multiple objects, in crowded and real world scenarios. Following this analogy, we demonstrate the effectiveness of Transformer [50] attention for the task of multi-object tracking (MOT) in videos.
The goal in MOT is to follow the trajectories of a set of objects, e.g., pedestrians, while keeping their identities dis-criminated as they are moving throughout a video sequence.
Due to the advances in image-level object detection [7, 38], most approaches follow the two-step tracking-by-detection paradigm: (i) detecting objects in individual video frames, and (ii) associating sets of detections between frames and
*Work done during an internship at Facebook AI Research (FAIR).
Figure 1. TrackFormer jointly performs object detection and tracking-by-attention with Transformers. Object and autoregres-sive track queries reason about track initialization, identity, and spatiotemporal trajectories. thereby creating individual object tracks over time. Tra-ditional tracking-by-detection methods associate detections via temporally sparse [22, 25] or dense [18, 21] graph opti-mization, or apply convolutional neural networks to predict matching scores between detections [8, 23].
Recent works [4,6,28,66] suggest a variation of the tradi-tional paradigm, coined tracking-by-regression [12]. In this approach, the object detector not only provides frame-wise detections, but replaces the data association step with a con-tinuous regression of each track to the changing position of its object. These approaches achieve track association im-plicitly, but provide top performance only by relying either on additional graph optimization [6, 28] or motion and ap-pearance models [4]. This is largely due to the isolated and local bounding box regression which lacks any notion of object identity or global communication between tracks.
In this work, we introduce the tracking-by-attention paradigm which not only applies attention for data associ-ation [11, 67] but jointly performs tracking and detection.
As shown in Figure 1, this is achieved by evolving a set of tracks from frame to frame forming trajectories over time.
We present a first straightforward instantiation of tracking-by-attention, TrackFormer, an end-to-end train-able Transformer [50] encoder-decoder architecture.
It encodes frame-level features from a convolutional neural network (CNN) [17] and decodes queries into bounding boxes associated with identities. The data association is performed through the novel and simple concept of track queries. Each query represents an object and follows it in space and time over the course of a video sequence in an autoregressive fashion. New objects entering the scene are detected by static object queries as in [7, 68] and subse-quently transform to future track queries. At each frame, the encoder-decoder computes attention between the input image features and the track as well as object queries, and outputs bounding boxes with assigned identities. Thereby,
TrackFormer performs tracking-by-attention and achieves detection and data association jointly without relying on any additional track matching, graph optimization, or ex-plicit modeling of motion and/or appearance.
In contrast to tracking-by-detection/regression, our approach detects and associates tracks simultaneously in a single step via at-tention (and not regression). TrackFormer extends the re-cently proposed set prediction objective for object detec-tion [7, 47, 68] to multi-object tracking.
We evaluate TrackFormer on the MOT17 [29] bench-mark where it achieves state-of-the-art performance for public and private detections. Furthermore, we demonstrate the extension with a mask prediction head and show state-of-the-art results on the Multi-Object Tracking and Seg-mentation (MOTS20) challenge [51]. We hope this simple yet powerful baseline will inspire researchers to explore the potential of the tracking-by-attention paradigm.
In summary, we make the following contributions:
• An end-to-end trainable multi-object tracking ap-proach which achieves detection and data association in a new tracking-by-attention paradigm.
• The concept of autoregressive track queries which em-bed an object’s spatial position and identity, thereby tracking it in space and time.
• The TrackFormer model which obtains state-of-the-art results on two challenging multi-object tracking (MOT17) and segmentation (MOTS20) benchmarks. 2.