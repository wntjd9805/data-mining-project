Abstract
Remarkable achievements have been attained with Gen-erative Adversarial Networks (GANs) in image-to-image translation. However, due to a tremendous amount of pa-rameters, state-of-the-art GANs usually suffer from low ef-ficiency and bulky memory usage. To tackle this challenge, firstly, this paper investigates GANs performance from a fre-quency perspective. The results show that GANs, especially small GANs lack the ability to generate high-quality high frequency information. To address this problem, we pro-pose a novel knowledge distillation method referred to as wavelet knowledge distillation. Instead of directly distilling the generated images of teachers, wavelet knowledge distil-lation first decomposes the images into different frequency bands with discrete wavelet transformation and then only distills the high frequency bands. As a result, the student
GAN can pay more attention to its learning on high fre-quency bands. Experiments demonstrate that our method leads to 7.08× compression and 6.80× acceleration on Cy-cleGAN with almost no performance drop. Additionally, we have studied the relation between discriminators and gen-erators which shows that the compression of discriminators can promote the performance of compressed generators. 1.

Introduction
Tremendous progress has been achieved with Generative adversarial networks (GANs) in generating high-fidelity, high-resolution, and photo-realistic images and videos with both paired and unpaired datasets [4,13,17,18,25,41,43,59].
The excellent performance of GANs has promoted its ap-plication in various image-to-image translation tasks, such as image style transfer [20, 21] and super-resolution [22].
Compared with other tasks such as image classification and object detection, image-to-image generation is more com-plex since it has a much larger output space. As a con-* Corresponding author. † This project is funded by Kuaishou Research
Program. This work was done during internship of Linfeng Zhang and Xin
Chen with Y-tech Kuaishou Technology. Codes are relased on Github.
Figure 1. Normalized L1 distance between the images gener-ated by GANs and the ground truth images on different frequency bands. Different colors indicate GANs with different FLOPs. Re-sults are averaged over 8 trials on Edge→Shoe dataset. sequence, existing GANs always have high computational demands and a huge amount of parameters, which lead to inefficient inference and intolerant memory footprint, and limit their usage in resource-constrained platforms.
Knowledge distillation (KD) has been an effective tool for improving the performance of small models [5, 14]. By imitating the prediction results and the intermediate features from a cumbersome teacher model, the performance of a lightweight student model can be improved significantly.
Following previous knowledge distillation methods on clas-sification [44], object detection [53], semantic segmenta-tion [31] and action recognition [28], some recent research has tried to directly apply knowledge distillation to GANs.
Unfortunately, most of them obtain very limited and even negative effects [23, 26].
Why does KD not work well on GAN? In this paper, we first study this question from a frequency perspective with the following experiment. Firstly, discrete wavelet
Figure 2. Comparison between knowledge distillation [14] (subfigure a) and the proposed wavelet knowledge distillation (subfigure b) on
Edges→Shoes. Wavelet knowledge distillation first applies discrete wavelet transformation (DWT) to the generated images and then only minimizes the difference on high frequency bands. transformation (DWT) is utilized to decompose the gen-erated images and the ground truth images into different frequency bands. Then, we compute the normalized L1-norm distance on each frequency band respectively1. As shown in Figure 1, all the GANs achieve very low error on the low frequency band but fail in the generation on high frequency bands, which is in line with the observation that images generated by GANs do not have good details. Be-sides, it is observed that compared with the large GAN, the tiny GAN achieves comparable performance on the low frequency band but much worse performance on high fre-quency bands. These two observations demonstrate that more attention should be paid to the high frequency during
GAN compression.
However, naive knowledge distillation in GANs applica-tion directly minimizes the difference between the images generated by students and teachers and ignores the prior-ity of high frequency. Motivated by these observations, we propose wavelet knowledge distillation, which highlights students learning on the high frequency in knowledge dis-tillation. As shown in Figure 2, we first apply discrete wavelet transformation to decompose the images generated by teachers and students into different frequency bands and then only minimize the L1 loss on the high frequency bands.
Abundant experiments on both paired and unpaired image-to-image translation demonstrate the effectiveness of our method both quantitatively and qualitatively. On
Horse→Zebra and Zebra→Horse datasets, our method leads to 7.08× compression and 6.80× acceleration on Cy-cleGAN with almost no performance drop. In the discus-sion section, we have further studied the effectiveness of different frequency bands and the influence of knowledge distillation schemes. Additionally, studies on the relation between discriminators and generators in model compres-sion have also been introduced, showing that the compres-sion of discriminators can significantly promote the perfor-mance of compressed generators. 1Details of this experiment can be found in the supplementary material.
Our main contributions can be summarized as follows.
• We have analyzed the performance of GANs from a frequency perspective, which quantitatively shows that
GAN, especially small GAN lacks the ability to gener-ate high-quality high frequency information in images.
• Based on the above observation, wavelet knowledge distillation is proposed to address this issue by only distilling the high frequency information, instead of all the information from images generated by the teacher.
• Quantitative and qualitative results on three models and eight datasets with six comparison methods have demonstrated the effectiveness of our method.
• We have studied the relation between discriminators and generators during model compression.
It shows that compression on discriminators is necessary for maintaining its competition with compressed genera-tors in adversarial learning. which further benefits the performance of generators. 2.