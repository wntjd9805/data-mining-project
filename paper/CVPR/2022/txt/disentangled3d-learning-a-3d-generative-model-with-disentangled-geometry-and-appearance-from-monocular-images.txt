Abstract
Learning 3D generative models from a dataset of monoc-ular images enables self-supervised 3D reasoning and con-trollable synthesis. State-of-the-art 3D generative models are GANs that use neural 3D volumetric representations for synthesis. Images are synthesized by rendering the volumes from a given camera. These models can disentangle the 3D scene from the camera viewpoint in any generated image.
However, most models do not disentangle other factors of image formation, such as geometry and appearance. In this paper, we design a 3D GAN which can learn a disentan-gled model of objects, just from monocular observations.
Our model can disentangle the geometry and appearance variations in the scene, i.e., we can independently sample from the geometry and appearance spaces of the genera-tive model. This is achieved using a novel non-rigid de-formable scene formulation. A 3D volume that represents an object instance is computed as a non-rigidly deformed canonical 3D volume. Our method learns the canonical volume, as well as its deformations, jointly during train-ing. This formulation also helps us improve the disentan-glement between the 3D scene and the camera viewpoints using a novel pose regularization loss defined on the 3D de-formation field. In addition, we model the inverse deforma-tions, enabling the computation of dense correspondences between images generated by our model. Finally, we design an approach to embed real images into the latent space of our model, enabling editing of real images. 1.

Introduction
State-of-the-art generative models directly operate in the image space using 2D CNNs. These models, such as Style-GAN and its variants [14â€“16] have achieved a high level of photorealism. However, image-based models do not of-fer direct control over the underlying 3D scene parame-ters, such as camera and geometry. While some methods add camera viewpoint control over pretrained image-based
GAN models [1, 5, 17, 34], the results are limited by the quality of 3D consistency of the pretrained models.
In contrast to the image-based methods, recent ap-proaches learn GAN models directly in the 3D space [2, 8, 24, 26, 31]. In this case, the generator network synthe-sizes a 3D representation of the scene as output, which can
then be rendered from a virtual camera to generate the im-age. Since the 3D scene is explicitly modeled, the cam-era parameters are disentangled from the scene itself in the image synthesis process. However, other scene proper-ties such as geometry and appearance remain entangled and cannot be controlled independently. While some 3D GAN approaches have attempted to disentangle geometry from appearance [26,31], their design choices are not physically-motivated, which leads to inaccurate solutions where ap-pearance information can leak through the geometry com-ponent. In contrast, our proposed approach is inspired by recent non-rigid formulations for novel viewpoint synthesis of dynamic scenes [28, 35]. These methods model the de-formations in a scene observed across time, by separating the 3D reconstruction of each frame into a canonical 3D re-construction and its deformations. Yet, even though these methods can learn to synthesize novel viewpoints of a de-forming scene, they are limited to modeling a single scene, and they cannot control the appearance of the scene.
In this work, we propose D3D, a GAN with two separate and independent components for geometry and appearance.
We extend the non-rigid formulation to the case of model-ing multiple instances of a deformable object category, such as human heads, cats, or cars. Each instance of the object class is modeled as a deformation of a canonical volume, which is shared across the object category. Our method learns the canonical volume, as well as the instance-specific geometric deformations jointly from datasets of monocular images. The canonical volume has a fixed geometry while its appearance can be changed independent of the geomet-ric deformations. This formulation by design motivates dis-entanglement between the geometric deformations and ap-pearance variations, which has been a challenging task, es-pecially as we are limited to monocular images for training.
In addition to the disentanglement of geometry and ap-pearance, our formulation allows for other advantages over state-of-the-art methods. Since our geometric deformations are explicit Euclidean transformations, we can enforce use-ful properties in the model, such as pose consistency over the generated 3D volumes. Existing 3D GANs do not al-ways manage to disentangle the camera viewpoint and the generated 3D volumes, especially when the hand-crafted prior camera distribution does not match the real distribu-tion of the training dataset. We design a pose regularization loss, which can enforce the consistency of the object pose, improving the quality of camera and scene disentanglement.
In addition, we learn an inverse deformation network, al-lowing us to compute dense correspondences between im-ages generated by our model. Finally, we allow editing of input photographs using D3D by mapping a given image to the corresponding geometry and appearance latent codes, as well as the camera pose. In summary, this paper presents the following contributions: 1. A generative model which can disentangle geometry, appearance, and camera pose in the generated images.
This is enabled by a generalization of the non-rigid scene formulation to deformable object categories. 2. A novel training framework for 3D GANs, which en-ables pose consistency of the generated volumes, as well as the computation of dense correspondences be-tween generated images. 3. Editing of real images by computing their embedding in our GAN space. This enables intuitive control over the camera pose, appearance and geometry in images. 2.