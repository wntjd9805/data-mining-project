Abstract
In recent years, we have witnessed significant perfor-mance boost in the image captioning task based on vision-language pre-training (VLP). Scale is believed to be an im-portant factor for this advance. However, most existing work only focuses on pre-training transformers with mod-erate sizes (e.g., 12 or 24 layers) on roughly 4 million im-, a LargE-scale ages. In this paper, we present LEMON iMage captiONer, and provide the first empirical study on the scaling behavior of VLP for image captioning. We use the state-of-the-art VinVL model as our reference model, which consists of an image feature extractor and a trans-former model, and scale the transformer both up and down, with model sizes ranging from 13 to 675 million parame-ters. In terms of data, we conduct experiments with up to 200 million image-text pairs which are automatically col-lected from web based on the alt attribute of the image (dubbed as ALT200M1). Extensive analysis helps to char-acterize the performance trend as the model size and the pre-training data size increase. We also compare different training recipes, especially for training on large-scale noisy data. As a result, LEMON achieves new state of the arts on several major image captioning benchmarks, including
COCO Caption, nocaps, and Conceptual Captions. We also show LEMON can generate captions with long-tail vi-sual concepts when used in a zero-shot manner. 1.

Introduction
Recent advances in image captioning [1, 5, 35] can be largely attributed to vision-language pre-training (VLP) [26, 30, 37, 40], the current prevailing training paradigm for vision-language (VL) research. VLP [6] is usually conducted on a combined image-text dataset comprising of several or tens of millions images in total, e.g., Visual Genome [20], SBU [32] and Conceptual Cap-tions [4, 35]. While previous studies [29, 48, 49] have ana-1The dataset is released at https://github.com/xiaoweihu/
ALT200M.
Figure 1. Image captioning performance on COCO when up-scaling model for each dataset size. The x-axis plots the num-ber of parameters for each model size (e.g., tiny, small, huge) in a logarithmic scale. The definition of model sizes is detailed in
Table 2. Increasing the model size is not significantly beneficial at small pre-training dataset scales. However, when we use suf-ficiently large datasets, we see strong performance boost from a larger model. lyzed various choices of pre-training objectives and model architectures, it remains unclear to what extent the pre-training dataset would impact the performance, and how it correlates with different model settings. Along the journey of pushing the limit of VLP, it becomes increasingly impor-tant to answer this question.
Scale is believed to be an important ingredient in attain-ing excellent performance [17, 33, 43]. Recent work has in-vestigated the Pareto frontier of training transformer mod-els, often referred to as the neural scaling law, in the do-mains of natural language processing [2, 18, 41] and com-puter vision [12,47], via unsupervised or weakly-supervised learning methods. These studies have observed consistent benefits of increasing the model size to billions of parame-(a) finetuned and evaluated on COCO (b) finetuned on COCO, evaluated on nocaps
Figure 2. Image captioning performance in data upscaling for each model size. The x-axis shows the number of image-text pairs used in pre-training. The y-axis shows the evaluation score (CIDEr) on COCO “Karpathy” test split and nocaps validation set, respectively.
The models are first pre-trained, then finetuned on COCO caption training split. Note that x-axis is plotted in a logarithmic scale. ters, given billion magnitude of pre-training data available.
More recently, contrastive image-text pre-training [17, 33] has also been scaled up to 400 million and 1.8 billion data sizes for image representation learning and image-text retrieval. Both CLIP [33] and ALIGN [17] employ two in-dividual networks to encode the image and the text sepa-rately for alignment, which well fits the image-text retrieval task, but little is known about the scaling properties when it comes to image captioning.
To study the characteristics of this scaling trend on the captioning task, we first construct a large-scale image-text dataset (dubbled as ALT200M), consisting of up to 200 mil-lion image-text pairs from web based on the alt attribute of the images. Then, we conduct extensive experiments to scale VLP for image captioning from both the data and model perspectives, and name our model as LEMON , short for a LargE-scale iMage captiONer. To simulate the process of data scaling, we create multiple subsets of
ALT200M, ranging from 3 to 200 million.
In terms of model, we use the state-of-the-art image captioning model
VinVL [48] as our reference model, composed of an image feature extractor and a transformer model. We adapt the pre-training task to be consistent with the captioning task, and then scale the width and depth of the transformer model with the number of parameters ranging from 13 (i.e., tiny) to 675 (i.e., huge) millions. Combining different models and pre-training data sizes, we summarize our results in Fig-ure 1 and 2, which characterize the linear-logarithmic scal-ing trend. Larger models tend to benefit more when we have more than 10 million data for pre-training. However, with only 3 million data, the performance starts to saturate early as the model size increases. Moreover, we also investigate other design choices of VLP, e.g., model architectures and training objectives.
Our contributions are summarized as follows.
• We present the VLP scaling rule for image captioning.
Not only does this prove the effectiveness of learning from large-scale noisy data, but it also sheds lights on how performance can be efficiently improved by increas-ing the model and pre-training data sizes together to avoid a saturation plateau.
• We achieve new state-of-the-art results for image caption-ing across several major benchmarks, including COCO
Caption, nocaps, and Conceptual Captions. 2.