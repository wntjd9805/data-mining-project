Abstract
In this work we demonstrate the vulnerability of vision transformers (ViTs) to gradient-based inversion attacks. Dur-ing this attack, the original data batch is reconstructed given model weights and the corresponding gradients. We intro-duce a method, named GradViT, that optimizes random noise into naturally looking images via an iterative process. The optimization objective consists of (i) a loss on matching the gradients, (ii) image prior in the form of distance to batch-normalization statistics of a pretrained CNN model, and (iii) a total variation regularization on patches to guide correct recovery locations. We propose a unique loss scheduling function to overcome local minima during optimization. We evaluate GadViT on ImageNet1K and MS-Celeb-1M datasets, and observe unprecedentedly high fidelity and closeness to the original (hidden) data. During the analysis we find that vision transformers are significantly more vulnerable than previously studied CNNs due to the presence of the attention mechanism. Our method demonstrates new state-of-the-art results for gradient inversion in both qualitative and quan-titative metrics. Project page at https://gradvit.github.io/. 1.

Introduction
Vision Transformers (ViTs) [8] have achieved state-of-the-art performance in a number of vision tasks such as image classification [39], object detection [6] and semantic segmentation [5]. In ViT-based models, visual features are split into patches and projected into an embedding space. A series of repeating transformer encoder layers, consisting of alternating Multi-head Self-Attention (MSA) and Multi-Layer Perceptron (MLP) blocks extract feature representa-tion from the embedded tokens for downstream tasks (e.g., classification). Recent studies have demonstrated the ef-fectiveness of ViTs in learning uniform local and global spatial dependencies [31]. In addition, ViTs have a great
˚Equal contribution. :Equal advising. (a) Recovering data from vision transformer gradient unveils intricate details.
Original
Gradient Recovery
GradInv. [37] (RN-50) GradInv. [37] (ViT) GradViT (VIT) - ours (b) GradViT improves noticeably over prior art. Example within a batch of size 8.
Figure 1. Inverting gradients for image recovery. We show vision transformer gradients encode a surprising amount of information such that high-fidelity original image batches of high resolution can be recovered, see 112 ˆ 112 pixel MS-Celeb-1M and 224 ˆ 224 pixel ImageNet1K sample recovery above and more in experiments.
Our method, GradViT, yields the first successful attempt to invert
ViT gradients, not achievable by previous state-of-the-art methods.
We demonstrate that ViTs, despite lacking batchnorm layers, suffer even more data leakage compared to CNNs. As insights we show that ViT gradients (i) encode uneven original information across layers, and (ii) attention is all that reveals. capability in learning pre-text tasks and can be scaled for distributed, collaborative, or federated learning scenarios. In this work, we study vulnerability of sharing ViT’s gradients in the above mentioned settings.
Recent efforts [9, 37, 43] have demonstrated the vulnera-bility of convolutional neural networks (CNN) to gradient-based inversion attacks. In such attacks, a malicious party can intercept local model gradients and reconstruct private training data in an optimization-based scheme via matching the compromised gradients. Most methods are limited to small image resolutions or non-linearity constraints amid the
hardness of the problem. Among these, GradInversion [37] demonstrated the first successful scaling of gradient inver-sion to deep networks on large datasets over large batches.
In addition to gradient matching, GradInversion [37] is con-strained to models with Batch Normalization layers to match feature distribution and bring naturality to the reconstructed images. However, vision transformers lack BN layers and are less vulnerable to previously proposed inversion meth-ods. Naively applying CNN-Based gradient matching [9, 37] techniques for ViT inversion results in sub-optimal solutions due to inherent differences in architectures. Fig. 1 compares reconstruction results obtained by applying current state-of-the-art method GradInversion [37] on the CNN and ViT models. We clearly see significantly degraded visual quality when inverting the ViT gradients.
Since ViT-based models have a different architecture, op-erate on image patches, and contain no BNs as in CNN counterparts, it might be assumed as if they are more secure to gradient-based inversion attacks. On the contrary to this assumption, in this work we quantitatively and qualitatively demonstrate that ViT-based models are even more vulnera-ble than CNNs. To show that, we first study the challenges introduced by ViT’s architectural difference, then propose a novel method, named GradViT, which addresses them and obtains unprecedented high-fidelity and closeness to the original (hidden) data (Fig. 1). Specifically, in GradViT, we tackle the absence of BN statistics by using an independently trained CNN to match the feature distributions of natural images and the images under optimization. We use a ResNet-50 model trained with contrastive loss and its associated BN statistic as an image prior. That is, another model can serve as an image prior instead of the exact BN statistics and their corresponding updates. Moreover, we discover that the pro-posed image prior generalizes to unseen domain (e.g., faces) which makes it universal.
In addition, while a gradient-based optimization attack can lead to a legitimate reconstruction of patches, their rel-ative location will most likely be incorrect. This happens due to the lack of inductive image bias and permutation in-variance in ViTs. To address this problem, we propose a patch prior loss that minimizes the total pixel distances of edges between patches. In other words, we enforce spatial constraints on shared borders (i.e., vertical and horizontal) across neighboring patches as we expect no significant visual discontinuities between them. Minimizing all three losses simultaneously leads to sub-optimal solutions. Therefore, we propose a tailored scheduler to balance the contribution of each loss during training, which is observed to be critical to achieve a valid image recovery.
We validate the effectiveness of GradViT across a wide range of ViT-based models over changing datasets. We start with batch reconstruction of training images from
ImageNet1K dataset [7] given the widely used ViT net-works (e.g., ViT-B/16,32, ViT-S, ViT-T, DeiT, etc.) as the base networks. Our results demonstrate new state-of-the-art benchmarks in terms of image reconstruction metrics.
Furthermore, we demonstrate the possibility of detailed re-covery of facial images by gradient inversion of a ViT-based model [42] from MS-Celeb-1M dataset [11]. Our findings demonstrate the vulnerabilities of ViT-based models to gra-dient inversion attacks and specifically for sensitive domains with human training data. With these concerns, we perform extensive studies to analyze the source of vulnerability in
ViTs by investigating both layer-wise and component-wise contributions. Our findings provide insights for the develop-ment of protection mechanisms against such attacks, which can be beneficial for securing distributed training of ViTs in applications such as multi-node training or federated learn-ing [14, 25].
Our main contributions are summarized as follows:
• We present GradViT, a first successful attempt at ViT gradient inversion, in which random noise is optimized to match shared gradients.
• We introduce an image prior based on CNNs trained with contrastive loss and show scalability across do-mains.
• We articulate a loss scheduling scheme to guide opti-mization out of sub-optimal solutions.
• We formulate a patch prior loss function tailored to ViT inversion that mitigates the issue of patch permutation invariance in the reconstructed image.
• We set a state-of-the-art benchmark for ViT gradient inversion across multiple ViT-based networks on Im-ageNet1K [7] and MS-Celeb-1M [11] datasets. Our method recovers high-resolution facial features with the most intricate details.
• We study the vulnerability of ViT components by per-forming layer-wise and component-wise analysis. Our findings show that gradients of deeper layers are more informative, and MSA gradients yield near-perfect in-put recovery. 2.