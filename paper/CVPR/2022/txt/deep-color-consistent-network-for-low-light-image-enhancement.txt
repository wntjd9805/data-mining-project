Abstract
Low-light image enhancement (LLIE) explores how to reﬁne the illumination and obtain natural normal-light im-ages. Current LLIE methods mainly focus on improving the illumination, but do not consider the color consistency by reasonably incorporating color information into the LLIE process. As a result, color difference usually exists between the enhanced image and ground-truth. To address this is-sue, we propose a new deep color consistent network termed
DCC-Net to retain the color consistency for LLIE. A new
“divide and conquer” collaborative strategy is presented, which can jointly preserve color information and enhance the illumination. Speciﬁcally, the decoupling strategy of our
DCC-Net decouples each color image into two main com-ponents, i.e., gray image plus color histogram. Gray image is used to generate reasonable structures and textures, and the color histogram is beneﬁcial for preserving the color consistency. That is, they both are utilized to complete the
LLIE task collaboratively. To match the color and content features, and reduce the color consistency gap between en-hanced image and ground-truth, we also design a new pyra-mid color embedding (PCE) module, which can better em-bed color information into the LLIE process. Extensive ex-periments on six real datasets show that the enhanced im-ages of our DCC-Net are more natural and colorful, and perform favorably against the state-of-the-art methods. 1.

Introduction
Low-light image enhancement (LLIE) is a task of re-ﬁning the illumination to obtain natural norm-light images, which aims at improving the perception and visual quality of low-light images captured in poor illumination environ-ment. Low-light images are rather common in reality, e.g., images captured in outdoor or indoor scenes with poor light-ing conditions, which suffers from the unclear contents and
Figure 1. Comparison of our DCC-Net and other deep LLIE meth-ods in terms of PSNR/SSIM metrics. We clearly see that there is a large color gap between the enhanced images of RetinexNet, Zero-DCE++, Kind++ and EnlightenGAN, and the ground-truth image.
In contrast, our DCC-Net can retain the color consistency effec-tively, and the enhanced image is more natural and colorful. textures, low contrast and noises. These degradations will not only have negative effect on human perception, but also will be not conducive to the subsequent multimedia comput-ing and computer vision tasks designed for high-quality im-ages, for instance face recognition [3], object detection [25] and semantic segmentation [4].
Traditional LLIE methods aim to build a model to reﬁne the illumination and obtain the enhanced image, which can be roughly categorized into histogram equalization (HE)-based and retinex-based methods. Traditional methods are relatively simple and easy, but they usually cannot restore the consistent colors and detailed textures.
With the impressive performance of deep neural net-works (DNN) in diverse high-level and low-level vision tasks [5, 10, 31, 36], deep LLIE methods also achieve great improvement [2,11,30]. Deep LLIE methods usually design
a deep neural network equipping with different modules to reverse the degradation process. Compared to traditional methods that usually produce undesirable illumination and noises, deep LLIE methods can obtain better results due to the strong ability of DNN. However, these methods tend to generate inconsistent colors, which can be seen in Figure 1. There is obvious color difference between the generated images of RetinexNet, Zero-DCE++, Kind++ and Enlight-enGAN and the ground-truth. While the result of our DCC-Net is more natural and conforms to the real color. We ask: what makes the enhanced images lose color consistency?
We attempt to answer this question from two respects: 1) Different architectures. There are two popular ways to handle the LLIE task in current studies: 1) end-to-end deep frameworks that directly handle the low-light image to obtain normal-light images; 2) retinex-based frameworks that decompose the image into reﬂectance and illumination for further processing. Both the two modes focus on reﬁning illumination, while ignoring the color consistency and naturalness. Thus, there will be color gap in the enhanced images. 2) Information mismatch. Color histogram describes color information globally, which does not contain any spatial information. As a result, we cannot ﬁnd suit-able color information for speciﬁc contents of images.
The connection between the color and content features is therefore unable to be directly built. This kind of information mismatch will make the enhanced images unnatural and contain inconsistent colors.
We therefore propose a new “divide and conquer” col-laborative strategy, which can jointly retain the color con-sistency and enhance the illumination. Generally, the main contributions of this paper are summarized as follows:
• Technically, we introduce a new strategy to retain the color consistency for LLIE, and also propose a deep color consistent network termed DCC-Net to reduce the color difference between the enhanced image and ground-truth. To the best of our knowledge, this is the
ﬁrst work to enhance the illumination of low-light im-age by directly exploring the color consistency. Ex-tensive experiments show that our DCC-Net can better enhance the illumination, and the enhanced images are more natural and consistent in color.
• To jointly retain the color consistency and enhance the illumination, DCC-Net designs a decoupling strategy to decouple a color image into a gray image and a color histogram that complete the LLIE task collaboratively.
We design three sub-nets for DCC-Net, i.e., G-Net, C-Net and R-Net, as shown in Figure 2. G-Net aims at recovering the gray image that can offer rich structure and texture information. C-Net aims to learn the color distributions, which will be conductive to the color co-herence. R-Net combines the gray image and color information to restore the normal-light image.
• To better overcoming the weakness of lacking spa-tial information for color histogram, we also design a pyramid color embedding (PCE) module that con-sists of six color embedding (CE) sub-modules with pyramid structure. CE can match the color and content features, according to the afﬁnities between them, so that the color information can be dynamically incorpo-rated, which can further reduce the color gap between the enhanced image and ground-truth image. 2.