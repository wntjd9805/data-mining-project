Abstract
We present High Dynamic Range Neural Radiance
Fields (HDR-NeRF) to recover an HDR radiance field from a set of low dynamic range (LDR) views with different expo-sures. Using the HDR-NeRF, we are able to generate both novel HDR views and novel LDR views under different ex-posures. The key to our method is to model the simplified physical imaging process, which dictates that the radiance of a scene point transforms to a pixel value in the LDR im-age with two implicit functions: a radiance field and a tone mapper. The radiance field encodes the scene radiance (val-ues vary from 0 to +∞), which outputs the density and radi-ance of a ray by giving corresponding ray origin and ray di-rection. The tone mapper models the mapping process that a ray hitting on the camera sensor becomes a pixel value.
The color of the ray is predicted by feeding the radiance and the corresponding exposure time into the tone mapper. We use the classic volume rendering technique to project the output radiance, colors and densities into HDR and LDR images, while only the input LDR images are used as the supervision. We collect a new forward-facing HDR dataset
*Work done during an internship at Tencent AI Lab. to evaluate the proposed method. Experimental results on synthetic and real-world scenes validate that our method can not only accurately control the exposures of synthesized views but also render views with a high dynamic range. 1.

Introduction
Novel view synthesis is one of the most pursued topics in computer graphics and computer vision. Limited by the dynamic range of camera sensors and input views, rendered novel views are often with a low dynamic range, while hu-man eyes are able to perceive a much higher dynamic range than what is possible by a regular camera. It is therefore highly desirable to render novel HDR views to improve the overall visual experience.
Recently, a series of works have been focused on recov-ering the radiance field of a scene to render photorealistic novel views using deep neural networks [3,37,42,62]. They implicitly encode volumetric densities and colors using a multi-layer perceptron (MLP), which is termed neural ra-diance field (NeRF). These methods produce high-quality novel views, yet the dynamic range of the obtained radi-ance in NeRF is limited to a low dynamic range (between
0 and 255), while the radiance in the physical world scene often covers a much broader (higher) dynamics range (e.g. from 0 to +∞). We also notice that ‘NeRF in the Dark’ tries to recover radiance field from raw images with noise [40], while it’s different from our method.
High Dynamic Range (HDR) imaging is the set of tech-niques that recover HDR images from multiple LDR im-ages with different exposures [55]. The most common way to reconstruct HDR images is to take a series of LDR images with different exposures at a fixed camera pose and then merge those LDR images into an HDR image
[12, 39, 48]. These methods produce compelling results for tripod-mounted cameras but may lead to ghost artifacts when the camera is hand-held. To overcome the limitations of conventional multi-exposure stack-based HDR synthesis, some deep learning methods have been proposed to solve this problem via a two-stage approach [26, 59]: 1) aligning the input LDR images using optical flow or removing plau-sible motion regions, 2) merging the processed images into an HDR image. However, in cases with large motion, their approach typically introduces artifacts in the final results.
Most critically, these HDR imaging methods are unable to render novel views and the learning-based methods re-quire HDR images as training supervision. To render novel views, some methods try to merge image-based rendering and HDR imaging techniques. [30,35,49,51]. However, the image-based methods struggle from preserving view con-sistency.
In this paper, we propose a method HDR-NeRF to re-cover the high dynamic range neural radiance field from a set of LDR images (Fig. 1a) with various exposures (the exposure is defined as the product of exposure time and radiance). To the best of our knowledge, this is the first end-to-end neural rendering system that can render novel
HDR views (Fig. 1c) and control the exposure of novel LDR views (Fig. 1b). Building upon NeRF, we introduce a dif-ferentiable tone mapper to model the process that radiance in the scene becomes pixel values in the image. We use an
MLP to model the tone-mapping operation. Overall, HDR-NeRF can be represented by two continuous implicit neu-ral functions: a radiance field for density and scene radi-ance and a tone mapper for color, as shown in Fig. 2. Our pipeline enables joint learning of the two implicit functions, which is critical to recovering the HDR radiance field from such sparse sampled LDR images. We use the classical vol-ume rendering technique [25] to accumulate radiance, col-ors, and densities into HDR and LDR images, but we only use LDR ground truth as supervision.
To evaluate our method, we collect a new HDR dataset that contains synthetic scenes and real-world scenes. We compare our method with original NeRF [42], NeRF-W (NeRF in the wild) [37], as well as NeRF-GT (a version of NeRF that is trained from LDR images with consis-tent exposures or HDR images). We provide quantitative and qualitative results and ablation studies to justify our main technical contributions. Our method achieves simi-lar scores across all major metrics on this dataset compared with NeRF-GT. Besides, compared to the recent state-of-the-art NeRF and NeRF-W, our method can render LDR novel views with arbitrary exposures and spectacular novel
HDR views. The main contributions of this paper can be summarized as follows: 1. An end-to-end method HDR-NeRF is proposed to re-cover the high dynamic range neural radiance field from multiple LDR views with different amounts of exposure. 2. The camera response function is modeled, both HDR views and LDR views with varying exposures are ren-dered from the radiance field. 3. A new HDR dataset including synthetic and real-world scenes is collected. Compared with SOTAs, our method achieves the best performance on this dataset.
The dataset and code will be released for further re-search purposes in this community. 2.