Abstract
Prior works on action representation learning mainly fo-cus on designing various architectures to extract the global representations for short video clips.
In contrast, many practical applications such as video alignment have strong demand for learning dense representations for long videos.
In this paper, we introduce a novel contrastive action repre-sentation learning (CARL) framework to learn frame-wise action representations, especially for long videos, in a self-supervised manner. Concretely, we introduce a simple yet efﬁcient video encoder that considers spatio-temporal con-text to extract frame-wise representations. Inspired by the recent progress of self-supervised learning, we present a novel sequence contrastive loss (SCL) applied on two cor-related views obtained through a series of spatio-temporal data augmentations. SCL optimizes the embedding space by minimizing the KL-divergence between the sequence simi-larity of two augmented views and a prior Gaussian dis-tribution of timestamp distance. Experiments on FineGym,
PennAction and Pouring datasets show that our method outperforms previous state-of-the-art by a large margin for downstream ﬁne-grained action classiﬁcation.
Sur-prisingly, although without training on paired videos, our approach also shows outstanding performance on video alignment and ﬁne-grained frame retrieval tasks. Code and models are available at https://github.com/ minghchen/CARL_code. (a) Fine-grained frame retrieval on FineGym dataset. (b) Phase boundary detection on Pouring dataset. (c) Temporal video alignment on PennAction dataset.
Figure 1. Multiple applications of our frame-wise representation learning on various datasets: (a) Fine-grained frame retrieval on
FineGym [37]. (b) Phase boundary detection on Pouring [36]. (c)
Temporal video alignment on PennAction [49]. As shown in the
ﬁgures, the representations obtained through our method (CARL) are invariant to the appearance, viewpoint and background. 1.

Introduction
In the last few years, deep learning for video understand-ing [1, 9, 17, 33, 39, 41, 44, 47] has achieved great success on video classiﬁcation task [9, 19, 40]. Networks such as
I3D [9] and SlowFast [17] always take short video clips
*Accomplished during Minghao Chen’s internship at MSRA.
†Corresponding author. (e.g., 32 frames or 64 frames) as input and extract global representations to predict the action category. In contrast, many practical applications, e.g., sign language transla-tion [4, 5, 13], robotic imitation learning [29, 36], action alignment [6, 21, 23] and phase classiﬁcation [16, 27, 37, 49] require algorithms having ability to model long videos with hundreds of frames and extract frame-wise representations rather than the global features (Fig. 1).
Previous methods [27, 35, 37] have made an effort to learn frame-wise representations via supervised learning, where sub-actions or phase boundaries are annotated. How-ever, it is time-consuming and even impractical to manually label each frame and exact action boundaries [21] on large-scale datasets, which hinders the generalization of mod-els trained with fully supervised learning in realistic sce-narios. To reduce the dependency of labeled data, some methods such as TCC [16], LAV [23] and GTA [21] ex-plored weakly-supervised learning by using either cycle-consistency loss [16] or soft dynamic time warping [21,23].
All these methods rely on video-level annotations and the training is conducted on the paired videos describing the same action. This setting obstructs them from applying on more generic video datasets where no labels are available.
The goal of this work is to learn frame-wise represen-tations with spatio-temporal context information for long videos in a self-supervised manner. Inspired by the recent progress of contrastive representation learning [8, 11, 12, 20], we present a novel framework named contrastive action representation learning (CARL) to achieve our goal. We as-sume no labels are available during training, and videos in both training and testing sets have long durations (hundreds of frames). Moreover, we do not rely on video pairs of the same action for training. Thus it is practical to scale up our training set with less cost.
Modeling long videos with hundreds of frames is chal-lenging. It is non-trivial to directly use off-the-shelf back-bones designed for short video clip classiﬁcation, since our task is to extract frame-wise representations for long videos.
In our work, we present a simple yet efﬁcient video encoder that consists of a 2D network to encode spatial information per frame and a Transformer [42] encoder to model tempo-ral interaction. The frame-wise features are then used for representation learning.
Recently, SimCLR [11] uses instance discrimina-tion [46] as the pretext task and introduces a contrastive loss named NT-Xent, which maximizes the agreement be-tween two augmented views of the same data. In their im-plementation, all instances other than the positive reference are considered as negatives. Unlike image data, videos pro-vide more abundant instances (each frame is regarded as an instance), and the neighboring frames have high seman-tic similarities. Directly regarding these frames as nega-tives may hurt the learning. To avoid this issue, we present a novel sequence contrastive loss (SCL), which optimizes the embedding space by minimizing the KL-divergence between the sequence similarity of two augmented video views and a prior Gaussian distribution.
The main contributions of this paper are summarized as follows:
• We propose a novel framework named contrastive ac-tion representation learning (CARL) to learn frame-wise action representations with spatio-temporal con-text information for long videos in a self-supervised manner. Our method does not rely on any data annota-tions and has no assumptions on datasets.
• We introduce a Transformer-based network to efﬁ-ciently encode long videos and a novel sequence con-trastive loss (SCL) for representation learning. Mean-while, a series of spatio-temporal data augmentations are designed to increase the variety of training data.
• Our framework outperforms the state-of-the-art meth-ods by a large margin on multiple tasks across dif-ferent datasets. For example, under the linear evalua-tion protocol on FineGym [37] dataset, our framework achieves 41.75% accuracy, which is +13.94% higher than the existing best method GTA [21]. On Penn-Action [49] dataset, our method achieves 91.67% for
ﬁne-grained classiﬁcation, 99.1% for Kendall’s Tau, and 90.58% top-5 accuracy for ﬁne-grained frame re-trieval, which all surpass the existing best methods. 2.