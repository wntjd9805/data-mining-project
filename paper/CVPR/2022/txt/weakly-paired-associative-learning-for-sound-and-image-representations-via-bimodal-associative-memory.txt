Abstract
Data representation learning without labels has at-tracted increasing attention due to its nature that does not require human annotation. Recently, representation learn-ing has been extended to bimodal data, especially sound and image which are closely related to basic human senses.
Existing sound and image representation learning methods necessarily require a large number of sound and image with corresponding pairs. Therefore, it is difﬁcult to ensure the effectiveness of the methods in the weakly paired condition, which lacks paired bimodal data. In fact, according to hu-man cognitive studies, the cognitive functions in the human brain for a certain modality can be enhanced by receiving other modalities, even not directly paired ones. Based on the observation, we propose a new problem to deal with the weakly paired condition: How to boost a certain modal representation even by using other unpaired modal data. To address the issue, we introduce a novel bimodal associa-tive memory (BMA-Memory) with key-value switching. It enables to build sound-image association with small paired bimodal data and to boost the built association with the eas-ily obtainable large amount of unpaired data. Through the proposed associative learning, it is possible to reinforce the representation of a certain modality (e.g., sound) even by using other unpaired modal data (e.g., images). 1.

Introduction
Data representation learning without labels is to learn general features from unlabeled data by exploiting automat-ically generated supervisory signals within the data. Since it is highly time-consuming and labor-intensive for people to annotate large-scale data manually, such representation learning methods have received increasing attention in in-dustry and research ﬁelds. In this context, representation learning has been applied to various areas such as computer vision [10, 15, 17], natural language processing [7, 12], and
∗Corresponding author
Figure 1. Concept of the proposed framework. The model can associate one modality (e.g., sound) with a different modality (e.g., image) through BMA-Memory to obtain abundant representations.
Unpaired modality can boost the association between modalities. sound signal processing [4, 39].
Recently, as data samples are acquired in various multi-sensory environments, representation learning methods for bimodal data have been proposed. They aimed to learn feature representation from exploiting correspondence be-tween bimodal data.
In particular, many bimodal repre-sentation learning methods investigated the correspondence between auditory and vision which are closely related to basic human senses. These methods mainly attempted to learn bimodal representations from audio-video [26, 31] or sound-image [34, 38] data without labels. However, exist-ing bimodal representation learning methods require a large number of data with corresponding pairs. Therefore, it is difﬁcult to ensure the effectiveness of the methods in the weakly paired condition, which lacks paired bimodal data.
According to neurobiological studies, the cognitive func-tions related to a certain modality can be enhanced by re-ceiving other modal stimuli in the human brain. There are several cases such as visual stimuli to multisensory cogni-tion [41], auditory stimuli to visual cognition [5], and tactile stimuli to visual cognition [18]. It is possible because hu-mans memorize multisensory modalities and associate them with each other in their brains. Bimodal cognitive functions are closely connected and inﬂuenced by each other.
Based on the observation, we introduce a new problem to deal with the weakly paired condition: How to boost a certain modal representation even by using other unpaired modal data, which has not been properly addressed in pre-vious works. It is needed to devise such a method in order to extend and generalize bimodal representation learning as
In terms of sound-image data, we can the human brain. expect to enhance the image representation even from un-paired sound data and vice versa. Based on this context, we focus on the representations of sound-image level rather than audio-video level because the weakly paired condition is more naturally observed in sound-image data. For exam-ple, we can acquire lots of animal images easily by web searching, whereas it is difﬁcult to obtain animal sound data. In such weakly paired condition, it is worth to rein-force difﬁcult-to-obtain modal (e.g., sound) representation from other easy-to-obtain modal data (e.g., image).
In this paper, we propose a novel bimodal associative memory (BMA-Memory) which enables to learn sound and image representations. BMA-Memory can store bi-modal features in sound-image sub-memories and associate with one another naturally through a key-value switching scheme. Since another modality can be recalled through
BMA-Memory, we can obtain abundant representation that includes both input and associated modalities from single modal input. Based on the memory, we introduce weakly paired associative learning to address weakly paired con-dition, which lacks paired data. BMA-Memory enables to build the sound-image association with small paired bi-modal data and to boost the built association with the eas-ily obtainable large amount of unpaired modal data. In un-paired associative learning, we construct pseudo bimodal pairs from unpaired data to enhance the bidirectional asso-ciation. As a result, the representation of certain modality can be enhanced even by using other unpaired modal data.
The concept of the proposed approach is shown in Figure 1.
The major contributions of the paper are as follows.
• We introduce a novel BMA-Memory with key-value switching to learn sound and image representations.
It stores bimodal sound-image features and associates with one another. It enables to obtain abundant repre-sentations including both input and associated modali-ties even from single modal input.
• We propose weakly paired associative learning to ad-dress the weakly paired condition.
It effectively en-ables to deal with boosting certain modal representa-tion even by using other unpaired modal data in the weakly paired condition. 2.