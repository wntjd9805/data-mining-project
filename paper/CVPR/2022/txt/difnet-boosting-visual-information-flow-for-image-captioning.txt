Abstract
Current Image Captioning (IC) methods predict textual words sequentially based on the input visual information from the visual feature extractor and the partially generated sentence information. However, for most cases, the par-tially generated sentence may dominate the target word pre-diction due to the insufficiency of visual information, mak-ing the generated descriptions irrelevant to the content of the given image. In this paper, we propose a Dual Informa-tion Flow Network (DIFNet1) to address this issue, which takes segmentation feature as another visual information source to enhance the contribution of visual information for prediction. To maximize the use of two information flows, we also propose an effective feature fusion module termed Iterative Independent Layer Normalization (IILN) which can condense the most relevant inputs while retrain-ing modality-specific information in each flow. Experiments show that our method is able to enhance the dependence of prediction on visual information, making word prediction more focused on the visual content, and thus achieves new state-of-the-art performance on the MSCOCO dataset, e.g., 136.2 CIDEr on COCO Karpathy test split. 1.

Introduction
Image captioning is a task of generating a description in natural language based on a given image. It needs a model to understand the given image from multiple aspects, in-cluding identifying objects, actions, as well as relationships, and generate a language description for that image.
*Equal Contribution
†Corresponding Author 1Source code is available at: https://github.com/mrwu-mac/
DIFNet
Figure 1. Comparison between the popular captioning paradigm (top) and the proposed Dual Information FLow Network (DIFNet). (bottom). Compared with existing methods, DIFNet introduces the visual representation of the dual information flow to facilitate reliable and accurate image understanding.
Inspired by the development of neural machine transla-tion, the encoder-decoder framework has been widely used in image captioning tasks. The encoder takes a set of visual features (such as grid feature [10]) extracted by an offline
CNN-based network as input and further encodes them into visual-language space. Then the decoder uses the visual information provided by the encoder and the partially gen-erated caption to predict the next word. Most existing ap-proaches [5, 9, 22] follow this paradigm to build their cap-tioning networks, , as shown in Fig.1 (top). However, they suffer from a main drawback: the visual information from the visual feature extractor is insufficient and sometimes in-accurate. Although the research of feature extractors has made great progress [15, 25], key visual information, such as action and depth information, may still be ignored, even using the powerful visual-language pre-trained models [8].
The above drawback leads to an insufficient visual infor-mation flow for the decoder, forcing the decoder to rely excessively on partially generated captions to predict the rest words in order to ensure the fluency of the generated description. This issue ultimately makes the generated de-scriptions irrelevant to the actual visual content, as shown in
Fig. 1 (top), the baseline model generates incorrect phrase
“talking on a cell phone” because the ‘remote control’ fea-ture is hard to be captured by only grid feature.
To overcome these shortcomings, recent works [15, 19, 31, 37] introduce high-level visual cues, such as concepts, to supplement visual information. However, due to seman-tic inconsistency [17] and spatial misalignment, an addi-tional fusion module is required to align these cues with visual features, which is inefficient and difficult to be com-bined with IC models with grid features. In contrast, this paper considers a new type of cues, i.e. the segmentation map, where region semantics are naturally aligned with grid features. As shown in Fig. 1 (bottom), segmentation map can be regarded as spatial semantic guidance and provide a coarse-grained context for grid features to facilitate image understanding. On the one hand, its pixel-level category in-formation helps correct categories that are misjudged due to unreliable information in the grid features. On the other hand, its spatial information also helps to infer the underly-ing semantic and spatial relationships.
Motivated by this, we propose a Dual Information Flow
Network (DIFNet), which takes the segmentation feature as another visual information source to supplement grid fea-tures, thereby enhancing the contribution of visual informa-tion for reliable prediction. Since it is easy to integrate grid features and segmentation features, we only need a sim-ple fusion method. To maximize the benefit of two visual information flows, we propose an effective feature fusion module named Iterative Independent Layer Normalization (IILN), which can condense the most relevant inputs by a common LN layer while retraining modality-specific infor-mation in each flow via private LN layer. Note that certain visual information that is difficult to be captured might be directly filtered out by the attention layer, we adopt addi-tional skip connections to further enhance the flow of infor-mation within and between the encoder and decoder.
We evaluate our method on the MSCOCO benchmark for image captioning, where the effectiveness of our pro-posals is well validated. In particular, our proposed model achieves the new state-of-the-art performance of MSCOCO.
DIFNet achieves 136.2 CIDEr score on the COCO Karpa-thy test split under the setting of single-model. To gain more insights, we apply Layerwise Relevance Propagation (LRP) [4] to estimate how the visual information and par-tially caption contexts contribute to prediction, whose re-sults demonstrate that our proposed model can enhance the contribution of visual information for prediction.
Our contributions are:
• We propose a Dual
Information Flow Network (DIFNet), which takes the segmentation feature as an additional visual information source. DIFNet can en-hance the contribution of visual content for prediction.
• We propose a feature fusion module termed Iterative
Independent Layer Normalization (IILN), which can condense the most relevant inputs by a common LN layer while retraining modality-specific information in each flow via the private LN layer.
• Experiments show that our method can enhance the dependence of prediction on visual information and achieve significant performance improvements over the state-of-the-art on MSCOCO benchmark. 2.