Abstract
Video inpainting remains a challenging problem to fill with plausible and coherent content in unknown ar-eas in video frames despite the prevalence of data-driven methods. Although various transformer-based architec-tures yield promising result for this task, they still suffer from hallucinating blurry contents and long-term spatial-temporal inconsistency. While noticing the capability of discrete representation for complex reasoning and predic-tive learning, we propose a novel Discrete Latent Trans-former (DLFormer) to reformulate video inpainting tasks into the discrete latent space rather the previous contin-uous feature space. Specifically, we first learn a unique compact discrete codebook and the corresponding autoen-coder to represent the target video. Built upon these rep-resentative discrete codes obtained from the entire target video, the subsequent discrete latent transformer is capa-ble to infer proper codes for unknown areas under a self-attention mechanism, and thus produces fine-grained con-tent with long-term spatial-temporal consistency. More-over, we further explicitly enforce the short-term consis-tency to relieve temporal visual jitters via a temporal aggre-gation block among adjacent frames. We conduct compre-hensive quantitative and qualitative evaluations to demon-strate that our method significantly outperforms other state-of-the-art approaches in reconstructing visually-plausible and spatial-temporal coherent content with fine-grained details.Code is available at https://github.com/
JingjingRenabc/dlformer. 1.

Introduction
Video inpainting aims to fill in corrupted regions with meaningful details such that the completed video is con-sistent both spatially and temporally.
It can be applied to various industrial applications, including video restora-tion [15, 34], unwanted object removal [18, 19] and video retargeting [32]. 1This work was done while Jingjing Ren was an intern at Tencent. 2Qingqing Zheng and Xuemiao Xu are the joint corresponding authors. (a) Input (b) VINet (c) STTN (d) Ours
Figure 1. Previous methods, like (b) VINet [9] and (c) STTN [34] formulate in a continuous feature space and usually produce arti-facts and blurry results around the occluded bars and background.
In contrast, our method (d) fills the unknown region with plausi-ble content even in the swift movement case by formulating this problem in a global discrete latent space (please zoom in for better visualization).
Recently, methods [2, 9, 37] have made great progress in this task thanks to the powerful CNN-based deep features extractors. These methods still suffer from limited recep-tive field along temporal domain and produce blurry and misplacement artifacts in the completed video, as shown in
Figure 1 (b). The state-of-the-art methods [12, 16, 34] tend to capture long-term correspondences with attention mecha-nism, so the available content at distant frames can be glob-ally propagated to the unknown regions. Although these attention-based methods yield promising results, trivially using pair-wise similarity in a continuous feature space, e,g., STTN [34], still suffers from blurry contents (refer to
Figure 1 (c)) degrading the visual quality in high frequency areas. It is still challenging to generate plausible and co-herent contents with fine-grained details, especially under complex and dynamic scenarios.
To tackle the aforementioned challenges, we propose a novel Discrete Latent Transformer (DLFormer) to model the video inpainting task as a code inference problem in a discrete latent space rather than in the continuous feature
space. Benefiting from the Vector Quantized Variational
AutoEncoder (VQ-VAE) [20], continuous representation of one image generated by an autoencoder can be quantized into limited discrete codes in latent space, spanned by a codebook to form a quantized feature. Such discrete codes, represented as the indices in the corresponding codebook, can be delivered back to the autoencoder to reconstruct the original image sufficiently.
Inspired by this work and in order to capture the fine-grained details, we learn a video-specific and discriminative codebook as well as the corre-sponding autoencoder to represent the target video in the discrete latent space, which is spanned by a context-rich and efficient codebook. In this way, the obtained codebook naturally captures global discriminative features among the entire video sequence, even for unknown regions.
Based on this discrete latent representation, inpainting unknown regions with plausible content can be regarded as inferring the proper discrete code indices with a certain codebook. By adopting a self-supervised training strategy, the latent code distribution in valid regions can be naturally propagated to unknown regions via the proposed discrete la-tent transformer. Moreover, to avoid spatial-temporal visual jitters caused by such discrete prediction, we further explic-itly enforce short-term consistency with a residual aggrega-tion block before delivering the code inference results back to the autoencoder to generate the final inpainting results.
We extensively evaluate our method in both video restoration and object removal tasks on Youtube-VOS [31] and DAVIS [24] datasets and the experimental results demonstrate the proposed method significantly outperforms the state-of-the-art methods. Thanks to the robust dis-crete representation, the proposed DLFormer is able to fill visually-plausible and spatial-temporal coherent content with fine-grained details in unknown regions.
We summarize our contributions of this work as follows:
• To the best of our knowledge, we are the first to for-mulate the video inpainting task as a discrete code in-ference problem in the discrete latent space. Benefit-ing from such discrete representation, our method is capable to synthesize more plausible and fine-grained details than previous methods formulating in the con-tinues feature space.
• Based on the aforementioned novel formulation, a dis-crete latent transformer is proposed to explicitly model the global code distribution among the entire video se-quence with a self-attention mechanism. The proposed transformer is allowed to propagate such distribution from valid regions toward corrupted regions regardless of the limited temporal receptive field.
• We further develop a residual temporal aggregation block to relieve temporal visual jitters caused by the discrete prediction across adjacent frames. 2.