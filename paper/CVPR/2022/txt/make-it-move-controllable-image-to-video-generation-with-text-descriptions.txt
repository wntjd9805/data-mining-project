Abstract
Generating controllable videos conforming to user in-tentions is an appealing yet challenging topic in computer vision. To enable maneuverable control in line with user in-tentions, a novel video generation task, named Text-Image-to-Video generation (TI2V), is proposed. With both con-trollable appearance and motion, TI2V aims at generat-ing videos from a static image and a text description. The key challenges of TI2V task lie both in aligning appear-ance and motion from different modalities, and in handling uncertainty in text descriptions. To address these chal-lenges, we propose a Motion Anchor-based video GEnera-tor (MAGE) with an innovative motion anchor (MA) struc-ture to store appearance-motion aligned representation. To model the uncertainty and increase the diversity, it further allows the injection of explicit condition and implicit ran-domness. Through three-dimensional axial transformers,
MA is interacted with given image to generate next frames recursively with satisfying controllability and diversity. Ac-companying the new task, we build two new video-text paired datasets based on MNIST and CATER for evalua-tion. Experiments conducted on these datasets verify the ef-fectiveness of MAGE and show appealing potentials of TI2V task. Datasets are available at https:// github.com/ Youncy-Hu/ MAGE. 1.

Introduction
Video generation has undergone revolutionary changes and has made great progress in recent years. Early research of unconditional video generation [24, 26, 28] focused on how to generate a video from noise or a latent vector from an aligned latent space. Recently, more emphases have been put on controllable video generation [4, 11, 30], which al-lows users to express their intentions about how the scene or the objects look like (appearance information) or how the objects move (motion information). Controllable video generation has many potential applications, including facil-itating designers in artistic creation and assisting machine learning practitioners for data augmentation.
*This work was done while Yaosi Hu was an intern at MSRA.
Figure 1. An illustration of the proposed TI2V task. An image and a detailed text description provide the appearance and motion information for video generation, respectively.
Existing controllable video generation tasks can be grouped into three categories, namely Image-to-Video gen-eration (I2V), Video-to-Video generation (V2V), and Text-to-Video generation (T2V). These tasks provide different ways for users to inject the appearance and the motion in-formation and therefore have different levels of control over these two factors. I2V and V2V have strong control over the appearance of generated video, as separate images are usually provided to set the scene. As for the motion, I2V shows limited controllability since the task is defined to accept only coarse-grained motion clues, such as prede-fined action labels or directions [4]. In contrast, V2V can generate videos with highly controllable motion because detailed motion guidance, such as trajectories [11] or ac-tion sequences [6, 19], are provided in the form of input video. But one drawback of V2V in practical use is that such motion guidance is hard to be obtained. Among all the three tasks, T2V has the weakest control over the generated video. Users provide both appearance and motion informa-tion through text, which is imprecise and sometimes am-biguous. Nevertheless, text description of motion is more in line with human habits [35] and leaves a lot of room for creation and imagination in video generation.
In this paper, we introduce a novel video generation task, named Text-Image-to-Video generation (TI2V). It provides a natural way for users to express their intentions, using a single static image to set the scene and a natural text de-scription to provide motion. TI2V is a more difficult task than I2V or T2V. It not only requires the separate under-standing of text and image, but also needs to align visual objects with corresponding text descriptions, and then trans-form the implied object motion to an explicit video. We aim to achieve two goals in the TI2V task: i) Controllable. Un-der the constraints of image and text, the generated video should have visually consistent appearance set by the given image and semantically aligned motion as described in the text. ii) Diverse. This goal resolves ambiguity and brings creativity, which are important and appealing features for video generation. In the example given in Fig.1, the text description does not specify which cone it wants to pick up and which exact position in “the fourth quadrant” it wants the metal sphere to slide to. Under such “constrained ran-domness”, we want to produce videos that match the de-scription but are also diverse.
We design an auto-regressive framework, named MAGE, to address the TI2V task. A VQ-VAE encoder-decoder ar-chitecture is adopted for efficient visual token representa-tion. The key challenge is how to merge the text-described motion into visual features to generate a controllable and diverse video. To achieve the controllable goal, we pro-pose a spatially aligned Motion Anchor (MA) to integrate the appearance and motion information through the cross-attention operation in the common latent space for image and text. Each position in the MA stores all necessary mo-tion information of the corresponding region for video gen-eration. We further introduce explicit condition and implicit randomness into MA. The explicit condition provides addi-tional constraint from an explicit input (e.g., speed) to im-prove both controllability and diversity, while the implicit randomness brings in uncertainty in the data distribution, allowing the model to generate diverse videos in a stochas-tic way. In the proposed MAGE framework, we adopt axial transformer to inject and fuse MA into visual tokens and generate videos in an auto-regressive manner.
To evaluate TI2V task and our generation model, appro-priate paired video-text datasets are in need. Different from
T2V task that often conducts experiments on action recog-nition datasets like KTH [25] or captioning datasets like
MSR-VTT [38] with action label or coarse-grained caption,
TI2V focuses more on the maneuvering capability to image and requires fine-grained text description. Therefore, we propose two datasets with synthetic videos and fine-grained text descriptions based on MNIST [14] and CATER [10] for
TI2V task. By controlling the uncertainty in descriptions, we can evaluate the performance of both deterministic and diverse video generation.
The contributions of this paper are concluded as follows:
• A novel Text-Image-to-Video generation task (TI2V) is introduced, aiming to generate visually consistent video from an image and a text description.
• A Motion Anchor-based video GEnerator (MAGE) is proposed to generate controllable and diverse videos.
The core structure, motion anchor (MA), addresses the challenging matching problem between the appear-ance in the image and the motion clues in the text.
• Two video-text paired datasets modified from MNIST and CATER are built for the evaluation of TI2V task. Moreover, experiments conducted on these two datasets verify the effectiveness of MAGE. 2.