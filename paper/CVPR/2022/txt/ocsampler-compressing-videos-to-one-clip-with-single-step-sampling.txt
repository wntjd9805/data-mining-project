Abstract
Videos incorporate rich semantics as well as redundant information. Seeking a compact yet effective video repre-sentation, e.g., sample informative frames from the entire video, is critical to efficient video recognition. There have been works that formulate frame sampling as a sequential decision task by selecting frames one by one according to their importance.
In this paper, we present a more effi-cient framework named OCSampler, which explores such a representation with one short clip. OCSampler designs a new paradigm of learning instance-specific video conden-sation policies to select frames only in a single step. Rather than picking up frames sequentially like previous methods, we simply process a whole sequence at once. Accordingly, these policies are derived from a light-weighted skim net-work together with a simple yet effective policy network.
Moreover, we extend the proposed method with a frame number budget, enabling the framework to produce correct predictions in high confidence with as few frames as possi-ble. Experiments on various benchmarks demonstrate the effectiveness of OCSampler over previous methods in terms of accuracy and efficiency. Specifically, it achieves 76.9% mAP and 21.7 GFLOPs on ActivityNet with an impressive throughput: 123.9 Video/s on a single TITAN Xp GPU. 1.

Introduction
With the explosive popularity of social media platforms as well as bountiful online video content, there comes wider attention on effective and scalable approaches that can deal with actions or events recognition in the face of the video data deluge. To this end, most efforts have been devoted to exploring a complicated temporal mod-ule to capture relationships across the time dimension by densely applying 2D-CNNs [11, 20, 22, 29, 34, 42] or 3D-(cid:0): Corresponding author.
Figure 1. Comparisons of other methods and our proposed
OCSampler. Most existing works reduce computational cost by regarding the frame selection problem as a sequential deci-sion task, while OCSampler aims to perform efficient inference by making one-step decision with holistic views. Our method achieves excellent performance on accuracy, theoretical compu-tational expense, and actual inference throughout.
CNNs [3,6,7,10,28,31,32]. Though achieving superior per-formance, the exorbitant computational expense limits the application of these models in real-world scenarios where the deployment is resource-constrained and requires to pro-cess high data volumes with stringent latency and through-put requirements.
To mitigate this issue, a large body of research has been focusing on designing light-weighted modules [9, 22, 27, 28, 33, 33, 40, 47] to bring efficiency improvements. Being unaware of the complexity of video contents and instance-specific difficulties for video recognition, these models treat
all videos equally and adopt naive sampling strategies. To overcome this limitation, extensive studies [8,12,14,39,41] have been conducted to devise adaptive mechanisms of frame selection on a per-video basis by either determining which frame to observe next, or conditional early exiting in a deterministic order. These approaches all model the frame selection problem as a sequential decision task and prefer to make per-frame decisions individually, leaving out the subsequent parts of the video. Thus, these methods re-quire more inference time even with theoretical computa-tional efficiency and lead to sub-optimal results. Recent methods [19, 25, 26, 30, 35, 38] rely on designing different preset transformations (e.g., process at a specific spatial res-olution [25], process at a specific patch [35], etc.) and deter-mining which action should be taken on each frame or net-work module to alleviate computational burden. However, the key to video recognition is aggregating features across different frames. Most of these methods rely on the assump-tion that several salient frames are equally important to an effective video representation for video recognition, which may introduce temporal redundancy and lack specific con-sideration for temporal modeling.
A promising alternative to reduce the computational complexity of video analysis, without sacrifice of recog-nition accuracy, is representing videos with one clip in a single step. Clip-level features [3, 10, 18, 32] commonly used in 3D-CNNs methods reveal the superiority owing to its spatio-temporal information extraction. However, tra-ditional clip-level sampling requires to average the predic-tions of multiple clips, and clips containing visual redun-dancy will pollute the final results.
Inspired by that, we design an efficient video recognition framework that com-presses trimmed/untrimmed videos into a single clip by evaluating a clip-based reward on a per-video basis in one pass. As shown in Figure 1, our basic idea is that modeling the selection problem as a one-step decision task can yield significant savings in both theoretic computation and actual inference time, and sampling an integrated clip is more rea-sonable than evaluating several frames individually.
Particularly, in this paper, we propose a novel OCSam-pler to dynamically localize and attend to the instance-specific condensed clip of each video. More specifically, our method first takes a quick skim over the whole video with a light-weight CNN to obtain coarse global informa-tion. Then we train a simple yet effective policy network to select the most valuable combination of the clip for the subsequent recognition. This module is learnt with rein-forcement learning due to its non-differentiability. Finally, we activate a high-capacity classifier to process the selected clip. Inference on clips constructed with a small number of frames, considerable computation overhead can be saved.
Our method allocates computation unevenly across the tem-poral locations of videos according to their contributions to the recognition task, leading to a significant improvement in efficiency yet still with preserved accuracy.
The vanilla OCSampler framework processes videos us-ing the same number of frames, while the only difference lies in the temporal locations of selected frames. We show that our method can be extended via an adaptive frame bud-get to reduce the computation spent on ”easy” videos, which can be classified precisely with few frames, owing to dis-criminative backgrounds or objects. This is achieved by in-troducing an additional budget network that estimates how many frames should be used for a video, which is optimized by pseudo-labels in a self-supervised way.
[2], Mini-Kinetics
We evaluate the effectiveness of OCSampler on four efficient video recognition benchmarks, namely Activ-ityNet
[17], FCVID [15], Mini-Sports1M [16]. Experimental results show that OCSam-pler consistently outperforms all the state-of-the-art by large margins in terms of accuracy and efficiency. Especially, we achieve 76.9% mAP and 21.7 GFLOPs on ActivityNet with an impressive throughput: 123.9 Video/s on a single TITAN
Xp GPU. We also demonstrate that the frames sampled by our method can be generalized to boost the efficacy and ef-ficiency of an arbitrary classifier. 2.