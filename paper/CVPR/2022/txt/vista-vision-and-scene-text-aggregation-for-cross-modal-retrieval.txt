Abstract
Visual appearance is considered to be the most impor-tant cue to understand images for cross-modal retrieval, while sometimes the scene text appearing in images can provide valuable information to understand the visual se-mantics. Most of existing cross-modal retrieval approaches ignore the usage of scene text information and directly adding this information may lead to performance degra-dation in scene text free scenarios. To address this is-sue, we propose a full transformer architecture to unify these cross-modal retrieval scenarios in a single Vision and
Scene Text Aggregation framework (ViSTA). Speciﬁcally,
ViSTA utilizes transformer blocks to directly encode image patches and fuse scene text embedding to learn an aggre-gated visual representation for cross-modal retrieval. To tackle the modality missing problem of scene text, we pro-pose a novel fusion token based transformer aggregation approach to exchange the necessary scene text information only through the fusion token and concentrate on the most important features in each modality. To further strengthen the visual modality, we develop dual contrastive learning losses to embed both image-text pairs and fusion-text pairs into a common cross-modal space. Compared to existing methods, ViSTA enables to aggregate relevant scene text semantics with visual appearance, and hence improve re-sults under both scene text free and scene text aware sce-narios. Experimental results show that ViSTA outperforms other methods by at least 8.4% at Recall@1 for scene text aware retrieval task. Compared with state-of-the-art scene text free retrieval methods, ViSTA can achieve better accu-racy on Flicker30K and MSCOCO while running at least three times faster during the inference stage, which vali-dates the effectiveness of the proposed framework.
*Equal Contributions. This work is done when Mengjun Cheng is a research intern at Baidu Inc.
†Corresponding author.
Figure 1. Given a text query, two images are close in visual se-mantics for (a) conventional cross-modal retrieval. By consider-ing visual appearance and scene text information, e.g.,“gummy hotdog”, into one framework, (b) the proposed Vision and Scene
Text Aggregation (ViSTA) approach enables to distinguish the se-mantic difference between images I1 and I2 (θ2 < θ1), and can be also adapted to conventional scene text free scenarios. 1.

Introduction
As one of the most important multi-modal understand-ing tasks, cross-modal retrieval attracts much attention due to its valuable applications, e.g., news search and product retrieval. Cross-modal text-to-image retrieval [10, 11, 22] aims to return the most relevant candidate based on the rel-evance between the text content of a query and the visual appearance of an image. The performance of this retrieval task is largely improved by better visual representation and detailed image-text alignment [3, 22, 25, 28].
In recent years, following the success of BERT [7] in natural language modeling, transformer-based single en-coder architectures [5, 15, 16, 20, 23, 26, 29, 36, 44, 47, 49] are adopted to fuse images and text, and image-text pre-training for ﬁne-tuning becomes the mainstream paradigm in modeling visual-language tasks, signiﬁcantly boosting 1
the cross-modal retrieval performance. However, these ap-proaches with deep interactions between images and text are orders of magnitudes slower and hence impractical for the large-scale cross-modal retrieval task. As dual-encoder architectures, CLIP [37], ALIGN [18] and WenLan [17] ex-ploit cross-modal contrastive pre-training by encoding im-ages and text separately, which allows that image and text features can be computed in an ofﬂine setting to efﬁciently calculate similarities between large-scale image-text pairs.
Even though the performance of the cross-modal retrieval task is greatly improved by the million-scale image-text contrastive pre-training [37], it is still difﬁcult and ineffec-tive to learn speciﬁc ﬁne-grained visual concepts, e.g., the scene text semantics from images [37]. More recently, a new cross-modal retrieval task [31] is proposed to enable the usage of scene text in an image together with its visual appearance. Speciﬁcally, an image in this task is paired with the corresponding scene text features to help to determine the similarity between the query’s textual content and the image’s visual appearance plus scene text. Beneﬁting from exploiting additional scene text features, this model can im-prove the cross-modal retrieval accuracy than those exploit-ing only visual appearance. Nevertheless, in a real-world image corpus, there are only a fraction of images contain-ing scene text instances. The model designed for the scene text aware retrieval task might fail to generate reliable sim-ilarities between the query and images without scene text instances, and can not be adapted to the conventional scene text free retrieval task.
To overcome this issue, we propose an effective Vision and Scene Text Aggregation (ViSTA) framework to tackle both scene text aware and scene text free cross-modal re-trieval tasks. Speciﬁcally, ViSTA utilizes a full transformer design to directly encode image patches and fuse scene text embedding to learn an aggregated visual representation.
To enforce each modality focusing on its most important features, we propose a novel token based aggregation ap-proach by sharing the necessary scene text information only through the fusion token. To tackle the modality missing problem of scene text, we further develop dual contrastive supervisions to strengthen the visual modality, and embed both image-text pairs and fusion-text pairs into a common cross-modal space. Compared to existing fusion methods,
ViSTA enables to aggregate relevant scene text semantics with visual appearance, and hence improve results under both scene text free and scene text aware scenarios.
The contributions of this paper are three-fold. 1) We propose a full transformer architecture to effectively ag-gregate vision and scene text, which is applicable in both scene text aware and scene text free retrieval scenarios. 2)
We propose a fusion token based transformer aggregation design to exchange the relevant information among visual and scene text features, and dual contrastive losses to en-hance visual features. 3) The proposed cross-modal re-trieval framework can remarkably surpass existing methods for the scene text aware retrieval task and achieve better per-formance than state-of-the-art approaches on scene text free retrieval benchmarks as well.
To the best of our knowledge, it is the ﬁrst time to solve scene text free and scene text aware cross-modal retrieval tasks with a vision and scene text aggregated transformer. 2.