Abstract
Progress in self-supervised learning has brought strong image representation learning methods. Yet so far, it has
In turn, tasks mostly focused on image-level learning. such as unsupervised image segmentation have not bene-fited from this trend as they require spatially-diverse rep-resentations. However, learning dense representations is challenging, as in the unsupervised context it is not clear how to guide the model to learn representations that cor-respond to various potential object categories. In this pa-per, we argue that self-supervised learning of object parts is a solution to this issue. Object parts are generalizable: they are a priori independent of an object definition, but can be grouped to form objects a posteriori. To this end, we leverage the recently proposed Vision Transformer’s capa-bility of attending to objects and combine it with a spatially dense clustering task for fine-tuning the spatial tokens. Our method surpasses the state-of-the-art on three semantic seg-mentation benchmarks by 17%-3%, showing that our rep-resentations are versatile under various object definitions.
Finally, we extend this to fully unsupervised segmentation – which refrains completely from using label information even at test-time – and demonstrate that a simple method for automatically merging discovered object parts based on community detection yields substantial gains. . 1.

Introduction
Defining what makes an object an object is hard.
In philosophy, Peirce defines an object as anything we can think and talk about [40]. In computer vision, object def-initions for semantic segmentation are more pragmatic and feature various notions of objectness as well as different lev-els of granularity. For instance, the COCO-Stuff benchmark distinguishes between stuff (objects without a clear shape) and things (objects with a “well-defined” shape) [3, 35] and features coarse and fine object categories. Others, like
Code: https://github.com/MkuuWaUjinga/leopart
Figure 1. ViTs and Resnets compared under foreground ex-traction and semantic segmentation. We use Jaccard distance as a measure for foreground extraction. Starting from a DINO ini-tialization, our method, Leopart, closes the performance gap be-tween self-supervised ViTs and their supervised counterparts as well as Resnets. Leopart (CBFE+CD) further improves a ViT’s object extraction capabilities and sets new state-of-the-art for fully unsupervised semantic segmentation.
Cityscapes [12], choose a segmentation that is most infor-mative for a specific application like autonomous driving and therefore also include sky and road as object classes.
This variedness of object definitions is challenging for self-supervised or unsupervised semantic segmentation as human annotations that carry the object definitions are, at most, used at test time. However, the ability to learn self-supervised dense representations is desirable as this would allow scaling beyond object-centric images and allow ef-fective learning on billions more generic images. Further-more, unsupervised segmentation can be highly useful as a starting point for more efficient data labeling, as segmen-tation annotations are even more expensive than image la-belling [35]. To tackle the lack of a principled object defini-tion during training, many methods resort to defining object priors such as saliency and contour detectors to induce a no-tion of objectness into their pretext tasks [9, 28, 30, 49, 56], effectively rendering such methods semi-supervised and po-tentially not generalizeable. In this paper, we instead stay in the fully unsupervised domain and explore a novel yet sim-ple alternative for training densely. We learn object parts (Leopart) through a dense image patch clustering pretext task. Object part learning promises a principled formula-tion for self-supervised dense representation learning as ob-ject parts can be composed to form objects as defined in each benchmark, after generic pretraining.
In this paper, we explore the use of a Vision Trans-former (ViT) with our new loss and excavate its unique apt-ness for self-supervised segmentation. While vision trans-formers have shown great potential unifying architectures and scaling well with data into billions, they have mostly been shown to work for image-level tasks [6, 8, 16] or dense tasks [11, 37, 50, 52] but in a supervised manner. Our work aims to close this gap by self-supervisedly learning dense
ViT models. We combine the recently discovered prop-erty of self-supervised ViTs to localise objects [6] with our dense loss to train spatial tokens for unsupervised segmen-tation.
We validate our method from two different angles: First, we conduct a transferability study and show that our repre-sentations perform well on downstream semantic segmen-tation tasks. Second, we tackle the more challenging fully unsupervised setup proposed in [49] based on directly clus-tering the pixel or patch embeddings. For that, two model characteristics are important: unsupervised foreground ex-traction and a semantically-structured embedding space, see
Figure 1. To our surprise, even though self-supervised ViTs excel at extracting objects, they do not learn a spatial token embedding space that is discriminative for different object categories. On the other hand, ViTs trained under super-vision achieve better semantic segmentation performance, but the attention heads perform poorly at localizing objects.
In contrast, our method outperforms self-supervised ViTs and Resnets in fully unsupervised semantic segmentation as well as in learning transferable dense representations.
Thus, our contributions are as follows:
• We propose a dense clustering pretext task to learn semantically-rich spatial tokens closing the gap be-tween supervised ViTs and self-supervised ViTs.
• We show that a ViT trained with our pretext task learns transferable representations that surpass the state-of-the-art on Pascal VOC, COCO-Thing and COCO-Stuff semantic segmentation at the same time by 17%-3%.
• We develop a novel cluster-based foreground extrac-tion and overclustering technique based on community detection to tackle fully unsupervised semantic seg-mentation and surpass the state-of-the-art by >3%. 2.