Abstract
Memory bank
Space-time memory (STM) based video object segmen-tation (VOS) networks usually keep increasing memory bank every several frames, which shows excellent perfor-mance. However, 1) the hardware cannot withstand the ever-increasing memory requirements as the video length increases. 2) Storing lots of information inevitably intro-duces lots of noise, which is not conducive to reading the most important information from the memory bank.
In this paper, we propose a Recurrent Dynamic Embedding (RDE) to build a memory bank of constant size. Speciﬁ-cally, we explicitly generate and update RDE by the pro-posed Spatio-temporal Aggregation Module (SAM), which exploits the cue of historical information. To avoid error accumulation owing to the recurrent usage of SAM, we pro-pose an unbiased guidance loss during the training stage, which makes SAM more robust in long videos. Moreover, the predicted masks in the memory bank are inaccurate due to the inaccurate network inference, which affects the seg-mentation of the query frame. To address this problem, we design a novel self-correction strategy so that the network can repair the embeddings of masks with different quali-ties in the memory bank. Extensive experiments show our method achieves the best tradeoff between performance and speed. Code is available at https://github.com/
Limingxing00/RDE-VOS-CVPR2022. (cid:1858)(cid:1870)(cid:1853)(cid:1865)(cid:1857) (cid:1846) (cid:3398) (cid:884)(cid:2016) (cid:1858)(cid:1870)(cid:1853)(cid:1865)(cid:1857) (cid:1846) (cid:3398) (cid:2016) (cid:1858)(cid:1870)(cid:1853)(cid:1865)(cid:1857) (cid:1846) (a) STM pattern memory bank
RDE
SAM
SAM (cid:1858)(cid:1870)(cid:1853)(cid:1865)(cid:1857) (cid:1846) (cid:3398) (cid:884)(cid:2016) (cid:1858)(cid:1870)(cid:1853)(cid:1865)(cid:1857) (cid:1846) (cid:3398) (cid:2016) (b) Our SAM pattern memory bank (cid:1858)(cid:1870)(cid:1853)(cid:1865)(cid:1857) (cid:1846)
Figure 1. The inference pipelines of the segmentation of frame c(cid:2) denotes concatenation. θ denotes the sampling interval
T . for the update of the memory bank. (a) shows the network read the space-time memory (STM) pattern memory bank to segment frame T . As the length of videos increases, the STM pattern mem-ory bank has an ever-increasing size. In (b), we update a recurrent dynamic embedding (RDE) to build a memory bank of the con-stant size, which is maintained by a spatio-temporal aggregation module (SAM). 1.

Introduction
Video object segmentation (VOS) is a fundamental task for video understanding, including lots of applications, such as autonomous driving and video editing. This work fo-cuses on semi-supervised VOS setting. In this setting, given the instances annotation of the ﬁrst frame, the VOS algo-rithms segment the instances in other frames.
Matching based networks [5, 13, 18, 20, 23–25, 30, 31,
∗ Equal contribution. † Corresponding author. This work was done during Mingxing Li’s internship at Alibaba. 39, 44] are popular for semi-supervised VOS. These net-works have a memory bank mechanism, which encodes some frames into embeddings and stores those embeddings in the memory bank to assist the segmentation of the query frame. Some methods only use the embeddings of a lim-ited number of frames, such as the ground-truth (GT) frame
[14], the latest frame (for brevity, the latest frame of the query frame is abbreviated as the latest frame) [27] and both of them [20, 24, 39]. These methods do not make full use of historical frames in the video. STM based meth-ods [5, 13, 18, 23, 25, 30, 31, 44] store the embeddings every
several (e.g., 5) frames in the STM pattern memory bank as shown in Figure 1(a). Although STM based methods utilize equal interval sampling to mine the historical information in the video, as the length of videos increases, the STM pattern memory bank has an ever-increasing size and inevitably in-troduces lots of noise. Exponential moving average (EMA) based methods [17, 19, 33] try to address the problems. The
EMA based methods index some pixel embeddings from the embeddings of the query frame and the memory bank ac-cording to certain criteria and fuse these pixel embeddings in the EMA way. However, the EMA based methods have a strong limitation because of the direct summation operation (see details in Sec. 3.1).
In this paper, we address two problems. 1) How to build and update a memory bank of the constant size to effec-tively and efﬁciently store historical information? 2) Ex-cept for the GT frame, other masks are inaccurate owing to the inaccurate network inference, how to correct the poor embedding encoded from the inaccurate masks?
For problem 1, we propose a recurrent dynamic embed-ding (RDE) to provide a richer representation for VOS. As shown in Figure 1(b), to generate and update RDE, we pro-pose a spatio-temporal aggregation module (SAM) to orga-nize the cue of the historical information (previous RDE) and the embedding of the latest frame adaptively. SAM includes three parts: extracting, enhancing and squeezing.
The extracting part is responsible for organizing the spatio-temporal relationship between previous RDE and the em-bedding of the latest frame. Then, the enhancing part re-inforces the spatio-temporal relationship and the squeezing part aggregates and compresses the spatio-temporal infor-mation. We refer to the memory bank maintained by SAM as the SAM pattern memory bank.
One potential risk of the SAM pattern memory bank is the recurrent update of RDE may cause error accumulation.
However, we have no GT for training the generated RDE directly. To tackle this problem, we propose to employ aux-iliary supervision for the distribution of RDE. In the train-ing process, we additionally build a STM pattern memory bank (see Figure 1(a)) to obtain the uncompressed infor-mation and its read results, which are used to estimate the distribution for RDE. Thus we design an unbiased guidance loss to control the approach degree of the two distributions.
Relying on the unbiased guidance loss, the training of the network is more stable and has higher performance with no extra computation overhead for deployment.
For problem 2, we design a novel self-correction strat-egy, which enforces the network to repair the embeddings of masks with different qualities in the memory bank. Speciﬁ-cally, we ﬁrst simulate different perturbated masks and then constrain the embeddings encoded by perturbated masks to be close to the embedding encoded by the GT mask with a mask consistency loss. The mask consistency loss enforce the network to learn the self-correction ability for inaccurate masks in the embedding space during the training stage.
To investigate the effectiveness of the proposed meth-ods, we conduct experiments on DAVIS 2017, DAVIS 2016 and YouTube-VOS 2019. The proposed method achieves state-of-the-art performance on DAVIS 2017 validation set (86.1% J &F, 27 FPS), DAVIS 2017 test set (78.9%
J &F), DAVIS 2016 (91.6% J &F, 35 FPS) and superior performance on YouTube-VOS 2019 (83.3% J &F) with-out the multi-scale inference. Furthermore, we demonstrate the effectiveness of our method in the synthetic long video.
For the synthetic long video, J &F and FPS of our method are almost unchanged as the length of the synthetic long video increases.
Our contributions can be summarized as follows:
• We propose an easy-to-extend recurrent dynamic em-bedding (RDE) to provide a richer representation for
VOS compared with the embedding of the GT frame and the latest frame, which is maintained by the pro-posed spatio-temporal aggregation module (SAM).
• To avoid error accumulation owing to the recurrent us-age of SAM, we propose an unbiased guidance loss during the training stage, which makes SAM more ro-bust in long videos.
• Considering inaccurately predicted masks in the mem-ory bank affect the segmentation performance due to the inaccurate network inference, we design a novel self-correction strategy, which enforces the network to learn the self-correction ability for inaccurate masks in the embedding space.
• Extensive experiments on several benchmarks and the synthetic long video show the effectiveness and supe-riority of our method. 2.