Abstract
A data augmentation module is utilized in contrastive learning to transform the given data example into two views, which is considered essential and irreplaceable.
However, the pre-determined composition of multiple data augmentations brings two drawbacks. First, the artificial choice of augmentation types brings specific representa-tional invariances to the model, which have different de-grees of positive and negative effects on different down-stream tasks. Treating each type of augmentation equally during training makes the model learn non-optimal repre-sentations for various downstream tasks and limits the flex-ibility to choose augmentation types beforehand. Second, the strong data augmentations used in classic contrastive learning methods may bring too much invariance in some cases, and fine-grained information that is essential to some downstream tasks may be lost. This paper proposes a gen-eral method to alleviate these two problems by considering
“where” and “what” to contrast in a general contrastive learning framework. We first propose to learn different aug-mentation invariances at different depths of the model ac-cording to the importance of each data augmentation in-stead of learning representational invariances evenly in the backbone. We then propose to expand the contrast content with augmentation embeddings to reduce the misleading ef-fects of strong data augmentations. Experiments based on several baseline methods demonstrate that we learn better representations for various benchmarks on classification, detection, and segmentation downstream tasks. 1.

Introduction
Contrastive learning has been proved to be able to learn meaningful visual representations without human annota-tions [4, 10]. Original methods regard two views trans-formed from the same example as a positive pair and other examples in the batch or the memory bank as negative sam-*Corresponding Author.
Figure 1. The influence of augmentation types. (a) The proportion of categories in ImageNet where the data augmentation has a pos-itive effect. (b) An example of the flower where color invariance has a negative effect and rotation invariance has a positive effect.
The conclusion is reversed for most categories in ImageNet. ples [30], then the model is trained with the contrastive loss.
Many techniques were previously considered important in this process, such as strong data augmentations [3], the se-lection of negative samples [14, 34], momentum-update en-coder [10,11], and training details like large batch sizes and long training epochs [3]. However, recent works prove that useful visual representations can be learned without neg-ative pairs, the momentum-update of parameters or large batch size [4, 10, 39]. The most indispensable process in contrastive learning is the data augmentation module. The essential principle of contrastive learning is to learn the rep-resentational invariance by making the network learn to be invariant to a set of data augmentations [29].
Previous works show that the composition of multi-ple types of data augmentations is crucial for contrastive learning [3]. A specific set of augmentations is deter-mined after extensive experiments to achieve the best re-sults on large-scale datasets (e.g., ImageNet). In most re-cent works [1,11,16,26,33], the data augmentation pipeline consists of random cropping and resizing, horizontal flip-ping, color jittering, converting to grayscale, and Gaussian blurring. However, the pre-determined and artificial choices of augmentation types and augmentation strength bring the corresponding problems as follows.
In terms of the augmentation type, after selecting specific
been proposed in unsupervised contrastive learning.
This paper proposes a generic method to tackle these two augmentation-related problems by considering “where” and “what” to contrast. First, we propose to treat vari-ous data augmentations differently and learn the “hierarchi-cal augmentation invariance”. By computing multiple con-trastive losses at different depths of the encoder, we make the fundamental augmentation invariances more widely dis-tributed and some generally insignificant invariances re-stricted to the deeper layers. By restricting the impact scope of data augmentation without weakening its strength, we demonstrate that adding a specific type of data augmen-tation that was not used in the classical augmentation set can simultaneously improve the performance on both large-scale datasets (e.g., ImageNet, COCO) and fine-grained datasets (e.g., VGG Flowers, iNaturelist-2019). Second, we propose to expand the contrast content with augmentation embeddings. By augmenting the original labels with input transformation, the label augmentation [18] method in su-pervised learning relaxes specific transformation invariant constraints and prevents the loss of transformation-related information. Inspired by this, we regard each view as the
“label” of the other view and expand the extracted view fea-tures with corresponding augmentation embeddings. The encoded augmentation information helps to reduce the un-necessary invariance and make up for some lost fine-grained information. The small network for embedding augmenta-tion parameters is simultaneously trained with the encoder and is discarded during the inference. Our analysis demon-strates that the augmentation embeddings learn useful infor-mation of specific augmentations and benefit the represen-tation learning in various benchmarks.
We apply our method to several baseline contrastive learning architectures and evaluate the representations on various classification, detection, and segmentation bench-marks. Our results reveal that the proposed method consis-tently improves the performance compared to baselines on various downstream tasks. 2.