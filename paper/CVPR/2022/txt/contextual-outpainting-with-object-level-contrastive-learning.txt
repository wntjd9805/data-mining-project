Abstract 1.

Introduction
We study the problem of contextual outpainting, which aims to hallucinate the missing background contents based on the remaining foreground contents. Existing image out-painting methods focus on completing object shapes or ex-tending existing scenery textures, neglecting the semanti-cally meaningful relationship between the missing and re-maining contents. To explore the semantic cues provided by the remaining foreground contents, we propose a novel
ConTextual Outpainting GAN (CTO-GAN), leveraging the semantic layout as a bridge to synthesize coherent and di-verse background contents. To model the contextual corre-lation between foreground and background contents, we in-corporate an object-level contrastive loss to regularize the learning of cross-modal representations of foreground con-tents and the corresponding background semantic layout, facilitating accurate semantic reasoning. Furthermore, we improve the realism of the generated background contents via detecting generated context in adversarial training. Ex-tensive experiments demonstrate that the proposed method achieves superior performance compared with existing so-lutions on the challenging COCO-stuff dataset. Project page: https://ddlee-cn.github.io/cto-gan.
*Corresponding author: zwxiong@ustc.edu.cn.
Image outpainting, also referred to as image extrapola-tion or image extension, is a long-lived task in computer vi-sion. Many real-world scenarios have a strong demand for high-quality image extrapolations, like simulating different views of the current visual content in virtual reality. Early image outpainting methods rely on a retrieval and stitch-ing process to extend image patches [18, 49, 69]. Recently, learning-based methods have made impressive progress in synthesizing visually pleasing results [13, 21, 53, 61]. How-ever, existing image outpainting methods mainly focus on completing object shapes or extending existing scenery tex-tures. The contextual relationship between foreground and background contents remains unexplored.
In this work, we study a variant of the outpainting prob-lem, named contextual outpainting, which aims to synthe-size coherent and natural background contents from the re-maining foreground contents, as shown in Fig. 1. As hu-mans, it is easy for us to hallucinate the empirical con-text given common objects, since we relate objects with their context unconsciously in everyday life. There are many potential applications of contextual outpainting tech-niques, such as generating plausible backgrounds for the salient objects in online advertising, ﬁlm making, and aug-mented reality. However, for machines, the task of contex-tual outpainting is signiﬁcantly harder than the previous im-age completion tasks (i.e. inpainting and outpainting) in two ways. Firstly, the assumption of information redundancy is violated because the foreground and background contents share almost nothing in common in terms of appearance.
Secondly, to utilize the constraint provided by the remain-ing foreground contents, it is necessary to understand the correlations inside the scene at the semantic level.
To address the above obstacles, we utilize the semantic layout as a bridge and exploit the contextual correlation be-tween foreground and background contents in a generative way. Speciﬁcally, as shown in Fig. 2, we propose a novel
ConTextual Outpainting GAN (CTO-GAN), that infers the possible semantic layout from the foreground contents ﬁrst and then synthesizes the corresponding background con-tents under its guidance. We predict diverse semantic lay-outs from the remaining foreground contents with a Varia-tional Auto-Encoder (VAE).
To better model the contextual correlation between fore-ground and background contents at the semantic level, we propose an object-level contrastive loss to assist the learn-ing of representations of the foreground contents and the background semantic layout. Speciﬁcally, we encode the features of foreground pixels and background semantic lay-outs into the same cross-modal embedding space and regu-larize the learning of their representations in a “relating-by-contrasting” paradigm, where the network is encouraged to pull the given foreground contents to the coherent semantic layouts and push out-of-context semantic layouts away.
Furthermore, to prevent the discriminator from making lazy decisions merely based on the untouched foreground contents, we incorporate an additional context-aware dis-criminator to detect which region of the generated image is fake, making it harder for the generator to fool the dis-criminator and thus improving the quality of generated im-ages. We conduct extensive experiments on the challenging
COCO-stuff dataset [5] and show that our method is able to generate coherent and diverse background contents, outper-forming existing solutions. 2.