Abstract
Auxiliary loss is additional loss besides the main branch loss to help optimize the learning process of neural net-works. In order to calculate the auxiliary loss between the feature maps of intermediate layers and the ground truth in the ﬁeld of semantic segmentation, the size of each fea-ture map must match the ground truth. In all studies us-ing the auxiliary losses with the segmentation models, from what we have investigated, they either use a down-sampling function to reduce the size of the ground truth or use an up-sampling function to increase the size of the feature map in order to match the resolution between the feature map and the ground truth. However, in the process of select-ing representative values through down-sampling and up-sampling, information loss is inevitable. In this paper, we introduce Class Probability Preserving (CPP) pooling to al-leviate information loss in down-sampling the ground truth in semantic segmentation tasks. We demonstrated the supe-riority of the proposed method on Cityscapes, Pascal VOC,
Pascal Context, and NYU-Depth-v2 datasets by using CPP pooling with auxiliary losses based on seven popular seg-mentation models.
In addition, we propose See-Through
Network (SeeThroughNet) that adopts an improved multi-scale attention-coupled decoder structure to maximize the effect of CPP pooling. SeeThroughNet shows cutting-edge results in the ﬁeld of semantic understanding of urban street scenes, which ranked #1 on the Cityscapes benchmark. 1.

Introduction
Increasing the depth of the convolutional neural network can introduce optimization difﬁculties as shown in [6, 24].
To solve this vanishing gradient problem in the ﬁeld of im-age classiﬁcation, directly connected auxiliary classiﬁers were used to provide extra gradients into some intermediate layers. The auxiliary classiﬁers have been widely used in the variations of the Inception architecture of GoogLeNet
∗Indicates equal contribution
†Corresponding author
Figure 1. Visualizations of the effect of Class Probability Preserv-ing (CPP) pooling. The images are from Cityscapes. For the oc-cluded train (in the red box), the occluded building (the bus stop in the orange box), and the poles in the distance (in the yellow box), the DeepLabV3+ [5] model w/ CPP shows ﬁner and stronger acti-vation in the Grad-CAM [34] visualization (the left column) than the DeepLabV3+ original model. In the inference results (the right column), the w/ CPP model shows correct and ﬁne segmentation.
The enlarged views are provided (the yellow box).
[38] and ResNet [18], where the motivation was to pro-vide useful gradients directly to intermediate layers as extra supervision during training to resist the vanishing gradient problem in very deep networks.
Similar attempts have been made in the ﬁeld of semantic segmentation. For example, [53, 55, 56] used ResNet-101 as a backbone and adopted an auxiliary head in the inter-mediate layer as speciﬁed in [53]. Since the feature map of the auxiliary head connected layer and ground truth are used to calculate the auxiliary loss, their resolutions should be the same. The auxiliary head is responsible for re-sizing the resolution of the intermediate feature map to that of the ground truth through a re-sizing operation.
Figure 2. Visualization of the effects of the conventional pooling (i.e. nearest neighbor [NN] pooling) and CPP pooling. NN pooling has been widely used for down-sampling ground truth for segmentation task since the ground truth ids are integer. A semantic ground truth from the Cityscapes dataset is used. As the scale factor of the pooling increases, more information is lost. Especially, in NN pooling, far object and boundary information are lost a lot, while they still remain as a probability in CPP pooling.
However, designing the auxiliary loss with the re-sizing operations has an inevitable ﬂaw. Each layer in deep neu-ral networks learns different data-driven features. For ex-ample, closer layers to the input image learn simpler low-dimensional features. As one moves away from the input layer, higher-order complex features are learned. If we re-size the ground truth resulting in one-dimensional pixel-wise class information to get the auxiliary loss for the inter-mediate layers, it would not be enough to guide the various-order features of each layer. For this reason, researchers in this ﬁeld decorate these re-sizing guided losses with the word auxiliary and use them as auxiliary solutions rather than complete solutions. For the same reason, the majority of scene segmentation papers [5, 14, 17, 29, 46, 48] that have been published recently do not aggressively use auxiliary loss (mostly using no or one auxiliary head).
Motivated by this limitation, we introduce Class Proba-bility Preserving (CPP) pooling to provide well-designed auxiliary guides, which help optimize the training process of the networks. CPP pooling keeps the class information within a receptive ﬁeld as a probability preventing the information loss (Figure 3). Through the comprehensive experiments with seven popular semantic segmentation models on various datasets such as Cityscapes [9], Pascal
VOC [11], Pascal Context [26] and NYU-Depth-v2 [28], we demonstrated that the proposed method is a model agnostic method that is simply applicable to any model but effective. In addition, we introduce See-Through Network (SeeThroughNet) with an improved multi-scale attention-coupled decoder structure to maximize the effect of CPP pooling.
SeeThroughNet shows state-of-the-art results in the ﬁeld of semantic understanding of high-resolution urban street scenes. The results of ablation and transfer learning experiments are also provided.
The main contributions of this paper are:
• We introduce Class Probability Preserving (CPP) pool-ing to alleviate information loss, where it keeps the class information within a receptive ﬁeld as a proba-bility.
• We demonstrated seven popular semantic segmenta-tion models with CPP pooling on Cityscapes, Pascal
VOC, Pascal Context, and NYU-Depth-v2 datasets to show the effect of the proposed method.
• We propose
SeeThroughNet with multi-scale attention-coupled decoder maximizing the effect of CPP pooling, and achieved the ﬁrst place on the
Cityscapes leaderboard. 2.