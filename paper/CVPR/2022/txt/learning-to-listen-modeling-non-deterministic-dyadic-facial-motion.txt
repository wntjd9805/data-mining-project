Abstract 1.

Introduction
We present a framework for modeling interactional com-munication in dyadic conversations: given multimodal inputs of a speaker, we autoregressively output multiple possibili-ties of corresponding listener motion. We combine the mo-tion and speech audio of the speaker using a motion-audio cross attention transformer. Furthermore, we enable non-deterministic prediction by learning a discrete latent repre-sentation of realistic listener motion with a novel motion-encoding VQ-VAE. Our method organically captures the mul-timodal and non-deterministic nature of nonverbal dyadic interactions. Moreover, it produces realistic 3D listener fa-cial motion synchronous with the speaker (see video). We demonstrate that our method outperforms baselines quali-tatively and quantitatively via a rich suite of experiments.
To facilitate this line of research, we introduce a novel and large in-the-wild dataset of dyadic conversations. Code, data, and videos available at https://evonneng.github.io/ learning2listen/.
“Thus the body of the speaker dances in time with his speech. Further, the body of the listener dances in rhythm with that of the speaker!”
— CONDON AND OGSTON, 1966
When we speak, it is rarely in a void — rather, there is often a listener at the other end of the conversation. As a speaker, we are acutely aware of what the listener is doing.
A slight off-sync motion or a diverted look may throw us off, suggesting the listener is bored or otherwise preoccu-pied, leaving us feeling misunderstood [36]. Indeed, success-ful conversations rely on a coordinated dance between the speaker and the listener in which the two signal to each other that they are communicating with one another and not with anyone else [36]. This chameleon effect [12] of nonverbal mimicry during conversation results in smoother interactions, increases the liking between interaction partners, establishes rapport [38], and may even predict the long term outcome of psychotherapy [49]. Interestingly, nonverbal feedback from a listener, such as head movement is more central to keep-ing a conversation flowing than content-based replies [11].
In this work, we propose a computational framework that can similarly provide nonverbal feedback in response to a speaker in a contextual and timely manner. Such an ability
is critical for virtual agents to meaningfully interact with humans, for whom nonverbal communication is central from infancy [54].
Modeling nonverbal feedback during dyadic interaction is a difficult problem, as listener responses are nondeterminis-tic in nature. Moreover, speakers are inherently multimodal, as they communicate both verbally via speech, and nonver-bally via face and body motion. Capturing interaction in its natural setting requires addressing both challenges. The task of modeling human conversations has a long history. How-ever, unlike traditional rule-based methods [5, 10, 22, 29] or methods that rely on modeling hand-defined simple motion characteristics such as smiles [51] or head nods [22, 29], we wish to model the true complexity of the interaction. This is hard to achieve and generalize using conventional database methods that generate motion via a lookup into a database of ground truth motion [35, 52, 58]. We, therefore, learn to model these dyadic conversational dynamics implicitly in a data-driven way by directly observing human conversations in in-the-wild videos.
Given a video of a speaker, we extract their speech audio, and facial motion (Figure 1(left)). We combine information from both modalities using a motion-audio cross-attention transformer. From this multimodal speaker input, we learn to autoregressively synthesize multiple modes of motion representing different possible responses of a listener who moves synchronously with the speaker (Figure 1(right)).
Modeling the nondeterminism in listener responses is a key element in capturing conversational dynamics. Previous attempts to tackle this problem applied various techniques but fell short of achieving realistic outputs [33]. We propose to learn a realistic manifold of listener motion by quantizing the space of listener motion with a novel sequence-encoding
VQ-VAE [56], which efficiently captures a wide range of motion in a discrete format that is well-suited for learning.
To the best of our knowledge, we are the first to extend VQ-VAE models to the domain of motion synthesis. The learned discrete codebook of listener motion allows us to predict a multinomial distribution of future motion. From this distribu-tion we can sample a wide range of possible modes of motion representing different perceptually-plausible listeners, cap-turing their inherent non-deterministic nature. Furthermore, we demonstrate our learned discrete latent codes can stay on the manifold of realistic motion ensuring no motion drift occurs even in long-horizon predictions. Meanwhile, the autoregressive nature of our method allows us to consider speaker sequences of any length.
To support our data-driven approach to modeling hu-man conversation, data is needed in the form of video-taped dyadic interactions where both parties are ideally filmed from a head-on frontal view. This kind of data is hard to come by. While the first investigation of interac-tional synchrony in conversation dates back to Condon and
Ogston in 1966 [15], current studies still mostly rely on in-lab footage [13, 19, 24, 29] or small-scale motion-capture datasets [7, 33]. Notable exceptions are [17, 44], yet the footage has not been made publicly available. We collect a large-scale source of data in the form of split-screen recorded online interviews where the speaker and listener are captured in frontal view. Our dataset, which consists of 72 hours of in-the-wild conversations, enables the investigation of dyadic communication using the latest machine learning methods.
We evaluate the synthesized listener motion compared to ground truth as well as baseline methods and ablations via an extensive quantitative study. We employ a wide array of metrics to test the realism and diversity of the synthesized motion, and the synchronization of the listener’s motion with that of the speaker. While measuring realism and di-versity centers on the generated motion of the listener in isolation, synchrony captures aspects of the dyad as a whole.
We further corroborate our qualitative findings by inviting human observers to evaluate our results. While we assess our method using the raw 3D mesh output, we additionally illustrate our results by translating the 3D output to pixels for viewing purposes only, as synthesized video provides a richer perceptual context. Under both quantitative and qual-itative measures, our method significantly outperforms all baselines. Our synthesized listeners were deemed plausible by human observers when compared to ground-truth motion.
This highlights our method’s ability to produce realistic-looking motion that is synchronous with a given speaker.
Our main contribution is in our learning-based approach towards understanding human interactional communication in conversation. We combine multimodal speaker inputs via motion-audio cross-attention. We extend vector quantiza-tion to the domain of motion synthesis and learn a quan-tized space of motion in which we autoregressively predict multiple modes of perceptually realistic listener motion. To support future endeavors in this direction we publicly re-lease a novel dataset of 72 hours of in-the-wild dyadic con-versational videos with detailed 3D annotations capturing subtleties in expression and fine-grain head motion. 2.