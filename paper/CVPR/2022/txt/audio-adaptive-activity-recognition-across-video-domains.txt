Abstract
This paper strives for activity recognition under domain shift, for example caused by change of scenery or camera viewpoint. The leading approaches reduce the shift in activ-ity appearance by adversarial training and self-supervised learning. Different from these vision-focused works we lever-age activity sounds for domain adaptation as they have less variance across domains and can reliably indicate which ac-tivities are not happening. We propose an audio-adaptive en-coder and associated learning methods that discriminatively adjust the visual feature representation as well as address-ing shifts in the semantic distribution. To further eliminate domain-speciﬁc features and include domain-invariant ac-tivity sounds for recognition, an audio-infused recognizer is proposed, which effectively models the cross-modal inter-action across domains. We also introduce the new task of actor shift, with a corresponding audio-visual dataset, to challenge our method with situations where the activity ap-pearance changes dramatically. Experiments on this dataset,
EPIC-Kitchens and CharadesEgo show the effectiveness of our approach. Project page: https://xiaobai1217. github.io/DomainAdaptation. 1.

Introduction
The goal of this paper is to recognize activities such as eating, sleeping or cutting under domain shift caused by change of scenery, camera viewpoint or actor, as shown in Figure 1. Existing solutions align distribution-shifted domains inside a single visual video network by adversarial training [5,20,27,29] and self-supervised learning [9,22,34].
Although successful, projecting the visual features from different source and target domains into a shared space can make the ability of the model to distinguish between classes in the target domain suffer. We observe that activity sounds can act as natural domain-invariant cues, as they carry rich activity information while exhibiting less variance across domains. We thus propose a video model which adapts to video distribution shifts with the aid of sound.
Many have considered sound in addition to visual analy-sis for activity recognition within a single domain [18, 24,25,
*Currently at Terminus Group, China.
Source domain
Target domain
Scenery shift
Cutting
Viewpoint shift
Sleeping
Actor shift
Eating
Figure 1. We recognize activities under domain shifts, caused by change of scenery, camera viewpoint or actor, with the aid of sound. 28, 31, 38, 39, 45, 46, 49]. For instance, both Gao et al. [18] and Korbar et al. [24] reduce the computational cost by pre-viewing the audio track, while Lee et al. [25] show that com-bining visual features with audio can better localize actions.
However, the cross-modal correspondences become harder to discover when shifting domains, causing existing cross-modal fusion schemes to degrade in performance. Yang et al. [48] and Planamente et al. [30] propose to directly fuse visual and audio features or predictions for cross-domain activity classiﬁcation. However, the effectiveness of these methods is reduced when not all activities make a charac-teristic sound. Different from previous works, we introduce audio-adaptive learning methods and a cross-modal interac-tion that utilizes the reliable domain-invariant cues within sound to help the video model adapt to the distribution shift.
We make three contributions in this paper. First, we propose an audio-adaptive encoder which exploits the rich information from sound to adjust the visual feature repre-sentation causing the model to learn more discriminative features in the target domain. This is done by preventing the model from over-ﬁtting to domain-speciﬁc visual con-tent, while simultaneously dealing with imbalanced seman-tic distributions between domains. Second, we introduce an audio-infused recognizer, which eliminates domain-speciﬁc features further and allows effective cross-modal interaction across domains by considering domain-invariant activity information within sound. As a third contribution, we in-troduce the new task of actor shift, and a corresponding audio-visual video dataset ActorShift, to challenge our ap-proach when the change in actors results in large variation in activity appearance. Experiments on EPIC-Kitchens [12],
CharadesEgo [33] and ActorShift, demonstrate the advantage of our approach under various video distribution shifts for both audible and silent activities. 2.