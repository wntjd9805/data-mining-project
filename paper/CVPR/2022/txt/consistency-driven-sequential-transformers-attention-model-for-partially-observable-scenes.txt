Abstract
Most hard attention models initially observe a complete scene to locate and sense informative glimpses, and pre-dict class-label of a scene based on glimpses. However, in many applications (e.g., aerial imaging), observing an en-tire scene is not always feasible due to the limited time and resources available for acquisition. In this paper, we de-velop a Sequential Transformers Attention Model (STAM) that only partially observes a complete image and pre-dicts informative glimpse locations solely based on past glimpses. We design our agent using DeiT-distilled [44] and train it with a one-step actor-critic algorithm. Further-more, to improve classification performance, we introduce a novel training objective, which enforces consistency be-tween the class distribution predicted by a teacher model from a complete image and the class distribution predicted by our agent using glimpses. When the agent senses only 4% of the total image area, the inclusion of the proposed consistency loss in our training objective yields 3% and 8% higher accuracy on ImageNet and fMoW datasets, respec-tively. Moreover, our agent outperforms previous state-of-the-art by observing nearly 27% and 42% fewer pixels in glimpses on ImageNet and fMoW. 1.

Introduction
High-performing image classification models such as Ef-ficientNet [42], ResNet [15] and Vision Transformers (ViT)
[13] assume that a complete scene (or image) is available for recognition. However, in many practical scenarios, a com-plete image is not always available at once. For instance, an autonomous agent may acquire an image only partially and through a series of narrow observations. The reasons may include a small field of view, high acquisition cost, lim-ited time for acquisition, or limited bandwidth between the sensor and the computational unit. Often, an agent would have partially acquired an image, and the system must per-form recognition based on incomplete information. Models trained on complete images prove inefficient while classify-Figure 1. Schematic diagram of Sequential Transformers Atten-tion Model (STAM). We divide an image (X) into equally-sized non-overlapping glimpses. STAM sequentially observes informa-tive glimpses (gt) from an image. While never observing an image entirely, STAM predicts the class-label of an image (y) based on glimpses. At each t, our agent encodes past glimpses and their locations (g0:t, l0:t) into a Markov state st. It uses state st to pre-dict class distribution p(yt|st) and attention policy π(lt+1|st). We sample the next glimpse location lt+1 from π(lt+1|st). ing incomplete images. For example, the accuracy of DeiT-Small [44] drops by around 10% when 50% of the image re-gions are unavailable [25]. Moreover, they cannot perform autonomous sensing.
Many developed autonomous agents that acquire a series of most informative sub-regions from a scene to perform classification from partial observations [5, 14, 24, 27]. Most existing scalable approaches [14, 27, 45] initially glance at an entire scene to locate the informative sub-regions. How-ever, in practice, glancing at an entire scene is not always feasible. Examples include time-sensitive rescue operations using aerial imagery, an autonomous car driving in a new territory, and a medical expert probing a tissue to find ab-normalities. We develop an autonomous agent that predicts locations of the most informative regions, called glimpses, without observing the entire scene initially. Starting from observing a glimpse at a random location, the autonomous agent decides which location to attend next solely based on the partial observations made so far.
We design our autonomous agent using a transformer ar-chitecture [13, 44, 47] and call it Sequential Transformers
Attention Model (STAM). Transformers efficiently model long-range dependencies and are ideal for aggregating in-formation from distant glimpses. At any given time, our
agent predicts an optimal location for the next glimpse and class-label of an image based on the glimpses col-lected so far. As glimpse acquisition is a discrete and non-differentiable process, we train our agent using reinforce-ment learning (RL). Further, we propose an additional train-ing objective where the agent is required to predict a class distribution from a set of glimpses consistent with the class distribution predicted from a complete image. To do so, we employ a teacher transformers model to predict the class distribution from a complete image and our agent (a student model) tries to reproduce this distribution using partial ob-servations. We perform experiments on two large-scale real-world datasets, namely, ImageNet [34] and fMoW [10].
Our main contributions are as follows.
• We develop a transformers-based RL agent called
STAM, which actively senses glimpses from a scene and predicts class-label based on partial observations.
Instead of locating informative glimpses by observ-ing an entire image, our agent sequentially predicts the next most informative glimpse location based on past glimpses.
• We propose a consistency-based training objective, where the agent must predict a class distribution con-sistent with the complete image using only partial ob-servations. With only 4% of the total image area ob-served, our proposed objective yields 8% gain in accuracy on ImageNet and fMoW, respectively. 3% and
∼
∼
• Our agent that never observes a complete image out-performs previous methods that initially glance at an entire image to locate informative glimpses. It starts exceeding the previous state-of-the-art while sensing 27% and 42% fewer pixels in glimpses on ImageNet and fMoW, respectively. 2.