Abstract
Neural image caption generation (NICG) models have received massive attention from the research community due to their excellent performance in visual understand-ing. Existing work focuses on improving NICG model ac-curacy while efﬁciency is less explored. However, many real-world applications require real-time feedback, which highly relies on the efﬁciency of NICG models. Recent re-search observed that the efﬁciency of NICG models could vary for different inputs. This observation brings in a new attack surface of NICG models, i.e., An adversary might be able to slightly change inputs to cause the NICG mod-els to consume more computational resources. To further understand such efﬁciency-oriented threats, we propose a new attack approach, NICGSlowDown, to evaluate the ef-ﬁciency robustness of NICG models. Our experimental re-sults show that NICGSlowDown can generate images with human-unnoticeable perturbations that will increase the
NICG model latency up to 483.86%. We hope this research could raise the community’s concern about the efﬁciency robustness of NICG models. 1.

Introduction
Neural Image Caption Generation (NICG) models have received wide attention from both academia and industry in recent years [1, 9, 36–38]. NICG model combines computer vision and natural language processing techniques for im-age understanding and textual description generation. De-signing NICG models is a challenging task but could have a massive impact in the real world [1, 7, 22, 30], such as helping people with visual impairment to understand visual inputs, enhancing the accuracy of image search engines, or transferring images to text/audio in social media, etc.
Real-world applications rely on real-time feedback (e.g., transferring image to audio for people with visual im-pairment, generating context caption of camera feed for robot). In such application scenarios, the responsiveness of NICG models is crucial. However, existing NICG tech-niques mainly focus on improving model accuracy or de-fending the adversarial accuracy-based attacks [6,9,36–38].
Whether the NICG model can maintain efﬁciency under ad-versarial pressure is still a blank domain.
In order to study the efﬁciency robustness of NICG mod-els, the ﬁrst thing we need to do is to ﬁgure out what factors will affect NICG model efﬁciency. In this paper, we investi-gate a natural property of NICG models. The NICG model producing output tokens is a Markov Process; hence the number of underlying decoder calls is non-deterministic.
Thus, the computational consumption of NICG models is naturally non-deterministic. This natural property discloses a potential vulnerability of NICG models. Adversaries may be able to design speciﬁc adversarial inputs to increase com-putational cost in NICG models signiﬁcantly. Such efﬁ-ciency vulnerability could lead to severe outcomes in real-world scenarios. For example, efﬁciency-based attacks may cause a large magnitude of redundant computational re-sources and affect the user experience, such as increasing the device battery consumption or extending the response latency. In this paper, we plan to investigate such potential vulnerability by answering the following questions:
Can we make unnoticeable modiﬁcations to image inputs to signiﬁcantly increase the computational consumption of NICG models and degrade the model efﬁciency? If so, how severe the efﬁciency degrada-tion can be?
Existing work on adversarial machine learning (ML) [3, 4, 10, 23, 24, 27, 29, 32] can not answer the aforementioned questions because of the following two reasons: (i) existing adversarial attacks mainly focus on the classiﬁcation DNN model, whose output is a deterministic numeric vector rep-resenting the likelihood for different categories. In contrast, our target model is the NICG model, whose output genera-tion process is a non-deterministic Markov process, and the
output is a sequence of numeric vectors. Existing accuracy-based adversarial ML techniques can not handle the depen-dency in the Markov process. Furthermore, (2) the goal of efﬁciency robustness evaluation is to increase the compu-tational cost to detect the possible computational resources leakage while existing accuracy-based work seeks to maxi-mize the DNNs errors. The natural difference between these two goals requires a totally new design of the optimization function for efﬁciency robustness evaluation.
In this paper, we propose a new methodology,
NICGSlowDown, to generate efﬁciency-oriented adversar-ial inputs for evaluating the NICG model efﬁciency ro-bustness. These adversarial inputs contain unnoticeable per-turbations and consume more computation resources than original inputs in NICG models. To be speciﬁc, NICGSlow-Down will apply the minimal perturbation on the benign inputs that could minimize the likelihood of End Of Sen-tence (EOS) token and delay the appearance of EOS ac-cordingly.
Evaluation. To evaluate the effectiveness of NICGSlow-Down, we perform NICGSlowDown on four subject mod-els with two datasets, Flickr8k [19], and MS-COCO [25].
We compare NICGSlowDown against six baseline tech-niques, including two accuracy-based attack algorithms and four natural image corruptions. To represent the efﬁciency degradation severity, we deﬁne I-Loops and I-Latency met-rics to measure the increment of the decoder calls of the target models and CPU/GPU response latency caused by NICGSlowDown and baselines. The evaluation results show that NICGSlowDown has achieved performance far exceeding all baselines on all subjects, increasing the loop numbers, CPU/GPU latency of NICG model up to 483.86%, 198.76% and 290.40% respectively.
Contribution. Our contributions are formalized as below:
• We state a new vulnerability of NICG models.
The computational consumption of NICG models is volatile for different inputs, thus the adversaries can decrease the efﬁciency of NICG models by increasing the computational resource consumption.
• We propose a new methodology to evaluate the efﬁ-ciency robustness of NICG models. To the best of our knowledge, NICGSlowDown is the ﬁrst technique to measure the efﬁciency robustness for NICG models. 2.