Abstract
From CNN, RNN, to ViT, we have witnessed remarkable advancements in video prediction, incorporating auxiliary inputs, elaborate neural architectures, and sophisticated training strategies. We admire these progresses but are con-fused about the necessity: is there a simple method that can perform comparably well? This paper proposes SimVP, a simple video prediction model that is completely built upon
CNN and trained by MSE loss in an end-to-end fashion.
Without introducing any additional tricks and complicated strategies, we can achieve state-of-the-art performance on five benchmark datasets. Through extended experiments, we demonstrate that SimVP has strong generalization and ex-tensibility on real-world datasets. The significant reduction of training cost makes it easier to scale to complex scenar-ios. We believe SimVP can serve as a solid baseline to stim-ulate the further development of video prediction. 1.

Introduction
A wise person can foresee the future, and so should an intelligent vision model do. Due to spatio-temporal infor-mation implying the inner laws of the chaotic world, video prediction has recently attracted lots of attention in climate change [71], human motion forecasting [3], traffic flow pre-diction [69] and representation learning [55]. Struggling with the inherent complexity and randomness of video, lots of interesting works have appeared in the past years. These methods achieve impressive performance gain by introduc-ing novel neural operators like various RNNs [65â€“69,71] or transformers [48, 70], delicate architectures like autoregres-sive [21, 28, 49, 55, 71] or normalizing flow [73], and apply-ing distinct training strategies such as adversarial training
[1, 8, 39, 40, 51, 52, 58, 64]. However, there is relatively little understanding of their necessity for good performance since many methods use different metrics and datasets. More-over, the increasing model complexity further aggravates this dilemma. A question arises: can we develop a simpler
*Equal contribution model to provide better understanding and performance?
Deep video prediction has made incredible progress in the last few years. We divide primary methods into four categories in Figure. 1, i.e., (1) RNN-RNN-RNN (2) CNN-RNN-CNN (3) CNN-ViT-CNN, and (4) CNN-CNN-CNN.
Some representative works are collected in Table. 1, from which we observe that RNN models have been favored since 2014.
Figure 1. Different architectures for video prediction. Red and blue lines help to learn the temporal evolution and spatial depen-dency. SimVP belongs to the framework of CNN-CNN-CNN, which can outperform other state-of-the-art methods.
In this context, lots of novel RNNs are proposed. Con-vLSTM [71] extends fully connected LSTMs to have con-volutional structures for capturing spatio-temporal corre-lations. PredRNN [67] suggests simultaneously extract-ing and memorizing spatial and temporal representations.
MIM-LSTM [69] applies a self-renewed memory module to model both non-stationary and stationary properties. E3D-LSTM [66] integrates 3D convolutions into RNNs. Phy-Cell [18] learns the partial differential equations dynamics in the latent space.
Recently, vision transformers (ViT) have gained tremen-dous popularity. AViT [70] merges ViT into the autoregres-sive framework, where the overall video is divided into vol-umes, and self-attention is performed within each block in-dependently. Latent AViT [48] uses VQ-VAE [44] to com-RNN-RNN-RNN
CNN-RNN-CNN
CNN-ViT-CNN
CNN-CNN-CNN 2014-2015 2016-2017 2018-2019 2020-2021
[41, 55, 71]
[38, 47, 63, 67]
[22, 43, 56, 65, 69, 74]
[68]
[36, 46]
[3, 9, 15, 32, 62]
[6, 10, 61, 66, 73]
[18, 23]
--[70]
[48]
[40]
[20, 35, 59]
[16, 29, 72]
[7, 54]
Table 1. Some representative video prediction works since 2014. press the input images and apply AViT in the latent space to predict future frames.
In contrast, purely CNN-based models are not as favored as the approaches mentioned above, and fancy techniques are usually required to improve the novelty and perfor-mance, e.g., adversarial training [29], teacher-student dis-tilling [7], and optical flow [16]. We admire their signif-icant advancements but expect to exploit how far a simple model can go. In other words, we have made much progress against the baseline results, but have the baseline results been underestimated?
We aim to provide a simpler yet better video prediction model, namely SimVP. This model is fully based on CNN and trained by the MSE loss end-to-end. Without intro-ducing any additional tricks and complex strategies, SimVP can achieve state-of-the-art performance on five benchmark datasets. The simplicity makes it easy to understand and use as a common baseline. The better performance provides a solid foundation for further improvements. We hope this study will shed light on future research. 2.