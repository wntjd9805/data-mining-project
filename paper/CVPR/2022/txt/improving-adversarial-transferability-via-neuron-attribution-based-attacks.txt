Abstract
Deep neural networks (DNNs) are known to be vulner-able to adversarial examples. It is thus imperative to de-vise effective attack algorithms to identify the deficiencies of
DNNs beforehand in security-sensitive applications. To effi-ciently tackle the black-box setting where the target model’s particulars are unknown, feature-level transfer-based at-tacks propose to contaminate the intermediate feature out-puts of local models, and then directly employ the crafted adversarial samples to attack the target model. Due to the transferability of features, feature-level attacks have shown promise in synthesizing more transferable adversarial sam-ples. However, existing feature-level attacks generally em-ploy inaccurate neuron importance estimations, which de-teriorates their transferability. To overcome such pitfalls, in this paper, we propose the Neuron Attribution-based At-tack (NAA), which conducts feature-level attacks with more accurate neuron importance estimations. Specifically, we first completely attribute a model’s output to each neuron in a middle layer. We then derive an approximation scheme of neuron attribution to tremendously reduce the computa-tion overhead. Finally, we weight neurons based on their attribution results and launch feature-level attacks. Exten-sive experiments confirm the superiority of our approach to the state-of-the-art benchmarks. Our code is available at: https://github.com/jpzhang1810/NAA . 1.

Introduction
Deep neural networks (DNNs) have been deployed in many safety-critical real-world applications, such as au-tonomous driving and medical diagnosis. However, recent research shows that DNNs are vulnerable to adversarial at-tacks [30], which add human-imperceptible perturbations to clean images to mislead DNNs. It is thus imperative to de-vise effective attack algorithms to identify the deficiencies
*Corresponding author.
Benign Image
Source Model Attention Target Model Attention
Adversarial Image
Source Model Attention Target Model Attention
Figure 1. Visualization of model attentions on both the benign image and adversarial image generated by our method. The atten-tions of both the source model and target model change dramati-cally on the adversarial image compared with the benign image. of DNNs beforehand, which serves as the first step to im-prove their robustness.
There are generally two categories of adversarial at-tacks: white-box and black-box attacks. Attackers under the white-box setting can fetch the structures and parameters of the target models to craft adversarial examples. In contrast, under the black-box setting, attackers have no access to the model structure and parameters. In real-world applications, the DNN models are generally deployed in the black-box situation. Therefore, we focus on black-box attacks in this work.
Black-box attacks can be roughly divided into query-based and transfer-based schemes. Query-based methods approximate the gradient information by queries [1, 14, 32] to generate adversarial examples. However, query-based methods are impractical since large quantities of queries are not allowed in reality. As a result, researchers turn to efficient transfer-bases attacks [6, 7, 9, 19, 38], which em-ploy white-box attacks to attack a local surrogate model,
and then directly transfer the resultant adversarial samples to the target model.
Instead of directly manipulating the local model’s final output, feature-level transfer-based at-tacks propose to destroy the intermediate feature maps of local models. Since the most critical features are shared among different DNN models [8,22], feature-level transfer-based attacks have shown promise in relieving the over-fitting issue and synthesizing more transferable adversarial samples [33].
However, existing feature-level transfer-based attacks still have limited transferability due to the reliance on inap-propriate neuron importance measures. NRDM [22] views all neurons as important neurons and tries to maximize the distortion of neuron activation after attacks. However, in a middle layer, there are positive and negative features that promote and suppress the correct prediction of models, re-spectively. As a result, maximizing the feature distortion destroys positive and negative features at the same time, while the negative features should be enhanced for gener-ating adversarial samples. FDA [8] differentiates the polar-ity of neuron importance by mean activation values. Un-fortunately, it still attaches the same importance to all neu-rons except for their signs. FIA [33] measures the neuron importance by the multiplication of neuron activation and back-propagated gradients. However, the back-propagated gradient on the original input suffers from the problem of saturation [5].
To address the drawbacks of existing feature-level transfer-based attacks, in this paper, we propose Neuron
Attribution-based Attack (NAA), which conducts feature-level attacks based on more accurate neuron importance measures. Specifically, inspired by the neuron attribu-tion method [5], we first attempt to completely attribute a model’s output to each neuron. It ensures that our neuron attribution results possess a good property of completeness that the sum of all the neuron attribution results equals to the output value. Consequently, the attribution results can accurately reflect the attribution of each neuron to the out-put, taking into consideration both the polarity and mag-nitude of neuron importance. From Figure 1, our attack method finds the important features on the mouse instead of the lake, which means our method can accurately find more important features that can craft more transferable ad-versarial examples. However, directly utilizing neuron attri-bution method is intractable due to extensive computation consumption. We then devise an approximation approach to conduct neuron attribution to tremendously reduce the computation cost. Finally, we weight each neuron accord-ing to their attribution results, and endeavor to minimize the weighted feature output. Comprehensive experiments con-firm the superiority of our method. Our contributions are:
• We deploy neuron attribution method to better mea-sure neuron importance when launching feature-level attacks. We further devise an approximation for neu-ron attribution, which largely reduces the time con-sumption and promotes the attack efficiency.
• Based on the proposed neuron importance mea-sure, we devise a novel feature-level attack, Neu-ron Attribution-based Attack (NAA), to overcome the drawbacks of existing feature-level attacks and im-prove the transferability of adversarial examples.
• Comprehensive experiments validate the effectiveness and efficiency of our method. We can achieve state-of-the-art performance on attacking both undefended and defended models. 2.