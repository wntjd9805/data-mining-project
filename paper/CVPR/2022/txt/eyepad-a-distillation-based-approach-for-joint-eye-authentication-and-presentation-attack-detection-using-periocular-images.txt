Abstract
A practical eye authentication (EA) system targeted for edge devices needs to perform authentication and be ro-bust to presentation attacks, all while remaining compute and latency efﬁcient. However, existing eye-based frame-works a) perform authentication and Presentation Attack
Detection (PAD) independently and b) involve signiﬁcant pre-processing steps to extract the iris region. Here, we in-troduce a joint framework for EA and PAD using periocular images. While a deep Multitask Learning (MTL) network can perform both the tasks, MTL suffers from the forget-ting effect since the training datasets for EA and PAD are disjoint. To overcome this, we propose Eye Authentication with PAD (EyePAD), a distillation-based method that trains a single network for EA and PAD while reducing the effect of forgetting. To further improve the EA performance, we introduce a novel approach called EyePAD++ that includes training an MTL network on both EA and PAD data, while distilling the ‘versatility’ of the EyePAD network through an additional distillation step. Our proposed methods outper-form the SOTA in PAD and obtain near-SOTA performance in eye-to-eye veriﬁcation, without any pre-processing. We also demonstrate the efﬁcacy of EyePAD and EyePAD++ in user-to-user veriﬁcation with PAD across network back-bones and image quality. 1.

Introduction
Eye Authentication (EA) using irises has been widely used for biometric authentication. With the current ad-vancements in head-mounted technology, eye-based au-thentication is likely to become an essential part of authen-ticating users against their wearable devices. While highly accurate, EA systems are also vulnerable to ’Presentation
Attacks’ (PA) [14, 47]. These attacks seek to fool the au-thentication system by presenting artiﬁcial eye images, such as printed iris images of an individual [5, 16], or cosmetic contacts [21]. While researchers have proposed methods to train networks that achieve SOTA performance in either
Figure 1. EA pipelines segment out the iris region from the pe-riocular image and normalize the iris image before feeding it to the network. Segmentation/detection is also used in most PAD pipelines. We train a single network for both EA and PAD without any pre-processing steps, using the entire periocular image.
EA or PAD, a practical eye-based biometric system that can be used on edge devices must be able to perform both of these tasks accurately and simultaneously, with low latency and in an energy efﬁcient manner. Therefore, in this work we propose strategies to develop a single deep network for both EA and PAD using periocular images.
One of the key steps in EA is pre-processing. Most EA frameworks [45, 53] use an auxiliary segmentation network to extract the iris region from the periocular image. The iris is then unwrapped into a rectangular image and is fed to the eye authentication system. This geometric normal-ization step was ﬁrst proposed in [7]. Pre-processing is also an important step in iris PAD pipelines that requires another segmentation network [14, 47], or a third party iris detection software [38, 41]. Such pre-processing steps potentially make the EA and PAD pipelines computationally expen-sive, making it impractical to embed these biometric sys-tems on edge devices with limited computational resources.
We investigate and propose techniques to perform EA and
PAD using the entire periocular image without any active pre-processing. In doing so, we adhere to our goal of using a single network in a truer sense.
For a given subject, irises of left and right eyes demon-strate different textural patterns.
In most of the existing works in EA [45, 53], the CNN models are trained on the left irises for classiﬁcation. During evaluation, a right iris is veriﬁed against the right irises of the same or other sub-jects. We refer to this evaluation method as ‘eye-to-eye ver-iﬁcation’. However, a more practical protocol would be to perform user-to-user veriﬁcation, i.e. consider both left and right eyes of a given test subject (query user) and verify it against one or more pairs of left-right irises of same or dif-ferent user (i.e. gallery user). To this end, we propose a new evaluation protocol to match the the left-right pair of a query user with that of a gallery user.
We consider the problem of EA and PAD as a disjoint multitask learning problem because the authentication task presumes real images, which is why the current datasets for
EA do not include PAD labels. A possible single-network solution is to train a deep multitask network for both tasks, alternately training the EA and PAD branches with their respective dataset each iteration (as done in [37]). How-ever, several works [18, 22, 27] have shown that Multitask
Learning (MTL) frameworks for disjoint tasks demonstrate forgetting effect (see Sec. 3). Hence, we propose two novel knowledge distillation-based techniques called Eye-PAD and EyePAD++ to incrementally learn EA and PAD tasks, while minimizing the forgetting effect. In summary, we make the following contributions in this work: 1. We propose a user-to-user veriﬁcation protocol that can be used to authenticate one query user against one or many samples of a gallery user. This is more practical than the existing protocol for eye-to-eye veriﬁcation. 2. To the best of our knowledge, we are the ﬁrst to explore the problem of EA and PAD using a single network. We introduce a new metric called Overall False Rejection
Rate (OFRR) to evaluate the performance of the entire system (EA and PAD), using only authentication data. 3. We propose a distillation-based method called Eye Au-thentication with Presentation Attack Detection (Eye-PAD) for jointly performing EA and PAD. To further improve the veriﬁcation performance, we propose Eye-PAD++. EyePAD++ inherits the versatility of the Eye-PAD network through distillation and combines it with the speciﬁcity of multitask learning. EyePAD++ con-sistently outperforms the existing baselines for MTL, in terms of OFRR. We show the efﬁcacy of Eye-PAD and EyePAD++ across different network back-bones (Densenet121 [20], MobilenetV3 [19] and HR-net64 [44]), and image quality degradation (blur and noise). Additionally, we apply our methods to jointly perform eye-to-eye veriﬁcation and PAD, following the commonly used train-test protocols. Although the cur-rent SOTA approaches use pre-processing, our proposed
Method
IrisCode [29]
Ordinal [42]
UniNet [53]
DRFnet [45]
[35]
[14]
[33]
DensePAD [47]
[17]
D-net-PAD [41]
[3]
PBS, A-PBS [11]
EyePAD (ours)
EyePAD++ (ours)
EA PAD (cid:51) (cid:51) (cid:51) (cid:51) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51)
Pre-processing
Segmentation, geometric normalization
Segmentation, geometric normalization
Segmentation, geometric normalization
Segmentation, geometric normalization
Segmentation, geometric normalization
Segmentation, geometric normalization
Cropping
Segmentation, geometric normalization
Segmentation with UIST [38]
Detection with VeriEye
Detection with [2]
None
None
None
Table 1. Pre-processing steps in recent EA/PAD frameworks methods outperform the existing SOTA in PAD task, and obtain comparable user-to-user veriﬁcation performance without any pre-processing. 2.