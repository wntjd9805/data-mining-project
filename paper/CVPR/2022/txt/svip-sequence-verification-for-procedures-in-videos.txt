Abstract
In this paper, we propose a novel sequence verification task that aims to distinguish positive video pairs performing the same action sequence from negative ones with step-level transformations but still conducting the same task. Such a challenging task resides in an open-set setting without prior action detection or segmentation that requires event-level or even frame-level annotations. To that end, we carefully reorganize two publicly available action-related datasets with step-procedure-task structure. To fully investigate the effectiveness of any method, we collect a scripted video dataset enumerating all kinds of step-level transformations in chemical experiments. Besides, a novel evaluation met-ric Weighted Distance Ratio is introduced to ensure equiva-lence for different step-level transformations during evalua-tion. In the end, a simple but effective baseline based on the transformer encoder with a novel sequence alignment loss is introduced to better characterize long-term dependency between steps, which outperforms other action recognition methods. Codes and data will be released1. 1.

Introduction
In recent years, short-form videos filming people’s daily life widely disseminated on social media, which leads to the spurt of activity videos and greatly facilitated the research on video understanding [6, 14, 32, 55, 69, 85] as well. One can see from these videos that most daily activities are ac-complished by serial steps instead of a single step. Such se-quential steps form a procedure of which key-steps obey in-trinsic consistency, while different participants may accom-plish the same activity by different procedures with step-† Corresponding Author 1https : / / github . com / svip - lab / SVIP - Sequence -VerIfication-for-Procedures-in-Videos
Figure 1. Comparison between traditional action tasks and se-quence verification. (a) Action recognition datasets generally con-sist of various action categories containing videos; (b) Results of action detection methods on different datasets in recent years.
[a/b] means the corresponding dataset contains a action classes and b action instances per video; (c) Sequence verification dataset aims to verify the procedures in the same task. Cross-task verifi-cation is dropped due to its simplicity. level divergence, as shown in Figure 1 (c). In this paper, we advocate a novel action task sequence verification which intends to verify whether the procedures in two videos are step-level consistent, which can be applied to multiple po-tential tasks such as instructional training and performance
scoring. To better demonstrate this task, we define related terms specifically. Step: a human-action or human-object-interaction atomic unit that is always labeled by a verb, a noun, and even prepositions, e.g., ’remove the old wrapper’; procedure: a sequence of steps performed in the chronolog-ical order to accomplish a certain goal, e.g., ’remove the old wrapper - wrap with the new wrapper’; task: an activ-ity that needs to be accomplished within a defined period of time or by a deadline, e.g., ’Rewrap battery’ and ’Change the car tire’ in COIN. We note that a task can be accom-plished by various procedures; video: each video performs one procedure of a certain task; P/N pairs: two videos per-forming an identical procedure form a positive pair, while those performing different procedures from the same task form a negative pair.
Why do we need sequence verification? Traditional ac-tion tasks such as action recognition [52,90,104], action lo-calization [11, 50, 75] and action segmentation [24, 47, 100] have achieved significant progress due to the development of CNN as well as recently prevalent visual transformer
[20]. However, most of these tasks follow a close-set set-ting with a limitation of predefined categories, illustrated in Figure 1 (a). Besides, accurate annotations of steps in numerous videos are extremely time-consuming and labor-intensive, followed by boundary ambiguities that have been studied in recent work [46, 72, 85, 107] though. However, our proposed sequence verification task circumvents both of these problems by verifying any video pair according to their distance in embedding space. In this way, the sequence verification task neither requires the predefined labels nor consumes the intensive step annotations, which can easily handle the open-set setting.
As shown in Figure 1 (c), our proposed sequence ver-ification aims to verify those procedures with semantic-similar steps rather than being associated with totally ir-relevant tasks, which enables it to concentrate more on action step association rather than background distinction.
Thus, an appropriate dataset is crucial to perform this task well. However, existing trimmed video datasets such as
UCF101 [82], Kinetics [10], and Moments in Time [59] etc. are leveraged to carry out single-label action recognition.
On the other hand, untrimmed video datasets like EPIC-KITCHENS [15], Breakfast [43], Hollywood Extended [7],
ActivityNet [8] provide videos composed by multiple sub-actions and the corresponding step annotations, but they do not collect videos that especially performs similar or iden-tical procedures. Thus, they cannot be used directly for se-quence verification. To this end, we rearrange some datasets such as COIN [85] and Diving48 [51] where each task con-tains multiple videos recording different procedures, and each video has step-level annotations. Generally, videos with the same procedure are assigned to an individual cate-gory for training. Positive pairs and negative ones for test-ing are collected within the same procedure and cross dif-ferent procedures in the same task, respectively. It should be noticed that these unscripted videos in the same proce-dure could be with a large appearance variance due to back-ground divergence and personal preference, which makes sequence verification more challenging. Apart from that, we introduce a scripted filming dataset performing chem-ical procedures, where it includes all kinds of step-level transformations such as deletions, additions, and order ex-changes. Thus, the effectiveness of any algorithm can be well justified by this newly proposed dataset. Additionally, since more step transformations may lead to larger feature distances which is unfair compared to less ones, we intro-duce a new evaluation metric Weighted Distance Ratio to make sure that every negative pair will be counted equally regardless of its step-level difference during evaluation.
As an unprecedented task, sequence verification may be solved by off-the-shelf action detectors [10,13,25,30,39,49, 57,62–64,77,83,84,94,96]. However, their performance on
Charades [78] or AVA [32], shown in Figure 1 (b), is not sat-isfactory to conduct step-level detection before verification.
Although [3, 5, 11, 28, 53, 54, 74, 75, 91, 95–97, 99, 101, 102] perform well on ActivityNet [8] or THUMOS14 [38], it lacks persuasion since the two datasets either contain a few action classes or action instances per video. Thus, we intro-duce a simple but effective baseline CosAlignment Trans-former (abbreviated as CAT), which leverages 2D convolu-tion to extract discriminative features from sampled frames and utilizes a transformer encoder to model inter-step tem-poral correlation in a video clip. Whereas representing the whole video with multiple steps as a single feature vector may lose information corresponding to the order of steps in a procedure. Thus, we introduce a sequence alignment loss that aligns each step in a positive video pair via the co-sine similarities between two videos. The results show that our proposed method significantly outperforms other action recognition methods in the sequence verification task.
We summarize our contributions as follows: i) Problem setting: We propose a new task, sequence verification. To our knowledge, this is the first task focusing on procedure-level verification between videos. ii) Benchmark: We rearrange two unscripted video datasets with significant diversity and propose a new scripted dataset with multiple step-level transformations to support this task. Moreover, a new evaluation metric is in-troduced especially for this novel task. iii) Technical contributions: We propose a simple but effective baseline that contains a transformer encoder to ex-plicitly model the correlations between steps. Besides, a sequence alignment loss is introduced to improve the sen-sitivity to step disorder and absence. This novel baseline significantly outperforms other action recognition methods.
Dataset
COIN-SV
Diving48-SV
CSV
# Tasks 36 1 14
# Videos 2,114 16,997 1,940
# Steps 749 24 106
# Procedures 37 / 268 / 285 20 / 20 / 8 45 / 25 / -# Split Videos 1,221 / 451 / 442 6,035 / 7,938 / 3,024 901 / 1,039 / -# Split Samples 21,741 / 1,000 / 400 50,000 / 1,000 / 400 8,531 / 1,000 / -Table 1. The statistical information of three datasets. It is listed with an order of training, testing, and validation. 2.