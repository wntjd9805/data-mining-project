Abstract
Outside-knowledge visual question answering (OK-VQA) requires the agent to comprehend the image, make use of relevant knowledge from the entire web, and digest all the information to answer the question. Most previ-ous works address the problem by first fusing the image and question in the multi-modal space, which is inflexible for further fusion with a vast amount of external knowl-edge. In this paper, we call for an alternative paradigm for the OK-VQA task, which transforms the image into plain text, so that we can enable knowledge passage retrieval, and generative question-answering in the natural language space. This paradigm takes advantage of the sheer vol-ume of gigantic knowledge bases and the richness of pre-trained language models. A Transform-Retrieve-Generate framework (TRiG) framework is proposed1, which can be plug-and-played with alternative image-to-text models and textual knowledge bases. Experimental results show that our TRiG framework outperforms all state-of-the-art super-vised methods by at least 11.1% absolute margin. 1.

Introduction
The visual question answering (VQA) task is to provide a natural language answer to a natural language question given an image [2]. This task has been well studied in the research communities, and numerous cross-modal methods have achieved state-of-the-art performance [6,14,21,30,31, 34, 50, 63, 67, 69]. The knowledge-based visual question answering (KB-VQA) task requires more extensive learn-ing since the questions can be answered only by referring to external general knowledge [35, 48, 48, 57, 58]. Most
KB-VQA datasets come with pre-defined knowledge bases, and each question is annotated with at least one supporting knowledge fact. Moreover, the recently proposed outside-knowledge visual question answering (OK-VQA) task is the most open in the sense that any external knowledge can be 1The code of this work will be made public.
Figure 1. An intuitive example of our TRiG framework on the
OK-VQA problem. Our Framework transform all information into language space and performs retrieved-based question answering through generative language models. used to answer the questions.
Consider the example in Figure 1. As a human, one needs to first identify objects like giraffes and trees in the image, and associate the giraffes to the word animal in the question. Second, the human needs to apply his/her ac-quired commonsense knowledge about giraffe’s character-istics and answer the question that giraffe is known for hav-ing a long neck. For machine learning models to solve the same problem, there are several unique challenges. First, in order to answer such a question, one has to align the image, the question, and the vast amount of knowledge passages into one common space. One solution is to first fuse the im-age and question information in the multi-modal space with pre-trained vision-language models, and then inject knowl-edge into the multi-modal space. Most previous work on
OK-VQA follow this paradigm, including directly injecting the knowledge embeddings [12,49] and fusing the output of a vision-language model with the knowledge graph through graph convolutional network [39]. However, this paradigm is at the cost of squeezing the rich representation of the textual knowledge, in the magnitude of hundreds of mil-lions, into a much smaller multi-modal space. Comparing to knowledge corpus such as BookCorpus (800M words)
and English Wikipedia (2,500M words), multi-modal pre-training datasets are much smaller such as Visual Genome with 0.01 million images and less than 2 million question-answer pairs [27], which leads to less knowledge. There-fore, we argue that it is possible to transform everything into the language space first, and then take advantage of the tremendous amount of textual knowledge for question an-swering. Although this seems counter-intuitive, our work proves its advantage. In this paradigm, the challenge is to be able to transform the image into language with minimum information loss. In order to tackle this, we propose three-level image-to-text transformations which significantly out-perform baselines that use only captions or object labels.
The second challenge of the OK-VQA task is how to effectively retrieve the most relevant knowledge passages from gigantic knowledge bases. Previous work has ex-plored various retrieval methods such as term-based BM25
[37], and network-based ranking [37, 61]. In the OK-VQA dataset, this task is problematic in that there is no ground-truth knowledge annotation for each question. The re-trieval has to rely on either transfer learning from similar knowledge-retrieval tasks or weak supervision from pseudo signals such as whether the passage contains the answer tokens [41]. Our preliminary study finds that there is no guarantee that a passage containing the ground-truth an-swer will essentially relate to the question or help the an-swer prediction. Such signals are very weak and may intro-duce more noise than useful information into the retrieval model.
Instead, we adopt the state-of-the-art dense pas-sage retrieval model (DPR) [24] that is pre-trained on large question-answering dataset Natural Questions (NQ) [28] as our knowledge retriever, which is shown to outperform the
BM25 method in terms of retrieval coverage rate.
The third challenge of the OK-VQA task is to consoli-date all the multi-source input, namely the question, visual context, and the retrieved knowledge passages, to predict answers. Since now everything is in the language space, the problem can be formulated as a multi-passage question an-swering problem. More specifically, the model needs to not only rank the retrieved passages but also predict an answer according to the ranked passages. Most existing work uti-lizes extractive methods to predict the answer span in the passage [5, 7, 29, 44, 45, 60, 62]. This is not applicable in the OK-VQA dataset because there is neither annotation of ground-truth passage nor answer span in any passage. In-stead, we use the generative question answering model [19] to avoid the defect in span prediction. Furthermore, we use beam-search for robust answer generation. Lastly, since the question-answering model is the last stage in the entire framework, any information distortion or loss in the image-to-text transformation and knowledge retrieval would prop-agate to the final question answering model. Therefore, it is important for the final question answering model to be more transparent and interpretable to diagnose the root cause of errors. We use cross-attention scores from the decoder of the generative model to rank and highlight the top support-ing knowledge passages, which helps to interpret the results of the model.
To bridge the above-mentioned research gaps, we pro-pose the Transform-Retrieve-Generate (TRiG) framework for the OK-VQA task. At the high level, the framework aligns all the information (image, question, and knowledge) into the language space in order to take advantage of the rich semantics of textual knowledge. The framework starts with three-level image-to-text transformations, followed by dense passage retrieval to retrieve the most relevant knowl-edge passages. Further, the TRiG aggregates the informa-tion from all passages and generates an answer that is rela-tively easy to interpret. Our contributions are as follows:
• We propose a new paradigm shift for the OK-VQA task, from aligning all the information in the multi-modal space, to first transforming an image into plain text and performing knowledge retrieval and question answering all in language space.
• We propose a robust framework Transform-Retrieve-Generate (TRiG), that achieves new state-of-the-art performance on the OK-VQA dataset and leading other supervised methods by 11.1%. 2.