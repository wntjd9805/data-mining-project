Abstract
We present a deformable prototypical part network (De-formable ProtoPNet), an interpretable image classifier that integrates the power of deep learning and the interpretabil-ity of case-based reasoning. This model classifies input images by comparing them with prototypes learned during training, yielding explanations in the form of “this looks like that.” However, while previous methods use spatially rigid prototypes, we address this shortcoming by proposing spatially flexible prototypes. Each prototype is made up of several prototypical parts that adaptively change their rel-ative spatial positions depending on the input image. Con-sequently, a Deformable ProtoPNet can explicitly capture pose variations and context, improving both model accu-racy and the richness of explanations provided. Compared to other case-based interpretable models using prototypes, our approach achieves state-of-the-art accuracy and gives an explanation with greater context. The code is available at https://github.com/jdonnelly36/Deformable-ProtoPNet. 1.

Introduction
Machine learning has been adopted in many domains, in-cluding high-stakes applications such as healthcare [2, 29], finance [46], and criminal justice [3]. In these critical do-mains, interpretability is essential in determining whether we can trust predictions made by machine learning models.
In computer vision, there is a growing stream of research that aims to produce accurate yet interpretable image clas-sifiers by integrating the power of deep learning and the in-terpretability of case-based reasoning [2, 4, 28, 45]. These models learn a set of prototypes from training images, and make predictions by comparing parts of the input image with prototypes learned during training. This enables expla-nations of the form “this is an image of a painted bunting, because this part of the image looks like that prototypical part of a painted bunting,” as in Figure 1(a). However, ex-isting prototype-based models for computer vision use spa-Figure 1. How an input image of a painted bunting is com-pared with (a) a regular (non-deformable) prototype and (b) a de-formable prototype of the painted bunting class (overlaid on its source image). tially rigid prototypes, which cannot explicitly account for geometric transformations or pose variations of objects.
Inspired by recent work on modeling geometric trans-formations in convolutional neural networks [5, 16, 17, 53], we propose a deformable prototypical part network (De-formable ProtoPNet), a case-based interpretable neural net-work that provides spatially flexible deformable prototypes.
In a Deformable ProtoPNet, each prototype is made up of several prototypical parts that adaptively change their relative spatial positions depending on the input image.
This enables each prototype to detect object features with a higher tolerance to spatial transformations, as the parts within a prototype are allowed to move. Figure 1(b) illus-trates the idea of a deformable prototype; when an input image is compared with a deformable prototype, the pro-totypical parts within the deformable prototype adaptively
change their relative spatial positions to detect similar parts of the input image. Consequently, a Deformable ProtoP-Net can explicitly capture pose variations, and improve both model accuracy and the richness of explanations provided.
The main contributions of our paper are as follows: (1)
We developed the first prototypical case-based interpretable neural network that provides spatially flexible deformable prototypes. (2) We improved the accuracy of case-based interpretable neural networks by introducing angular mar-(3) We showed that De-gins to the training algorithm. formable ProtoPNet can achieve state-of-the-art accuracy on the CUB-200-2011 bird recognition dataset [47] and the
Stanford Dogs [18] dataset. 2.