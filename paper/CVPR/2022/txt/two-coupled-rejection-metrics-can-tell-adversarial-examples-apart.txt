Abstract
Correctly classifying adversarial examples is an essential but challenging requirement for safely deploying machine learning models. As reported in RobustBench, even the state-of-the-art adversarially trained models struggle to exceed 67% robust test accuracy on CIFAR-10, which is far from practical. A complementary way towards robustness is to introduce a rejection option, allowing the model to not re-turn predictions on uncertain inputs, where conﬁdence is a commonly used certainty proxy. Along with this routine, we
ﬁnd that conﬁdence and a rectiﬁed conﬁdence (R-Con) can form two coupled rejection metrics, which could provably distinguish wrongly classiﬁed inputs from correctly classiﬁed ones. This intriguing property sheds light on using coupling strategies to better detect and reject adversarial examples.
We evaluate our rectiﬁed rejection (RR) module on CIFAR-10,
CIFAR-10-C, and CIFAR-100 under several attacks includ-ing adaptive ones, and demonstrate that the RR module is compatible with different adversarial training frameworks on improving robustness, with little extra computation. 1.

Introduction
The adversarial vulnerability of machine learning models has been widely studied because of its counter-intuitive be-havior and the potential effect on safety-critical tasks [2, 17, 43]. Towards this end, many defenses have been proposed, but most of them can be evaded by adaptive attacks [1, 45].
Among the previous defenses, adversarial training (AT) is recognized as an effective defending approach [30, 53].
Nonetheless, as reported in RobustBench [10], the state-of-the-art AT methods still struggle to exceed 67% robust test accuracy on CIFAR-10, even after exploiting extra data [18, 35, 39, 47], which is far from practical.
An improvement can be achieved by incorporating a rejec-tion or detection module along with the adversarially trained classiﬁer, which enables the model to refuse to make predic-tions for abnormal inputs [7, 23, 25, 42]. Although previous rejectors trained via margin-based objectives or conﬁdence calibration can capture some aspects of prediction certainty,
∗Corresponding author.
Figure 1. PGD-10 examples crafted against an adversarially trained
ResNet-18 on the CIFAR-10 test set. As described in Theorem 1, these adversarial examples are ﬁrst ﬁltered by the conﬁdence value 1 2−ξ for each ξ. Namely, they pass if the predicted conﬁdence at is larger than 1 2−ξ ; otherwise rejected. Then among the remaining examples, the R-Con metric can provably separate correctly and wrongly classiﬁed inputs. In Fig. 3 we show that tuning the logits temperature τ can increase the number of remaining examples. they may overestimate the certainty, especially on wrongly classiﬁed samples. Furthermore, [44] argues that learning a robust rejector could suffer from a similar accuracy bottle-neck as learning robust classiﬁers, which may be caused by data insufﬁciency [38] or poor generalization [49].
−
To solve these problems, we ﬁrst observe that the true log fθ(x)[y] reﬂects how well the clas-cross-entropy loss siﬁer fθ(x) is generalized on the input x [16], assuming that we can access its true label y. Thus, we propose to treat true conﬁdence (T-Con) fθ(x)[y], i.e., the predicted probability on the true label as a certainty oracle. Note that T-Con is different from the commonly used conﬁdence, which is ob-tained by taking the maximum as maxl fθ(x)[l]. As we shall see in Table 1, executing the rejection based on T-Con can largely increase the test accuracy under a given true positive rate for both standardly and adversarially trained models.
An instructive fact about T-Con is that if we ﬁrst thresh-old conﬁdence by 1 2 , then T-Con can provably distinguish wrongly classiﬁed inputs from correctly classiﬁed ones, as stated in Lemma 1. This inspires us to couple two connected metrics like conﬁdence and T-Con to execute rejection op-tions, instead of employing a single metric.
Table 1. Test accuracy (%) on all examples and under given true positive rate of 95% (TPR-95). The model is ResNet-18 that stan-dardly or adversarially trained on CIFAR-10.
Inputs
All
Clean 95.36
PGD-10 0.22 82.67
PGD-10 53.58
Clean
Standard
Adversarial
Availability
TPR-95
Conﬁdence T-Con 100.0 100.0 96.55 88.75 (cid:55) 98.40 0.18 87.39 57.23 (cid:51) (AT) via margin-based objectives, whereas they abandon the ready-made information from the conﬁdence that is shown to be a simple but good solution of rejection for PGD-AT [48].
On the other hand, [42] propose conﬁdence-calibrated AT (CCAT) by adaptive label smoothing, leading to preciser rejection on unseen attacks. However, this calibration acts on the true classes in training, while the conﬁdences obtained by the maximal operation during inference may not follow the calibrated property, especially on the misclassiﬁed inputs.
In contrast, we exploit true conﬁdence (T-Con) as a certainty oracle, and propose to learn T-Con by rectifying conﬁdence.
Our RR module is also compatible with CCAT, where R-Con is trained to be aligned with the calibrated T-Con. [8] used similarly rectiﬁed conﬁdence (R-Con) for failure prediction, while we prove that R-Con and conﬁdence can be coupled to provide provable separability in the adversarial setting.
In Appendix B, we introduce more backgrounds on the adversarial training and detection methods, where several representative ones are involved as our baselines. 3. Classiﬁcation with a rejection option
Consider a data pair (x, y), with x
Rd as the input and y as the true label. We refer to fθ(x) : Rd
∆L as a classiﬁer parameterized by θ, where ∆L is the probability simplex of L classes. Following [14], a classiﬁer with a rejection module can be formulated as
→
∈ (fθ,
)(x) (cid:44)
M fθ(x), don’t know, if if t; (x)
≥ (x) < t,
M
M (1) (x) is a certainty proxy com-where t is a threshold, and puted by auxiliary models or statistics.
What to reject? The design of
M
M is principally decided by what kinds of inputs we intend to reject. In the adversarial setting, most of the previous detection methods aim to reject adversarial examples, which are usually misclassiﬁed by standardly trained models (STMs) [6].
In this case, the misclassiﬁed and adversarial characters are considered as associated by default. However, for adversarially trained models (ATMs) on CIFAR-10, more than 50% adversarial
M (cid:40)
Figure 2. Construction of the objective LRR in Eq. (4) for training the RR module, which is the binary cross-entropy (BCE) loss be-tween T-Con and R-Con. The RR module shares a main backbone with the classiﬁer, introducing little extra computation.
The property of T-Con is compelling, but its computation is unfortunately not realizable during inference due to the absence of the true label y. Thus we construct the rectiﬁed conﬁdence (R-Con) to learn to predict T-Con, by rectifying conﬁdence via an auxiliary function. We prove that if R-Con is trained to align with T-Con within ξ-error where ξ
[0, 1), then a ξ-error R-Con rejector and a 1 2−ξ conﬁdence rejector can be coupled to distinguish wrongly classiﬁed inputs from correctly classiﬁed ones. This property generally holds as long as the learned R-Con rejector performs better than a random guess, as described in Section 4.2.
∈
Technically, as illustrated in Fig. 2, we adopt a two-head structure to model the classiﬁer and our rectiﬁed rejection (RR) module, while adversarially training them in an end-to-end manner. Our rejection module is learned by minimizing an extra BCE loss between T-Con and R-Con. The design of a shared main body saves computation and memory costs.
Stopping gradients on the conﬁdence fθ(x)[ym] when the predicted label ym = y can avoid focusing on easy examples and keep the optimal solution of classiﬁer unbiased.
Empirically, we evaluate the performance of our RR mod-ule on CIFAR-10, CIFAR-10-C, and CIFAR-100 [22, 24] with extensive experiments. In Section 4, we verify the prov-able rejection options obtained by coupling conﬁdence and
R-Con. To fairly compare with previous baselines, we also use R-Con alone as the rejector, and report both the accuracy for a given true positive rate and the ROC-AUC scores in
Section 6. We perform ablation studies on the construction of
R-Con, and design adaptive attacks to evade our RR module.
Our results demonstrate that the RR module is well com-patible with different AT frameworks, and can consistently facilitate the returned predictions to achieve higher robust accuracy under several attacks and threat models, with little computational burden, and is easy to implement. 2.