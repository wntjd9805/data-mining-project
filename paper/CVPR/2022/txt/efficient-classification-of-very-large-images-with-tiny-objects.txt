Abstract
WSI
Target Tiles (ROI)
Target Sub-tiles (ROI)
An increasing number of applications in computer vi-sion, specially, in medical imaging and remote sensing, be-come challenging when the goal is to classify very large im-ages with tiny informative objects. Speciﬁcally, these classi-ﬁcation tasks face two key challenges: i) the size of the input image is usually in the order of mega- or giga-pixels, how-ever, existing deep architectures do not easily operate on such big images due to memory constraints, consequently, we seek a memory-efﬁcient method to process these im-ages; and ii) only a very small fraction of the input im-ages are informative of the label of interest, resulting in low region of interest (ROI) to image ratio. However, most of the current convolutional neural networks (CNNs) are de-signed for image classiﬁcation datasets that have relatively large ROIs and small image sizes (sub-megapixel). Existing approaches have addressed these two challenges in isola-tion. We present an end-to-end CNN model termed Zoom-In network that leverages hierarchical attention sampling for classiﬁcation of large images with tiny objects using a single GPU. We evaluate our method on four large-image histopathology, road-scene and satellite imaging datasets, and one gigapixel pathology dataset. Experimental results show that our model achieves higher accuracy than existing methods while requiring less memory resources. 1.

Introduction
Neural networks have achieved state-of-the-art perfor-mance in many image classiﬁcation tasks [24]. However, there are still many scenarios where neural networks can still be improved. Using modern deep neural networks on image inputs of very high resolution is a non-trivial problem due to the challenges of scaling model architectures [44].
Such images are common for instance in satellite or medi-cal imaging. Moreover, these images tend to become even bigger due to the rapid growth in computational and mem-ory availability, as well as the advancements in camera sen-sor technology. Speciﬁcally challenging are the so called score score
Manually Annotated
ROI Mask
Figure 1. Illustration of processing a typical WSI using our zoom-in strategy. We see that i) there are large regions with little in-formation (mostly background), and ii) small informative regions have high-resolution details. Leveraging the above characteristics of WSI, we derive a method that gradually zooms-in to the ROI.
The proposed approach ﬁrst process the down-sampled WSI to sample the target tiles, and then repeats this procedure to sample target sub-tiles. The sampled sub-titles contain the ﬁne-grained information for classiﬁcation. The bottom images show that the manually annotated ROIs are captured by the proposed approach without the need for pixel level annotations. tiny object image classiﬁcation tasks, where the goal is to classify images based on the information of very small ob-jects or regions of interest (ROIs), in the presence of a much larger and rich (non-trivial) background that is uncorrelated or non-informative of the label. Consequently, constituting an input image with a very low ROI-to-image ratio. Recent work [37] showed that with a dataset of limited size, convo-lutional neural networks (CNNs) have poor performance on very low ROI-to-image ratio problems. In these settings, the input resolution is increased from typical image sizes, e.g., 224 × 224 pixels, to gigapixel images of size ranging from 45, 056 × 35, 840 to 217, 088 × 111, 104 pixels [30], which not only require signiﬁcantly more computational process-ing power per image than a typical image given a ﬁxed deep architecture, but in some cases, become prohibitive for cur-rent GPU-memory standards.
Figure 1 shows an example of a gigapixel pathology im-age, from which we see that manually annotated ROIs (with cancer metastases), not usually available for model train-ing, constitute a small proportion of the whole slide image (WSI). Moreover, many tasks in satellite imagery [5] and medical image analysis [30] are still challenging due to the scarce methodology available for such big images.
Other recent works have addressed the computational re-source bottlenecks associated with models for very large images by proposing approaches such as the streaming neu-ral network [38] and gradient checkpoint [33]. However, these methods do not take advantage of the characteris-tics of very large images in tiny object image classiﬁcation tasks, i.e., those in which only a small portion of the image input is informative for the classiﬁcation label of interest.
Alternatively, other approaches use visual attention models to exploit these characteristics and show that discriminative information may be sparse and scattered across various im-age scales [12, 20, 36], which suggests that in some scenar-ios, processing the entire input image is unnecessary, and specially true in tiny object image classiﬁcation tasks. For instance, [20] leverages attention to build image classiﬁers using a small collection of tiles (image patches) sampled from the matrix of attention weights generated by an atten-tion network. Unfortunately, despite the ongoing efforts, existing approaches are either prohibitive or require severe resolution trade-offs that ultimately affect classiﬁcation per-formance, for tasks involving very large (gigapixel) images.
The purpose of this work is to address these limitations simultaneously. Speciﬁcally, we propose a neural network architecture termed Zoom-In network, which as we will show, yields outperforming memory efﬁciency and classi-ﬁcation accuracy on various tiny object image classiﬁcation datasets. We build upon [20] by proposing a two-stage hi-erarchical attention sampling approach that is effectively able to process gigapixel images, while also leveraging con-trastive learning as a means to improve the quality of the at-tention mechanisms used for sampling. This is achieved by building aggregated representations over a small fraction of high-resolution content (sub-tiles) that is selected from an attention mechanism, which itself leverages a lower reso-lution view of the original image. In this way, the model can dramatically reduce the data acquisition and storage re-quirements in real-world deployments. This is possible be-cause low resolution views can be used to indicate which re-gions of the image should be acquired (attended) at higher resolution for classiﬁcation purposes, without the need of acquiring the entire image at full resolution. Moreover, we show that the proposed approach can be easily extended to incorporate pixel-level-annotations when available for ad-ditional performance gains. Results on ﬁve challenging datasets demonstrate the capabilities of the Zoom-In net-work in terms of accuracy and memory efﬁciency. 2. Zoom-In Network
Below we present the construction of the proposed
Zoom-In network model, which aims to efﬁciently process gigapixel images for classiﬁcation of very large images with tiny objects. We start by brieﬂy describing the one-stage at-tention sampling method proposed in [20], which we lever-age in our formulation. Then, we introduce our strategy consisting in decomposing the attention-based sampling into two stages as illustrated in Figure 2. This two-stage hierarchical sampling approach enables computational efﬁ-ciency without the need to sacriﬁce performance due to loss of resolution, when used in applications with very large im-In the experiments, ages and small ROI-to-image ratios. we will show that the Zoom-In network results in improved performance relative to existing approaches on several tiny object image classiﬁcation datasets, and importantly, with-out the need for any pixel-level annotation. 2.1. Attention Sampling
Let Ts1 (x, c) denote a function that extracts a tile of size h1 × w1 from the input, full-resolution, image x ∈ RH×W corresponding to the location (coordinates) c = {i, j} in a lower resolution view V (x, s1) ∈ Rh×w of x at scale s1 ∈ (0, 1), so h = (cid:3)s1H(cid:4) and w = (cid:3)s1W (cid:4), where (cid:3)·(cid:4) is the ﬂoor operator. More speciﬁcally, Ts1 (x, c) maps c to a location in x via {(cid:3)1 + (i − 1)(W − 1)/(w − 1)(cid:4), (cid:3)1 + (j − 1)(H − 1)/(h − 1)(cid:4)}, and returns a tile of size h1 × w1.
Note that i) the map of locations between V (x, s1) and x only depends on the size of x (H × W ) and s1 and not on the tile size (h1 × w1); ii) h1, w1 > 1/s1, to guaran-tee full coverage of x; iii) this strategy requires to zero-pad x on all sides by (cid:3)h1/2(cid:4) and (cid:3)w1/2(cid:4) pixels accordingly; and iv) we have omitted the (color) channel dimension in x and V (x, s1) for notational simplicity, however, we con-sider color images (with an additional dimension) in our experiments. Then, let ΨΘ(x) = gΘ(fΘ(Ts1 (x, c))) be a neural network parameterized by Θ whose intermediate rep-resentation z ∈ RK is obtained via feature extracting func-tion z = fΘ(Ts1 (x, c)), e.g., a convolutional neural network (CNN). Further, gΘ(z) is a classiﬁcation function also spec-iﬁed as a neural network and parameterized by Θ. We can provide ΨΘ(x) with an attention mechanism as follows
α = aΘ(V (x, s1)) : Rh×w → Rh×w (cid:4)
ΨΘ(x) = gΘ
αcfΘ(Ts1 (x, c))
, (cid:2) (cid:3) c∈C (1) (2) where α is the matrix of attention weights such that (cid:5) c∈C αc = 1, aΘ(V (x, s1)) is the attention function, also speciﬁed as a neural network, and C (of length |C| = h · w) is the collection of all index pairs for view V (x, s1). In or-der to avoid computing the features z from all |C| tiles im-plied by view V (x, s1), which can be a very large number
(cid:2183)(cid:2216)(cid:4666)(cid:521)(cid:4667)
Input image
Stage I
Sampled tiles 0 0 0  7 4 0 9 4 0 6 0 0 0 8 1 8 0 0 (cid:2184)(cid:2216)(cid:4666)(cid:521)(cid:4667)
Stage II 1 0 0  0 0 1 1 1 1 0 0 1 1 0 1 1
Sampled sub-tiles (cid:2188)(cid:2216)(cid:4666)(cid:521)(cid:4667)
Feature 
Aggregation (cid:2189)(cid:2216)(cid:4666)(cid:521)(cid:4667) (cid:2235)
Samples Map
Contrastive Samples 
Map (cid:2236)(cid:2185)
Samples Map
Contrastive Samples 
Map
Figure 2. Illustration of the Zoom-In network. In Stage I, attention network aΘ(·) generates an attention map for the input image down-scaled by s1, from which N tiles are sampled with replacement (see samples map). In Stage II, attention network bΘ(·) generates an attention map for each selected tile and selects a sub-tile, thus N sub-tiles are selected (without replacement). Then all sub-tiles are fed to feature extractor fΘ(·), feature maps are aggregated using their corresponding attention weights, and predictions are obtained from aggregated features using a classiﬁcation module gΘ(·). Further, both attention maps are also used to draw contrastive samples with minimal computational overhead (during training). if x is big, as in our pathology and remote sensing scenar-ios, [20] proposed to leverage Monte Carlo estimation by only considering a small set of tiles from the original in-put image sampled via the attention function. This strategy leverages that α deﬁnes a discrete distribution over the set of |C| tiles. Speciﬁcally, [20] approximates (2) by sampling from (1) via
ΨΘ(x) ≈ gΘ
⎛
⎝ 1
N (cid:3) c∈Q
⎞
⎠ , (3) fΘ(Ts1 (x, c)) where Q is a collection of N (cid:7) |C| index pairs for view
V (x, s1) drawn independently and identically distributed (iid) from the distribution deﬁned by the attention weights, i.e., Q = {(i, j) ∼ aΘ(V (x, s1))|i = 1, 2, ..., N }. In [20], they consider tiles of size h1 = w1 = 27, s1 = 0.2 and
N = 10 for the colon cancer dataset. See Experiments be-low for additional details.
Using the approximation in (3), the attention mechanism uses a lower resolution view V (x, s1) of the original im-age x for computing the attention distribution and outputs an aggregated feature vector by averaging over the features
{zn}N n=1 of a small amount of N tiles. Unfortunately, this approach is still prohibitive for gigapixel images because feasible combinations of h1, w1 and s result in unrealistic memory needs for current GPU-memory standards. Below, we introduce the proposed two-stage hierarchical sampling to improve the memory efﬁciency of attention sampling. 2.2. Two-stage Hierarchical Attention Sampling
Multistage and hierarchical sampling strategies are often preferred in practice. For instance, the cost of interviewing or testing people are enormously reduced if these people are geographically or organizationally grouped, thus sampling is performed within groups (clusters). Such sampling de-sign has many real-world applications such as household and mortality surveys, as well as high-resolution remote sensing applications [6, 13, 50]. Motivated by this idea, we design a two-stage hierarchical sampling approach to reduce memory requirements when processing very large, gigapixel, images without severe resolution trade-offs.
Speciﬁcally, let V (x, s2, c) ∈ Ru×v be a view of
Ts1 (x, c) at scale s2 ∈ (0, 1), so u = (cid:3)s2h(cid:4) = (cid:3)s1s2H(cid:4) and v = (cid:3)s2w(cid:4) = (cid:3)s1s2W (cid:4). Further, we deﬁne a function
Ts2 (Ts1 (x, c), c(cid:3)) that extracts a sub-tile of size h2 × w2 at location c(cid:3) = {i(cid:3), j(cid:3)} in V (x, s2) from tile Ts1 (x, c) at location c = {i, j} in V (x, s1). The mapping function
Ts2 (Ts1 (x, c), c(cid:3)) is deﬁned similarly to Ts1 (x, c), but re-turns tiles of size h2 × w2 instead of h1 × w1, and is such that h2 < h1, w2 < w1, and h2, s2 > 1/s2. Moreover, we can also deﬁne an attention mechanism for V (x, s2, c) as in (1) as follows
βc = bΘ(V (x, s2, c)) : Ru×v → Ru×v
+ , (4) c(cid:2)∈C(cid:2) β(cid:3) where β is the matrix of attention weights for the tile at loca-(cid:5) tion c of V (x, s1) such that c = 1, bΘ(V (x, s2, c)) is the attention function, also speciﬁed as a neural network, and C (cid:3) (of length |C (cid:3)| = u · v) is the collection of all in-dex pairs for view V (x, s2, c) of Ts1 (x, c). Provided that (cid:5) c = 1 in (4), it is easy to c(cid:2)∈C(cid:2) aΘ(V (x, s1))bΘ(V (x, s2, c)) = 1 see that and that the attention for location c(cid:3) = {i(cid:3), j(cid:3)} in V (x, s2, c) relative to the entire image x is αcβc(cid:2) . Consequently, we can rewrite (2) as (cid:2) c∈C αc = 1 in (1) and c∈C c(cid:2)∈C(cid:2) β(cid:3) (cid:5) (cid:5) (cid:5) (cid:4) (cid:3) (cid:3)
ΨΘ(x) = gΘ
αc c∈C c(cid:2)∈C(cid:2)
βc c(cid:2) fΘ(Ts2 (Ts1 (x, c), c(cid:3)))
, (5) where now, the aggregated representation is a weighted av-erage of all tiles of size h2 × w2 of x, and like in (3), we can approximate as
ΨΘ(x) ≈ gΘ
⎛
⎝ 1
N
⎞ (cid:3) c∈Q fΘ(Ts2 (Ts1 (x, c), c(cid:3)))
⎠ , (6) where c(cid:3) ∼ bΘ(V (x, ss, c)) is drawn iid from distribution bΘ(V (x, ss, c)) for every location c ∈ Q.
Note that the approximation in (6) uses full-resolution sub-tiles from x that are drawn hierarchically from the two-level discrete distribution implied by α and {βc}|C| c=1, which are obtained from low-resolution views V (x, s1) and
{V (x, s2, c)}|C| c=1. Importantly, in practice we do not need to instantiate the tiles Ts1 (x, c) but only Ts2 (Ts1 (x, c), c(cid:3)), and the second-level attention matrix in (4) can be obtained as needed (on the ﬂy). However, this can cause computa-tional inefﬁciency if multiple samples from the same lo-cation c are selected for level-two sampling in (6). Inefﬁ-ciency occurs because such procedure will require to instan-tiate view V (x, s2, c) multiple times to obtain βc on a single model update (iterations), and then when a sub-tile is sam-pled multiple times when obtaining fΘ(Ts2 (Ts1 (x, c), c(cid:3)).
We can mitigate the inefﬁciency by ordering the samples in Q to prevent recalculating βc, and we can reuse features fΘ(Ts2 (Ts1 (x, c), c(cid:3)) for a given c and c(cid:3) as needed. Alter-natively, we can avoid reusing sub-tiles by sampling loca-tions c(cid:3) in (6) without replacement. However, such sampling strategy will not be iid and as a result, it will cause bias in the Monte Carlo approximation in (6). Fortunately, us-ing a formulation similar to that of [20], we can still obtain an unbiased estimator of the average (expectation) in (6) from a non-iid sample, without replacement, by leveraging the Gumbel-Top-k trick [23], which is extended from the
Gumbel-Max trick for weighted reservoir sampling [11].
Speciﬁcally, from (5) we can write
Ec(cid:2)∼bΘ(V (x,s2,c))[fΘ(Ts2 (Ts1 (x, c), c(cid:3)))] = (7) c(cid:2) fΘ(Ts2 (Ts1 (x, c), c(cid:3))),
βc (cid:3) c(cid:2)∈C(cid:2) from which we can see that the sum on the right is an unbi-ased estimator of the expectation on the left. Alternatively, we can write
Ec(cid:2)∼bΘ(V (x,s2,c))[fΘ(Ts2 (Ts1 (x, c), c(cid:3)))] = (8) (cid:3) (cid:3) c(cid:2)∈C(cid:2) i(cid:5)=c(cid:2)
βc c(cid:2)
βc i 1 − βc c(cid:2)
+ (1 − βc (βc c(cid:2) fΘ(Ts2 (Ts1 (x, c), c(cid:3)))) c(cid:2) )fΘ(Ts2 (Ts1 (x, c), i))), i /(1 − βc where βc c(cid:2) ) is the attention weight for the i-th sub-tile reweighted to exclude sub-tile c(cid:3), which is equivalent to having already sampled it. The proof of (8) can be found in the Supplementary Material (SM). We can then approxi-mate (5) like in (6) but sampling without replacement using
ΨΘ(x) ≈ gΘ( 1
N
⎡
N(cid:3) i−1(cid:3) i=1 j=1
βci c(cid:2) j
⎤ fΘ(Ts2 (Ts1 (x, ci), c(cid:3) j))
⎣1 −
+ i−1(cid:3) j=1
βci c(cid:2) j
⎦ fΘ(Ts2 (Ts1 (x, ci), c(cid:3) j))), (9) and c(cid:3) j is sampled via j ∼ p(c(cid:3)|c(cid:3) c(cid:3) 1, . . . , c(cid:3) j−1) ∝ (cid:14)
βc i 0 1, . . . , c(cid:3) j−1} if i /∈ {c(cid:3) otherwise
, 1, . . . , c(cid:3) where p(c(cid:3)|c(cid:3) j−1) represents sampling location c(cid:3) j without replacement, by having already sampled locations 1, . . . , c(cid:3) c(cid:3) j−1. 1HW + N (cid:3)s2
Memory requirements
In practice, the memory require-ments of the attention sampling model are determined by the model parameters, feature maps, gradient maps and workspace variables [41]. For neural-network-based im-age models, memory allocation is mainly dominated by the size of the input image, i.e., H and W . Speciﬁcally, the peak memory usage at inference for N samples scales with O(s2HW + N h2w2) and O(s2 2HW +
N h2w2) for both, the one-stage [20] and the proposed two-stage hierarchical model. Here, we use N (cid:3) to denote the number of unique tiles in Q and s to indicate the scale of the view for the one-stage approach.
In fact, we can show that our model requires signiﬁcantly less GPU mem-ory than one-stage attention sampling by choosing s1 < s and s2 = s. Note that the number of selected tiles in the
ﬁrst stage decreases dramatically as the attention map is be-ing optimized. We use the term peak memory to refer to the worse case scenario. Empirically, we have observed that the average number of selected tiles is N (cid:3) ≈ N/2. A detailed analysis of memory requirements is presented in the SM. 1s2 2.3. Efﬁcient Contrastive Learning with Attention
Sampling
Motivated by [22], we introduce a contrastive learning objective for the proposed Zoom-In network consisting on encouraging the model to make predictions for cases (y = 1), e.g., images with cancer metastases (see Experiments for details), but using sub-tiles with low attention weights while inverting the image labels (y = 1 → 0). Conveniently, we can generate these (negative) contrastive samples without the need for additional modules or model parameters.
Speciﬁcally, we leverage the existing attention functions in (1) and (4). To generate the contrastive feature vectors for image x such that y = 1, we ﬁrst sample (with replacement) tile locations via 1 − aΘ(V (x, s1)) similar to (1). Then, we sample N sub-tiles via 1 − bΘ(V (x, s2, c)) without replace-ment similar to (4).
The sampled contrastive sub-tiles are passed through the feature network and then processed by the classiﬁer to make predictions ΨΘ(x|y = 1) using (9), where the condition-ing y = 1 is used to emphasize that we use images x of class y = 1 as contrastive examples. In general, the num-ber of contrastive examples (per training batch) is equal to the number of samples such that y = 1. For these
(cid:5) contrastive sample, we optimize the following objective,
Lcon(ΨΘ(x|y = 1)) = n − log(1 − ΨΘ(xn|yn = 1)).
Note that Lcon(ΨΘ(x|y = 1)) encourages contrastive sam-ples for images x with label y = 1 to be predicted as y = 0.
In multi-class scenarios, this contrastive learning approach can be readily extended by letting one of the classes be the reference, or in general, by using a complete, cross-entropy-based contrastive loss, in which contrastive sam-ples are generated for both classes, i.e., y = {0, 1}, instead of just one class (half the cross-entropy loss) as in our case. 3.