Abstract
As clean ImageNet accuracy nears its ceiling, the re-search community is increasingly more concerned about ro-bust accuracy under distributional shifts. While a variety of methods have been proposed to robustify neural networks, these techniques often target models trained on ImageNet classiﬁcation. At the same time, it is a common practice to use ImageNet pretrained backbones for downstream tasks such as object detection, semantic segmentation, and image classiﬁcation from different domains. This raises a ques-tion: Can these robust image classiﬁers transfer robustness to downstream tasks? For object detection and semantic segmentation, we ﬁnd that a vanilla Swin Transformer, a variant of Vision Transformer tailored for dense prediction tasks, transfers robustness better than Convolutional Neu-ral Networks that are trained to be robust to the corrupted version of ImageNet. For CIFAR10 classiﬁcation, we ﬁnd that models that are robustiﬁed for ImageNet do not re-tain robustness when fully ﬁne-tuned. These ﬁndings sug-gest that current robustiﬁcation techniques tend to empha-size ImageNet evaluations. Moreover, network architecture is a strong source of robustness when we consider transfer learning. 1.

Introduction
ImageNet [7] serves as an important benchmark in the
ﬁeld of computer vision. Numerous models and training techniques have emerged out of this benchmark [11, 17]. A newly proposed vision architecture, including recent Vision
Transformer [8], is ﬁrst tested against ImageNet to demon-strate a good performance before it gains popularity within the community. While accuracy on ImageNet has been con-sidered as a surrogate for measuring progress in machine vi-sion systems, the research community is now aware of the lack of robustness of vision models towards small input per-turbations. [33] ﬁrst reported that imperceptible adversarial perturbations can easily fool image classiﬁers. Recent stud-ies show that even simpler, more natural noises such as blur, contrast change, and snow can signiﬁcantly degrade the per-Figure 1. It is a common practice to use ImageNet classiﬁers as initialization for downstream tasks such as object detection and semantic segmentation. When we ﬁne-tune robust ImageNet clas-siﬁers for downstream tasks, should we expect that the resulting vision system still maintains robustness? We tackle this question in the settings of ﬁxed-feature and full-network transfer learning. formance of models [13]. A typical strategy to increase robustness is data augmentation, where a vision model is trained with additional data, which are artiﬁcially corrupted during training. Examples include ANT [29], AugMix [14], and DeepAug [12]. However, these techniques often focus on improving robust accuracy for ImageNet classiﬁcation.
In fact, there are now a variety of ImageNet-scale robust-ness benchmarks, and the community is striving to improve accuracy on these benchmarks [2, 12, 15].
Due to the scale of ImageNet, it is a common prac-tice to use ImageNet pretrained weights for downstream tasks such as object detection [16] and image segmentation
[5, 10]. This practice of using pretrained ImageNet weights for transfer learning raises a fundamental question from a robustness perspective: When we use pretrained weights that are made to be robust to ImageNet benchmarks, do these models necessarily show robustness for downstream tasks as well? (See Figure 1 for the problem setting we consider.)
Contributions.
We ﬁnd that when we freeze the backbone of Ima-geNet models, robustiﬁed Convolutional Neural Networks (CNNs) maintain robustness for object detection and se-mantic segmentation. These robustiﬁed CNNs continue to
demonstrate higher robustness than the regular model even when we fully ﬁne-tune the whole network, which is practi-cally more relevant. However, perhaps more notably, we observe that Swin Transformer [23], a variant of Vision
Transformer tailored to dense prediction tasks, transfers ro-bustness better than robustiﬁed CNNs in this fully-ﬁnetuned setting. Moreover, it seems difﬁcult to transfer corruption robustness from ImageNet to CIFAR10 [21].
In fact, we
ﬁnd that a non-robustiﬁed ImageNet pretrained ResNet per-forms the best when ﬁne-tuned for CIFAR10. We hope these ﬁndings encourage the community to reconsider how we evaluate the robustness of vision systems, as existing data augmentation techniques for robustifying neural net-works might be overﬁtting to ImageNet benchmarks. Fur-thermore, it is noteworthy that, for robustness transfer, the robustness contribution from Swin Transformer architecture is more signiﬁcant than the existing robustiﬁcation methods.
Scope. While there are various kinds of distributional shifts and robustness that the vision community studies, we focus on common corruption robustness in this paper, because we are interested in robustness transfer from ImageNet classi-ﬁcation to downstream tasks such as object detection and segmentation. See Section 3.1 for more details about why we speciﬁcally choose common corruptions as a topic of our study. 2.