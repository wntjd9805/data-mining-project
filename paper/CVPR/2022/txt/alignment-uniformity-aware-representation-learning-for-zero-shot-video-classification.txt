Abstract
Most methods tackle zero-shot video classiﬁcation by aligning visual-semantic representations within seen classes, which limits generalization to unseen classes. To enhance model generalizability, this paper presents an end-to-end framework that preserves alignment and uniformity properties for representations on both seen and unseen classes. Speciﬁcally, we formulate a supervised contrastive loss to simultaneously align visual-semantic features (i.e., alignment) and encourage the learned features to distribute uniformly (i.e., uniformity). Unlike existing methods that only consider the alignment, we propose uniformity to pre-serve maximal-info of existing features, which improves the probability that unobserved features fall around observed data. Further, we synthesize features of unseen classes by proposing a class generator that interpolates and extrap-olates the features of seen classes. Besides, we introduce two metrics, closeness and dispersion, to quantify the two properties and serve as new measurements of model gener-alizability. Experiments show that our method signiﬁcantly outperforms SoTA by relative improvements of 28.1% on
UCF101 and 27.0% on HMDB51. Code is available1. 1.

Introduction
Mimicking human capability to recognize things never seen before, zero-shot video classiﬁcation (ZSVC) only trains models on videos of seen classes and makes pre-dictions on unobserved ones [13, 19, 24, 27, 28, 51, 52, 54].
Correspondingly, existing ZSVC models map visual and se-mantic features into a uniﬁed representation, and hope the association can be generalized to unseen classes [2, 3, 6, 14, 35, 54]. However, these methods learn associated represen-tations within limited classes, thus facing the following two critical problems [11, 13]: (1) semantic-gap: manifolds in-consistency between visual and semantics features, and (2)
*These authors contributed equally. 1https://github.com/ShipuLoveMili/CVPR2022-AURL (cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:5)(cid:4)(cid:4)(cid:7) (cid:8)(cid:9)(cid:8)(cid:10)(cid:11) (cid:12)(cid:13)(cid:4)(cid:14)(cid:5)(cid:15)(cid:4)(cid:13)(cid:3)(cid:6)(cid:7) (cid:8)(cid:9)(cid:16)(cid:16)(cid:8) (cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:5)(cid:4)(cid:4)(cid:7) (cid:8)(cid:9)(cid:8)(cid:10)(cid:8) (cid:12)(cid:13)(cid:4)(cid:14)(cid:5)(cid:15)(cid:4)(cid:13)(cid:3)(cid:6)(cid:7) (cid:8)(cid:9)(cid:16)(cid:17)(cid:18) (cid:9)(cid:16)(cid:12)(cid:15)(cid:6)(cid:17)(cid:6)(cid:7)(cid:8) (cid:14)(cid:17)(cid:9)(cid:19)(cid:6)(cid:7)(cid:8) (cid:21)(cid:3)(cid:10)(cid:11) (cid:20)(cid:9)(cid:3)(cid:23)(cid:6)(cid:7)(cid:8) (cid:6)(cid:20)(cid:15) (cid:14)(cid:10)(cid:1)(cid:1)(cid:6)(cid:7)(cid:8) (cid:5)(cid:9)(cid:17)(cid:17)(cid:14)(cid:9)(cid:14)(cid:15)(cid:3) (cid:4)(cid:7) (cid:5)(cid:9)(cid:17)(cid:17) (cid:11)(cid:9)(cid:13)(cid:6)(cid:7)(cid:8) (cid:12)(cid:10)(cid:12)(cid:2)(cid:6) (cid:2) (cid:6) (cid:5) (cid:4) (cid:3) (cid:6) (cid:4) (cid:3) (cid:6) (cid:7) (cid:4) (cid:3) (cid:3) (cid:6) (cid:7) (cid:4) (cid:3) (cid:1) (cid:6) (cid:4) (cid:3) (cid:1) (cid:6) (cid:5) (cid:4) (cid:3) (cid:1) (cid:2) (cid:1) (cid:11)(cid:9)(cid:13)(cid:6)(cid:7)(cid:8) (cid:12)(cid:10)(cid:12)(cid:2)(cid:6) (cid:21)(cid:6)(cid:3)(cid:15)(cid:20)(cid:1)(cid:6)(cid:7)(cid:8) (cid:1)(cid:3)(cid:9)(cid:22)(cid:22)(cid:6)(cid:20) (cid:9)(cid:16)(cid:12)(cid:15)(cid:6)(cid:17)(cid:6)(cid:7)(cid:8) (cid:14)(cid:10)(cid:1)(cid:1)(cid:6)(cid:7)(cid:8) (cid:5)(cid:9)(cid:17)(cid:17)(cid:14)(cid:9)(cid:14)(cid:15)(cid:3) (cid:4)(cid:7) (cid:5)(cid:9)(cid:17)(cid:17) (cid:20)(cid:9)(cid:3)(cid:23)(cid:6)(cid:7)(cid:8) (cid:6)(cid:20)(cid:15) (cid:12)(cid:13)(cid:6)(cid:14)(cid:14)(cid:6)(cid:7)(cid:8) (cid:3)(cid:4)(cid:14)(cid:15) (cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8) (cid:1)(cid:9)(cid:7)(cid:1)(cid:3)(cid:10)(cid:11) (cid:14)(cid:17)(cid:9)(cid:19)(cid:6)(cid:7)(cid:8) (cid:21)(cid:3)(cid:10)(cid:11) (cid:18)(cid:10)(cid:11)(cid:16)(cid:9) (cid:1)(cid:2) (cid:1)(cid:3)(cid:4)(cid:5)(cid:6) (cid:1)(cid:3)(cid:4)(cid:6)(cid:3) (cid:1)(cid:3)(cid:4)(cid:7)(cid:6) (cid:3) (cid:19)(cid:20)(cid:21) (cid:22)(cid:3)(cid:23)(cid:24) (cid:8)(cid:19)(cid:11)(cid:7)(cid:9)(cid:12)(cid:1)(cid:6)(cid:20)(cid:12) (cid:1)(cid:3)(cid:10)(cid:11)(cid:16)(cid:17)(cid:6)(cid:7)(cid:8) (cid:3)(cid:4)(cid:7)(cid:6) (cid:3)(cid:4)(cid:6)(cid:3) (cid:3)(cid:4)(cid:5)(cid:6) (cid:1)(cid:2) (cid:2) (cid:1)(cid:3)(cid:4)(cid:6)(cid:3) (cid:1)(cid:3)(cid:4)(cid:5)(cid:6) (cid:2) (cid:6) (cid:5) (cid:4) (cid:3) (cid:6) (cid:4) (cid:3) (cid:6) (cid:7) (cid:4) (cid:3) (cid:3) (cid:6) (cid:7) (cid:4) (cid:3) (cid:1) (cid:6) (cid:4) (cid:3) (cid:1) (cid:6) (cid:5) (cid:4) (cid:3) (cid:1) (cid:2) (cid:1) (cid:2) (cid:3)(cid:4)(cid:5)(cid:6) (cid:3)(cid:4)(cid:6)(cid:3) (cid:3)(cid:4)(cid:7)(cid:6) (cid:3) (cid:1)(cid:3)(cid:4)(cid:7)(cid:6) (cid:21)(cid:6)(cid:3)(cid:15)(cid:20)(cid:1)(cid:6)(cid:7)(cid:8) (cid:1)(cid:3)(cid:9)(cid:22)(cid:22)(cid:6)(cid:20) (cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8) (cid:1)(cid:9)(cid:7)(cid:1)(cid:3)(cid:10)(cid:11) (cid:8)(cid:19)(cid:11)(cid:7)(cid:9)(cid:12)(cid:1)(cid:6)(cid:20)(cid:12) (cid:1)(cid:10)(cid:11)(cid:16)(cid:17)(cid:6)(cid:7)(cid:8) (cid:18)(cid:10)(cid:11)(cid:16)(cid:9) (cid:2) (cid:3)(cid:4)(cid:5)(cid:6) (cid:3)(cid:4)(cid:6)(cid:3) (cid:3)(cid:4)(cid:7)(cid:6) (cid:3) (cid:1)(cid:3)(cid:4)(cid:7)(cid:6) (cid:1)(cid:2) (cid:1)(cid:3)(cid:4)(cid:5)(cid:6) (cid:1)(cid:3)(cid:4)(cid:6)(cid:3) (cid:1)(cid:3)(cid:4)(cid:7)(cid:6) (cid:3) (cid:12)(cid:13)(cid:6)(cid:14)(cid:14)(cid:6)(cid:7)(cid:8) (cid:3)(cid:4)(cid:14)(cid:15) (cid:3)(cid:4)(cid:7)(cid:6) (cid:3)(cid:4)(cid:6)(cid:3) (cid:19)(cid:25)(cid:21) (cid:26)(cid:27)(cid:15)(cid:4) (cid:3)(cid:4)(cid:5)(cid:6) (cid:1)(cid:2) (cid:2) (cid:1)(cid:3)(cid:4)(cid:6)(cid:3) (cid:1)(cid:3)(cid:4)(cid:5)(cid:6) 4
Figure 1. Visual-semantic representations: Comparisons of the learned representations between the SoTA [3] and our method. • represent visual and semantic features separately; colors and are for different classes. Besides, we use two metrics to quantify feature qualities on alignment (closeness better) and uniformity (dispersion better). We observe that ours show better closeness within classes and more separations among semantic clusters.
#
" domain-shift: the representations learned from training sets are biased when applied to the target sets due to disjoint classes between two groups. In ZSVC, these two problems cause side effects on model generalizability.
Reviewing the literature, we observe that most methods focus on tackling the semantic-gap by learning alignment-aware representations, which ensure visual and semantic features of the same class close. To improve the alignment,
MSE loss [3], ranking loss [14], and center loss [13] are commonly used to optimize the similarity between visual and semantic features. Apart from the loss, improvements for alignment are attributed mainly to the designs of archi-tectures. For instance, [13, 16, 28] ﬁrst project global vi-sual features to local object attributes, then optimize simi-larity between the attributes and ﬁnal semantics. In contrast,
URL [54], Action2Vec [14], and TARN [2] directly align vi-sual and ﬁnal semantic features, which are improved via at-tention modules. Since video features are hard to learn, the above methods utilize pre-trained models to extract visual features. The recent model [3] beneﬁts from the efﬁcient
R(2+1)D module [43] in video classiﬁcation and achieves the state-of-the-art (SoTA) results in ZSVC. However, the
SoTA [3] neglects to learn semantic features; thus, it is still
not a true end-to-end (e2e) framework for visual-semantic feature learning. We claim that e2e is critical for alignment since ﬁxed visual/semantic features will bring obstacles to adjusting one to approach another.
Noteworthily, the latest MUFI [35] and ER [6] get down to addressing the domain-shift problem by involv-ing more semantic information, thus consuming extra re-sources. In particular, MUFI [35] augments semantics by training multi-stream models on multiple datasets. ER [6] expands class names by annotating amount of augmented words crawled from the website. Freeing complex models or additional annotations, we will design a compact model that preserves maximal semantic info of existing classes while synthesizing features of unseen classes.
To tackle the two problems with one stone, we present an end-to-end framework that jointly preserves alignment and uniformity properties for representations on both seen and unseen classes. Here, alignment ensures closeness of visual-semantic features; uniformity encourages the fea-tures to distribute uniformly (maximal-info preserving), which improves the possibility that unseen features stand around seen features, mitigating the domain-shift implicitly.
Speciﬁcally, we formulate a supervised contrastive loss as a combination of two separate terms: one regularizes align-ment of features within classes, and the other guides unifor-mity between semantic clusters. To alleviate the domain-shift explicitly, we generate new features of unseen syn-thetic classes by our class generator that interpolates and ex-trapolates features of seen classes. In addition, we introduce closeness and dispersion scores to quantify the two proper-ties and provide new measurements of model generalizabil-ity. Fig. 1 illustrates the representations of our method and the SoTA alternative [3]. We train the two models on ten classes sampled from Kinetics-700 [5] and map features on 3D hyperspheres. We observe that our representation shows better closeness within classes and preserves more disper-sion between semantic clusters. Experiments validate that our method signiﬁcantly outperforms SoTA by relative im-provements of 28.1% on UCF101 and 27.0% on HMDB51. 2.