Abstract
Model pre-training is a cornerstone of modern visual recognition systems. Although fully supervised pre-training on datasets like ImageNet is still the de-facto standard, re-cent studies suggest that large-scale weakly supervised pre-training can outperform fully supervised approaches. This paper revisits weakly-supervised pre-training of models us-ing hashtag supervision with modern versions of residual networks and the largest-ever dataset of images and corre-sponding hashtags. We study the performance of the result-ing models in various transfer-learning settings including zero-shot transfer. We also compare our models with those obtained via large-scale self-supervised learning. We find our weakly-supervised models to be very competitive across all settings, and find they substantially outperform their self-supervised counterparts. We also include an investi-gation into whether our models learned potentially trou-bling associations or stereotypes. Overall, our results pro-vide a compelling argument for the use of weakly supervised learning in the development of visual recognition systems.
Our models, Supervised Weakly through hashtAGs (SWAG), are available publicly. 1.

Introduction
Most modern visual-recognition systems are based on machine-learning models that are pre-trained to perform a task that is different from the downstream task that the system aims to solve. Such pre-training allows the sys-tem to leverage (annotated) image or video datasets that are much larger than the datasets available for the down-stream task. Arguably the most popular pre-training task is supervised image classification on datasets such as Im-ageNet and JFT [20, 42, 76], but recent studies have also explored self-supervised [11–14, 27, 29, 31, 51] and weakly supervised [37, 38, 44, 49, 57] tasks for pre-training.
There are trade-offs between these three types of pre-training.
Fully supervised pre-training benefits from a strong semantic learning signal for each training example, but does not scale well because manual labeling of training data is time-consuming. By contrast, self-supervised pre-training receives hardly any semantic information on the training examples, but can be scaled to billions of training examples relatively easily [27, 31]. Weakly-supervised ap-proaches fall somewhere in between: for example, hashtags or other text associated with visual data generally provide a noisy semantic learning signal but can be obtained at large scale with relative ease [49, 57].
Following the success of prior work [49], this paper per-forms an in-depth study of weakly-supervised pre-training using hashtag supervision. We pre-train modern image-recognition models on the largest-ever-dataset of images and associated hashtags, and evaluate the resulting models in a range of transfer-learning experiments. Specifically, we transfer our models to a variety of image-classification tasks and evaluate the performance of the resulting models. We also evaluate the models in zero-shot transfer and few-shot transfer settings [57]: that is, we evaluate the “off-the-shelf performance” of these models without finetuning them on the target tasks. The overall goal of our study is to shed light on the trade-offs between fully supervised, self super-vised, and weakly supervised pre-training. Throughout our experiments, we find the weakly-supervised approach to be very competitive: our best models perform on par with the state-of-the-art on a range of visual-perception tasks despite employing a relatively simple training pipeline.
A potential downside of weakly-supervised pre-training is that models may inherit or amplify harmful associations from the underlying supervisory signal. We perform a series of experiments aimed at assessing the extent to which this happens. Our results do not provide conclusive answers, but they do suggest that the risks involved may not be as large as in language modeling [6, 9]. Overall, we believe our study presents a compelling argument for weakly-supervised pre-training of visual-recognition systems.
2.