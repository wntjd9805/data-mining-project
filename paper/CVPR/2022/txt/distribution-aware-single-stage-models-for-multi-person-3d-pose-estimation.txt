Abstract
In this paper, we present a novel Distribution-Aware for tackling the challenging
Single-stage (DAS) model multi-person 3D pose estimation problem. Different from existing top-down and bottom-up methods, the proposed
DAS model simultaneously localizes person positions and their corresponding body joints in the 3D camera space in a one-pass manner. This leads to a simplified pipeline with enhanced efficiency. In addition, DAS learns the true distribution of body joints for the regression of their posi-tions, rather than making a simple Laplacian or Gaussian assumption as previous works. This provides valuable pri-ors for model prediction and thus boosts the regression-based scheme to achieve competitive performance with volumetric-base ones. Moreover, DAS exploits a recur-sive update strategy for progressively approaching to re-gression target, alleviating the optimization difficulty and further lifting the regression performance. DAS is imple-mented with a fully Convolutional Neural Network and end-to-end learnable. Comprehensive experiments on bench-marks CMU Panoptic and MuPoTS-3D demonstrate the su-perior efficiency of the proposed DAS model, specifically 1.5x speedup over previous best model, and its stat-of-the-art accuracy for multi-person 3D pose estimation. 1.

Introduction
Estimating 3D poses of multiple persons from a single
RGB image is a fundamental yet challenging task in com-puter vision, which targets at localizing 3D positions of per-sons and their body joints in the camera space. Recently, it has drawn much attention thanks to the broad applications in AR/VR [2, 14], Gaming [30, 31, 33], Human-Computer
Interaction [7, 32], etc.
Prior works address this task via two-stage strategies.
They either adopt the top-down scheme [22, 28, 35, 41] that
â€ Corresponding author.
Figure 1. Overview of our Distribution-Aware Single-stage (DAS) model for multi-person 3D pose estimation. i) The brief pipeline of DAS. ii) Comparison with state-of-the-arts on MuPoTs-3D.
DAS outperforms two-stage models in efficiency. It achieves supe-rior performance with SOTA bottom-up method while competitive with SOTA top-down one. For PCKrel, the higher, the better. first localizes the absolute 3D person positions and then sep-arately estimates the root-relative body joints for each per-son; or the bottom-up scheme [8,21,47] that in the first stage detects all 3D body joints and groups them into the corre-sponding persons in the second stage. Although achieving good accuracy, these methods suffer redundant computation and complex postprocessing, caused by the sequential man-agement for person position and body joint localization in a two-stage manner. This leads to unsatisfied efficiency for deployment in real scenarios.
Given the above motivation, we propose to simplify the pipeline for multi-person 3D pose estimation, further push-ing forward the frontier of its applications.
Inspired by schemes in the 2D counterpart [23, 40, 48], we aim to de-sign a single-stage model to simultaneously localize 3D per-son positions and body joints. However, the extension from 2D to 3D scenes is non-trival, due to the ill-posed setup for deriving depth information from a monocular RGB image without prior knowledge on the data distribution.
To achieve the goal, we present a novel Distribution-Aware Single-stage (DAS) model in this paper. The pro-posed DAS tackles the ill-posed problem of multi-person 3D pose estimation from two aspects: 1) DAS represents the 3D pose with a 2.5D human center together with 3D center-relative joint offsets. This adapts to direct depth pre-diction from the image domain as well as enables to unify the 3D localization of person position and body joints, mak-ing the monocular-based one-pass solution feasible; 2) DAS learns the true distribution of body joints during the model optimization. This provides valuable guidance to predict the joint position, thus boosting the performance of the regression-based scheme. To alleviate the distribution es-timation difficulty, DAS exploits a recursive update strategy to progressively approach to the targets. In this way, DAS can efficiently generate accurate 3D poses of multiple per-sons from a single RGB image.
In particular, we build DAS with a regression-based pipeline that outputs the 3D human poses via a single for-ward inference from an input image. As depicted in Fig-ure 1 i) (a) and (b), DAS models the human center with a center confidence map and a center coordinate map. DAS uses the center confidence map for localizing the projected human centers in 2D image coordinate space, while the cen-ter coordinate map for pixel-wisely estimating the absolute center positions in 3D camera coordinate space. DAS uti-lizes joint offset maps to densely encode 3D center-relative locations of body joints, as shown in Figure 1 i) (c). DAS can produce these three kinds of maps in parallel and then easily reconstruct multiple 3D human poses with them, avoiding redundant computation and complex association.
With this compact single-stage pipeline, DAS can achieve superior efficiency over prior two-stage ones.
To optimize the regression-based model, prior works al-ways adopt the conventional L1 or L2 loss. However, re-searches [12] have proven that this kind of supervision ac-tually makes a simple Laplacian or Gaussian assumption on the data distribution, which is far way from the true one.
In contrast, DAS learns the underlying distribution of 3D body joint locations via exploiting the normalizing flows [1, 5, 27]. This helps to derive a suitable distribution for model output, thus providing valuable priors to guide the regression of body joint coordinates. Together with the pose regression modules, DAS optimizes the distribution learn-ing module through the maximum likelihood estimation in the training phase, while removes it in the inference phase.
In this way, DAS boosts the regression performance without additional computation cost. Besides, DAS iteratively up-dates joint offsets by ensemble of informative predictions around regression targets to further facilitate the localiza-tion of body joints. With this distribution-aware design,
DAS can achieve outperforming accuracy with bottom-up methods while competitive with top-down ones, as shown in Figure 1 ii).
We implement DAS with a fully Convolutional Neu-ral Network, which is end-to-end learnable. Comprehen-sive experiments on benchmarks CMU Panoptic [10] and
MuPoTS-3D [21] show the superiority of our DAS model.
In summary, our contribution are in three folds: (1) We present a novel single-stage model for estimating 3D poses of multiple persons from a monocular RGB image, which overcomes the drawbacks on computation cost and model complexity occurred in two-stage methods; (2) We present a novel distribution-aware way to boost the body joint regres-sion in a recursive manner; (3) We set new state-of-the-art on multiple benchmarks with superior efficiency. 2.