Abstract
Visual Emotion Analysis (VEA) is attracting increasing attention. One of the biggest challenges of VEA is to bridge the affective gap between visual clues in a picture and the emotion expressed by the picture. As the granularity of emo-tions increases, the affective gap increases as well. Existing deep approaches try to bridge the gap by directly learning discrimination among emotions globally in one shot. They ignore the hierarchical relationship among emotions at dif-ferent affective levels, and the variation in the affective level of emotions to be classified. In this paper, we present the multi-level dependent attention network (MDAN) with two branches to leverage the emotion hierarchy and the corre-lation between different affective levels and semantic levels.
The bottom-up branch directly learns emotions at the high-est affective level and largely prevents hierarchy violation by explicitly following the emotion hierarchy while predict-ing emotions at lower affective levels. In contrast, the top-down branch aims to disentangle the affective gap by one-to-one mapping between semantic levels and affective lev-els, namely, Affective Semantic Mapping. A local classifier is appended at each semantic level to learn discrimination among emotions at the corresponding affective level. Then, we integrate global learning and local learning into a uni-fied deep framework and optimize it simultaneously. More-over, to properly model channel dependencies and spatial attention while disentangling the affective gap, we care-fully designed two attention modules: the Multi-head Cross
Channel Attention module and the Level-dependent Class
Activation Map module. Finally, the proposed deep frame-work obtains new state-of-the-art performance on six VEA benchmarks, where it outperforms existing state-of-the-art methods by a large margin, e.g., +3.85% on the WEBEmo dataset at 25 classes classification accuracy. 1.

Introduction
Visual Emotion Analysis (VEA) is a high-level abstrac-tion task, which aims to recognize the emotion induced via
Figure 1. Illustration of a three-level emotion hierarchy defined by
Parrott in psychological studies. visual content. Recently, it raises more and more research attention due to the trend that social network users become more likely to express their opinions and emotions via vi-sual content. VEA has many practical applications, such as opinion mining [27], business intelligence, entertainment assistant [32] and personalized emotion prediction [31].
One of the biggest challenges of VEA is to bridge the affective gap between pixel-level information and the high-level emotion semantics [10, 29, 32]. To bridge the affec-tive gap, the key is to extract discriminative features [30].
Recent efforts have been devoted to improving the discrim-ination of learned features for fine-grained emotion classifi-cation in one shot globally, i.e., learning one discriminative feature through one global classifier [16, 28, 30, 34].
However, as the granularity of emotions increases, the affective gap becomes larger because of the higher affective level, described by the arrow in Fig. 1, making the global learning in one shot difficult and unreliable. In this work, we try to disentangle the affective gap into several smaller steps. Instead of merely learning the discriminative feature for fine-grained emotions globally, we introduce an extra top-down branch to learn level-wise discrimination. Specif-ically, we append a local classifier at each semantic level of the top-down branch and each of the local classifiers con-centrates on learning the discrimination among emotions at a particular affective level. In this paper, the term ‘lo-cal’ is equivalent to ‘level-wise’, since both of them refer to a particular affective level in an emotion hierarchy. The above-mentioned setting for the top-down branch is based on an assumption that there is a correlation between differ-down branch. For MHCCA, we refine and abridge multi-head attention mechanism [21], and extend it from mod-eling pixel interdependencies [2, 4, 18, 19, 22] into explor-ing channel attribute dependencies between feature maps at adjacent levels, Cross Attention. For L-CAM, it is similar to CAM proposed in [33] but we leverage the subordinate relationship between emotions at adjacent affective levels.
Specifically, the computed attention map is subject to the prediction results from the former affective level.
Our contributions are summarized as follow: First, we provide a new perspective for VEA, which is to disentangle the affective gap by learning the level-wise discrimination.
We study the Affective Semantic Mapping to achieve this.
Second, We purpose a novel multi-level dependent attention network consisting of two branches, classifying emotions both globally and locally in a simultaneous manner. Two new attention modules: the MHCCA module and the L-CAM module are appended at each semantic level to select important channel attributes and to highlight spatial details for each affective level. Finally, the proposed deep frame-work obtains new state-of-the-art performance on six VEA benchmarks, where it outperforms existing state-of-the-art methods by a large margin, e.g., +3.85% on WEBEmo, and
+2.07% on Emotion-6 at classification accuracy. 2.