Abstract
Self-supervised learning is a promising unsupervised learning framework that has achieved success with large floating point networks. But such networks are not read-ily deployable to edge devices. To accelerate deployment of models with the benefit of unsupervised representation learning to such resource limited devices for various down-stream tasks, we propose a self-supervised learning method for binary networks that uses a moving target network. In particular, we propose to jointly train a randomly initialized classifier, attached to a pretrained floating point feature ex-tractor, with a binary network. Additionally, we propose a feature similarity loss, a dynamic loss balancing and mod-ified multi-stage training to further improve the accuracy, and call our method BURN. Our empirical validations over five downstream tasks using seven datasets show that BURN outperforms self-supervised baselines for binary networks and sometimes outperforms supervised pretraining. Code is availabe at https://github.com/naver-ai/burn. 1.

Introduction
Self-supervised learning (SSL) has achieved great suc-cess with floating point (FP) networks in recent years [4, 5, 7, 9, 13, 14, 16, 18, 20, 28, 41, 42, 44, 50]. Models learned by SSL methods perform on par with or even outperform the ones learned by supervised pretraining by the help of large scale unlabeled data in a number of downstream tasks such as image classification [1, 5], semi-supervised fine-tuning [5, 7, 18] and object detection [20]. While recent works [5, 7, 18, 20] from resourceful research groups have shown that the gains from SSL scale up with model size and/or dataset size used for pretraining, there is little work where the resulting pretrained models are small in size, i.e., quantized. SSL for such small models is important since it could expedite the AI deployment for a wide range of appli-This work was done while DK and JC were an intern and an AI technical advisor at NAVER AI Lab., respectively. † indicates corresponding author.
Figure 1. Comparison of various representation learning meth-ods on multiple downstream tasks (pretrained with ImageNet).
‘Obj. Det.’ refers to object detection, ‘Lin. Eval’ refers to linear evaluation, ’SS 1/10% refers to semi-supervised fine-tuning with 1 or 10% data respectively,‘FS K=1’ refers to few-shot learning with 1 shot, and ‘Transfer (CUB)’ means transfer learning to the
CUB dataset. ‘Tuned MoCov2’ and ‘S2-BNN’ are SSL methods from [39]. Proposed BURN outperforms all comparable methods in various tasks, and even the Supervised Pre. in certain tasks. cations onto models with high efficiency in computational and memory costs, and energy consumption [12]. At the extreme of resource constrained scenarios, binary networks exhibit superior efficiency and the accuracy is being sig-nificantly improved [2, 3, 23, 31–34, 38]. Thus, developing an SSL method for binary networks could further acceler-ate the deployment of models to edge devices for various downstream tasks, yet is seldom explored.
Providing additional supervisory signals with a pre-trained FP network by using the KL divergence loss be-tween the softmax outputs from the classifiers of the FP tar-get network and binary network, which we denote as ‘super-vised KL div.’, has become a popular and effective method for training binary networks [2, 3, 32, 34]. Recently, [39] propose an unsupervised representation learning method for binary networks based on the supervised KL div. method.
To extract meaningful softmax probabilities from the FP network, they pretrain the classifier as well as the feature
extractor using SSL. Then, the FP network is completely frozen when used as the target network, which could lead to stale targets [18] or be dependent on the pretraining dataset used for the fixed FP network being similar to the dataset used for training the binary network.
Thus, to avoid the potential pitfalls of a fixed target, we are motivated to develop an SSL method for binary net-works that uses a moving FP network as the target, similar to other SSL methods [8, 9, 18, 20], and call our method
Binary Unsupervised RepresentatioN learning or BURN.
Specifically, we first construct the FP target network by combining a fixed FP feature extractor pretrained in an SSL manner and a randomly initialized FP classifier. We then use the outputs of the randomly initialized FP classifier as targets for the binary network and jointly optimize both the FP classifier and the binary network, using the KL di-vergence loss, to keep updating the FP network overtime.
But the gradients provided by the randomly initialized FP classifier could have unexpectedly large magnitudes, espe-cially during early training phase. To alleviate this problem, we additionally propose to enforce feature similarity across both precision, providing stable gradients that bypass the randomly initialized classifier. As relative importance of the feature similarity loss decreases as the FP classifier gets jointly trained to provide less random targets, we further propose to dynamically balance the KL divergence term and the feature similarity term in the loss function. Finally, we modify the multi-stage training scheme [34] for BURN to further improve performance.
We conduct extensive empirical validations with a wide variety of downstream tasks such as object detection on Pas-cal VOC, linear evaluation on ImageNet, semi-supervised fine-tuning on ImageNet with 1% and 10% labeled data,
SVM classification and few-shot SVM classification on
Pascal VOC07, and transfer learning to various datasets such as CIFAR10, CIFAR100, CUB-200-2011, Birdsnap, and Places205.
In the validations, the binary networks trained by our method outperforms other SSL methods by large margins (see Fig. 1 and Sec. 4.1).
We summarize our contributions as follows:
• We propose a novel SSL method for binary networks that uses a jointly trained FP classifier to obtain targets that can adapt overtime to the current training scenario.
• We propose to use a feature similarity loss and dynamic balancing with modified multi-stage training to signifi-cantly improve the accuracy.
• Our BURN outperforms prior arts by large margins on a wide variety of downstream tasks.
• We analyze our proposed BURN by in-depth investiga-tions. 2.