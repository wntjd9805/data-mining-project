Abstract
We present Neural Kernel Fields: a novel method for reconstructing implicit 3D shapes based on a learned kernel ridge regression. Our technique achieves state-of-the-art results when reconstructing 3D objects and large scenes from sparse oriented points, and can reconstruct shape categories outside the training set with almost no drop in accuracy.
The core insight of our approach is that kernel methods are extremely effective for reconstructing shapes when the chosen kernel has an appropriate inductive bias. We thus factor the problem of shape reconstruction into two parts: (1) a backbone neural network which learns kernel parameters from data, and (2) a kernel ridge regression that ﬁts the input points on-the-ﬂy by solving a simple positive deﬁnite linear system using the learned kernel. As a result of this factorization, our reconstruction gains the beneﬁts of data-driven methods under sparse point density while maintaining interpolatory behavior, which converges to the ground truth shape as input sampling density increases. Our experiments demonstrate a strong generalization capability to objects outside the train-set category and scanned scenes. Source code and pretrained models are available at https:// nv-tlabs.github.io/nkf. 1.

Introduction of a shape to a 3D implicit surface for that shape. Surface reconstruction from point clouds is a well studied topic in computer vision and graphics, with applications in robotics, entertainment, and manufacturing. Techniques for surface reconstruction broadly fall into two types: implicit meth-ods which aim to recover a volumetric function whose zero level-set encodes the surface, and explicit methods which directly recover a triangle mesh from the input points. While implicit approaches can adapt to arbitrary topologies, the requirement to store a dense volumetric ﬁeld led many past works to favor explicit approaches [42, 19]. More recently, implicit approaches have regained popularity due to a num-ber of works demonstrating that neural networks are com-pact and effective at encoding signed-distance [43, 55] and occupancy ﬁelds [39, 46]. These works pair neural ﬁeld1 rep-resentations with modern advances in point cloud processing architectures to produce powerful reconstruction techniques.
Current state-of-the-art shape reconstructions methods can be categorized along three axes (Fig. 3): (1) Feed-forward vs. test-time optimization: Feed-forward methods leverage shape priors to directly predict a surface from input points. While these methods are fast, they are not strictly constrained by their input and thus may perform a task more akin to retrieval than reconstruction (see [57]
The goal of 3D reconstruction is to recover geometry from partial measurements of a shape. In this work, we aim to map a sparse set of oriented points sampled from the surface 1A neural ﬁeld refers to the parameterization of a continuous function of spatial coordinates using a neural network. In this work we focus on scalar functions mapping coordinates to real numbers.
*Denotes equal contribution.
and Fig. 2, top). This results in decreased generalization performance on out-of-distribution shapes and input point densities. In contrast, test-time optimization via latent space traversal allows adaptation to the input, but is slow and can converge to poor local minima (See e.g. [16] and Fig. 2, bottom). (2) Whether or not to leverage data priors: Data-free meth-ods recover the surface by minimizing the residuals between the reconstructed surface and input points, leveraging a pre-determined prior to control the behavior away from the input points (e.g. a smooth space of functions [33, 62] or, emergent regularization arising from neural architectures [61, 25]).
Such ﬁxed priors are, however, difﬁcult to tailor to speciﬁc tasks, like completion of partial shapes (Fig. 2, middle).
Data-driven approaches, on the other hand, can learn task-speciﬁc priors to predict shapes that resemble a given dataset. (3) Which scale to process and represent data. Local-scale methods [32, 4] use the idea that complex structures can be reduced to a collection of simpler geometric primitives.
These methods learn local models which are used to recon-struct a surface in patches. While this approach can general-ize better, patch-size plays a critical role and must be care-fully tuned per object (Fig. 2, bottom). Furthermore, without any notion of global context, these methods are unable to complete larger missing regions, leaving a fundamental gap in their generalization performance.
Based on these axes and the motivating examples in Fig. 2, we identify the need for a method that can learn good priors from a simple collection of shapes to drive 3D reconstruc-tion of both in-distribution and out-of distribution shapes and scenes. In particular, the priors learned by this method should respect the input points, performing reconstruction rather than retrieval.
We thus propose a method using a novel representation of neural ﬁelds based on learned kernels, which we call Neural
Kernel Fields (NKFs). In brief, NKFs work by learning a pos-itive deﬁnite kernel conditioned on an input point cloud, and then using that kernel to predict an implicit shape by solv-ing a simple linear system (Fig. 4). Our approach provides several key beneﬁts: First, since predicted kernels are con-ditioned on the input and learned from data, they enjoy the versatility of learning-based methods. Second, since NKFs leverage a kernel for shape prediction, any reconstructed surfaces respect the input points by construction. Third, unlike gradient descent-based latent space optimization, at test-time NKF kernel weights are solved in closed form via a simple convex least-squares problem, guaranteeing good minima. Finally, our kernel acts as a global aggregator of spatially local features, allowing our method to work at a wide variety of sampling densities without tuning any scale parameters. The result is a generalizable method that can be trained only on synthetic shapes to seamlessly reconstruct out of distribution shapes and large scale scenes, while being
SPSR [33]
NS [62]
Ours
OccNet [39]
C-OccNet [46]
Ours
LIG [32] (0.1)
LIG [32] (0.3)
Ours
Figure 2: Comparison of our approach with methods along three Axes in Sec. 1. Top Row: Data free methods [33, 62] respect the input points but their simple ﬁxed priors cannot complete the partial shape. Middle Row: Feed-forward methods [46, 46] learn from data, but miss the slats on the slightly out of distribution canoe. Bottom Row: LIG [32], a local method which performs test-time optimization, is very sensitive to the choice of patch size (0.3 left vs 0.1 middle), and gets stuck in bad local minima (bumpy artefacts). robust to changes in input point density. Compared with the baselines, our method achieves a marked improvement re-construction detail on both in and out-of distribution shapes.
We summarize our contributions as follows:
• We introduce Neural Kernel Fields, a novel represen-tation of neural ﬁelds for 3D reconstruction, which outputs highly detailed surfaces that respect the input points.
• Our NKF representation achieves state of the art perfor-mance on ShapeNet reconstruction (Section 4.1).
• We show state-of-the-art generalization performance on out-of-distribution shapes (Section 4.3), scenes (Sec-tion 4.4) and point densities (Section 4.5) 2.