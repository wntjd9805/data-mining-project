Abstract
In this paper, we present a system to train driving policies from experiences collected not just from the ego-vehicle, but all vehicles that it observes. This system uses the behav-iors of other agents to create more diverse driving scenarios without collecting additional data. The main difficulty in learning from other vehicles is that there is no sensor in-formation. We use a set of supervisory tasks to learn an intermediate representation that is invariant to the viewpoint of the controlling vehicle. This not only provides a richer sig-nal at training time but also allows more complex reasoning during inference. Learning how all vehicles drive helps pre-dict their behavior at test time and can avoid collisions. We evaluate this system in closed-loop driving simulations. Our system outperforms all prior methods on the public CARLA
Leaderboard by a wide margin, improving driving score by 25 and route completion rate by 24 points. 1.

Introduction
Autonomous driving has been one of the most anticipated technologies since the advent of modern-day artificial in-telligence. However, even after decades of exploration, we have yet to see self-driving cars deployed at scale. One main reason is the generalization. The world and its drivers are more diverse than current planning approaches can han-dle. Hand-designed classical planning [3, 16, 29, 45] does not generalize gracefully to unseen or unfamiliar scenarios.
Learning based methods [4, 9, 11, 14, 37] fare better, but suffer from a long tail of driving scenarios. The majority of driving data consist of easy and uninteresting behaviors.
After all, humans drive thousands of hours before observing a traffic accident [43], especially when driving an expensive autonomous test vehicle. How do we tame the long-tail of driving scenes? While many approaches rely on carefully crafted safety-critical scenarios in simulation [33, 36, 42], or collect massive data in the real world [4, 41], in this paper we focus on an orthogonal direction.
We observe that, although many of us have not experi-enced traffic accidents ourselves, everyone has at least ob-served several accidents throughout our driving career. The
Figure 1. We present LAV, a mapless, learning-based end-to-end driving system. LAV takes as input multi-modal sensor readings and learns from all nearby vehicles in the scene for both perception and planning. At test time, LAV predicts multi-modal future trajec-tories for all detected vehicles, including the ego-vehicle. Picture credit – Waymo open dataset [41]. same applies to safety-critical driving scenarios: While the data-collecting ego-vehicle might not experience accident-prone situations itself, it is likely its driving logs contain states that are interesting or safety-critical, but experienced by other vehicles. Training on other vehicles’ trajectories helps not only with sample efficiency, but also greatly in-crease the chance that the model sees interesting scenarios.
Moreover, knowing other vehicles’ future trajectories helps the ego-vehicle avoid collisions.
The main challenge with training on all vehicles lies in the partial observability of other vehicles. Unlike the ego-vehicle, other vehicles have only partially observed motion trajectories, exposing no control commands or higher-level goals. This makes direct training [10, 11, 12, 14, 38] on other vehicles’ traces close to impossible. More importantly, other vehicles have no accessible sensors. To learn from other vehicles, a model has to infer their surrounding state using the ego-vehicle’s sensors.
Our framework, Learning from All Vehicles (LAV), han-dles the partial observability of both perception and motion in one joint recognition, prediction, and planning stack. We decouple the partial observability challenge of perception and action using a privileged distillation approach [11]. LAV first learns a perception model that outputs a viewpoint in-variant representation using auxiliary supervision from 3D detection and segmentation tasks. By definition, this auxil-iary task does not distinguish between the ego-vehicle and other vehicles in the scene and thus learns a viewpoint in-variant representation. It handles the partial observability of sensors. In parallel, LAV learns a privileged motion plan-ner [11]. Instead of predicting steering and acceleration, which are only available for the ego-vehicle, we use future waypoints to represent the motion plan. We use ground-truth computer-vision labels as inputs to the privileged motion planner. Computer-vision labels ensure viewpoint invari-ance, waypoints provide an invariant representation of mo-tion. The privileged motion planner predicts trajectories of all nearby vehicles and infers their high-level commands.
Finally, we combine the two models in a joint framework using privileged distillation [11]. This final distillation learns a motion prediction model from all vehicles using the view-point invariant vision features of the perception model. The distilled policy drives from raw sensor inputs alone.
We validate our method in the CARLA driving simu-lator [17]. At the time of submission, our method ranks first on the CARLA public leaderboard1. It attains a 61.85 driving score and a 94.46 route completion rate. Both are the highest among all methods and outperform the prior state-of-the-art method by a wide margin, increas-ing driving score and route completion rate by 25 and 24 points respectively. Our method has also won the 2021
CARLA Autonomous Driving challenge2. Code available at https://github.com/dotchen/LAV. 2.