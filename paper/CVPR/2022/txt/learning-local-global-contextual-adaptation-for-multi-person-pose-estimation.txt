Abstract
This paper studies the problem of multi-person pose es-timation in a bottom-up fashion. With a new and strong observation that the localization issue of the center-offset formulation can be remedied in a local-window search scheme in an ideal situation, we propose a multi-person pose estimation approach, dubbed as LOGO-CAP, by learn-ing the LOcal-GlObal Contextual Adaptation for human
Pose. Specifically, our approach learns the keypoint attrac-tion maps (KAMs) from the local keypoints expansion maps (KEMs) in small local windows in the first step, which are subsequently treated as dynamic convolutional kernels on the keypoints-focused global heatmaps for contextual adap-tation, achieving accurate multi-person pose estimation.
Our method is end-to-end trainable with near real-time in-ference speed in a single forward pass, obtaining state-of-the-art performance on the COCO keypoint benchmark for bottom-up human pose estimation. With the COCO trained model, our method also outperforms prior arts by a large margin on the challenging OCHuman dataset. 1.

Introduction 2D human pose estimation is a classical computer vision problem that aims to parsing articulated structures of hu-man parts from natural images. With rich and longstanding studies, we have witnessed great successes on single-person pose estimation [21, 31] by convolutional neural networks.
Therefore, it is of great interest in pushing the pose estima-tion from the single-person to multi-person configuration.
The problem of multi-person pose estimation from images has been extensively studied in top-down paradigms [21, 31, 36] that formulate the problem as single-person pose estimation with an off-the-shelf person detector with an impressively high AP (e.g, 56% on the COCO-2017 validation dataset [36]). Although the top-down paradigm has been dramatically pushed into a high-performing stages, it remains several problems in both aspects of efficiency and accuracy due to the dependency of detecting person bounding boxes. Motivated by this,
*Corresponding author
Figure 1. An illustrative example of multi-person pose estimation by the proposed LOGO-CAP with the HRNet-W32 backbone. For each initial pose obtained by the center-offset regression, LOGO-CAP learns 11 × 11 local filters for each joint, and then refines the initial keypoint by convolution with the learned kernels: The local filters are learned to refocusing those initially-less-accurate pose keypoints towards better placement. In more detailed, we show an example of the initial center-offset pose that only obtains the
OKS of 0.826 due to the misplacement between the keypoints of the right elbow and the right wrist. We mark the residual vectors between the predictions and the groundtruth with yellow and red dash lines respectively. The LOGO-CAP improves the OKS by 10.4%. Please see text for details. we are interested in studying the problem of multi-person pose estimation without incurring extra priors of bounding boxes, which is conventionally termed as bottom-up pose estimation.
Bottom-up pose estimation approaches pay much atten-i
P
A s t n o p y e
K
O
C
O
C 88.9
W48-512 70.0
W32-512 65.0
W32-384 60.0 50
An Empirical Upper Bound
LOGO-CAP (Ours)
DEKR [11]
PifPaf [14]
CenterNet [40]
AE+HrHRNet-W32 [5]
Baseline-W32
W48-640 100 150
Inference time (in ms) 200
≥ 500
Figure 2. Our LOGO-CAP is motivated by a strong empirical ob-servation that a vanilla center-offset baseline with an AP of 60.1 (marked by green dashed line) can be improved by leveraging a searching scheme in 11 × 11 local windows to an AP of 88.9 (marked by red dashed line). In the meanwhile, we illustrate the speed-accuracy comparisons between our LOGO-CAP and prior arts on the COCO val-2017 dataset. Wx-Y (e.g. W32-384) means that a model uses the backbone HRNet-Wx [31] and is tested with the image resolution Y in the short side. tion on estimating pose parameters from any given multi-person image instead of using the cropped single-person ones, which poses several challenges on accurately iden-tifying the learned bottom-level keypoints as person parts and grouping them into different individuals by learning part affinity fields [2, 3], part association fields [14], asso-ciative embedding [20]. Those grouping approaches are accurate but sophisticated due to the necessity of ad-hoc grouping/decoding schemes. Recently, many researchers attempted to learning center-offsets [6, 11, 30, 40] for pose estimation as its intrinsic simplicity and efficiency. How-ever, most of the center-offset approaches are suffering from the main challenge of localization inaccuracy due to the large structural variations of human pose, thus leading to inferior performance than the grouping ones.
In this paper, we are interested in studying the bottom-up pose estimation using the center-offset formulation for its simplicity and efficiency. We directly address the afore-mentioned main challenges for center-offset based multi-person pose estimation. Our proposed approach is moti-vated by a surprisingly strong empirical observation via an-alyzing what the fundamental issues are with the vanilla center-offset pose estimation network.
An Empirical Observation. Our analyses are based on results in the fully-annotated subset of the COCO val-2017 dataset1. The vanilla center-offset regression method uti-lizes the HRNet-W32 [31] as the feature backbone to di-rectly predict keypoints center heatmap and the offset vec-1Note that the COCO-val-2017 dataset contain many partially-annotated images (with only ground-truth bounding boxes), we use 2346 testing samples that are fully annotated with keypoint annotations. tors. As shown in Fig. 2, it obtains 60.1 average preci-sion (AP), which is not great, but reasonably good.
It clearly shows that the pose keypoints center and the off-set vectors can be learned reasonably well. We ask: How exactly bad are the center-offset estimation? We want to know (i) whether some of keypoints are truly bad being far away from the underlying ground-truth (GT) locations, or (ii) whether most of them are already in the close proxim-ity of the GT ones? We observe that the latter is true. To quantitatively characterize the close proximity, instead of directly utilizing the learned offset vectors for human pose estimation, we treat them as human pose keypoint initializa-tion and do a local window search to compute the empirical upper-bound of performance. More specifically, based on the initially predicted human poses, by introducing a local window (e.g., 11 × 11) centered at each detected key point and by computing the single keypoint similarity with the ground-truth keypoint, an empirical upper-bound of 88.9
AP is obtained, which is significantly higher than the state of the art and shows the potential of improving the vanilla center-offset regression paradigm.
A Direct Solution Leveraging the Observation. The im-plication of the observation is significant: It reveals that the fundamental challenge of improving the center-offset ap-proach for pose estimation is to resolve the local misplace-ment. To that end, a straightforward way is just to learn a local heatmap (e.g., 11 × 11) for each human pose keypoint based on the learned center and offset vectors, and then to compute the refined keypoints by taking arg max within the local heatmap. Although appealing, this does not work as observed during our development of the LOGO-CAP. The underlying reason is also straightforward: if this can work, the original offset vector regression should work at the first place since no additional information is introduced through learning the local heatmap.
We hypothesize that on the one hand, in addition to the local heatmap, the structural relationship between different pose keypoints needs to be taken into account, and on the other hand, the intrinsic uncertainty of the local informa-tion in a local heatmap needs to be resolved. The former is the key challenge of structured output prediction problems.
Many message passing algorithms have been developed in the literature. The latter can not be addressed by simply in-creasing the local window size. It entails learning stronger local-global information interaction and adaptation.
To verify the two hypotheses, the proposed LOGO-CAP lifts the initial keypoints via the center-offset predic-tion to keypoint expansion maps (KEMs) to counter their lack of localization accuracy in two modules (Section 3).
The KEMs extend the star-structured representation of the center-offset formulation to the pictorial structure represen-tation [9, 10]. As shown in Fig. 1, in our LOGO-CAP, one module computes local KEMs and learns to account for the
structured output prediction nature of the human pose esti-mation problem, resulting in the keypoint attraction maps (KAMs), i.e., the local filters in Fig. 1. Another mod-ule computes global KEMs and learns to refine the global
KEMs by integrating the KAMs.
Our LOGO-CAP is a fully end-to-end bottom-up hu-man pose estimation method with near real-time inference speed. It obtains 70.0 AP in the fully-annotated subset of the COCO val-2017 dataset, which is an absolute increase of 9.9 AP compared to the vanilla center-offset method, making a significant step forward. Fig. 2 shows the advan-tage of the proposed LOGO-CAP in terms of overall speed-accuracy comparisons between our LOGO-CAP and prior arts. Meanwhile, we should notice that there is still a sig-nificant gap towards the empirical upper bound in Fig. 2, which encourages more work to be investigated. 2.