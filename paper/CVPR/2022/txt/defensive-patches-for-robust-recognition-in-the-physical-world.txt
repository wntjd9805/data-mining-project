Abstract
To operate in real-world high-stakes environments, deep learning systems have to endure noises that have been con-tinuously thwarting their robustness. Data-end defense, which improves robustness by operations on input data in-stead of modifying models, has attracted intensive attention due to its feasibility in practice. However, previous data-end defenses show low generalization against diverse noises and weak transferability across multiple models. Motivated by the fact that robust recognition depends on both local and global features, we propose a defensive patch genera-tion framework to address these problems by helping mod-els better exploit these features. For the generalization against diverse noises, we inject class-specific identifiable patterns into a confined local patch prior, so that defensive patches could preserve more recognizable features towards specific classes, leading models for better recognition un-der noises. For the transferability across multiple models, we guide the defensive patches to capture more global fea-ture correlations within a class, so that they could activate model-shared global perceptions and transfer better among models. Our defensive patches show great potentials to im-prove application robustness in practice by simply sticking them around target objects. Extensive experiments show that we outperform others by large margins (improve 20+% accuracy for both adversarial and corruption robustness on average in the digital and physical world).1 1.

Introduction
Though deep neural networks (DNNs) have achieved significant successes in multiple areas [25, 56, 58], their robustness is challenged by noises, especially in physical
*Corresponding author 1Our codes are available at https : / / github. com / nlsde -safety-team/DefensivePatch. (a) (c) (b) (d)
Figure 1. (a) Different guideboards in the physical world. (b) Sam-ples with generated defensive patches in the digital world. (c) The model prediction is misled into Turn Right when it is snowy in the physical world. (d) Defensive patches can help models to conduct correct predictions during snow in the physical world. world scenarios. Adversarial noise, an imperceptible per-turbation designed to mislead the decision of DNNs, is now becoming a great threats [9,44]. Besides adversarial attacks,
DNNs also show weak robustness against common corrup-tions in the daily environment (e.g., snow, rain, brightness etc) [14, 15]. For example, the guide boards will be incor-rectly classified as Turn Right when it is snowy (Figure (1c)). What’s worse, these inevitable noises have caused dozens of self-driving accidents with casualties and are cast-ing a shadow over the deep learning applications in prac-tice [22]. This urges us to investigate feasible defenses for building robust deep learning models in the physical world.
In the past years, a great number of efforts have been made to defend against the adversarial perturbations and further improve model robustness [12, 28, 32, 39, 57]. Most of the existing works focus on enhancing robustness from model-end (e.g., data augmentation, adversarial training),
which require an additional cost of the model architecture modification or model retraining. In contrast, another line of studies performs defenses from the data-end without im-posing any model modification (e.g., input transformation), which has shown great potential in practice [38, 54]. For example, by simply sticking a patch on the traffic sign, our proposed defensive patch can help DNNs to make ro-bust recognition under noises (Figure 1d). Though showing great application potential, existing data-end defenses show several limitations when applied in practice: (1) Weak gen-eralization for diverse noises. Existing works show a signif-icant drop when facing different unseen noises (e.g., adver-sarial attacks, common corruptions). (2) Low transferabil-ity across multiple models. In other words, these works fail to perform defenses for black-box models and even being counteractive. We attribute this phenomenon to the under-utilization of robust recognition characteristics.
To address the problems mentioned above, this paper proposes a data-end defensive patch generation framework, which could be effective against diverse noises and work among different models (Figure (1b) and Figure (1d)). Pre-vious studies have revealed strong evidence that robust recognition highly depends on the exploitation of local and global features [35, 36, 55], we thereof improve the de-fense ability of our defensive patches by promoting better exploitation of both local and global features. Regarding the generalization against diverse noises, since deep learn-ing models rely strongly on the local patterns for predic-tions [17, 24, 30], we optimize the locally confined patch priors to contain more class-specific identifiable patterns via reducing model uncertainty. Based on these class-level patch priors, the defensive patches can preserve more rec-ognizable features for a specific class and help models to better resist the influence of different noises, i.e., better gen-eralization. As for the transferability across multiple mod-els, recent studies found that different models share sim-ilar global perception during decision-making [2, 21, 48], we thus guide the defensive patches to capture more class-wise global feature correlations. In other words, the defen-sive patches could contain more global features correlated to the class. Thus, the generated defensive patches could better activate the model-shared global perception and en-joy stronger transferability among multiple models. In con-clusion, our main contributions can be summarized as:
• To the best of our knowledge, we are the first to gener-ate data-end defensive patches that could improve ap-plication robustness against diverse noises (adversarial attacks and corruptions) among different models.
• Our defensive patches improve robustness by injecting local identifiable patterns and enhancing global per-ceptual correlations, which can be easily deployed via sticking them around target objects.
• Extensive experiments show that our defensive patch outperforms others by large margins (+20% accuracy for both adversarial and corruption robustness on aver-age in digital and physical world). 2.