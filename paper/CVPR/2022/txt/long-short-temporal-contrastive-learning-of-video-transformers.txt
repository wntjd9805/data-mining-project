Abstract
Video transformers have recently emerged as a competi-tive alternative to 3D CNNs for video understanding. How-ever, due to their large number of parameters and reduced inductive biases, these models require supervised pretrain-ing on large-scale image datasets to achieve top perfor-mance. In this paper, we empirically demonstrate that self-supervised pretraining of video transformers on video-only datasets can lead to action recognition results that are on par or better than those obtained with supervised pretrain-ing on large-scale image datasets, even massive ones such as ImageNet-21K. Since transformer-based models are ef-fective at capturing dependencies over extended temporal spans, we propose a simple learning procedure that forces the model to match a long-term view to a short-term view of the same video. Our approach, named Long-Short Tem-poral Contrastive Learning (LSTCL), enables video trans-formers to learn an effective clip-level representation by predicting temporal context captured from a longer tempo-ral extent. To demonstrate the generality of our findings, we implement and validate our approach under three differ-ent self-supervised contrastive learning frameworks (MoCo v3, BYOL, SimSiam) using two distinct video-transformer architectures, including an improved variant of the Swin
Transformer augmented with space-time attention. We conduct a thorough ablation study and show that LSTCL achieves competitive performance on multiple video bench-marks and represents a convincing alternative to supervised image-based pretraining. 1.

Introduction
Since the introduction of AlexNet [36], deep convolu-tional neural networks (CNNs) have emerged as the promi-nent model in numerous computer vision tasks [21, 22, 30, 54, 64, 65]. More recently, the Transformer model [61] has received much attention due to its impressive performance in the field of natural language processing (NLP) [15].
While CNNs rely on the local operation of convolution, the building block of transformers is self-attention [61] which is particularly effective at modelling long-range de-pendencies. In the image domain, the Vision Transformer (ViT) [16] was proposed as a convolution-free architecture which uses self-attention between non-overlapping patches in all layers of the model. ViT was shown to be competi-tive with state-of-the-art CNNs on the task of image cate-gorization. In the last few months, several adaptations of
In order to
ViT to video have been proposed [3, 6, 44]. capture salient temporal information from the video, these works typically extend the self-attention mechanism to op-erate along the time axis in addition to within each frame.
Since video transformers have a larger numbers of param-eters and fewer inductive biases compared to CNNs, they typically require large-scale pretraining on supervised im-age datasets, such as ImageNet-21K [52] or JFT [3], in or-der to achieve top performance.
Self-supervised learning has been shown to be an effec-tive solution to eliminate the need for large-scale supervised pretraining of transformers both in NLP [15] as well as in image-analysis [9, 59]. In this work, we show that, even in the video domain, self-supervised learning provides an ef-fective way for pretraining video transformers. Specifically, we introduce Long-Short Temporal Contrastive Learning (LSTCL), a contrastive formulation that maximizes repre-sentation similarity between a long video clip (say, 8 sec-onds long) and a much shorter clip (say, 2 seconds long) where both clips are sampled from the same video. We ar-gue that by training the short-clip representation to match the long-clip representation, the model is forced to extrap-olate from a short extent the contextual information exhib-ited in the longer temporal span. As the long clip includes temporal segments not included in the short clip, this self-supervised strategy trains the model to anticipate the future and to predict the past from a small temporal window in or-der to match the representation extracted from the long clip.
We believe that this is a good pretext for video representa-tion learning, as it can be accomplished only by a successful understanding and recognition of the structure and correla-tion of atomic actions in a long video. Furthermore, such framework is particularly suitable for video transformers as they have been recently shown to effectively capture long-term temporal cues [6]. In this work we demonstrate that these long-term temporal cues can be effectively encoded
into a short-range clip-level representation leading to a sub-stantial improvement in video classification performance.
To demonstrate the generality of our findings, we ex-periment with two different video transformer architec-tures whose code is publicly available. The first is TimeS-former [6], which reduces the computational cost of self-attention over the 3D video volume by means of a space-time factorization. The second architecture is the Swin transformer [39], which we further extend into a 3D ver-sion, dubbed Space-Time Swin transformer, that computes hierarchical spatiotemporal self-attention by using 3D shift-ing windows. We show that our unsupervised LSTCL pre-training scheme allows both of these video transformers to outperform their respective counterparts pretrained with full supervision on the large-scale ImageNet-21K dataset.
In summary, the contributions of this paper can be sum-marized as follows:
• We introduce Long-Short Temporal Contrastive Learning (LSTCL), which enables encoding temporal context from the longer video into a short-range clip representation.
• We demonstrate that for recent video transformer models, our proposed LSTCL pretraining provides an effective al-ternative to large-scale supervised pretraining on images.
• We propose a Space-Time Swin transformer for spa-tiotemporal feature learning, and show that it achieves strong results on multiple action recognition benchmarks. 2.