Abstract 3D-aware generative models have shown that the intro-duction of 3D information can lead to more controllable image generation. In particular, the current state-of-the-art model GIRAFFE [38] can control each object’s rotation, translation, scale, and scene camera pose without corre-sponding supervision. However, GIRAFFE only operates well when the image resolution is low. We propose GI-RAFFE HD, a high-resolution 3D-aware generative model that inherits all of GIRAFFE’s controllable features while generating high-quality, high-resolution images (5122 res-olution and above). The key idea is to leverage a style-based neural renderer, and to independently generate the foreground and background to force their disentanglement while imposing consistency constraints to stitch them to-gether to composite a coherent final image. We demonstrate state-of-the-art 3D controllable high-resolution image gen-eration on multiple natural image datasets. 1.

Introduction
In image generation, two of the most important objec-tives are image realism and controllability. Style-based
GANs (i.e., StyleGAN [25] and its variants [24, 27]) can generate high-resolution, photorealistic images. However, while their latent style code design provides a level of dis-entanglement and controllability in 2D space (e.g., color and shape changes), their lack of explicit 3D information makes it difficult to impose 3D-level control over the gen-erated image content. Meanwhile, the recent NeRF [35] based GANs [5, 38, 43] have shown that explicit modeling of the scene in 3D space conditioned on camera pose can enable effective 3D-level control. However, the computa-tionally expensive nature of 3D representations has limited current 3D-aware generative models from directly learning and rendering images in high resolutions.
GIRAFFE [38] is the current state-of-the-art 3D-aware generative model for both image realism and controllability.
It models the foreground and background as two separate 3D objects, uses volume rendering to render the combined 3D features into low-resolution 2D feature maps, and finally uses a neural renderer to further render the feature maps into higher resolution images. These design choices enable
GIRAFFE to change the background’s appearance indepen-dent of the foreground, translate or rotate the foreground object in 3D, and change the foreground object’s shape and color. However, the neural renderer is specifically designed to provide only spatially small refinements in order to avoid entangling global scene properties and losing controllabil-ity. Thus, it is significantly less powerful than style-based renderers, and hence the highest image resolution that GI-RAFFE can generate is 2562.
In this work, we propose a two-stage style-based 3D-aware generative model that inherits all of GIRAFFE’s con-trollability while generating high-quality, high-resolution images (up to 10242 resolution); see Fig. 1. Our design is motivated by three key observations when replacing GI-RAFFE’s neural renderer with a style-based neural render (based on StyleGAN2 [27]): 1) Using the style renderer to upsample the volume-rendered low-res 2D feature maps leads to high-quality, high-resolution image generations while still preserving controllability over the foreground ob-ject’s 3D properties (translation, rotation). However, due to its high capacity, the style renderer 2) now gains full control over color as well as some control over shape, and 3) entan-gles and loses controllability over the foreground and back-ground features (i.e., changing the foreground color/shape also changes the background color/shape).
In order to regain controllability over the foreground and background, we generate them independently using two dif-ferent style-based renderers and combine them into a co-herent image by imposing geometric and photometric com-patibility constraints that eliminate inconceivable combi-nations. Furthermore, to disentangle color and shape, we exploit the well-known emergent properties of StyleGAN, namely that the early layers control coarse shape, mid lay-ers control fine-grained shape, and later layers control color.
Specifically, we inject the shape code into the 3D feature generator as well as the early layers of the style renderer to control shape, and the appearance code into the later layers of the style renderer to control color.
Contributions. Our approach, GIRAFFE HD, preserves the 3D controllability of GIRAFFE, including independent control over foreground and background, while generating much higher-resolution and higher-quality images (up to 10242 vs. GIRAFFE’s 2562). We validate our approach on multiple natural image datasets (CompCar [51], FFHQ [26],
AFHQ Cat [12], CelebA-HQ [23], LSUN Church [52]) and demonstrate better foreground-background disentan-glement and image realism compared to GIRAFFE in higher resolution domains. Finally, we perform ablation studies to justify the different design choices for our model. 2.