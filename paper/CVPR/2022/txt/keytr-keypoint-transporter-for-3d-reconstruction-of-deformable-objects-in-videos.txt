Abstract
We consider the problem of reconstructing the depth of dynamic objects from videos. Recent progress in dynamic video depth prediction has focused on improving the out-put of monocular depth estimators by means of multi-view constraints while imposing little to no restrictions on the deformation of the dynamic parts of the scene. However, the theory of Non-Rigid Structure from Motion prescribes to constrain the deformations for 3D reconstruction. We thus propose a new model that departs signiﬁcantly from this prior work. The idea is to ﬁt a dynamic point cloud to the video data using Sinkhorn’s algorithm to associate the 3D points to 2D pixels and use a differentiable point renderer to ensure the compatibility of the 3D deformations with the measured optical ﬂow. In this manner, our algorithm, called
Keypoint Transporter, models the overall deformation of the object within the entire video, so it can constrain the re-construction correspondingly. Compared to weaker defor-mation models, this signiﬁcantly reduces the reconstruction ambiguity and, for dynamic objects, allows Keypoint Trans-porter to obtain reconstructions of the quality superior or at least comparable to prior approaches while being much faster and reliant on a pre-trained monocular depth esti-mator network. To assess the method, we evaluate on new datasets of synthetic videos depicting dynamic humans and animals with ground-truth depth. We also show qualitative results on crowd-sourced real-world videos of pets. 1.

Introduction
We are interested in the problem of reconstructing 3D dynamic scenes from casually recorded videos. A scene is dynamic if it contains moving objects, including deforming
Frame 1 time t1
Frame 2 time t2
Shape predictor
Shape predictor s t n i o p y e k
D 3
D 2 s n o i t c e j o r p w o l
F y c n e t s i s n o c
Frame N time tN
Shape predictor
...
...
...
Optimal transport
Optimal transport
Optimal transport
... t n i o p y e k
D 2 s e t a d i d n a c
Figure 1. Keypoint Transporter reconstructs the depth of 3D non-rigid objects from a casually recorded video. Unlike prior work, we model the deformations of the object globally from the beginning to the end of the video. The key technical contribution is a robust mechanism to track these long-range deformations. This is based on estimating a dynamic cloud of 3D keypoints that are (1) encouraged to optimally cover a set of candidate 2D points in every frame via differentiable optimal transport and (2) to describe a 2D trajectory compatible with the measured optical ﬂow. ones such as people or animals. This is a very challeng-ing reconstruction scenario which has traditionally been ad-dressed by making use of specialized hardware, such as multi-camera domes or 3D scanners. However, with ad-vancements in virtual and augmented reality, we can en-visage a future in which non-experts may wish to create content to experience in 3D. In these scenarios, dynamic            
3D scenes must be reconstructed from limited observations, such as a monocular video captured by a phone camera.
Casual 3D reconstruction is much more challenging than reconstruction in a controlled capture setup. While the including recon-problem has many interesting aspects, structing the shape and appearance of the visible parts of the scene and extrapolating the parts that are not visible (for new-view synthesis), in this work we focus on the task of reconstructing depth from videos. Furthermore, we focus speciﬁcally on reconstructing the depth of dynamic objects which deform over time as these are often the focus of at-tention, while being challenging to reconstruct.
Several works have recently considered the problem of estimating depth in casual videos of dynamic scenes [20,29, 49]. The general approach is to ﬁrst apply a deep network such as MiDaS [23] in order to obtain a per-frame depth estimation. Given this initial, and often unreliable, depth estimate, principles from multi-view geometry are then ap-plied to reﬁne the solution, leveraging the information con-tained in the whole video. The latter usually starts by es-timating image correspondences by an off-the-shelf optical
ﬂow method such as RAFT [44]. The simplest approach, adopted by Consistent Video Depth (CVD) [29] assumes dynamic objects behave rigidly across neighbouring video frames, which limits its applicability to slowly moving ob-jects. The follow up Robust CVD [20] does not apply geo-metric constraints to the dynamic objects. Other approaches such as Dynamic Video Depth (DVD) [49] explicitly esti-mate and constrain the 3D deformation of the dynamic parts of the scene by means of a small neural network.
While prior works have settled on variants of the pipeline discussed above, in this paper we take a step back and ques-tion their assumptions. Since the problem is to reconstruct the 3D shape of a non-rigid object, we re-consider Non-Rigid Structure from Motion (NRSfM) methods [4, 5, 11, 33]. The key lesson form NRSfM is that reconstructing a deformable object is possible only if the space of deforma-tions is sufﬁciently constrained. The simplest of such con-straints is to assume that the 3D deformations span a low-rank linear subspace. By comparison, (Robust) CVD do not model non-rigid deformations explicitly, and DVD only en-forces local smoothness of the 3D deformation ﬁeld.
We thus propose Keypoint Transporter (KeyTr): built on the idea of constraining the deformations that the object un-dergoes throughout the video. This is a signiﬁcant departure from recent works because it requires to track deformations across the entire video (not just instantaneously as in, e.g.,
DVD). In contrast to traditional NRSfM, which utilizes 2D feature trackers, KeyTr works by maintaining a set of 3D keypoints that can be deformed within a low-rank subspace.
The keypoints and deformations are learned so that: (1) the 2D projections of the deformed keypoints cover the object region well in each frame, and (2) their 2D trajectories are compatible with the measured optical ﬂow. The latter con-straint is enforced in a differentiable and occlusion-aware manner by using an off-the-shelf point cloud renderer.
Compared to prior methods that model only local de-formations, we empirically show that globally modelling and constraining object deformations signiﬁcantly reduces the reconstruction ambiguity. Because of this, we show that KeyTr obtains superior or at least comparable recon-struction quality without using a pre-trained depth estima-tion model such as MiDaS; instead, our model reconstructs videos individually from scratch without any prior learning.
We also introduce several new datasets for measuring the quality of dynamic video depth algorithms. We quantita-tively evaluate on synthetic datasets of humans and animals and we further evaluate the reconstructions qualitatively, on real videos of pets collected for the study. 2.