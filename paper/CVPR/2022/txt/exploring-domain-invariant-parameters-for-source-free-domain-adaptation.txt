Abstract
Source-free domain adaptation (SFDA) newly emerges to transfer the relevant knowledge of a well-trained source model to an unlabeled target domain, which is critical in various privacy-preserving scenarios. Most existing meth-ods focus on learning the domain-invariant representations depending solely on the target data, leading to the obtained representations are target-speciﬁc. In this way, they can-not fully address the distribution shift problem across do-mains. In contrast, we provide a fascinating insight: rather than attempting to learn domain-invariant representations, it is better to explore the domain-invariant parameters of the source model. The motivation behind this insight is clear: the domain-invariant representations are dominated by only partial parameters of an available deep source model. We devise the Domain-Invariant Parameter Exploring (DIPE) approach to capture such domain-invariant parameters in the source model to generate domain-invariant representa-tions. A distinguishing method is developed correspond-ingly for two types of parameters, i.e., domain-invariant and domain-speciﬁc parameters, as well as an effective update strategy based on the clustering correction technique and a target hypothesis is proposed. Extensive experiments verify that DIPE successfully exceeds the current state-of-the-art models on many domain adaptation datasets. 1.

Introduction
Unsupervised domain adaptation (UDA) has been gain-ing momentum in the past decade, effectively addressing the distribution shift problem across domains. Thanks to the free access to labeled source data, previous UDA stud-∗Contribute to this work equally
†Corresponding author
Figure 1. The comparison between existing methods and our method. Existing methods (top) optimize the model parameters without distinction, obtaining the target-speciﬁc representations which would not be well adapted to the source classiﬁer, i.e., some samples are classiﬁed wrongly. Our method (bottom) would obtain domain-invariant representations by exploring domain-invariant parameters, guaranteeing the generalization of the source model. ies have achieved remarkable achievements [10, 25, 49].
However, the source data is unavailable in various privacy-preserving scenarios: data privacy protection laws and data silos in clinical practice [38]. Moreover, the fully test-time adaptation [44] assumes that the model could be sensitive to changing conditions, e.g., domain shift, during testing with-out the training data. In such practical limitations, source-free domain adaptation (SFDA) relaxes the source data re-quirement and leverages the source model’s knowledge for domain adaptation.
The fundamental challenge of SFDA is that the domain-invariant presentations are challenging to be explored di-rectly depending solely on the target data, as previous works have attempted to do. Both SHOT [23] and PPDA [17] utilize various techniques, e.g., entropy functions and self-perform a passive update which will penalize their values to be near zero and gradually make them lose activity.
We summarize our main contributions as follows:
• To the best of our knowledge, we, for the ﬁrst time, ex-plore domain-invariant parameters stored in the given source model, opening up a new perspective in SFDA.
• We propose a novel DIPE framework for exploring domain-invariant parameters, and introduce a domain-balanced identifying criterion to determine domain-invariant and domain-speciﬁc parameters.
• A simple and general technique, clustering correction, is proposed to promote the learning process. 2.