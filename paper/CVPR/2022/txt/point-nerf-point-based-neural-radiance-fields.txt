Abstract
Volumetric neural rendering methods like NeRF [34] generate high-quality view synthesis results but are opti-mized per-scene leading to prohibitive reconstruction time.
On the other hand, deep multi-view stereo methods can quickly reconstruct scene geometry via direct network in-ference. Point-NeRF combines the advantages of these two approaches by using neural 3D point clouds, with associ-ated neural features, to model a radiance field. Point-NeRF can be rendered efficiently by aggregating neural point fea-tures near scene surfaces, in a ray marching-based render-ing pipeline. Moreover, Point-NeRF can be initialized via direct inference of a pre-trained deep network to produce a neural point cloud; this point cloud can be finetuned to surpass the visual quality of NeRF with 30× faster train-ing time. Point-NeRF can be combined with other 3D re-construction methods and handles the errors and outliers in such methods via a novel pruning and growing mechanism. 1.

Introduction
Modeling real scenes from image data and rendering photo-realistic novel views is a central problem in com-puter vision and graphics. NeRF [34] and its extensions
†This work is partially done during the internship at Adobe Research.
Code and results: xharlie.github.io/projects/project sites/pointnerf.
[28, 31, 58] have shown great success on this by modeling neural radiance fields. These methods [34, 37, 58] often re-construct radiance fields using global MLPs for the entire space through ray marching. This leads to long reconstruc-tion times due to the slow per-scene network fitting and the unnecessary sampling of vast empty space.
We address this issue using Point-NeRF, a novel point-based radiance field representation that uses 3D neural points to model a continuous volumetric radiance field. Un-like NeRF that purely depends on per-scene fitting, Point-NeRF can be effectively initialized via a feed-forward deep neural network, pre-trained across scenes. Moreover, Point-NeRF avoids ray sampling in the empty scene space by leveraging classical point clouds that approximate the ac-tual scene geometry. This advantage of Point-NeRF leads to more efficient reconstruction and more accurate render-ing than other neural radiance field models [8, 34, 50, 57].
Our Point-NeRF representation consists of a point cloud with per-point neural features: each neural point encodes the local 3D scene geometry and appearance around it.
Prior point-based rendering techniques [2] use similar neu-ral point clouds but perform rendering with rasterization and 2D CNNs operating in image space. We instead treat these neural points as local neural basis functions in 3D to model a continuous volumetric radiance field which enables high-quality rendering using differentiable ray marching. In
particular, for any 3D location, we propose to use an MLP network to aggregate the neural points in its neighborhood to regress the volume density and view-dependent radiance at that location. This expresses a continuous radiance field.
We present a learning-based framework to efficiently ini-tialize and optimize the point-based radiance fields. To generate a initial field, we leverage deep multi-view stereo (MVS) techniques [54], i.e., applying a cost-volume-based network to predict depth which is then unprojected to 3D space. In addition, a deep CNN is trained to extract 2D fea-ture maps from input images, naturally providing the per-point features. These neural points from multiple views are combined as a neural point cloud, which forms a point-based radiance field of the scene. We train this point gen-eration module with the point-based volume rendering net-works from end to end, to render novel view images and supervise them with the ground truth. This leads to a gen-eralizable model that can directly predict a point-based ra-diance field at inference time. Once predicted, the initial point-based field is further optimized per scene in a short period to achieve photo-realistic rendering. As shown in
Fig. 1 (left), 21 minutes of optimization with Point-NeRF outperforms a NeRF model trained for days.
Besides using the in-built point cloud reconstruction, our approach is generic and can also generate a radiance field based on a point cloud of other reconstruction techniques.
However, the reconstructed point cloud produced by tech-niques like COLMAP [42], in practice, contain holes and outliers that adversely affect the final rendering. To address this issue, we introduce point growing and pruning as part of our optimization process. We leverage the geometric rea-soning during volume rendering [13] and grow points near the point cloud boundary in high volume density regions and prune points in low-density regions. The mechanism effectively improves our final reconstruction and rendering quality. We show an example in Fig. 1 (right) where we convert COLMAP points to a radiance field and success-fully fill large holes and produce photo-realistic renderings.
We train our model on the DTU dataset [17] and evalu-ate on DTU testing scenes, NeRF synthetic, Tanks & Tem-ples [22], and ScanNet [11] scenes. The results demonstrate that our approach can achieve state-of-the-art novel view synthesis, outperforming many prior arts including point-based methods [2], NeRF, NSVF [28], and many other gen-eralizable neural methods [8, 50, 57] (see (Tab. 1 and 2)). 2.