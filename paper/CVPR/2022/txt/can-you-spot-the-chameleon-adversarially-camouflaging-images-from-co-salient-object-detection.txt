Abstract
Co-salient object detection (CoSOD) has recently achieved significant progress and played a key role in retrieval-related tasks. However, it inevitably poses an en-tirely new safety and security issue, i.e., highly personal and sensitive content can potentially be extracting by power-ful CoSOD methods. In this paper, we address this prob-lem from the perspective of adversarial attacks and identify a novel task: adversarial co-saliency attack. Specially, given an image selected from a group of images contain-ing some common and salient objects, we aim to generate an adversarial version that can mislead CoSOD methods to predict incorrect co-salient regions. Note that, compared with general white-box adversarial attacks for classifica-tion, this new task faces two additional challenges: (1) low success rate due to the diverse appearance of images in the group; (2) low transferability across CoSOD methods due to the considerable difference between CoSOD pipelines.
To address these challenges, we propose the very first black-box joint adversarial exposure and noise attack (Jadena), where we jointly and locally tune the exposure and addi-tive perturbations of the image according to a newly de-signed high-feature-level contrast-sensitive loss function.
Our method, without any information on the state-of-the-art
CoSOD methods, leads to significant performance degra-dation on various co-saliency detection datasets and makes the co-salient objects undetectable. This can have strong practical benefits in properly securing the large number of personal photos currently shared on the Internet. Moreover, our method is potential to be utilized as a metric for evalu-ating the robustness of CoSOD methods.
*Ruijun Gao and Qing Guo are co-first authors and contribute equally to this work.
â€ Wei Feng is the corresponding author: wfeng@ieee.org
Figure 1. Overview of the novel problem, adversarial co-saliency attack, and our solution. We expect the perturbed image to be undiscoverable in an even dynamically growing group of images across multiple CoSOD meth-ods (e.g., GICD [52], GCAGC [50], PoolNet [31]), which is much more challenging and practical in real-world scenarios. Note that our attack is black-box and can be performed without references provided in the group. 1.

Introduction
Co-saliency is typically defined as the common and salient (usually in foreground) visual stimulus found across a given group of images. Co-salient object detection (CoSOD) therefore aims at detecting and highlighting the common and salient foreground region (object) in the given group of image [47]. Different from the traditional single-image saliency detection problem, the key to solving the co-saliency problem is to discover the correspondence (based on various cues) of the common and similar salient regions across multiple images in a group. With unknown semantic categories of the co-salient objects, the co-saliency task is a rather challenging one, and a good co-saliency detection al-gorithm should consider both the intra-image saliency cues and the inter-image common saliency cues [14].
At the moment, co-saliency detection is still an emerg-ing research topic, with many recently proposed methods to
solve this challenging problem. We will discuss several of the current CoSOD algorithms, which range from low-level features to high-level semantic features, as well as deep learning-based models, in Section 2. Co-saliency detection plays a key role in many practical applications of computer vision and multimedia, including object co-segmentation
[54], foreground discovery in video sequences [5], weakly supervised localization [48], image retrieval [14], multi-camera surveillance [32], etc.
However, the extensive and powerful CoSOD methods inevitably pose a new safety and security problem, i.e., high-profile and personal-sensitive content may be subject to extraction and discovery by these methods. In order to prevent content-sensitive images from being discovered by co-saliency detection, in this work, we address the problem from the perspective of adversarial attacks and identify a novel task, i.e., adversarial co-saliency attack. Specially, given an image selected from a group containing some com-mon and salient objects, the aim of the task is to generate an adversarial version of the image that can mislead CoSOD methods to predict incorrect co-salient regions, thus evading co-saliency detection, and making the images undiscover-able by the co-saliency detection algorithms. Our method is able to conceal personal content containing sensitive infor-mation from the CoSOD methods. Moreover, our method can potentially be used as a metric to evaluate the robustness of the CoSOD methods and can be coupled with adversarial training to improve the performance as well.
It is worth noting that, compared with general white-box adversarial attacks for classification, this new task faces two additional challenges: (1) low success rate due to the di-verse appearance of images in the group of images; (2) low transferability across CoSOD methods due to the consid-erable difference between CoSOD pipelines. To overcome these challenges, we propose the very first black-box joint adversarial exposure and noise attack (Jadena), where we jointly and locally tune the exposure and additive pertur-bations of the image according to a newly designed high-feature-level contrast-sensitive loss function. Our method, without needing any information on the state-of-the-art
CoSOD methods, leads to significant performance degra-dation on various co-saliency detection datasets and makes the co-salient objects undetectable, as shown in Fig. 1. This has strong practical value nowadays where large-scale per-sonal multimedia contents are shared on the public domain
Internet and should be properly and securely preserved and protected from malicious extraction. We have released our code in this GitHub repo: https://github.com/ tsingqguo/jadena. 2.