Abstract 1.

Introduction
Deep generative models, like GANs, have considerably improved the state of the art in image synthesis, and are able to generate near photo-realistic images in structured domains such as human faces. Based on this success, re-cent work on image editing proceeds by projecting images to the GAN latent space and manipulating the latent vector.
However, these approaches are limited in that only images from a narrow domain can be transformed, and with only a limited number of editing operations. We propose FlexIT, a novel method which can take any input image and a user-defined text instruction for editing. Our method achieves flexible and natural editing, pushing the limits of seman-tic image translation. First, FlexIT combines the input im-age and text into a single target point in the CLIP multi-modal embedding space. Via the latent space of an autoen-coder, we iteratively transform the input image toward the target point, ensuring coherence and quality with a variety of novel regularization terms. We propose an evaluation protocol for semantic image translation, and thoroughly evaluate our method on ImageNet. Code will be available at https://github.com/facebookresearch/
SemanticImageTranslation/.
The old saying goes: “You can’t make a silk purse from a sow’s ear.” Or can you? Truly flexible and powerful se-mantic image editing is elusive, and current work is limited in terms of possible input images and edit operations. Re-search in deep generative image models has seen significant progress in recent years, with GANs in particular generating near photo-realistic samples in domains such as human and animal faces [26] or object-centric images [4]. Moreover, recent “style-based” GANs, like StyleGAN [27–29], have an impressively disentangled latent space, where perform-ing copy-pastes between two latent vectors transfers the cor-responding styles in the image space.
Consequently, significant research efforts have been put into using pre-trained GANs for semantic image edition.
Through specific latent-space manipulation, high-level at-tributes such as age or gender can be identified and edited in a realistic manner [1,22,41,57]. These approaches, how-ever, present several caveats. First, contrary to generated latents, inferred latent codes representing real images have been shown to react poorly to latent editing operations [19].
Although recent methods [19, 45, 55] improve editability, input images are still highly limited to the distribution of the generative network. Moreover, edit operations are also 1
limited to the semantics identified in the latent space via a pre-trained classifier [1,41,57] or through a semi-automatic manner [22, 48]. These semantics are specific to the single domain the GAN was trained on, such as age or apparent gender in the case of faces. Some flexibility w.r.t. the input images can be obtained by training a GAN to directly mod-ify the images, known as image-to-image translation. These methods learn a transformation between two domains, using paired data [23, 38, 49] or unpaired data [6, 56]. However, these models only learn a single transformation, or combi-nations thereof [50], specific to the training data, limiting the scope of their applicability.
We tackle these challenges with a unified framework which modifies an input image based on a user-defined text query of the form (S → T ), like cat → dog. For this se-mantic image translation task, the goal is to make mini-mal image modifications while transforming the image as requested. We leverage CLIP [40], which combines text and image representations in one powerful multimodal em-bedding space. This space is used to define our target point, based on the embeddings from the user input. We perform a per-image optimization procedure, using specific strategies to ensure image quality and relevance to the transformation query. Our method requires only fixed pre-trained compo-nents, and can thus be used off-the-shelf without requiring any training. The image is optimized in the latent space of an auto-encoder, rather than a GAN, which greatly enlarges the scope of possible input images. This allows for truly flexible image edits; as Figure 1 shows, even a sow’s ear can be changed into a silk purse.
We also propose a quantitative evaluation protocol for the task of semantic image translation. Evaluation is based on three criteria: (i) the transformed image should correctly correspond to the text query, (ii) the output image should look natural, and (iii) visual elements irrelevant to the text query should remain unchanged. We thoroughly evaluate our model on ImageNet, and demonstrate quantitatively and qualitatively the superiority of our method against base-lines, broadening the horizon of text-driven image editing. 2.