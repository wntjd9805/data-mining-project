Abstract
Far beyond learning long-range interactions of natural language, transformers are becoming the de-facto standard for many vision tasks with their power and scalability. Es-pecially with cross-modal tasks between image and text, vector quantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB image into a sequence of feature vectors. To better leverage the correlation be-tween image and text, we propose L-Verse, a novel archi-tecture consisting of feature-augmented variational autoen-coder (AugVAE) and bidirectional auto-regressive trans-former (BiART) for image-to-text and text-to-image gener-ation. Our AugVAE shows the state-of-the-art reconstruc-tion performance on ImageNet1K validation set, along with the robustness to unseen images in the wild. Unlike other models, BiART can distinguish between image (or text) as a conditional reference and a generation target. L-Verse can be directly used for image-to-text or text-to-image genera-tion without any finetuning or extra object detection frame-work. In quantitative and qualitative experiments, L-Verse shows impressive results against previous methods in both image-to-text and text-to-image generation on MS-COCO
Captions. We furthermore assess the scalability of L-Verse architecture on Conceptual Captions and present the ini-tial result of bidirectional vision-language representation learning on general domain. 1.

Introduction
Image-to-text and text-to-image generation and can be summarized as a task of learning cross-modal represen-tations of image and text. Recent studies [7, 10, 11, 32] on vision-language tasks have highly improved the perfor-mance of each target task, in particular with various trans-former architectures [3, 4, 9, 45]. Initially designed to un-derstand natural language, the dot-product multi-head at-tention mechanism [45] effectively learns long-range inter-actions of sequential data. To leverage transformer archi-*Correspondence to: taehoon.kim@lgresearch.ai
Figure 1. Examples of L-Verse on zero-shot text-to-image genera-tion (256 × 256 pixels) on Conceptual Captions (top) and image-to-text generation on MS-COCO Captions (bottom). Trained in bidirectional manner, L-verse can both generate well-conditioned synthetic images and detailed captions without any finetuning. tectures [45] also in vision domains, an input image is fac-torized into a sequence of latent feature vectors.
To encode an image into a sequence of latent fea-ture vectors, vector quantized variational autoencoder (VQ-VAE) [44] can be used to learn a discrete latent repre-sentation with quantized embedding vectors from the vi-sual codebook. VQ-VAE is a simple and powerful repre-sentation learning method to make image sequential and is widely used in conditional image generation tasks with auto-regressive pairs like RNNs [33, 44] or transformers
[10–12, 32]. Improving the reconstruction quality of VQ-VAE is also an active area of research [12, 32, 33].
Combining an auto-regressive transformer [3] with a fea-ture extractor like VQ-VAEs or other deep convolutional neural networks (CNNs) is becoming a popular approach training a for various vision-language tasks. However, model for unidirectional image-to-text [7] or text-to-image
[10, 32] generation task still requires a large amount of data
or an extra object detection framework. We hypothesize that learning bidirectional cross-modal representation of image and text can alleviate this problem via better data efficiency.
This paper proposes an approach, L-Verse (latent verse), for learning a bidirectional vision-language cross-modal representation. The key idea of L-Verse is two-fold: (i) aug-ment a visual codebook with diverse features and (ii) enable an auto-regressive transformer to learn bidirectional image-text generation. Our novel cross-level feature augmenta-tion technique effectively increase the diversity of a visual codebook with unique feature embedding vectors. We fur-thermore add a segment embedding to an auto-regressive transformer [3] to teach the difference between image (or text) as given condition or generation target. Specifically, our contribution for vision-language cross-modal represen-tation learning are summarized as follows:
• We introduce a feature-augmented variational autoen-coder (AugVAE), a VQ-VAE trained with cross-level feature augmentation. With the feature-augmented visual codebook, AugVAE shows the state-of-the-art reconstruction performance on both in-domain Im-ageNet1K [8] validation set (Figure 2) and out-of-domain image datasets (Figure 5).
• We propose a bidirectional auto-regressive transformer (BiART) for bidirectional image-text generation. We index each token with two different embedding vec-tors according to its role as a conditional reference ([REF]) or a generation target ([GEN]). With this segment embedding, our BiART can both generate corresponding images to given texts or meaningful captions to given images without any finetuning.
• L-Verse, consisting of AugVAE and BiART, outper-forms previously proposed image captioning models in most of the machine evaluation metrics on MS-COCO
Captions [24] Karpathy test split. It is also notable that
L-Verse does not require any object-detection frame-work, such as Faster-RCNN [34].
• L-Verse shows comparable text-to-image generation results to other generative models on MS-COCO Cap-tions [24]. We also assess the scalability of L-Verse for zero-shot text-to-image generation by training on
Conceptual Captions [39].
Section 2 briefly reviews previous works on VQ-VAE and cross-modal vision-language tasks. Section 3 explains how we design AugVAE and BiART to learn the bidirec-tional cross-modal representation between image and text.
Section 4 shows quantitative and qualitative results on im-age reconstruction, image-to-text generation, and text-to-image generation. Section 5 summarizes our paper with conclusion and discussion for future works. 2.