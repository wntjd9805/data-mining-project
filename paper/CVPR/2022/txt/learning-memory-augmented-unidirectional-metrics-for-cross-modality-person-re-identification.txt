Abstract tackles
This paper the cross-modality person re-identification (re-ID) problem by suppressing the modality discrepancy. In cross-modality re-ID, the query and gallery images are in different modalities. Given a training identity, the popular deep classification baseline shares the same proxy (i.e., a weight vector in the last classification layer) for two modalities. We find that it has considerable toler-ance for the modality gap, because the shared proxy acts as an intermediate relay between two modalities. In response, we propose a Memory-Augmented Unidirectional Metric (MAUM) learning method consisting of two novel designs, i.e., unidirectional metrics, and memory-based augmen-tation. Specifically, MAUM first learns modality-specific proxies (MS-Proxies) independently under each modality.
Afterward, MAUM uses the already-learned MS-Proxies as the static references for pulling close the features in the counterpart modality. These two unidirectional metrics (IR image to RGB proxy and RGB image to IR proxy) jointly al-leviate the relay effect and benefit cross-modality associa-tion. The cross-modality association is further enhanced by storing the MS-Proxies into memory banks to increase the reference diversity. Importantly, we show that MAUM im-proves cross-modality re-ID under the modality-balanced setting and gains extra robustness against the modality-imbalance problem. Extensive experiments on SYSU-MM01 and RegDB datasets demonstrate the superiority of MAUM over the state-of-the-art. The code will be available. 1.

Introduction
This paper considers cross-modality re-identification (re-ID). Re-ID aims to retrieve images of the person-of-interest from the database. Real-world re-ID systems sometimes require recognizing the same person across daytime and night. To this end, they use two person
*Work done during an internship at Baidu Research.
†Corresponding author.
Figure 1. Comparison between baseline and MAUM. We visual-ize the embedding space of baseline and MAUM with t-SNE [27], respectively. (a) In baseline, each identity has a modality-agnostic proxy for two modalities, which acts as a relay between IR and
RGB features. The relay effect of baseline is revealed in the t-SNE visualization, where the modality gap between IR and RGB features is pretty large. (b) MAUM has two modality-specific proxies (MS-proxies, the orange solid dot for RGB and the blue solid dot for IR). Each MS-Proxy is fixed as a static reference for pulling close the features in the counterpart modality (dotted ar-row). Further, MAUM stores historical MS-Proxies (void dot) into two memory banks, one for IR modality and one for RGB modal-ity. Correspondingly, each identity has multiple IR and RGB prox-ies. The farthest MS-Proxies from the modality boundary become hard positive references and thus have a stronger “pulling close” effect (solid arrow). Consequently, as the visualization shows,
MAUM suppresses the modality discrepancy. different devices, i.e., the RGB camera at daytime and the
Infra-Red (IR) camera at night. When the query and the gallery images are from different modalities, the significant modality discrepancy stands out as the most prominent challenge. In this paper, we try to improve cross-modality re-ID by addressing the modality discrepancy problem.
From the metric learning viewpoint, the keynote of re-ID is to learn an embedding space with both within-class com-pactness and between-class separability. A popular deep learning baseline [8, 17, 23, 23, 29, 41] for re-ID and face recognition task is based on deep classification learning.
During training, it pulls all the features of the same iden-tity toward a corresponding proxy (i.e., the weight vector in the classification layer).
When we apply this baseline to the cross-modality re-ID, we find that the modality discrepancy problem significantly hinders the within-class compactness, as illustrated in Fig. 1 (a). In the baseline, all the instances of the same identity share a single proxy, regardless of the underlying modality.
The modality-agnostic proxy strives to accommodate both the IR and RGB features and acts as an intermediate relay between them. Such relay effects result in considerable tol-erance for the modality discrepancy. From the t-SNE [27] visualization in Fig. 1 (a), we observe that there is an appar-ent modality discrepancy between the features of the two modalities. The features with different identities but same modality are even closer than those with same identity but different modality. For example, the between-class distance between ID-116 and ID-129 is smaller than the within-class distance of ID-116.
To suppress the modality discrepancy, we propose a Memory-Augmented Unidirectional Metric (MAUM) learning method. It is featured for two novel designs, i.e., 1) learning unidirectional metrics and 2) enhancing the uni-directional metrics with the memory bank.
First, we learn two unidirectional metrics (“IR to RGB” and “RGB to IR”) to alleviate the relay effect of the base-line. To this end, MAUM learns two modality-specific prox-ies (MS-Proxies) for each identity, as illustrated in Fig. 1 (b). The RGB (IR) proxies only receive gradients from the
RGB (IR) features and thus represent the dedicated modal-ity. Afterward, we freeze them and use the RGB proxies as the static references for pulling IR features, and vice versa.
These two unidirectional metrics promote the better cross-modality association.
Second, we further enhance these two unidirectional metrics through memory-based augmentation. MAUM stores the IR and RGB proxies into a respective memory bank after every iteration. Since the MS-proxies keep on changing iteration by iteration (i.e., the “drift” phenomenon
[30]), each person has multiple diverse IR and RGB prox-ies in the memory bank, as illustrated in Fig. 1 (b). Some historical MS-Proxies are farther away from the modality boundary (than the up-to-date MS-Proxies) and thus lay stronger “pulling close” effect on the counterpart-modality features.
In a word, the memory bank enhances MAUM with hard positive references and consequentially promotes cross-modality association. We point out that memory-based learning in MAUM reveals a previously unknown yet important potential of the memory bank. Specifically, we employ the “drift” to enhance the references. In contrast, the previous works [10,15,25,30] consider the “drift” bring-ing negative impact and try to avoid it (as detailed in Section 2.2). As the visualization in Fig. 1 (b) shows, the features with the same identity distribute compactly, which indicates that the modality discrepancy is suppressed. For example, the within-class embedding of ID-116 is significantly more compact than that in baseline (Fig). 1 (a).
In addition to the effectiveness of mitigating modality discrepancy, the proposed MAUM has a particular advan-tage under the modality imbalance scenario. In the train-ing data, the IR images are usually scarcer than the RGB images because people have less movement at night, and the IR images are harder to annotate. In MAUM, both the unidirectional metrics and the memory-based augmentation are modality-specific. The augmentation on IR proxies is independent of that on RGB proxies and vice versa. There-fore, MAUM may re-balance the enhancement for the IR and RGB modalities. By re-balancing the augmentation, the MAUM compensates for the shortage of IR images and gains strong robustness against the modality imbalance.
Our main contributions are summarized as follows:
• We propose a novel memory-augmented unidirectional metric learning method for cross-modality re-ID. It learns explicit cross-modality metrics in two uni-directions and further enhances them with memory-based augmentation.
• We consider the modality imbalance, which is an es-sential realistic problem in cross-modality re-ID. By ad-justing the modality-specific augmentation, MAUM shows strong robustness against modality imbalance.
• We comprehensively evaluate our method under modality-balance and modality-imbalance scenarios. Ex-perimental results confirm that MAUM improves cross-modality re-ID under both settings, surpassing the state-of-the-art significantly. 2.