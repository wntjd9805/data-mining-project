Abstract
We present TubeFormer-DeepLab, the ﬁrst attempt to tackle multiple core video segmentation tasks in a uniﬁed manner. Different video segmentation tasks (e.g., video se-mantic/instance/panoptic segmentation) are usually consid-ered as distinct problems. State-of-the-art models adopted in the separate communities have diverged, and radically different approaches dominate in each task. By contrast, we make a crucial observation that video segmentation tasks could be generally formulated as the problem of assign-ing different predicted labels to video tubes (where a tube is obtained by linking segmentation masks along the time axis) and the labels may encode different values depend-ing on the target task. The observation motivates us to de-velop TubeFormer-DeepLab, a simple and effective video mask transformer model that is widely applicable to mul-tiple video segmentation tasks. TubeFormer-DeepLab di-rectly predicts video tubes with task-speciﬁc labels (either pure semantic categories, or both semantic categories and instance identities), which not only signiﬁcantly simpliﬁes video segmentation models, but also advances state-of-the-art results on multiple video segmentation benchmarks. 1.

Introduction
We observe that video segmentation tasks could be for-mulated as partitioning video frames into tubes with differ-ent predicted labels, where a tube contains segmentation masks linked along the time axis. Based on the target task, the predicted labels may encode only semantic categories (e.g., Video Semantic Segmentation (VSS) [7, 58]), or both semantic categories and instance identities (e.g., Video In-stance Segmentation (VIS) [68, 77] for only foreground
‘things’, or Video Panoptic Segmentation (VPS) [41,73] for both foreground ‘things’ and background ‘stuff’) (Fig. 1).
However, the underlying similarity of several video seg-mentation tasks (i.e., assigning tubes with predicted labels) has been long overlooked, and thus models developed for video semantic, instance, and panoptic segmentation have
∗Work done during an internship at Google.
Figure 1. Video segmentation tasks can be formulated as partition-ing video frames (e.g., a clip) into tubes (i.e., segmentation masks linked along time) with different labels. TubeFormer-DeepLab di-rectly predicts class-labeled tubes, providing a simple and general solution to Video Semantic Segmentation (VSS), Video Instance
Segmentation (VIS), and Video Panoptic Segmentation (VPS).
Figure 2. Our proposed hierarchical dual-path transformer per-forms attention on three consecutive input frames (a) for VSS,
VIS, and VPS tasks. While the global memory learns the spatio-temporally clustered attention for individual tube regions (b), our latent memory learns task-speciﬁc attention (c).
fundamentally diverged. For example, some VSS meth-ods [26, 86] warp features between video frames, while the modern VIS model [5] predicts hundreds of frame-level in-stance masks [31] and then propagates them to other neigh-boring frames. To make matters more complicated, state-of-the-art VPS methods [62, 74] adopt separate prediction branches, speciﬁc to semantic segmentation, instance seg-mentation, and object tracking, respectively.
In this work, instead of exacerbating the bifurcation be-tween video segmentation models, we take a step back and rethink the following question: Can we exploit the simi-lar nature between video segmentation tasks, and develop a single model that is both effective and generally applica-ble? To answer this, we propose TubeFormer-DeepLab that builds upon mask transformers [69] for video segmen-tation by directly predicting class-labeled tubes, where the labels encode different values depending on the target task. to other Transformer architec-tures [9, 67], TubeFormer-DeepLab extends the mask trans-former [69] to generate a set of pairs, each containing a class prediction and a tube embedding vector. The tube embed-ding vector, multiplied by the video pixel embedding fea-tures obtained by a convolutional network [45], yields the tube prediction. As a result, TubeFormer-DeepLab presents the ﬁrst attempt to tackle multiple core video segmentation tasks in a general framework without the need to adapt the system for any task-speciﬁc design.
Speciﬁcally, similar
Na¨ıvely applying the image-level mask transformer [69] to the video domain does not yield a satisfactory result, mainly due to the difﬁculty of learning attentions for video-clip (i.e., multi-frames) features with large spatial resolu-tions. To alleviate the issue, we introduce the latent dual-path transformer block that is in charge of passing mes-sages between video-frame (i.e., single-frame) features and a latent memory, followed by the global dual-path trans-former block that learns the attentions between video-clip features and a global memory. This hierarchical dual-path transformer framework facilitates the attention learning and signiﬁcantly improves the video segmentation results. In-terestingly, as shown in Fig. 2, our latent memory learns task-speciﬁc attention, while the global memory learns the spatio-temporally clustered attention for individual tube re-gions. Additionally, we split the global memory into two sets, thing-speciﬁc and stuff-speciﬁc global memory, with the motivation to exploit the different nature of ‘thing’ (countable instances) and ‘stuff’ (amorphous regions).
During inference, practically we could only ﬁt a video clip (i.e., a short video sequence) for video segmentation.
The whole video sequence segmentation result is thus ob-tained by applying the video stitching [63] to merge clip segmentation results. To enforce the consistency between video clips, we additionally propose a Temporal Consis-tency loss that encourages the model to learn consistent pre-dictions in the overlapping frames between clips.
Finally, we propose a simple and effective data augmen-tation policy by extending the image-level thing-speciﬁc copy-paste [24, 29]. Our method, named clip-paste (clip-level copy-paste), randomly pastes either ‘thing’ or ‘stuff’ (or both) regions from a video clip to the target video clip.
To demonstrate the effectiveness of our proposed
TubeFormer-DeepLab, we conduct experiments on multiple core video segmentation datasets, including KITTI-STEP (VPS) [73], VSPW (VSS) [58], YouTube-VIS (VIS) [77], and SemKITTI-DVPS (depth-aware VPS) [63]. Our single model not only signiﬁcantly simpliﬁes video segmentation systems (e.g., the proposed model is end-to-end trained and does not require any task-speciﬁc design), but also advances state-of-the-art performance on several benchmarks. In par-ticular, TubeFormer-DeepLab outperforms published works
Motion-DeepLab [73] by +13.1 STQ on KITTI-STEP test set, TCB [58] by +21 mIoU on VSPW test set, IFC [37] by
+2.9 track-mAP on YouTube-VIS-2019 val set, and ViP-DeepLab [63] by +3.6 DSTQ on SemKITTI-DVPS test set.
Our experimental results validate TubeFormer-DeepLab’s general efﬁcacy for video segmentation tasks. 2.