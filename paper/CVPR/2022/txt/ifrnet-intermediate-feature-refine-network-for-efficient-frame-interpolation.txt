Abstract
Prevailing video frame interpolation algorithms, that generate the intermediate frames from consecutive inputs, typically rely on complex model architectures with heavy parameters or large delay, hindering them from diverse real-time applications. In this work, we devise an efﬁcient encoder-decoder based network, termed IFRNet, for fast in-termediate frame synthesizing. It ﬁrst extracts pyramid fea-tures from given inputs, and then reﬁnes the bilateral in-termediate ﬂow ﬁelds together with a powerful intermedi-ate feature until generating the desired output. The gradu-ally reﬁned intermediate feature can not only facilitate in-termediate ﬂow estimation, but also compensate for con-textual details, making IFRNet do not need additional syn-thesis or reﬁnement module. To fully release its potential, we further propose a novel task-oriented optical ﬂow dis-tillation loss to focus on learning the useful teacher knowl-edge towards frame synthesizing. Meanwhile, a new ge-ometry consistency regularization term is imposed on the gradually reﬁned intermediate features to keep better struc-ture layout. Experiments on various benchmarks demon-strate the excellent performance and fast inference speed of proposed approaches. Code is available at https:
//github.com/ltkong218/IFRNet. 1.

Introduction
Video frame interpolation (VFI), that converts low frame rate (LFR) image sequences to high frame rate (HFR) videos is an important low-level computer vision task. Re-lated techniques are widely applied to various practical ap-plications, such as slow-motion generation [22], novel view synthesis [55] and cartoon creation [42]. Although it has been studied by a large number of researches, there are still
∗ Equal contribution. This work was done when Lingtong Kong was an intern at Tencent Youtu Lab.
† Corresponding author: Jie Yang (jieyang@sjtu.edu.cn). This re-search is partly supported by NSFC, China (No: 61876107, U1803261).
Figure 1. Speed, accuracy and parameters comparison. Pro-posed IFRNet achieves state-of-the-art frame interpolation accu-racy with fast inference speed and lightweight model size. great challenges when dealing with complicated dynamic scenes, which include large displacement, severe occlusion, motion blur and abrupt brightness change.
Recently, with the development of optical ﬂow net-works [13, 24, 45, 46], signiﬁcant progress has been made by ﬂow-based VFI approaches [22, 33, 37, 49], since op-tical ﬂow can provide an explicit correspondence to reg-ister frames in a video sequence. Successful ﬂow-based approaches usually follow a three-step pipeline: 1) Esti-mate optical ﬂow between target frame and input frames. 2) Warp input frames or context features by predicted ﬂow
ﬁelds for spatial alignment. 3) Reﬁne warped frames or fea-tures and generate the target frame by a synthesis network.
Denoting input frames and target frame to be I0, I1 and
It (0 < t < 1), existing methods either ﬁrst estimate optical
ﬂow F0→1, F1→0 [3,22,32,33,36], and then approximate or reﬁne bilateral intermediate ﬂow Ft→0, Ft→1 [9, 22, 40, 49] as shown in Figure 2 (a), or throw the intractable intermedi-ate ﬂow estimation sub-task to a learnable ﬂow network for end-to-end training [20, 50, 54] as depicted in Figure 2 (b).
Their common step is to further employ an image synthesis network to encode spatial aligned context feature [32] for target frame generation or reﬁnement.
Figure 2. Different ﬂow-based VFI paradigms. We roughly classify existing ﬂow-based VFI methods based on encoder-decoders with speciﬁc function. In (a) [3,22,32,33,36,37,40,49], FlowNet estimates conventional optical ﬂow F0→1, F1→0, the middle part approximates or further reﬁnes ﬂow ﬁelds Ft→0, Ft→1. In (b) [20, 50, 54], the Intermediate FlowNet directly predicts intermediate ﬂow of Ft→0, Ft→1.
Both (a) and (b) contain a separate synthesis network for target frame generation. In (c), proposed IFRNet jointly reﬁnes the intermediate
ﬂow Ft→0, Ft→1 together with a powerful intermediate feature ˆφt to generate the target frame in a single encoder-decoder.
Although above pipeline that ﬁrst estimates intermediate
ﬂow and then context feature has become the most popular paradigm for ﬂow-based VFI approaches [9,32,33,37,40], it suffers from several defects. First, they divide intermediate
ﬂow and context feature reﬁnement into separate encoder-decoders, which ignores the mutual promotion of these two crucial elements for frame interpolation. Second, their cas-caded architecture based on above design concept can sub-stantially increase the inference delay and model parame-ters, blocking them from mobile and real-time applications.
In this paper, we propose a novel Intermediate Fea-ture Reﬁne Network (IFRNet) for VFI to overcome the above limitations. For the ﬁrst time, we merge above sep-arated ﬂow estimation and feature reﬁnement into a single encoder-decoder based model for compactness and fast in-ference, abstracted in Figure 2 (c). It ﬁrst extracts pyramid features from given inputs by the encoder, and then jointly reﬁnes the bilateral intermediate ﬂow ﬁelds together with a powerful intermediate feature through coarse-to-ﬁne de-coders. The improved architecture can beneﬁt intermediate
ﬂow and intermediate feature with each other, endowing our model with the ability to not only generate sharper moving objects but also capture better texture details.
For better supervision, we propose task-oriented ﬂow distillation loss and feature space geometry consistency loss to effectively guide the multi-scale motion estimation and intermediate feature reﬁnement. Speciﬁcally, our ﬂow distillation approach adjusts the robustness of distillation loss adaptively in space and focuses on learning the useful teacher knowledge for frame synthesizing. Besides, pro-posed geometry consistency loss can employ the extracted intermediate features from ground truth to constrain the re-constructed intermediate features for keeping better struc-ture layout. Figure 1 gives a speed, accuracy and parameters comparison among advanced VFI methods, demonstrating the state-of-the-art performance of our approaches. In sum-mary, our main contributions are listed as follows:
• We devise a novel IFRNet to jointly perform interme-diate ﬂow estimation and intermediate feature reﬁne-ment for efﬁcient video frame interpolation.
• Task-oriented ﬂow distillation loss and feature space geometry consistency loss are newly proposed to pro-mote intermediate motion estimation and intermediate feature reconstruction of IFRNet, respectively.
• Benchmark results demonstrate that our IFRNet not only achieves state-of-the-art VFI accuracy, but also enjoys fast inference speed and lightweight model size. 2.