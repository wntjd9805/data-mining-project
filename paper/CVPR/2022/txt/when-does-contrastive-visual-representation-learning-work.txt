Abstract
Recent self-supervised representation learning tech-niques have largely closed the gap between supervised and unsupervised learning on ImageNet classiﬁcation. While the particulars of pretraining on ImageNet are now rela-tively well understood, the ﬁeld still lacks widely accepted best practices for replicating this success on other datasets.
As a ﬁrst step in this direction, we study contrastive self-supervised learning on four diverse large-scale datasets.
By looking through the lenses of data quantity, data do-main, data quality, and task granularity, we provide new insights into the necessary conditions for successful self-supervised learning. Our key ﬁndings include observations such as: (i) the beneﬁt of additional pretraining data beyond 500k images is modest, (ii) adding pretraining images from another domain does not lead to more general representa-tions, (iii) corrupted pretraining images have a disparate impact on supervised and self-supervised pretraining, and (iv) contrastive learning lags far behind supervised learn-ing on ﬁne-grained visual classiﬁcation tasks. 1.

Introduction
Self-supervised learning (SSL) techniques can now pro-duce visual representations which are competitive with rep-resentations generated by fully supervised networks for many downstream tasks [18]. This is an important mile-stone for computer vision, as removing the need for large amounts of labels at training time has the potential to scale up our ability to address challenges in domains where super-vision is currently too difﬁcult or costly to obtain. However, with some limited exceptions, the vast majority of current state-of-the-art approaches are developed and evaluated on standard datasets like ImageNet [40]. As a result, we do not have a good understanding of how well these methods work when they are applied to other datasets.
Under what conditions do self-supervised contrastive representation learning methods produce “good” visual representations? This is an important question for computer vision researchers because it adds to our understanding of
SSL and highlights opportunities for new methods. This is
Figure 1. What conditions are necessary for successful self-supervised pretraining on domains beyond ImageNet? We investigate the impact of self-supervised and supervised training dataset size, the downstream domain, image quality, and the gran-ularity of downstream classiﬁcation tasks. also an important question for domain experts with limited resources who might be interested in applying SSL to real-world problems. With these objectives in mind, we attempt to answer the following questions: (i) What is the impact of data quantity? How many un-labeled images do we need for pretraining, and when is it worthwhile to get more? How much labeled data do we need for linear classiﬁer training or end-to-end ﬁne-tuning on a downstream task? In which regimes do self-supervised features rival those learned from full supervision? (ii) What is the impact of the pretraining domain? How well do self-supervised representations trained on one do-main transfer to another? Can we learn more general repre-sentations by combining datasets? Do different pretraining datasets lead to complementary representations? (iii) What is the impact of data quality? How robust are self-supervised methods to training time image corruption such as reduced resolution, compression artifacts, or noise?
Does pretraining on corrupted images lead to poor down-stream performance on uncorrupted images? (iv) What is the impact of task granularity? Does SSL
result in features that are only effective for “easy” classiﬁ-cation tasks, or are they also useful for more challenging,
“ﬁne-grained” visual concepts?
We address the above questions through extensive quan-titative evaluation across four diverse large-scale visual datasets (see Figure 1). We make several interesting ob-servations and recommendations including:
• For an ImageNet-scale dataset, decreasing the amount of unlabeled training data by half (from 1M to 500k images) only degrades downstream classiﬁcation performance by 1-2% (Figure 2). In many contexts this trade-off is rea-sonable, allowing for faster and cheaper pretraining. This also indicates that current self-supervised methods cou-pled with standard architectures may be unable to take advantage of very large pretraining sets.
• Self-supervised representations that are learned from im-ages from the same domain as the test domain are much more effective than those learned from different domains (Table 1). Self-supervised training on our current datasets may not be sufﬁcient to learn representations that readily generalize to many contexts.
• Neither (i) combining datasets before pretraining (Ta-ble 2) nor (ii) combining self-supervised features learned from different datasets (Table 3) leads to signiﬁcant per-formance improvements. More work may be required be-fore self-supervised techniques can learn highly general-izable representations from large and diverse datasets.
• Pretraining on corrupted images affects supervised and self-supervised learning very differently (Figure 4). For instance, self-supervised representations are surprisingly sensitive to image resolution.
• Current self-supervised methods learn representations that can easily disambiguate coarse-grained visual con-cepts like those in ImageNet. However, as the granu-larity of the concepts becomes ﬁner, self-supervised per-formance lags further behind supervised baselines (Fig-ure 5). The contrastive loss may lead to coarse-grained features which are insufﬁcient for ﬁne-grained tasks. 2.