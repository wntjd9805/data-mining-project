Abstract
Contrastive language image pretraining (CLIP) en-coders have been shown to be beneficial for a range of visual tasks from classification and detection to caption-ing and image manipulation. We investigate the effective-ness of CLIP visual backbones for Embodied AI tasks. We build incredibly simple baselines, named EmbCLIP, with no task specific architectures, inductive biases (such as the use of semantic maps), auxiliary tasks during train-ing, or depth maps—yet we find that our improved base-lines perform very well across a range of tasks and sim-ulators. EmbCLIP tops the RoboTHOR ObjectNav leader-board by a huge margin of 20 pts (Success Rate). It tops the iTHOR 1-Phase Rearrangement leaderboard, beating the next best submission, which employs Active Neural Map-ping, and more than doubling the % Fixed Strict metric (0.08 to 0.17). It also beats the winners of the 2021 Habi-tat ObjectNav Challenge, which employ auxiliary tasks, depth maps, and human demonstrations, and those of the 2019 Habitat PointNav Challenge. We evaluate the abil-ity of CLIP’s visual representations at capturing semantic information about input observations—primitives that are useful for navigation-heavy embodied tasks—and find that
CLIP’s representations encode these primitives more ef-fectively than ImageNet-pretrained backbones. Finally, we extend one of our baselines, producing an agent capable of zero-shot object navigation that can navigate to objects that were not used as targets during training. Our code and models are available at https://github.com/ allenai/embodied-clip. 1.

Introduction
The CLIP family of neural networks have produced very impressive results at a series of visual recognition tasks, in-cluding an astonishing zero-shot performance on ImageNet that matches the accuracy of a fully supervised ResNet-50 model [22]. Unsurprisingly, visual representations pro-*Equal contribution
Figure 1. We show powerful visual encoders are important for
Embodied AI tasks. We consider four navigation-heavy tasks and show CLIP-based encoders provide massive gains over ResNet ar-chitectures trained on ImageNet. More interestingly, using only
RGB images as input, they outperform approaches employing depth images, maps, and more sophisticated architectures. vided by CLIP have now also been shown to provide im-provements across other computer vision tasks such as open vocabulary object detection [11], image captioning, visual question answering, and visual entailment [27]. In this work, we investigate the effectiveness of CLIP’s visual representations at tasks in the domain of Embodied AI.
Embodied AI tasks involve agents that learn to navi-gate and interact with their environments. Shen et al. [27] demonstrated that CLIP features can provide gains in in-struction following along a navigation graph: a task where an agent traverses the graph using linguistic instructions such as Walk past the piano. Building upon this outcome, we provide a thorough investigation of CLIP-powered Em-bodied AI models at tasks that require agents to use low level instructions (such as Move Ahead, Turn, Look
Down, and Pick Up) to take steps in a scene and inter-act with objects. Such tasks, including Object Goal Nav-igation [3] and Room Rearrangement [2], are inherently navigation-heavy. As a result, visual representations of ob-servations in such tasks must not just be effective for rec-ognizing categories, but also at encoding primitives such as walkable surfaces, free space, surface normals, and geomet-ric structures.
We build a series of simple baselines using the CLIP
ResNet-50 visual encoder. Our embodied agents input CLIP representations of observations that are frozen and not fine-tuned for the task at hand. These representations are com-bined with the goal specification and then passed into a re-current neural network (RNN) unit to provide memory. A linear layer transforms the hidden activations of the RNN to a distribution over the agent’s actions. These simple base-lines have very few task specific design choices, do not use depth maps (which are commonly regarded as criti-cal for good results), use no spatial or semantic maps, and employ no auxiliary tasks during training. And yet, with-out these design choices that have been empirically proven to be effective, our simple CLIP-based baselines are very effective—vaulting to the top of two leaderboards for the tasks of Object Goal Navigation and Room Rearrangement in the AI2-THOR environment, and outperforming the
Habitat simulator challenge winners for Object Goal Navi-gation in 2021 and Point Goal Navigation in 2019.
These results are surprising and suggest that CLIP’s vi-sual representations are powerful and encode visual primi-tives that are useful to downstream Embodied AI tasks. In-deed, we find that CLIP’s representations outperform those from ImageNet pretraining by using a linear probe at four primitive tasks: object presence, object localization, reach-ability and free space estimation. In particular, CLIP’s rep-resentations provide a +3 point absolute improvement on three of our probing studies and +6.5 point (+16 % relative) at the object localization probe.
We analyze 4 visual encoders (2 ResNet models pre-trained on ImageNet and 2 ResNet models pretrained with
CLIP) and use them to train 4 Object Goal Navigation agents. We then study the correlation between ImageNet
Top-1 Accuracy and Success Rate for Object Goal Navi-gation. We find that ImageNet accuracy alone is not a good indicator of an encoder’s suitability for Embodied AI tasks, and that it may be more useful to probe its representations for semantic and geometric information. However, our re-sults do indicate that Embodied AI agents can continue to benefit from better visual encodings.
Finally, we use CLIP’s visual and textual encoders in a simple architecture with few learnable parameters to train an agent for zero-shot Object Goal Navigation (i.e. navi-gating to objects that are not targets at training time). Our results are promising: the agent achieves roughly half the
Success Rate for unseen objects compared to that for seen objects. They suggest that using visual representations that are strongly grounded in language can help build Embodied
AI models that can generalize not only to new scenes, but also to new objects. 2.