Abstract
Fine-tuning pre-trained models for downstream tasks is mainstream in deep learning. However, the pre-trained models are limited to be ﬁne-tuned by data from a speciﬁc modality. For example, as a visual model, DenseNet cannot directly take the textual data as its input. Hence, although the large pre-trained models such as DenseNet or BERT have a great potential for the downstream recognition tasks, they have weaknesses in leveraging multimodal informa-tion, which is a new trend of deep learning. This work fo-cuses on ﬁne-tuning pre-trained unimodal models with mul-timodal inputs of image-text pairs and expanding them for image-text multimodal recognition. To this end, we propose the Multimodal Information Injection Plug-in (MI2P) which is attached to different layers of the unimodal models (e.g.,
DenseNet and BERT). The proposed MI2P unit provides the path to integrate the information of other modalities into the unimodal models. Speciﬁcally, MI2P performs cross-modal feature transformation by learning the ﬁne-grained corre-lations between the visual and textual features. Through the proposed MI2P unit, we can inject the language infor-mation into the vision backbone by attending the word-wise textual features to different visual channels, as well as inject the visual information into the language backbone by at-tending the channel-wise visual features to different textual words. Armed with the MI2P attachments, the pre-trained unimodal models can be expanded to process multimodal data without the need to change the network structures. 1.

Introduction
In social media such as Twitter, a tweet usually contains both the text and image contents which share the same con-∗ Corresponding author: F. Lv (email: fengmaolv@126.com). cept. With the increased use of social media, a massive number of multimodal user-generated contents can be avail-able for training deep models. It is clear that multimodal classiﬁcation can gain a nontrivial advantage over the uni-modal counterpart by using information from both the vi-sual and language modalities [22]. Over the past years, image-text multimodal classiﬁcation has been widely ap-plied to different social media projects such as emergency response [1, 2], emotional recognition [31], fake news de-tection [25], etc.
The core idea in image-text multimodal classiﬁcation is to integrate the image and texts together.
In general, the current works for image-text multimodal recognition can be categorized into two strategies. The ﬁrst strategy main-tains two separate backbones (e.g., DenseNet or BERT) to process each modality and performs multimodal fusion on the classiﬁcation scores or the high-level features produced by each backbone [1, 7, 15]. On the other hand, the sec-ond strategy goes in-depth into the intermediate layers of the backbones and performs multimodal fusion on the ﬁne-grained mid-level features of each modality [13, 16, 17, 29].
However, the current works along this line mainly focus on the homogeneous setting in which the modalities are just different views of the same input (e.g., RGB and depth im-ages) [13, 29, 34]. Due to the strong heterogeneity between the mid-level features of images and texts, the second strat-egy is less studied for the image-text multimodal fusion task. The recently proposed multimodal BERT can model the inter-modal interactions between the ﬁne-grained mid-level features of the visual and language modalities based on the recent advances of Transformer [12, 16–19, 23]. As large pre-trained models, multimodal BERT can be ﬁne-tuned for image-text multimodal recognition.
The previous works have shown that an efﬁcient multi-modal classiﬁcation algorithm needs to consider both the intra-modal processing and inter-modal interaction [13,29].
To be speciﬁc, the intra-modal processing requires to extract the discriminative semantic information from each modal-ity, which is crucial for the classiﬁcation task, while the inter-modal interaction requires to fully integrate the con-tent of each modality. In general, the ﬁrst strategy does well in intra-modal processing by maintaining separate unimodal backbones to process each modality, but has weaknesses in modeling sufﬁcient inter-modal interaction [1, 7, 15]. On the other hand, the multimodal BERT models from the sec-ond strategy do well in inter-modal interaction by attending to the ﬁne-grained token features of each modality, but un-derestimate the end-to-end intra-modal processing of each modality (e.g., directly take the region features extracted from faster-RCNN as visual inputs [17–19]; directly aggre-gate the original image patches or textual features [16]). Al-though the recent PixelBERT proposes to leverage an end-to-end CNN backbone to extract the image features [12], the intra-modal processing is still prone to be underesti-mated once the mid-level features of each modality have been input into the transformer layers [29]. The stacked transformer layers cut off the direct connection between the
CNN backbone and the ﬁnal prediction. Compared with the pre-trained multimodal BERT models, the large pre-trained unimodal models (e.g., DenseNet or BERT) care-fully consider the end-to-end intra-modal processing and have a strong ability in extracting the discriminative seman-tic information from each modality.
Motivated by the above discussion, this work focuses on directly expanding the large pre-trained unimodal mod-els for image-text multimodal recognition, with the consid-eration of both effective intra-modal processing and inter-modal interaction. Our core idea is to integrate the features from other modalities to augment the mid-level features of the unimodal models. To this end, we propose the Multi-modal Information Injection Plug-in (MI2P) attached to the mid-level layers of the unimodal networks (e.g., DenseNet or BERT). In order to bridge the heterogeneity across differ-ent modality features, MI2P performs cross-modal feature transformation by learning the ﬁne-grained cross-modal at-tentions between the visual and textual features. Through the MI2P unit, the language information can ﬂow into the visual backbone by attending the word-wise textual features to different visual channels. Similarly, the visual infor-mation can also ﬂow into language backbone by attending the channel-wise visual features to different textual words.
By ﬁne-tuning the unimodal backbone together with the at-tached MI2P units, the injected multimodal information can be adapted to augment the mid-level features in a proper manner, i.e., enrich the semantic patterns of the mid-level features but not suppress their intra-modal processing.
Compared with the existing image-text multimodal clas-siﬁcation methods [1, 9, 14, 17, 18], our approach can bet-ter balance the inter-modal interaction and intra-modal pro-cessing. For the former purpose, the ﬁne-grained cross-modal interactions are explicitly modeled within the MI2P attachment. In practice, the visual and textual modalities are usually correlated on different abstraction levels (e.g., in the sentence of “An elephant is drinking from the stream with its long nose”, the word nose may relate to the mid-level vi-sual features of images, while the word elephant may relate to the high-level features of images). The MI2P plug-ins can be ﬂexibly attached to multiple layers of the unimodal networks, in order to model the cross-modal interactions of different abstraction levels. For the latter purpose, our ap-proach completely preserves the original network structures of the large pre-trained unimodal models. As plug-ins, the
MI2P units will not suppress the intra-modal processing of the unimodal models.
To sum up, the contributions of this work are three-fold:
• We propose to expand the large pre-trained unimodal models for image-text multimodal classiﬁcation by arming them with the introduced Multimodal Infor-mation Injection Plug-in units. The proposed imple-mentation of multimodal recognition can preserve the strong intra-modal processing ability of the large pre-trained unimodal models.
• Our approach can model the cross-modal interactions of different abstraction levels by attaching the MI2P units to multiple layers of the unimodal models, with the consideration of sufﬁcient inter-modal interaction.
• Our approach can obtain state-of-the-art performance across different image-text multimodal classiﬁcation benchmarks. 2.