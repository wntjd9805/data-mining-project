Abstract
The ability to generalize learned representations across significantly different visual domains, such as between real photos, clipart, paintings, and sketches, is a fundamental capacity of the human visual system. In this paper, differ-ent from most cross-domain works that utilize some (or full) source domain supervision, we approach a relatively new and very practical Unsupervised Domain Generalization (UDG) setup of having no training supervision in neither source nor target domains. Our approach is based on self-supervised learning of a Bridge Across Domains (BrAD) -an auxiliary bridge domain accompanied by a set of seman-tics preserving visual (image-to-image) mappings to BrAD from each of the training domains. The BrAD and mappings to it are learned jointly (end-to-end) with a contrastive self-supervised representation model that semantically aligns each of the domains to its BrAD-projection, and hence im-plicitly drives all the domains (seen or unseen) to semanti-cally align to each other. In this work, we show how using an edge-regularized BrAD our approach achieves signifi-cant gains across multiple benchmarks and a range of tasks, including UDG, Few-shot UDA, and unsupervised general-ization across multi-domain datasets (including generaliza-tion to unseen domains and classes). 1.

Introduction
When observing some technical manual with schematic drawings of equipment for the first time, people do not need any supervision to associate these schematic drawings to real complex objects. This demonstrates the importance and efficiency of one of the basic human capacities - ability to learn with little to no supervision across multiple visual do-mains, as well as to effectively generalize to new domains and even new object classes without additional supervision.
Figure 1. Originally, same domain instances are closer to each other than to instances of the same class in other domains. Naive application of popular self-supervised learning techniques tends to separate domains before classes (Section 4). Our approach is to learn a BrAD - an auxiliary bridge domain (e.g. of edge-like images) that helps aligning instances of the same class across do-mains. Arrows indicate forces in feature space applied by our training losses (Section 3). green = attraction; red = repulsion.
Recent literature, extensively discussed in Section 2, is rich in works on learning to semantically generalize across domains without supervision in the target domain(s). The target domains can be observed as a collection of unla-beled images in Unsupervised Domain Adaptation (UDA), or even completely unseen during training in Domain Gen-eralization (DG). For both UDA and DG, success in gen-eralization would mean that the desired downstream tasks (classification, detection, etc.) would successfully transfer to a new seen or unseen domain. However, in most UDA and DG works an abundant source domain supervision for
*Equal contribution corresponding authors: sivangl∥elisch∥assaf.arbelle∥leonidka@il.ibm.com
the intended downstream task is assumed. But do we al-ways have that in real-life use-cases? In many situations, such as in the technical manuals example above, we need our systems to generalize not only to new domains, but also to completely new kinds of objects for which we might have very little data (images and/or labels) - not sufficient to train the standard UDA or DG methods. Few recent Unsuper-vised DG (UDG) and Few-Shot UDA (FUDA) works, re-alized the importance of this issue, and restrict the source domain(s) supervision to few or even zero labeled exam-ples. In this paper we target the least restrictive (in terms of labeling requirements) UDG setting, where we do not re-quire any source domain supervision at training, and which also implicitly supports generalization to completely new unseen visual domains with new unseen classes.
We all draw an edge-like image when asked to draw something quickly. Object edges seems to be our shared universal visual representation for all the domains we ob-serve. This drove the basic intuition behind our BrAD ap-proach: teach the machine to represent all the visual do-mains (in feature space) in the same way as it represents this seemingly universally shared visual domain of edge-like images. Our approach (Figure 1) is based on our proposed concept of a learnable Bridge Across Domains (BrAD) - an auxiliary visual ‘bridge’ domain to which it is relatively easy to visually map (in an image-to-image sense) all the domains of interest. The BrAD is used only during contrastive self-supervised training of our model for semantically aligning the representations (features) of each of the training domains to the ones for the shared BrAD.
By transitivity of this semantic alignment in feature space, the learned model representations for all the domains are all
BrAD-aligned and hence implicitly aligned to each other.
This makes the learned mappings to the BrAD unneces-sary for inference, making the trained model generalize-able even to new unseen domains (for which we do not have BrAD mappings) at test time. Moreover, not depend-ing on BrAD-mapping for inference allows exploiting ad-ditional non-BrAD-specific features (e.g. color) which are also learned by our representation model encoder (Sec. 3).
We show that even a simple heuristic implementation of the
BrAD idea, mapping images to their edge maps, already gives a nice improvement over strong self-supervised base-lines not utilizing BrAD. We further show that with learn-able BrAD, our method demonstrates good gains across var-ious datasets and tasks: UDG, FUDA, and a proposed task of generalization to different domains and classes.
To summarize our contributions are as follows: (i) we propose a new concept of a learnable BrAD - an auxiliary visual ‘bridge’ domain which is relatively easily mapable from domains of interest, seen or unseen, and can drive the learned representation features to be semantically aligned (generalize) across domains; (ii) we show how using the concept of BrAD combined with self-supervised contrastive learning and some additional ideas allows to train an ef-fective (and efficient) model for different kinds of source-labels-limited cross-domain tasks, including UDG, FUDA, and even generalization across multi-domain benchmarks without supervision; (iii) we demonstrate significant gains of up to 14% over UDG and up to 13.3% over FUDA re-spective state-of-the-art (SOTA) on several benchmarks, as well as showing significant advantages in transferring the learned representations without any additional fine-tuning to new unseen domains and object categories. 2.