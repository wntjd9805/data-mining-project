Abstract
Channel (or 3D filter) pruning serves as an effective way to accelerate the inference of neural networks. There has been a flurry of algorithms that try to solve this practical problem, each being claimed effective in some ways. Yet, a benchmark to compare those algorithms directly is lacking, mainly due to the complexity of the algorithms and some custom settings such as the particular network configura-tion or training procedure. A fair benchmark is important for the further development of channel pruning.
Meanwhile, recent investigations reveal that the chan-nel configurations discovered by pruning algorithms are at least as important as the pre-trained weights. This gives channel pruning a new role, namely searching the optimal channel configuration. In this paper, we try to determine the channel configuration of the pruned models by ran-dom search. The proposed approach provides a new way to compare different methods, namely how well they be-have compared with random pruning. We show that this simple strategy works quite well compared with other chan-nel pruning methods. We also show that under this setting, there are surprisingly no clear winners among different channel importance evaluation methods, which then may tilt the research efforts into advanced channel configura-tion searching methods. Code will be released at https:
//github.com/ofsoundof/random_channel_ pruning. 1.

Introduction tention to, i.e. random channel pruning. By random prun-ing, we mean that the pruning ratio of each layer is ran-domly selected and the channels to be pruned within the layer are determined by some criterion. Random prun-ing is frequently referred as a baseline to show the im-provements of the state-of-the-art channel pruning meth-ods [11, 12, 35, 39, 42, 47, 49, 61, 64]. Yet, the power of ran-dom pruning is not fully released. By the rigorous study in this paper, we have several striking findings as follows.
F1 When brought to the same setting under random prun-ing, the recent proposed channel pruning criteria [19, 36, 44, 49] performs just comparable with the simple
L1 and L2 norm based pruning criteria.
F2 Compared with channel pruning algorithms that start with a pre-trained model [9, 17–19, 23, 37–39, 45, 60, 66] (See results in Table 3), random pruning can find a pruned model with comparable or even superior per-formances.
F3 Even compared with advanced pruning methods that optimize the overall network architecture such place-ment of pooling layers [40] and expansion of available network width [57], random pruning still narrows the performance gap (less than 0.5% on ImageNet classi-fication).
F4 Fine-tuning epochs has a strong influence on the per-formance of the pruned network. High-performing pruned networks usually comes with prolonged fine-tuning epochs.
Since the advent of deep learning based computer vision solutions, network compression has been at the core of re-ducing the computational complexity of neural networks, accelerating their inference, and enabling their deployment on resource constrained devices [20,21,31,41,55,56,63,65].
Channel pruning (or structured pruning, filter pruning) is one of the approaches that can achieve the acceleration of convolutional neural networks (CNNs) [10, 18, 30, 32, 40].
The goal of this paper is to conduct an empirical study on channel pruning procedure that is not paid enough at-Those findings lead to several implications. First of all, considering F1, since L1/L2 based channel pruning could perform as well as the other pruning criteria, by the law of Occam’s razor, most of the cases, the simple L1 and
L2 based pruning criteria can just serve the purpose of channel pruning. Secondly, combining F2 and F3, ran-dom pruning as a neutral baseline, reveals the funda-mental development in the field of network pruning. For algorithms that rely on the predefined network architec-ture and pre-trained network weight, we haven’t gone far
since the advent of network pruning. Beyond that, overall network architecture optimization brings additional bene-fits. The performance difference of most methods fall into a narrow range of 1%, which is close to the performance of the original network. This on the one hand shows the characteristic of channel pruning, i.e. the performance of the channel pruned network is upper bounded by the orig-inal network 1. On the other hand, it shows the difficulty of the problem, i.e. every small improvement comes with huge efforts (mostly computation). Thirdly, considering F4, for a fair comparison and a long-lasting development of the field, fine-tuning epoch should be standardized. We encourage researchers in this field to explain in detail the training and fine-tuning protocol especially the number of epochs. As such, computational cost could be kept in mind for both researchers and industrial practitioners.
The discussion above leads to the unique role that ran-dom pruning could play in channel pruning, i.e. serving as a baseline to benchmark different channel pruning meth-ods [5]. On the one hand, random channel pruning could bring different pruning criteria under the same regime. As such, the different channel importance estimation methods becomes a meta component which is fit to work with the existing methods. On the other hand, random pruning can become a baseline for other algorithms. Since the perfor-mance of channel pruning algorithms can be influenced by a couple of factors especially the fine-tuning procedure, de-coupling the influential factors and neutrally showing costs and benefits helps creating clarity. Random channel prun-ing also simplifies the pruning algorithm. Instead of resort-ing to sophisticated algorithms such as reinforcement learn-ing [18], evolutionary algorithms [40], and proximal gradi-ent descent [30], channel pruning can be simplified to ran-domly sampling a pool of sub-networks and selecting the best from them.
In this paper, random pruning is studied in two settings.
In the first setting, the task is to prune a pre-trained network.
In the second setting, a pre-trained network is not needed and the pruning algorithm starts with a randomly initial-ized network. The problem is formulated as an architecture search problem. To cope with the searching, the network is reparameterized with an architecture similar to that of the original network. Since the network is trained and pruned from scratch, the second setting is referred to as ‘pruning from scratch’ in this paper.
In both cases, random prun-ing aims at searching the optimal numbers of channels for a compact network, by randomly sampling the space of all possible channel configurations. Although being extremely easy, random pruning performs surprisingly well compared to the carefully designed pruning algorithms. The surpris-ing success of random pruning also call for an optimized sampling method that improves the search efficiency. 1More discussion in the supplementary.
In short, the contributions of this paper are as follows. 1) We present random pruning, a simplified channel prun-ing method as a strong baseline to benchmark other channel pruning methods. The properties of random pruning are analyzed in this paper. 2) We formalize the basic concepts in channel pruning and try to analyze the reason why random pruning could lead to results comparable to those of carefully designed algorithms. 3) We benchmark a number of channel pruning methods, incl. criteria for random pruning, to get a feel for the current status of channel pruning. 2.