Abstract 3D visual grounding aims to locate the referred target object in 3D point cloud scenes according to a free-form language description. Previous methods mostly follow a two-stage paradigm, i.e., language-irrelevant detection and cross-modal matching, which is limited by the isolated ar-chitecture. In such a paradigm, the detector needs to sample keypoints from raw point clouds due to the inherent proper-ties of 3D point clouds (irregular and large-scale), to gen-erate the corresponding object proposal for each keypoint.
However, sparse proposals may leave out the target in de-tection, while dense proposals may confuse the matching model. Moreover, the language-irrelevant detection stage can only sample a small proportion of keypoints on the target, deteriorating the target prediction.
In this paper, we propose a 3D Single-Stage Referred Point Progressive
Selection (3D-SPS) method, which progressively selects keypoints with the guidance of language and directly lo-cates the target. Specifically, we propose a Description-aware Keypoint Sampling (DKS) module to coarsely focus on the points of language-relevant objects, which are sig-nificant clues for grounding. Besides, we devise a Target-oriented Progressive Mining (TPM) module to finely con-centrate on the points of the target, which is enabled by progressive intra-modal relation modeling and inter-modal target mining. 3D-SPS bridges the gap between detection and matching in the 3D visual grounding task, localizing the target at a single stage. Experiments demonstrate that 3D-SPS achieves state-of-the-art performance on both ScanRe-fer and Nr3D/Sr3D datasets. 1.

Introduction
Visual Grounding (VG) aims to localize the target ob-ject in the scene based on an object-related linguistic de-scription.
In recent years, the 3D VG task has received increasing attention owing to its wide applications, such
*Equal contribution
†Corresponding author: Chen Gao.
Figure 1. Traditional two-stage 3D VG methods are limited by the isolation of the detection stage and the matching stage. (a) Sparse proposals may leave out the target in detection. (b) Dense proposals could confuse the matching model. (c) 3D-SPS progres-sively selects keypoints (blue points→red points→green points) and performs referring at a single stage. Noted that dense surfaces are utilized only to help readers understand the example 3D scene, while the input of our method only contains sparse point clouds. as autonomous robots and human-machine interaction in
AR/VR/Metaverse. Even though much progress [29,33–38, 40, 41, 43] has been achieved in the 2D VG task, it is still challenging to locate the referred target object in 3D scenes since point clouds are irregular and large-scale.
Existing 3D VG methods [2, 7, 11, 39, 42, 44] are mainly based on the detection-then-matching two-stage pipeline.
The first stage is language-irrelevant detection, where gen-eral 3D object detectors [4, 20, 23] are adopted to produce numerous object proposals. The second stage is cross-modal matching, where specific vision-language attention
mechanisms are usually designed to match the proposal and the description. Previous methods primarily focus on the second stage, i.e., exploring relations among proposals to distinguish the target object.
We argue that the separation of the two stages limits the existing methods. Previous 2D detection methods adopt data-independent anchor boxes as proposals on regular and well-organized images. However, the anchor-based fashion is generally impractical for the large-scale and irregular 3D point clouds. Consequently, the 3D detector utilized in the first stage needs to sample a limited number of keypoints to represent the whole scene and generate the corresponding proposal for each keypoint. However, sparse proposals may leave out the target in the detection stage (e.g., the sofa chair in Figure 1 (a)), which leads to the inability to locate the tar-get in the matching stage. Meanwhile, dense proposals may contain redundant objects, causing the inter-proposal rela-tionship so complex that the matching module struggles to distinguish the target. As shown in Figure 1 (b), it is difficult to select the right sofa chair from these numerous proposals with similar appearances. Therefore, the two-stage ground-ing methods face a dilemma of deciding the proposal num-ber. Besides, the keypoint sampling strategy (e.g., Farthest
Point Sampling (FPS) [25]) usually adopted in the detec-tor at the first stage is also language-irrelevant. The strat-egy aims to sample keypoints to cover the entire scene as much as possible to detect all potential objects. Thus, the proportion of target keypoints is relatively small, which is unfavorable for the target prediction.
To address the aforementioned issues, we propose a 3D Single-Stage Referred Point Progressive Selection (3D-SPS) method in this paper. Our main idea is to progres-sively select keypoints under the guidance of the language description throughout the whole process, as shown in Fig-ure 1 (c). Based on this idea, we propose a Description-aware Keypoint Sampling (DKS) module to coarsely fo-cus on the points of language-relevant objects, e.g., sofa chair, couch, and table in Figure 1 (c). These keypoints provide significant clues for localizing the grounding target in the following cross-modal interaction. Besides, we de-vise a Target-oriented Progressive Mining (TPM) module, which conducts progressive mining to finely figure out the target. We leverage the self/cross-attention mechanism to model intra/inter-modal relationships respectively. In addi-tion, we fuse the keypoint features with point features of the whole scene to achieve global localization perception. To progressively select keypoints of the target, we utilize the language-points cross-attention map to select the keypoints that the language pays more attention to and discard irrele-vant points. The model gradually concentrates on the target and obtains a condensed set of keypoints through multiple layers. Thus, the proportion of target points will gradually increase with richer target-related features, which benefits the target box regression. Finally, 3D-SPS distinguishes the target from the condensed keypoint set and regresses its bounding box. Note that 3D-SPS is also consistent with the commonsense of how human finds the target object. Com-monly, a human first selects a coarse candidate set accord-ing to the language description and then finely recognize and judge it to select the target object. [16, 31]
In summary, we make the following contributions:
• We propose the 3D-SPS method, which directly per-forms 3D VG at a single stage to bridge the gap be-tween detection and matching. To the best of our knowledge, 3D-SPS is the first work investigating single-stage 3D VG.
• We treat the 3D VG task as a keypoint selection prob-lem. Two selection modules, i.e., DKS and TPM, are designed to progressively select target-related key-points. DKS samples the coarse language-relevant keypoints, and TPM finely mines the cross-modal re-lationship to distinguish the target.
• Extensive experiments confirm the effectiveness of our 3D-SPS achieves the state-of-the-art per-method. formance on both ScanRefer [2] and Nr3D/Sr3D [1]
The code is provided in https : / / datasets. github.com/fjhzhixi/3D-SPS. 2.