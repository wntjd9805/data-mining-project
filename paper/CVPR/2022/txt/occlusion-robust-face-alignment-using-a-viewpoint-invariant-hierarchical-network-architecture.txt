Abstract
The occlusion problem heavily degrades the localization performance of face alignment. Most current solutions for this problem focus on annotating new occlusion data, in-troducing boundary estimation, and stacking deeper mod-els to improve the robustness of neural networks. How-ever, the performance degradation of models remains un-der extreme occlusion (i.e. average occlusion of over 50%) because of missing a large amount of facial context infor-mation. We argue that exploring neural networks to model the facial hierarchies is a more promising method for deal-ing with extreme occlusion. Surprisingly, in recent stud-ies, little effort has been devoted to representing the facial hierarchies using neural networks. This paper proposes a new network architecture called GlomFace to model the fa-cial hierarchies against various occlusions, which draws in-spiration from the viewpoint-invariant hierarchy of facial structure. Specifically, GlomFace is functionally divided into two modules: the part-whole hierarchical module and the whole-part hierarchical module. The former captures the part-whole hierarchical dependencies of facial parts to suppress multi-scale occlusion information, whereas the latter injects structural reasoning into neural networks by building the whole-part hierarchical relations among facial parts. As a result, GlomFace has a clear topological inter-pretation due to its correspondence to the facial hierarchies.
Extensive experimental results indicate that the proposed
GlomFace performs comparably to existing state-of-the-art methods, especially in cases of extreme occlusion. Models are available at https://github.com/zhuccly/
GlomFace-Face-Alignment. 1.

Introduction
Although great effort [4, 9, 16, 21, 46, 47, 51, 54] has been devoted to face alignment, the localization accuracy
*Corresponding author.
Figure 1. Insight of the proposed GlomFace. remains unsatisfactory under various occlusions. Especially for the current situation, under which people have to wear medical masks due to the COVID-19 pandemic. The fol-lowing reasons cause the problem. Firstly, some landmarks are inevitably invisible, and partial facial information is not available. Secondly, large-scale occlusion datasets (average occlusion of over 50%) are scarce because annotating land-marks under occlusion is a great challenge. Thirdly, general neural network architectures cannot model the spatial rela-tionship between facial components [35].
Some studies [30, 53] deal with the occlusion problem by enhancing the coupling of facial context features. How-ever, the excessive coupling may introduce occlusion in-formation over the whole face, resulting in the degrada-tion of localization accuracy on non-occluded areas. Meth-ods such as [22, 23, 40, 41] integrate related tasks (e.g. vis-ibility estimation and uncertainty prediction) to improve the occlusion robustness. However, the related tasks can-not directly impose the shape constraint over all landmarks and may even introduce additional annotation and compu-tational costs. Recently, boundary estimation has been in-tegrated into heatmap-based models [18, 19, 42, 46] and has become a mainstream solution to occlusion, which predicts facial boundaries to provide the shape constraint. Nonethe-less, boundary estimation is prone to failure due to the loss of boundary information under extreme occlusion, leading to drift in all landmarks, as shown in the second image of Figure 2. In addition, boundary estimation is computa-tionally complex. Therefore, the performance degradation caused by occlusion remains an unsolved problem.
There is the fact that facial landmarks describe the physi-ological structure of a human face, which inherently possess viewpoint-invariant hierarchies. The hierarchies are not dis-turbed by any external environments and thus can be con-sidered to be powerful clues for structural reasoning. There appears to be some works [25, 26, 55] that focus on the fa-cial hierarchies, but they only fine-tune the predicted results for the backbone networks rather than actually model the hierarchies. Geoffrey Hinton indicated that a general neu-ral network can hardly represent viewpoint-invariant hier-archies and proposed GLOM [15] to do so. Unfortunately,
GLOM [15] only presents a single idea about representa-tion without describing any working network. Inspired by
GLOM, we thought over how a neural network with a fixed architecture can model the viewpoint-invariant hierarchies to handle occlusion. To achieve this, we propose a new neu-ral network architecture called GlomFace, which is func-tionally divided into two modules: the part-whole hierarchi-cal module (PHM) and the whole-part hierarchical module (WHM).
We first define the face hierarchies into different levels, and further divide a face into the different facial parts at each level. The part-whole hierarchical module (PHM) hi-erarchically captures the part-whole spatial dependencies of each facial part, as shown on the left of Figure 1. The shape-indexed patches are fed into the PHM as the lowest-level fa-cial part, and then, this module captures the short-range spa-tial dependencies within each patch. Subsequently, adjacent patches are combined into neighborhood parts. PHM then enlarges the range of spatial dependencies to the neighbor-hood level. The above operation is repeated until all patches are combined into a whole. With hierarchical spatial depen-dencies, PHM can suppress multi-scale occlusion informa-tion. Using shape-indexed patches instead of the raw image as input is done for two reasons, 1) providing a clear part-whole hierarchy and 2) the low computational complexity when capturing spatial dependencies in each part. When
PHM outputs a whole representation, the whole-part hierar-chical module (WHM) starts to build hierarchical relations among facial parts for structural reasoning. To achieve this,
WHM hierarchically disentangles the whole representation into low-level part representations, as shown on the right of Figure 1.
In each representation disentangling, WHM considers the coupling relations (part-part relationship) be-Figure 2. Hourglass v.s. GlomFace on a Masked face. “BE” de-notes boundary estimation, which imposes shape constraint [42, 46]. We can see that the boundary estimation will make the global shape drift when real facial boundaries are lost. Note that three models are fairly trained on 300W [34], not masked faces. tween adjacent facial parts at the same level and the con-strained relations (whole-part relationship) of a high-level facial part over its internal low-level facial parts. When rep-resentation disentangling is completed level by level, part-part and whole-part relations are simultaneously built. With hierarchical relations, WHM can achieve structure reason-ing against the shape damage of facial landmarks. Finally, the predicted landmarks are used to update the position of all shape-indexed patches that will be fed into GlomFace again to refine all spatial dependencies and relations. With the viewpoint-invariant hierarchical architecture, the pro-posed GlomFace can handle various occlusions (e.g. self-occlusion and external occlusion) and achieve promising performance even for extreme occlusion cases. Figure 2 shows an example compared with the mainstream Hour-glass [32] backbone equipped with boundary estimation.
Experiments demonstrated that GlomFace is more robust to occlusion and has a smaller number of FLOPS compared with hourglass-based methods [32, 42, 46]. 2.