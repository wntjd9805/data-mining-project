Abstract
Clinical outcome or severity prediction from medical im-ages has largely focused on learning representations from single-timepoint or snapshot scans. It has been shown that disease progression can be better characterized by tempo-ral imaging. We therefore hypothesized that outcome pre-dictions can be improved by utilizing the disease progres-sion information from sequential images. We present a deep learning approach that leverages temporal progression in-formation to improve clinical outcome predictions from
In our method, a self-attention single-timepoint images. based Temporal Convolutional Network (TCN) is used to learn a representation that is most reflective of the disease trajectory. Meanwhile, a Vision Transformer is pretrained in a self-supervised fashion to extract features from single-timepoint images. The key contribution is to design a recal-ibration module that employs maximum mean discrepancy loss (MMD) to align distributions of the above two contex-tual representations. We train our system to predict clini-cal outcomes and severity grades from single-timepoint im-ages. Experiments on chest and osteoarthritis radiography datasets demonstrate that our approach outperforms other state-of-the-art techniques. 1.

Introduction
Predicting clinical outcomes from medical images is a long standing goal in the medical vision community
[1, 6, 39, 77]. For the past half a decade, researchers have employed various deep neural networks (DNNs) [10,30,38] to improve diagnostic and prognostic performance. Pre-viously, DNNs were trained from scratch [56] for classi-fication and detection tasks on various medical imaging datasets. These multi-organ datasets can range from 2D ra-diographs (x-rays) [50] to 3D magnetic resonance imaging (MRI) [41] or computerized tomography (CT) [70] scans.
More recent frameworks have employed knowledge distil-lation [48,55] and self-supervision techniques [4,64] to pre-train models which are then finetuned on limited medical imaging data. This has led to improved model performance.
However, most medical imaging datasets contain only single-timepoint or ‘snapshot’ images. Although a snap-shot image plays an essential role for describing a disease, sequential scans provide a more comprehensive character-ization of the evolution and prognosis of a pathology. The temporal evolution of imaging biomarkers are highly corre-lated with disease progression trajectory. We hypothesize that this rich underlying domain information can be lever-aged by deep learning approaches to make accurate predic-tions about the disease trajectory even when temporal data is limited/unavailable.
In practice, temporal medical data can be very limited because patients are often lost to follow-up or suffer from chronic diseases with infrequent re-evaluations of their con-dition. Temporal models usually overfit on these small datasets leading to poor generalizability. Hence, they are limited in their use as a standalone source for training recur-rent neural networks (RNN), Temporal ConvNets (TCN), etc. Recently there have been many deep learning–based works that aim to learn representations from sequential medical imaging data [20, 21, 33, 63]. The bottleneck of limited training samples is evident in all of them. Unlike the video vision community where the presence of large scale temporal datasets facilitates temporal modeling approaches, pursuing similar problems (for e.g., future timepoint sever-ity prediction, object evolution) in medical imaging sce-nario is technically challenging.
In this work we propose to learn disease progression patterns from limited temporal imaging data, and use this auxiliary knowledge to enhance predictive performance of methods that use snapshot scans. Since the representations are obtained from two different domains - snapshot and temporal - the challenges lie in how to optimally adapt and align these feature distributions. Because each image in a temporal sequence contributes unequally, we first extract an
‘optimal’ embedding of the entire sequence. An ‘optimal’
embedding should retain maximum information focused on the key transition stages over the course of a disease. The temporal feature representation can then be aligned against a snapshot feature. Our next step involves employing an appropriate feature matching technique to re-calibrate the two different domain representations (snapshot and tempo-ral). We build a framework that leverages partially available temporal data to re-calibrate the representations learned by the single-timepoint pipeline. A temporal network that em-ploys multi-head self-attention at each layer is incorporated in our architecture. We eventually obtain a global attention distribution that aids in selecting an optimal representation from the whole sequence. Meanwhile, a vision transformer is pretrained in a self-supervised fashion to extract features from snapshot images. Finally, during the finetuning phase, maximum mean discrepancy (MMD) loss is proposed as a feature matching tool to minimize the distance between the two representations.
The main contributions of this work are as follows:
• This is the first work that learns representations from limited temporal medical images, and eventually uti-lizes them to improve clinical prediction tasks from single-timepoint datasets.
• We use a Temporal ConvNet that employs hierarchical attention to obtain the most optimal representation of a temporal image sequence, so that it can be compared with the features from a single image–based pipeline.
• In our study, intermediate representations are available from temporal and snapshot images. We propose to use MMD loss for the first time in this domain, to align the snapshot feature space with the optimal temporal representations selected through an attention mecha-nism. 2.