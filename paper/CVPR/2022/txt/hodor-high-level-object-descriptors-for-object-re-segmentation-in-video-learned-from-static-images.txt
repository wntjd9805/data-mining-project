Abstract
Existing state-of-the-art methods for Video Object Seg-mentation (VOS) learn low-level pixel-to-pixel correspon-dences between frames to propagate object masks across video. This requires a large amount of densely annotated video data, which is costly to annotate, and largely redun-dant since frames within a video are highly correlated. In light of this, we propose HODOR: a novel method that tack-les VOS by effectively leveraging annotated static images for understanding object appearance and scene context. We encode object instances and scene information from an im-age frame into robust high-level descriptors which can then be used to re-segment those objects in different frames. As a result, HODOR achieves state-of-the-art performance on the DAVIS and YouTube-VOS benchmarks compared to ex-isting methods trained without video annotations. With-out any architectural modification, HODOR can also learn from video context around single annotated video frames by utilizing cyclic consistency, whereas other methods rely on dense, temporally consistent annotations. Source code: https://github.com/Ali2500/HODOR 1.

Introduction
Current state-of-the-art Video Object Segmentation (VOS) methods learn ‘space-time correspondences’ (STC), i.e. pixel-to-pixel correspondences, between the image frames in a video. These methods [7,28,47] achieve impres-sive results, but require a large amount of temporally dense annotated video for training. Such datasets require signifi-cant human effort, and the annotations are largely redundant since image frames within a video are highly correlated.
The largest publicly available VOS dataset [46] contains only a few thousand videos. Single image datasets [14, 20], in contrast, exist with hundreds of thousands of annotated images. In this work, we explore the following question: can VOS be learned with only single-image annotations?
To this end, we propose HODOR: High-level Object (a) Space-time correspondence [6, 7, 19, 23, 28, 32, 33, 36, 47, 48]. (b) High-level Object Descriptors for Object Re-segmentation (ours).
Figure 1. Previous methods (a) learn low-level pixel-pixel corre-spondence to propagate object masks. HODOR (b) learns high-level object descriptors to re-segment objects in a different frame.
Descriptors for Object Re-segmentation, a novel VOS framework which extracts a robust, high-level descriptor for the given objects and background in an image. These de-scriptors are then used to find and segment those objects in another video frame, i.e. re-segment them, even if the object moves or changes appearance (Fig. 1b). This differs funda-mentally from STC methods which learn low-level, pixel-to-pixel correspondences (Fig. 1a). The underlying idea is that high-level object descriptors can be learned without sequential video data, as this only requires understanding object appearance, and not reasoning about motion. Thus,
HODOR can be trained for VOS using only single images without any video motion augmentation (Fig 2a), and still be applied to video (Fig 2b). This is inherently not possible with STC methods since learning correspondences requires comparing multiple, different frames.
The key to our approach is that it forces object appear-ance information to pass through a concise descriptor, i.e. an information bottleneck. This prevents the descriptor from trivially summarizing the object mask shape and location.
The network thus learns to concisely encode object appear-ance, and also to match the descriptor to each pixel in order
Figure 2. HODOR train and inference strategies. HOD: High-level Object Descriptor Encoder. OR: Object Re-segmentation Decoder.
Left: HODOR can be trained with single annotated images (without sequence augmentations). Center: HODOR is run on video by feeding features from a different frame to the decoder. Right: Training HODOR can take advantage of unlabeled frames using cycle-consistency. to re-segment the object in the same image.
If we add sequential augmentation to our single im-age training strategy to increase the network’s robustness,
HODOR out-performs all existing methods trained with similar augmented image sequences on the DAVIS [30] and YouTube-VOS [46] benchmarks. This is because STC methods can only learn correspondences of simple motion from augmented frames, and thus cannot generalize well to the complex motion of real video. HODOR however, being based on high-level object appearance and scene context, is much more resilient to this discrepancy.
HODOR can also be trained using cycle consistency on video where only a single frame is annotated (Fig 2c). With-out modifying the approach at all, we can simply propagate masks through unlabeled frames and then in reverse back to the labeled frame to apply the loss. This is enabled by a fully differentiable formulation for attending to soft in-put masks which allows gradients to flow through multiple frame predictions. Based on this, our network can learn to be more robust to appearance changes that occur in natural video, while only requiring single annotated frames. Cur-rent STC methods cannot be trained under this setting.
There are two further advantages: The encoder can pro-cess, and model interactions between, an arbitrary number of objects. This improves performance and makes the in-ference speed largely independent of the number of objects.
This is in contrast to many works [6, 7, 28, 47] where part of the network requires separate forward passes per ob-ject. (2) The decoder can jointly attend to object descriptors over multiple past frames with negligible overhead. Thus, we can incorporate temporal history during inference even though the method can be trained on just single images.
To summarize: we propose a novel VOS framework that uses high-level descriptors to propagate objects across video. This enables training using just single images, with or without other unlabeled video frames. Our model pro-cesses an arbitrary number of objects simultaneously, and can readily incorporate temporal context during inference.
We achieve state-of-the-art results on DAVIS and YouTube-VOS among methods trained without video annotation. 2.