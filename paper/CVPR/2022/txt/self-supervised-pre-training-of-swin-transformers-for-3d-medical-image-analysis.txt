Abstract
Vision Transformers (ViT)s have shown great performance in self-supervised learning of global and local representations that can be transferred to downstream applications. Inspired by these results, we introduce a novel self-supervised learning framework with tailored proxy tasks for medical image analy-sis. Speciﬁcally, we propose: (i) a new 3D transformer-based model, dubbed Swin UNEt TRansformers (Swin UNETR), with a hierarchical encoder for self-supervised pre-training; (ii) tailored proxy tasks for learning the underlying pattern of human anatomy. We demonstrate successful pre-training of the proposed model on 5,050 publicly available computed tomography (CT) images from various body organs. The ef-fectiveness of our approach is validated by ﬁne-tuning the pre-trained models on the Beyond the Cranial Vault (BTCV)
Segmentation Challenge with 13 abdominal organs and seg-mentation tasks from the Medical Segmentation Decathlon (MSD) dataset. Our model is currently the state-of-the-art on the public test leaderboards of both MSD1 and BTCV 2 datasets. Code: https://monai.io/research/swin-unetr. 1.

Introduction
Vision Transformers (ViT)s [16] have started a revolu-tionary trend in computer vision [12, 54] and medical image analysis [6, 19]. Transformers demonstrate exceptional capability in learning pre-text tasks, are effective in learning of global and local information across layers, and provide
˚Work completed during internship at NVIDIA
:Co-senior advising
;Corresponding author: ahatamizadeh@nvidia.com 1https : / / decathlon - 10 . grand - challenge . org / evaluation/challenge/leaderboard/ 2https://www.synapse.org/#!Synapse:syn3193805/ wiki/217785/ (cid:2278)(cid:3010)(cid:3041)(cid:3043)(cid:3028)(cid:3036)(cid:3041)(cid:3047) (cid:3397) (cid:2278)(cid:3004)(cid:3042)(cid:3041)(cid:3047)(cid:3045)(cid:3028)(cid:3046)(cid:3047) (cid:3397) (cid:2278)(cid:3019)(cid:3042)(cid:3047)
Self-Supervised 
Heads
Inpainting Contrastive Rotation (cid:3036) (cid:1878)(cid:3013) (cid:3037) (cid:1878)(cid:3013)
Swin Transformer Encoder
Patch Partition
Cutout + Rot (cid:1876)(cid:3036) (cid:1876)(cid:3037)
Input CT
Sub-Volume
Figure 1. Overview of our proposed pre-training framework.
Input CT images are randomly cropped into sub-volumes and augmented with random inner cutout and rotation, then fed to the
Swin UNETR encoder as input. We use masked volume inpainting, contrastive learning and rotation prediction as proxy tasks for learning contextual representations of input images. scalability for large-scale training [38, 52]. As opposed to convolutional neural networks (CNNs) with limited receptive
ﬁelds, ViTs encode visual representations from a sequence of patches and leverage self-attention blocks for modeling long-range global information [38]. Recently, Shifted windows (Swin) Transformers [30] proposed a hierarchical
ViT that allows for local computing of self-attention with
This architecture achieves non-overlapping windows. linear complexity as opposed to quadratic complexity of self-attention layers in ViT, hence making it more efﬁcient. In addition, due to the hierarchical nature of Swin Transformers, they are well-suited for tasks requiring multi-scale modeling.
In comparison to CNN-based counterparts, transformer-based models learn stronger features representations during pre-training, and as a result perform favorably on ﬁne-tuning downstream tasks [38]. Several recent efforts on ViTs [5, 48] have achieved new state-of-the-art results by self-supervised pre-training on large-scale datasets such as ImageNet [15].
In addition, medical image analysis has not beneﬁted from these advances in general computer vision due to: (1) large domain gap between natural images and medical imaging modalities, like computed tomography (CT) and magnetic resonance imaging (MRI); (2) lack of cross-plane contextual information when applied to volumetric (3D) images (such as CT or MRI). The latter is a limitation of 2D transformer models for various medical imaging tasks such as segmen-tation. Prior studies have demonstrated the effectiveness of supervised pre-training in medical imaging for different ap-plications [9, 39]. But creating expert-annotated 3D medical datasets at scale is a non-trivial and time-consuming effort.
To tackle these limitations, we propose a novel self-supervised learning framework for 3D medical image analysis. First, we propose a new architecture dubbed
Swin UNEt TRansformers (Swin UNETR) with a Swin
Transformer encoder that directly utilizes 3D input patches.
Subsequently, the transformer encoder is pre-trained with tailored, self-supervised tasks by leveraging various proxy tasks such as image inpainting, 3D rotation prediction, and contrastive learning (See Fig. 1 for an overview). Speciﬁcally, the human body presents naturally consistent contextual infor-mation in radiographic images such as CT due to its depicted anatomical structure [43, 50]. Hence, proxy tasks are utilized for learning the underlying patterns of the human anatomy.
For this purpose, we extracted numerous patch queries from different body compositions such as head, neck, lung, abdomen, and pelvis to learn robust feature representations from various anatomical contexts, organs, tissues, and shapes.
Our framework utilizes contrastive learning [35], masked volume inpainting [37], and 3D rotation prediction [17] as pre-training proxy tasks. The contrastive learning is used to differentiate various ROIs of different body compositions, whereas the inpainting allows for learning the texture, structure and correspondence of masked regions to their surrounding context. The rotation task serves as a mechanism to learn the structural content of images and generates various sub-volumes that can be used for contrastive learning. We utilize these proxy tasks to pre-train our proposed framework on a collection of 5,050 CT images that are acquired from various publicly available datasets.
Furthermore, to validate the effectiveness of pre-training, we use 3D medical image segmentation as a downstream application and reformulate it as a 1D sequence-to-sequence prediction task. For this purpose, we leverage the Swin
UNETR encoder with hierarchical feature encoding and shifted windows to extract feature representations at four different resolutions. The extracted representations are then connected to a CNN-based decoder. A segmentation head is attached at the end of the decoder for computing the
ﬁnal segmentation output. We ﬁne-tune Swin UNETR with pre-trained weights on two publicly available benchmarks of Medical Segmentation Decathlon (MSD) and the Beyond the Cranial Vault (BTCV). Our model is currently the state-of-the-art on their respective public test leaderboards.
Our main contributions in this work are summarized as follows:
• We introduce a novel self-supervised learning frame-work with tailored proxy tasks for pre-training on
CT image datasets. To this end, we propose a novel 3D transformer-based architecture, dubbed as Swin
UNETR, consisting of an encoder that extracts feature representations at multiple resolutions and is utilized for pre-training.
• We demonstrate successful pre-training on a cohort of 5,050 publicly available CT images from various applications using the proposed encoder and proxy tasks. This results in a powerful pre-trained model with robust feature representation that could be utilized for various medical image analysis downstream tasks.
• We validate the effectiveness of proposed framework by ﬁne-tuning the pre-trained Swin UNETR on two public benchmarks of MSD and BTCV and achieve state-of-the-art on the test leaderboards of both datasets. 2.