Abstract
In this paper, we propose a solution to the task of generating dynamic 3D facial expressions from a neutral 3D face and an expression label. This involves solving two sub-problems: (i) modeling the temporal dynamics of expressions, and (ii) deforming the neutral mesh to ob-tain the expressive counterpart. We represent the tempo-ral evolution of expressions using the motion of a sparse set of 3D landmarks that we learn to generate by train-ing a manifold-valued GAN (Motion3DGAN). To better en-code the expression-induced deformation and disentangle it from the identity information, the generated motion is represented as per-frame displacement from a neutral con-figuration. To generate the expressive meshes, we train a Sparse2Dense mesh Decoder (S2D-Dec) that maps the landmark displacements to a dense, per-vertex displace-ment. This allows us to learn how the motion of a sparse set of landmarks influences the deformation of the overall face surface, independently from the identity. Experimen-tal results on the CoMA and D3DFACS datasets show that our solution brings significant improvements with respect to previous solutions in terms of both dynamic expression gen-eration and mesh reconstruction, while retaining good gen-eralization to unseen data. Code and models are available at https://github.com/CRISTAL-3DSAM/Sparse2Dense.
Figure 1. 3D dynamic facial expression generation: A GAN generates the motion of 3D landmarks from an expression label and noise; A decoder expands the animation from the landmarks to a dense mesh, while keeping the identity of a neutral 3D face, 1.

Introduction
Synthesizing dynamic 3D (4D) facial expressions aims at generating realistic face instances with varying expres-sions or speech-related movements that dynamically evolve across time, starting from a face in neutral expression. It finds application in a wide range of graphics applications spanning from 3D face modeling, to augmented and vir-tual reality for animated films and computer games. While recent advances in generative neural networks have made possible the development of effective solutions that oper-ate on 2D images [17, 37], the literature on the problem of generating facial animation in 3D is still quite limited.
To perform a faithful and accurate 3D facial animation,
three main challenges arise. First, the identity of the sub-ject whose neutral face is used as starting point for the se-quence should be maintained across time. Second, the ap-plied deformation should correspond to the specified ex-pression/motion that is provided as input, and should be applicable to any neutral 3D face. Incidentally, these are major challenges in 3D face modeling, which require dis-entangling structural face elements related to the identity, e.g., nose or jaw shape, from deformations related to the movable face parts, e.g., mouth opening/closing. Finally, it is required to model the temporal dynamics of the specified expression so to obtain realistic animations.
Some previous works tackled the problem by capturing the facial expression of a subject frame-by-frame and trans-ferring it to a target model [7]. However, in this case the temporal evolution is not explicitly modeled, so the prob-lem reduces to transferring a tracked expression to a neutral 3D face. Some other works animated a 3D face mesh given an arbitrary speech signal and a static 3D face mesh as in-put [13, 29]. Also in this case, the temporal evolution is guided by an external input, similar to a tracked expression.
Instead, here we are interested in animating a face just start-ing from a neutral face and an expression label.
In our solution, which is illustrated in Figure 1, the tem-poral evolution and the mesh deformation are decoupled and modeled separately in two network architectures. A manifold-valued GAN (Motion3DGAN) accounts for the expression dynamics by generating a temporally consistent motion of 3D landmarks corresponding to the input label from noise. The landmarks motion is encoded using the
Square Root Velocity Function (SRVF) and compactly rep-resented as a point on a hypersphere. Then, a Sparse2Dense mesh Decoder (S2D-Dec) generates a dense 3D face guided by the landmarks motion for each frame of the sequence. To effectively disentangle identity and expression components, the landmarks motion is represented as a per-frame dis-placement from a neutral configuration. Instead of directly generating a mesh, the S2D-Dec expands the landmarks dis-placement to a dense, per-vertex displacement, which is fi-nally used to deform the neutral mesh. The intuition that led to this architecture is the following: the movement in-duced on the face surface by the underlying facial muscles is consistent across subjects. In addition, it causes the ver-tex motion to be locally correlated as muscles are smooth surfaces. We thus train the decoder to learn how the dis-placement of a sparse set of points influences the displace-ment of the whole face surface. This has the advantage that structural face parts, e.g., nose or forehead, which are not influenced by facial expressions are not deformed, helping in maintaining the identity traits stable. Furthermore, the network can focus on learning expressions at a fine-grained level of detail and generalize to unseen identities.
In summary, the main contributions of our work are: (i) we propose an original method to generate dynamic se-quences of 3D expressive scans given a neutral 3D mesh and an expression label. Our approach has the capability of generating strong and diverse expression sequences, with high generalization ability to unseen identities and expres-sions; (ii) we adapt a specific GAN architecture [37] for dynamic 3D landmarks generation, and design a decoder for expressive mesh reconstruction from a neutral mesh and landmarks. Differently from common auto-encoders, the proposed S2D-Dec learns to generate a per-vertex dis-placement map from a few control points, allowing accu-rate mesh deformations where the structural face parts re-main stable; (iii) we designed a novel reconstruction loss that weighs the contribution of each vertex based on its dis-tance from the landmarks. This proved to augment the de-coder capability of generating accurate expressions. 2.