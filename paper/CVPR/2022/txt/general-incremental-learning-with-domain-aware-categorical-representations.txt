Abstract
Continual learning is an important problem for achiev-ing human-level intelligence in real-world applications as an agent must continuously accumulate knowledge in re-sponse to streaming data/tasks. In this work, we consider a general and yet under-explored incremental learning prob-lem in which both the class distribution and class-specific domain distribution change over time.
In addition to the typical challenges in class incremental learning, this setting also faces the intra-class stability-plasticity dilemma and intra-class domain imbalance problems. To address above issues, we develop a novel domain-aware continual learn-ing method based on the EM framework. Specifically, we introduce a flexible class representation based on the von
Mises-Fisher mixture model to capture the intra-class struc-ture, using an expansion-and-reduction strategy to dynam-ically increase the number of components according to the class complexity. Moreover, we design a bi-level balanced memory to cope with data imbalances within and across classes, which combines with a distillation loss to achieve better inter- and intra-class stability-plasticity trade-off. We conduct exhaustive experiments on three benchmarks: iDig-its, iDomainNet and iCIFAR-20. The results show that our approach consistently outperforms previous methods by a significant margin, demonstrating its superiority. 1.

Introduction
In order to achieve human-level intelligence, it is in-dispensable for a learning system to continuously accumu-late knowledge over time in an ever-changing environment, known as continual or incremental learning [5]. To cope with real-world scenarios, we consider a general incremen-tal learning problem [3, 18], where both the class distribu-*Both authors contributed equally. This work was supported by Shang-hai Science and Technology Program 21010502700. tion and class-specific domain distributions of the incom-ing data continuously change across sequential learning ses-sions. This requires a model to incrementally learn not only novel class concepts but also new variants of previously-learned concepts.
Existing works on the general incremental learning typ-ically focuse on the online class incremental learning set-ting [3, 18], which has to sacrifice the performance due to its strict computation/memory constraints. In this work, we instead aim to tackle the offline incremental learning set-ting, which has the potential to achieve significantly higher performance than the online counterpart. We note that, un-like the offline class incremental learning [7], this general incremental learning problem additionally faces an intra-class stability-plasticity dilemma which refers to the trade-off between adapting novel examples and preserving current knowledge of the class, and an intra-class domain imbal-ance problem, where the model is biased toward incoming domains due to a limited memory. The intra-class problem is particularly challenging since the domain labels are usu-ally unknown in practice.
The majority of current research on the class incre-mental learning either focuses on improving the inter-class stability-plasticity trade-off and imbalance issue [7, 25] or mainly attempts to tackle the intra-class stability-plasticity dilemma [29, 30, 33]. Recent works on the online gen-eral class incremental learning [3, 18] typically ignore the intra-class structure of data distribution. In particular, those methods usually adopt the same feature representation for the data from both incoming and existing domains of a class, which makes it difficult to learn new domains with-out interference with previously-learned representation of that class. Such domain-invariant representations sacrifice the intra-class plasticity, often resulting in a poor intra-class trade-off between plasticity and stability.
In this work, we develop a novel domain-aware learning framework for the general incremental learning problem, which enables us to address both inter-class and intra-class
challenges in a unified manner. To this end, we introduce a flexible class representation based on the von Mises-Fisher (vMF) mixture model to capture the intra-class structure and a bi-level balanced memory to cope with data imbal-ance within and across classes. In detail, we build a vMF mixture model on deep features of each class to learn a domain-aware representation and design an expansion-and-reduction strategy to dynamically increase the number of its components in new sessions. Combining with an inter-and intra-class forgetting resistence strategy like distilla-tion, our design is capable of achieving better inter- and intra-class stability-plasticity trade-off. Moreover, based on the learned class representation, we propose a balanced memory at both inter- and intra-class level to mitigate bias toward new classes and new domains.
To learn our domain-aware representation, we devise an iterative training procedure for model update at each incre-mental session. Specifically, when new data comes, we first inherit the learned model from last session and allocate new components for the mixture model of each incoming cat-egory. We then adopt the Expectation-Maximization (EM) algorithm to jointly learn the backbone and mixture models, treating the component assignments of input data as latent variables. We incorporate strategies overcoming inter-class forgetting like [13, 27, 37] and adopt intra-class knowledge distillation for alleviating inter- and intra-class catastrophic forgetting, respectively. After each model update, we fur-ther perform a mixture reduction step based on hierarchical clustering to maintain a compact clustering result. During inference, we first extract input features via the backbone network and then infer its component assignment in class, followed by taking the class with the maximal component probability as prediction.
We validate our approach by extensive comparisons with prior incremental learning methods on three benchmarks: iDigits, iDomainNet and iCIFAR-20. For each benchmark, we conduct experiments on splits with varying class and do-main distributions over time. The empirical results and ab-lation study show that our method consistently outperforms other approaches across all benchmarks.
In summary, the main contributions of our work are three-folds as follows:
• We formulate a new offline general incremental learn-ing problem where both class distribution and intra-class domain distribution continuously change over time. This problem has the stability-plasticity dilemma and imbalance issue at both inter- and intra-class level.
• We propose a method based on vMF mixture models to learn a domain-aware representation for addressing the general stability-plasticity dilemma and develop a bi-level balanced memory strategy to mitigate both the inter- and intra-class data imbalance issue.
• Extensive experiments on three benchmarks show that our strategy consistently outperforms existing methods by a sizable margin. 2.