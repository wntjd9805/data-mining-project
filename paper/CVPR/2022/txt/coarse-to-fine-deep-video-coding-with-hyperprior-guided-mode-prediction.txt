Abstract
The previous deep video compression approaches only use the single scale motion compensation strategy and rarely adopt the mode prediction technique from the tra-ditional standards like H.264/H.265 for both motion and residual compression.
In this work, we first propose a coarse-to-fine (C2F) deep video compression framework for better motion compensation, in which we perform mo-tion estimation, compression and compensation twice in a coarse to fine manner. Our C2F framework can achieve better motion compensation results without significantly in-creasing bit costs. Observing hyperprior information (i.e., the mean and variance values) from the hyperprior net-works contains discriminant statistical information of dif-ferent patches, we also propose two efficient hyperprior-guided mode prediction methods. Specifically, using hyper-prior information as the input, we propose two mode pre-diction networks to respectively predict the optimal block resolutions for better motion coding and decide whether to skip residual information from each block for better resid-ual coding without introducing additional bit cost while bringing negligible extra computation cost. Comprehensive experimental results demonstrate our proposed C2F video compression framework equipped with the new hyperprior-guided mode prediction methods achieves the state-of-the-art performance on HEVC, UVG and MCL-JCV datasets. 1.

Introduction
Video compression systems are becoming more and more important for various practical applications due to the rapidly increasing demand for transmitting and storing huge amount of videos. While the conventional methods like
H.264 [43], H.265 [33] and the recent standard H.266 [32] have achieved promising results based on different hand-crafted techniques, they cannot be end-to-end optimized by using large-scale video datasets.
Recently, a large number of deep video compression works [3, 12, 15, 23] have been proposed (see Section 2 for
† Dong Xu is the corresponding author. more details), and most of them follow the hybrid coding framework [32, 33, 43], in which both motion compensa-tion and residual compression modules are used to reduce the spatio-temporal redundancy. Therefore, two aspects are critical when designing new deep video codecs: 1) how to generate more accurate motion information for better mo-tion compensation and 2) how to design more effective mo-tion compression and residual compression approaches.
The state-of-the-art learning based video compression methods [3, 15, 23] only use the single scale motion es-timation and compensation strategy. Considering that the motion patterns in videos may be complex, these single-scale deep video codecs may not work well for compress-ing videos from complex scenarios with significant mo-tion patterns. Motivated by the successful applications of the coarse-to-fine strategy for various tasks (e.g., optical flow estimation [29] and video super-resolution [41]), in this work, we first propose a new coarse-to-fine deep video compression framework by adopting a two-stage motion compensation strategy to better generate the predicted fea-ture. At the coarse level, given the low-resolution fea-tures from the reference frame and the current frame, we perform motion estimation to produce the low-resolution offset features, which are then compressed after using the motion compression module. After upsampling the recon-structed offset features, we further perform coarse-level mo-tion compensation to wrap the high-resolution reference feature as the intermediate predicted feature. Based on this intermediate predicted feature and the high-resolution fea-ture from the current frame, we perform these major opera-tions (i.e., motion estimation, compression, and compensa-tion) again at the fine level to additionally warp this interme-diate predicted feature for better motion compensation. Our two-stage coarse-to-fine motion compensation strategy can generate better predicted feature for the subsequent residual compression module without significantly increasing the bit cost, which leads to better video compression performance.
To further improve video compression performance, we also propose two efficient mode prediction methods for both motion compression and residual compression, which are motivated by the success of the rate-distortion (RD) opti-mization based mode prediction methods in the traditional codecs [32,33,43] and the recent work [14] (see Section 2.1 for more details).
Instead of using the computationally expensive RD optimization technology as in [14], in this work, we propose to train two prediction networks for cod-ing mode prediction, which bring negligible extra compu-tational cost and can thus support more types of coding modes. Specifically, we use discriminant hyperprior infor-mation (i.e., the mean and variance values from the hyper-prior network [27]) as the the input of the mode predic-tion networks as it represents the statistical characteristics of different patches and it does not introduce any additional bit cost. Our proposed mode prediction network can be readily used to adaptively select the optimal resolution of each block in motion compression or decide whether to skip residual information from each block in residual compres-sion.
Our contributions are summarized as follows: (1) We propose a simple and strong C2F deep video compression framework by performing two-stage motion compensation in a coarse-to-fine fashion. (2) We propose two hyperprior-guided mode prediction methods, in which we learn two mode prediction networks by using discriminant hyperprior information as the input. Our hyperprior-guided mode pre-diction methods do not introduce any additional bit cost, bring negligible computational cost, and can be readily used to predict the optimal coding modes (i.e., the optimal block resolution for motion coding and the “skip”/“non-skip” mode for residual compression). (3) Comprehen-sive experiments on the HEVC, UVG and MCL-JCV datasets demonstrate our C2F framework equipped with the newly proposed hyperprior-guided mode prediction meth-ods achieves comparable video compression performance with H265(HM) [1] in terms of PSNR and generally outper-forms the latest standard VTM [2] in terms of MS-SSIM. 2.