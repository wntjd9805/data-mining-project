Abstract
State-of-the-art document dewarping techniques learn to predict 3-dimensional information of documents which are prone to errors while dealing with documents with irreg-ular distortions or large variations in depth. This paper presents FDRNet, a Fourier Document Restoration Network that can restore documents with different distortions and improve document recognition in a reliable and simpler manner. FDRNet focuses on high-frequency components in the Fourier space that capture most structural informa-tion but are largely free of degradation in appearance. It dewarps documents by a flexible Thin-Plate Spline trans-formation which can handle various deformations effec-tively without requiring deformation annotations in train-ing. These features allow FDRNet to learn from a small amount of simply labeled training images, and the learned model can dewarp documents with complex geometric dis-tortion and recognize the restored texts accurately. To facil-itate document restoration research, we create a benchmark dataset consisting of over one thousand camera documents with different types of geometric and photometric distor-tion. Extensive experiments show that FDRNet outperforms the state-of-the-art by large margins on both dewarping and text recognition tasks. In addition, FDRNet requires a small amount of simply labeled training data and is easy to de-ploy. The proposed dataset is available at https://sg-vilab.github.io/event/warpdoc/. 1.

Introduction
Automated document recognition is critical in many ap-plications such as library digitization, office automation, e-business, etc. It has been well solved by optical character recognition (OCR) technology if documents are properly scanned by document scanners. But for increasing docu-ment images captured by various camera sensors, OCR soft-ware often encounters various recognition problems due to two major factors. First, document texts captured by cam-eras often lie over a curved or folded surface and suffer from different types of geometric distortions such as docu-ment warping, folding, and perspective views as illustrated in Fig. 1. Second, document texts captured by cameras of-ten suffer from different types of photometric distortion due to uneven illuminations, motion, shadows, etc. Accurate recognition of document texts captured by camera sensors remains a grand challenge in the document analysis and recognition research community.
Document restoration has been investigated extensively for better recognition of documents captured by various camera sensors. Recent data-driven methods [8, 11] syn-thesize 3D document images with various distortions and learn document distortions by predicting the 3D coordinates of each pixel in warped documents which have achieved very impressive performances on document dewarping task.
However, these methods are facing three challenges. First, most pixels in document images suffer from regular dis-tortions of perspective or curvature, whereas only a small portion of pixels exhibit irregular deformations (e.g. pix-els around crumples). Such pixel-level data imbalance of-ten leads to degraded performance for existing pixel-level regression-based models while handling documents with ir-regular deformations as shown in the first row on the right of Fig. 1. Second, most existing document dewarping meth-ods perform poorly when documents are far away from the camera as illustrated in the second row on the right of Fig. 1.
This is largely because existing methods often struggle in predicting document 3D coordinates when the document depth has large variations. Third, most existing models are trained on large amounts of synthetic images, where the synthesis is complicated requiring to collect 3D coordinates by special hardware (i.e. depth camera) and a large number of scanned document images (i.e. 100,000 synthetic images from 3D coordinates of 1,000 documents and 7,200 scanned images in [8]). This makes it challenging to generalize ex-isting methods to new tasks and domains.
We design FDRNet, an end-to-end trainable document restoration network that focuses on document contents and aims for better document recognition. FDRNet is inspired by the observation that geometric distortions in document images can be largely inferred from high-frequency com-ponents in Fourier space whereas appearance degradation is largely encoded in low-frequency components. Docu-ment restoration and recognition should therefore focus on high-frequency components capturing document structures and contents and ignoring interfering low-frequency com-ponents capturing largely appearance noises. We thus de-sign FDRNet to learn geometric distortions by focusing on high-frequency information of the whole document in-stead of 3D coordinates of each pixel which helps tackle the challenge of pixel-level data imbalance and document depth variation effectively. FDRNet is powered by Thin-Plate Spline transformation which helps not only reduce training data significantly but also eliminate the need for 3D document coordinates ground-truthing and the complex data collection process in training. Furthermore, we intro-duce WarpDoc, a benchmarking dataset with more than one thousand document images with different types of degrada-tion in geometry and appearance that greatly help for better validation of document dewarping models. Extensive ex-periments show that FDRNet achieves superior document restoration as illustrated in Fig. 1.
The contributions of this work are three-fold. First, we design FDRNet, an end-to-end trainable document restora-tion network that can remove geometric and appearance degradation from camera images of documents and im-prove document recognition significantly. Second, FDR-Net handles document restoration and recognition by fo-cusing on high-frequency components in the Fourier space which helps reduce training data and improve model gen-eralization and usability greatly. Third, we create a dataset with more than one thousand camera images of documents which is very valuable to future research in the restoration and recognition of documents captured by cameras. 2.