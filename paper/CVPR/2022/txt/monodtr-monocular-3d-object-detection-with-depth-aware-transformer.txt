Abstract
Monocular 3D object detection is an important yet chal-lenging task in autonomous driving. Some existing meth-ods leverage depth information from an off-the-shelf depth estimator to assist 3D detection, but suffer from the ad-ditional computational burden and achieve limited perfor-mance caused by inaccurate depth priors. To alleviate this, we propose MonoDTR, a novel end-to-end depth-aware transformer network for monocular 3D object detection. It mainly consists of two components: (1) the Depth-Aware
Feature Enhancement (DFE) module that implicitly learns depth-aware features with auxiliary supervision without re-quiring extra computation, and (2) the Depth-Aware Trans-former (DTR) module that globally integrates context- and depth-aware features. Moreover, different from conven-tional pixel-wise positional encodings, we introduce a novel depth positional encoding (DPE) to inject depth positional hints into transformers. Our proposed depth-aware mod-ules can be easily plugged into existing image-only monocu-lar 3D object detectors to improve the performance. Exten-sive experiments on the KITTI dataset demonstrate that our approach outperforms previous state-of-the-art monocular-based methods and achieves real-time detection. Code is available at https:// github.com/ kuanchihhuang/ MonoDTR. 1.

Introduction
Three-dimensional (3D) object detection is a fundamen-tal problem and enables various applications such as au-tonomous driving. Previous methods have achieved supe-rior performance based on the accurate depth information from multiple sensors, such as LiDAR signal [16,22,39,40] or stereo matching [6, 23, 44, 52].
In order to lower the sensor costs, some image-only monocular 3D object detec-tion methods [2, 7, 20, 31, 33, 50] have been proposed and made impressive progress relying on geometry constraints between 2D and 3D. However, the performance is still far from satisfactory without the aid of depth cues.
Recently, several works have tried to produce estimated depth from the pre-trained depth estimation models to as-sist monocular 3D object detection. Pseudo-LiDAR-based
Figure 1. Comparison of different depth-assisted monocular 3D object detection frameworks. (a) Pseudo-LiDAR-based methods [31, 52, 53] lift images to 3D coordinate via monocular depth estimation, followed by a 3D LiDAR-based detector to re-cover object locations. (b) Fusion-based methods [10, 34, 48] ex-tract features from images and estimated depth maps, then fuse them to predict objects. (c) Our MonoDTR learns depth-aware features via additional depth supervision and performs 3D object detection in an end-to-end manner. Note that our depth supervi-sion is only leveraged in the training stage. approaches [31, 52, 53] convert estimated depth maps into 3D point clouds to imitate LiDAR signals, followed by the existing LiDAR-based detector for 3D object detection (see
Figure 1(a)). Some fusion-based approaches [10,34,48] ap-ply several fusion strategies to combine features extracted from depths and images to detect objects (see Figure 1(b)).
These methods, though better localize objects with the help of estimated depth, may suffer from the risk of learning 3D detection on inaccurate depth maps. Also, the additional computational cost of the depth estimator makes it imprac-tical for real-world applications [32].
To address the above issues, we propose MonoDTR, a novel end-to-end depth-aware transformer network for monocular 3D object detection (see Figure 1(c)). A depth-aware feature enhancement (DFE) module is introduced to
learn depth-aware features with the auxiliary depth super-vision, which avoids obtaining inaccurate depth priors from the pre-trained depth estimator. Furthermore, the DFE mod-ule is lightweight yet effective in assisting 3D object de-tection without constructing the complicated architecture to extract features from off-the-shelf depth maps.
It signiﬁ-cantly reduces computational time compared with previous depth-assisted methods [10, 31, 48] (see Table 1).
In addition, unlike previous fusion-based methods (e.g.,
D4LCN [10] and DDMP-3D [48]) that apply carefully de-signed convolution kernels for context- and depth-aware features, we develop the ﬁrst transformer-based fusion mod-ule to globally integrate the image and depth information.
The transformer encoder-decoder structure [47] has been proven to capture long-range dependency effectively; thus, we apply it to model the relationship between context- and depth-aware features. To better represent the property of the 3D object, we utilize depth-aware features to replace the commonly used object queries [3, 18, 60] as input of the transformer decoder, which can provide more meaningful cues for 3D reasoning. Furthermore, we introduce a novel depth positional encoding (DPE) to involve depth-aware hints to the transformer, achieving better performance than conventional pixel-wise positional encodings.
We summarize our contributions as follows: 1. We propose a novel framework, MonoDTR, learning depth-aware features via auxiliary supervision to assist monocular 3D object detection, which avoids introduc-ing high computational cost and inaccurate depth pri-ors from using the off-the-shelf depth estimator. 2. We present the ﬁrst depth-aware transformer module to integrate context- and depth-aware features efﬁciently.
A novel depth positional encoding (DPE) is proposed to inject depth positional hints into the transformer. 3. Experimental results on the KITTI dataset show that our approach outperforms state-of-the-art monocular-based methods and achieves real-time detection. Fur-thermore, the proposed depth-aware modules can be easily plug-and-play in existing image-only frame-works to improve performance. 2.