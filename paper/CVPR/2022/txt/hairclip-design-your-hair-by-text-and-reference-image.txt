Abstract
Hair editing is an interesting and challenging problem in computer vision and graphics. Many existing methods require well-drawn sketches or masks as conditional inputs for editing, however these interactions are neither straight-forward nor efficient.
In order to free users from the te-dious interaction process, this paper proposes a new hair editing interaction mode, which enables manipulating hair attributes individually or jointly based on the texts or ref-erence images provided by users. For this purpose, we encode the image and text conditions in a shared embed-ding space and propose a unified hair editing framework by leveraging the powerful image text representation ca-pability of the Contrastive Language-Image Pre-Training (CLIP) model. With the carefully designed network struc-tures and loss functions, our framework can perform high-quality hair editing in a disentangled manner. Extensive experiments demonstrate the superiority of our approach in terms of manipulation accuracy, visual realism of editing results, and irrelevant attribute preservation. 1.

Introduction
Human hair, as the critical yet challenging component of the face, has long attracted the interest of researchers. In re-† Dongdong Chen is the corresponding author. Our code is available at https://github.com/wty-ustc/HairCLIP cent years, with the development of deep learning, many conditional GAN-based hair editing methods [26, 40, 50] can produce satisfactory editing results. Most of these methods use well-drawn sketches [20, 40, 50] or masks [26, 40] as the input of image-to-image translation networks to produce the manipulated results.
However, we think that these interaction types are not intuitive or user-friendly enough. For example, in order to edit the hairstyle of one image, users often need to spend several minutes to draw a good sketch, which greatly limits the large-scale, automated use of these methods. We there-fore wonder “Can we provide another more intuitive and convenient interaction way, just like human communication behaviors?”. And the language (or“text”) naturally meets our requirements.
Benefiting from the development of cross-modal vision and language representations [28, 37, 38], text-guided im-age manipulation has become possible. Recently, Style-CLIP [31] has achieved amazing image manipulation re-sults by leveraging the powerful image text representation capabilities of CLIP [32]. CLIP has an image encoder and a text encoder, by joint training on 400 million image text pairs, they can measure the semantic similarity between an input image and a text description. Based on this observa-tion, StyleCLIP proposes to use them as the loss supervision to make the manipulated results match the text condition.
Although StyleCLIP inherently supports text description based hair editing, they are not exactly suitable for our task.
It suffers from the following drawbacks: 1) For each spe-cific hair editing description, it needs to train a separate mapper, which is not flexible in real applications; 2) The lack of tailored network structure and loss design makes the method poorly disentangled for hairstyle, hair color, and other unrelated attributes; 3) In practical applications, some hairstyles or colors are difficult to describe in text. At this time, users may prefer to use reference images, but Style-CLIP does not support reference image based hair editing.
To overcome the aforementioned limitations, we propose a hair editing framework that simultaneously supports dif-ferent texts or reference images as the hairstyle/color con-ditions within one model. Generally, we follow StyleCLIP and utilize the StyleGAN [24] pre-trained on a large-scale face dataset as our generator, and then the key is to learn a mapper network to map the input conditions into corre-sponding latent code changes. But different from Style-CLIP, we explore the potential of CLIP to go beyond mea-suring image text similarity, along with some new designs: 1) Shared Condition Embedding. To unify the text and im-age conditions into the same domain, we leverage the text encoder and image encoder of CLIP to extract their embed-ding as the conditions for the mapper network respectively. 2) Disentangled Information Injection. We explicitly sepa-rate hairstyle and hair color information and feed them into different sub hair mappers corresponding to their seman-tic levels. This helps our method achieve disentangled hair editing; 3) Modulation Module. We design a conditional modulation module to accomplish the direct control of in-put conditions on latent codes, which improves the manip-ulation ability of our method.
Since our goal is to achieve the hair editing based on the text or reference image condition while ensuring other irrelevant attributes unchanged, three types of losses are in-troduced: 1) Text manipulation loss is used to guarantee the similarity between the editing result and the given text description; 2) Image manipulation loss is used to guide hairstyle or hair color transfer from the reference image to the target image; 3) Attribute preservation loss is used to keep irrelevant attributes (e.g., identity and background) un-changed before and after editing.
Quantitative and qualitative comparisons and user study demonstrate the superiority of our method in terms of ma-nipulation accuracy, manipulation fidelity, and irrelevant at-tribute preservation. And some example editing results are shown in Figure 1. We also conduct extensive ablation anal-ysis and well justify the designs of our network structure and loss function.
To summarize, our contributions are three-fold as below:
• We push the frontiers of interactive hair editing, i.e., unifying text and reference image conditions within one framework. It supports a wide range of text and image conditions in one single model without the need of training many independent models, which has never been achieved before.
• In order to perform various hairstyle and hair color ma-nipulation in a disentangled manner, we propose some new network structure designs and loss functions tai-lored for our task.
• Extensive experiments and analysis are conducted to show the better manipulation quality of our method and the necessity of each new design. 2.