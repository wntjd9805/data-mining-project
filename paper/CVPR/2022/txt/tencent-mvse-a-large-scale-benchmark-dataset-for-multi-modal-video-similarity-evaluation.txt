Abstract
Multi-modal video similarity evaluation is important for video recommendation systems such as video de-duplication, relevance matching, ranking, and diversity control. However, there still lacks a benchmark dataset that can support supervised training and accurate evaluation. In this paper, we propose the Tencent-MVSE dataset, which is the first benchmark dataset for the multi-modal video simi-larity evaluation task. The Tencent-MVSE dataset contains video pairs similarity annotations, and diverse metadata in-cluding Chinese title, automatic speech recognition (ASR) text, as well as human-annotated categories/tags. We pro-vide a simple baseline with a multi-modal Transformer ar-chitecture to perform supervised multi-modal video simi-larity evaluation. We also explore pre-training strategies to make use of the unpaired data. The whole dataset as well as our baseline will be released to promote the de-velopment of the multi-modal video similarity evaluation.
The dataset has been released in https://tencent-mvse.github.io/. 1.

Introduction
Recent years have witnessed the rapid development of online video-sharing platforms. More and more platforms such as YouTube, Youku, iQIYI, Tencent Video, and Tik-Tok have emerged to become a crucial part of our daily life.
To satisfy the diverse requirements of users, these platforms implement complicated video recommendation systems to perform diverse tasks, including video de-duplication, rel-evance matching, ranking, diversity control, etc. All of these applications rely on effective similarity evaluation al-gorithms that process a thorough understanding of video contents.
The “similarity” of video content is reflected in multiple modalities, including visual content and metadata. Figure 1 shows some examples of video pairs that might be “sim-ilar”. For the first example, the two videos have similar
Figure 1. Some examples of similar video pairs. The video pairs in the three rows are similar in their visual contents, titles, and semantic information. visual contents about “playing football”. Videos from the second pair have different visual contents, while they are both crosstalks acted by the same troupe according to their titles. For the third example, the two videos have different visual contents and titles, while they share “similar” visual and text information related to the same game. Since the similarity exists in such diverse manners, in real application scenarios, video similarity should be evaluated by consider-ing multi-modal information. Inspired by the recent success in the field of natural language processing and computer vi-sion, large-scale labeled datasets are mandatory to advance research progress. However, when creating a video similar-ity benchmark dataset, the multiple modalities bring signif-icant challenges for data annotation and evaluation.
Learning video representations for similarity evaluation requires the supervision of video pairs similarity. Most existing approaches learn video representations via multi-label classification by using the semantic tags as supervi-sion [1, 19, 33]. The tags summarize the videos from var-ious semantic levels and perceptions, and thus can briefly estimate the similarities of video pairs. However, in real video recommendation systems, these semantic tags can not satisfy the higher precision requirements. CDML [23] and
GCML [22] try to involve user behaviours to estimate the video pair similarity. Their idea is conceptually aligned with collaborative filtering, where many users implicitly collaborate to filter relevant items. However, user behav-ior relevance is affected by many factors, not only video content. What’s worse, user behavior differs in different platforms. Moreover, there does not exist a video similar-ity evaluation benchmark in the research community. Such restrictions greatly limit the development of multi-modal video similarity evaluation.
In this paper, we propose a large-scale Tencent-MVSE dataset, which is the first benchmark dataset for the multi-modal video similarity evaluation task, to promote the de-velopment of multi-modal video similarity evaluation. We collect 135, 705 video pairs, and finely annotate their simi-larity scores. A detailed specification for the similarity an-notation is provided to make sure that the annotated sim-ilarity score aligns with the human’s perception. We pro-vide videos as well as rich metadata including Chinese ti-tles, automatic speech recognition (ASR) text, and human-annotated categories and tags to support the evaluation of the video similarity in a multi-modal manner. The anno-tated video pairs data is separated into a pairwise split, a test-dev split, and a test-std split for supervised train-In ad-ing, validation, and final evaluation, respectively. dition, we also collect a pointwise split, which contains 1 million individual videos with video frames and metadata.
The collected pointwise split is to encourage researchers to explore advanced annotation-free approaches by lever-aging more accessible unlabeled data. Compared with ex-isting video understanding datasets with language annota-tions, the Tencent-MVSE dataset has two main character-istics. First, Tencent-MVSE regards video-text as a whole item, and annotates the similarity between items, while ex-isting datasets [29, 39, 42] focus on exploring the relation between video and text. Second, Tencent-MVSE provides 328 categories and 64, 903 tags, which is much larger than existing datasets [1, 19, 21, 33]. All the categories and tags are manually annotated by humans to guarantee high qual-ity. The Tencent-MVSE dataset has been validated in the competition of one of the leading international data mining conferences. It enabled hundreds of participants to imple-ment innovative methods of measuring.
Except for the collection of the Tencent-MVSE dataset, we also provide a simple baseline for the multi-modal video similarity evaluation task. Inspired by the great suc-cess of vision-language understanding approaches such as
UNITER [6], VL-BERT [34], SOHO [16] and VideoBERT
[36], we adopt the advanced single-stream multi-modal
Transformer (MMT) as the base model architecture. Tak-ing the concatenation of sentence token embeddings and video frame features as input, the MMT learns joint video-text embeddings for the input video-text item by using the multi-modal attention mechanism. The annotated similar-ity scores are utilized as the supervision signal to opti-mize the embedding cosine distance between video pairs by mean squared error (MSE) loss. The joint video-text embeddings learned through this method have the rich dis-criminate ability, and thus can evaluate the video similarity much preciser. Additionally, inspired by the effective pre-training strategies of recent works [6, 10, 16, 34, 36], we at-tempt to leverage the pointwise split to perform multi-modal pre-training for MMT. We adopt widely-used masked lan-guage modeling (MLM), masked frame modeling (MFM), and video-text matching (VTM) pre-training tasks to pre-train the MMT. Our results show that all the pre-training strategies can boost the model performance by a large mar-gin, which reveals the potential of the annotation-free data.
Summarily, the contributions of this paper include:
• We collect and annotate the Tencent-MVSE dataset, which is the first multi-modal video similarity evalua-tion benchmark in the research community;
• We build a simple baseline which adopts the advan-tages Transformer for multi-modal learning, and con-ducts sufficient ablation experiments to show the effec-tiveness of each module;
• We adopt the advanced multi-modal pre-training strategies to mine the potential of the MMT model.
The experiment results demonstrate the effectiveness of the pre-training strategies on the multi-modal video similarity evaluation task. 2.