Abstract
Built on top of self-attention mechanisms, vision trans-formers have demonstrated remarkable performance on a variety of tasks recently. While achieving excellent perfor-mance, they still require relatively intensive computational cost that scales up drastically as the numbers of patches,
In self-attention heads and transformer blocks increase. this paper, we argue that due to the large variations among images, their need for modeling long-range dependencies between patches differ. To this end, we introduce AdaViT, an adaptive computation framework that learns to derive usage policies on which patches, self-attention heads and transformer blocks to use throughout the backbone on a per-input basis, aiming to improve inference efficiency of vision transformers with a minimal drop of accuracy for image recognition. Optimized jointly with a transformer backbone in an end-to-end manner, a light-weight decision network is attached to the backbone to produce decisions on-the-fly.
Extensive experiments on ImageNet demonstrate that our method obtains more than 2× improvement on efficiency compared to state-of-the-art vision transformers with only 0.8% drop of accuracy, achieving good efficiency/accuracy trade-offs conditioned on different computational budgets.
We further conduct quantitative and qualitative analysis on learned usage polices and provide more insights on the redundancy in vision transformers. Code is available at https://github.com/MengLcool/AdaViT. 1.

Introduction
Transformers [40], the dominant architectures for a va-riety of natural language processing tasks, have been at-tracting an ever-increasing research interest in the computer vision community since the success of the Vision Trans-former (ViT) [7]. Built on top of self-attention mechanisms,
*Equal contributions.
†Corresponding author.
Figure 1. A conceptual overview of our method. Exploiting the redundancy in vision transformers, AdaViT learns to produce instance-specific usage policies on which patches, self-attention heads and transformer blocks to keep/activate throughout the net-work for efficient image recognition. Fewer computational re-sources are allocated for easy samples (top) while more are used for hard samples (bottom), reducing the overall computational cost with a minimal drop of classification accuracy. Green patches are activated in both figures. transformers are capable of capturing long-range dependen-cies among pixels/patches from input images effectively, which is arguably one of the main reasons that they out-perform standard CNNs in vision tasks spanning from im-age classification [4, 12, 20, 22, 38, 49, 56] to object detec-tion [3, 5, 43, 44], action recognition [9, 23, 58] and so forth.
Recent studies on vision transformers [4, 7, 38, 56] typ-ically adopt the Transformer [40] architecture from NLP
with minimal surgery. Taking a sequence of sliced im-age patches analogous to tokens/words as inputs, the trans-former backbone consists of stacked building blocks with two sublayers, i.e. a self-attention layer and a feed-forward network. To ensure that the model can attend to information from different representation subspaces jointly, multi-head attention is used in each block instead of a single attention function [40]. While these self-attention-based vision trans-formers have outperformed CNNs on a multitude of bench-marks like ImageNet [6], the competitive performance does not come for free—the computational cost of the stacked attention blocks with multiple heads is large, which further grows quadratically with the number of patches.
But are all patches needed to be attended to throughout the network for correctly classifying images? Do we need all the self-attention blocks with multiple heads to look for where to attend to and model the underlying dependencies for all different images? After all, large variations exist in images such as object shape, object size, occlusion and background complexity. Intuitively, more patches and self-attention blocks are required for complex images contain-ing cluttered background or occluded objects, which require sufficient contextual information and understanding of the whole image so as to infer their ground-truth classes (e.g. the barber shop in Figure 1), while only a small number of informative patches and attention heads/blocks are enough to classify easy images correctly.
With this in mind, we seek to develop an adaptive com-putation framework that learns which patches to use and which self-attention heads/blocks to activate on a per-input basis. By doing so, the computational cost of vision trans-formers can be saved through discarding redundant input patches and backbone network layers for easy samples, and only using full model with all patches for hard and complex samples. This is an orthogonal and complemen-tary direction to recent approaches on efficient vision trans-formers that focus on designing static network architec-tures [4, 11, 22, 56].
To this end, we introduce Adaptive Vision Transformer (AdaViT), an end-to-end framework that adaptively deter-mines the usage of patches, heads and layers of vision trans-formers conditioned on input images for efficient image classification. Our framework learns to derive instance-specific inference strategies on: 1) which patches to keep; 2) which self-attention heads to activate; and 3) which transformer blocks to skip for each image, to improve the inference efficiency with a minimal drop of classification accuracy.
In particular, we insert a light-weight multi-head subnetwork (i.e. a decision network) to each trans-former block of the backbone network, which learns to pre-dict binary decisions on the usage of patch embeddings, self-attention heads and blocks throughout the network.
Since binary decisions are non-differentiable, we resort to
Gumbel-Softmax [26] during training to make the whole framework end-to-end trainable. The decision network is jointly optimized with the transformer backbone with a us-age loss that measures the computational cost of the pro-duced usage policies and a normal cross-entropy loss, which incentivizes the network to produce policies that reduce the computational cost while maintaining classification accu-racy. The overall target computational cost can be con-trolled by hyperparameter γ ∈ (0, 1] corresponding to the percentage of computational cost of the full model with all patches as input during training, making the framework flexible to suit the need of different computational budgets.
We conduct extensive experiments on ImageNet [6] to validate the effectiveness of AdaViT and show that our method is able to improve the inference efficiency of vision transformers by more than 2× with only 0.8% drop of clas-sification accuracy, achieving good trade-offs between effi-ciency and accuracy when compared with other standard vi-sion transformers and CNNs. In addition, we conduct quan-titative and qualitative analyses on the learned usage poli-cies, providing more intuitions and insights on the redun-dancy in vision transformers. We further show visualiza-tions and demonstrate that AdaViT learns to use more com-putation for relatively hard samples with complex scenes, and less for easy object-centric samples. 2.