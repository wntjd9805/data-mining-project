Abstract
Image harmonization aims to achieve visual consistency in composite images by adapting a foreground to make it compatible with a background. However, existing methods always only use the real image as the positive sample to guide the training, and at most introduce the corresponding composite image as a single negative sample for an aux-iliary constraint, which leads to limited distortion knowl-edge, and further causes a too large solution space, mak-ing the generated harmonized image distorted. Besides, none of them jointly constrain from the foreground self-style and foreground-background style consistency, which exacerbates this problem. Moreover, recent region-aware adaptive instance normalization achieves great success but only considers the global background feature distribu-tion, making the aligned foreground feature distribution bi-ased. To address these issues, we propose a self-consistent style contrastive learning scheme (SCS-Co). By dynam-ically generating multiple negative samples, our SCS-Co can learn more distortion knowledge and well regularize the generated harmonized image in the style representa-tion space from two aspects of the foreground self-style and foreground-background style consistency, leading to a
*Equal contribution.
†Corresponding author. more photorealistic visual result. In addition, we propose a background-attentional adaptive instance normalization (BAIN) to achieve an attention-weighted background fea-ture distribution according to the foreground-background feature similarity. Experiments demonstrate the superiority of our method over other state-of-the-art methods in both quantitative comparison and visual analysis. 1.

Introduction
Image composition is widely used in image editing
[6, 45] and data augmentation [7, 46], which targets syn-thesizing a composite image by extracting the foreground of one image and pasting it on the background of another image. However, since the foreground and background ap-pearance will be distinct due to different capture conditions, the composite image often looks unrealistic, i.e., suffers from the inharmony problem. Therefore, image harmoniza-tion, which aims to adjust the appearance of the foreground to make it compatible with the background in the composite image, is signiﬁcant and challenging.
Numerous deep learning-based methods have been pro-posed for image harmonization. However, most methods
[6, 14, 15, 35, 39, 48] do not consider this problem from the perspective of visual style. Hence, they fail to ensure a vi-sual style consistency between the foreground and the back-ground [24]. Methods based on domain translation [4, 5] implicitly consider this problem from the perspective of domain-consistency, but do not directly transform the fore-ground feature in the generator.
Recently, Ling et al. [24] explicitly introduce the con-cept of visual style and ﬁrst regard image harmonization as a background-to-foreground style transfer problem1. In-spired by AdaIN [18], they propose a region-aware adap-tive instance normalization (RAIN) for image harmoniza-tion and achieve great success. However, as shown in Fig-ure 1(d), the distortion still exists or even is very severe in some cases.
We argue that two issues lead to the above dilemma: (1)
Just like the problem with AdaIN, RAIN only considers the global style distribution in the background and aligns the foreground feature distribution with it. However, as a com-mon intuition, areas in the background that feature-similar to the foreground need more attention. For example, in the ﬁrst row of Figure 1, the foreground object reappears twice in the background. The model should pay more atten-tion to the local style distributions of these two areas. (2)
The second is a general issue, not limited to the style-based method, and is the core issue we want to solve. Most ex-isting methods [6, 14, 15, 35, 39, 48] only use real images to guide the training via an L1 loss, which is too simple and cannot constrain the solution space well [42]. Toward this end, DoveNet [5] and RainNet [24] adopt a domain veriﬁcation loss. However, it only regards the foreground-background feature similarity of the real/harmonized image as positive/negative, and the input composite image is not
In used, which contains important distortion knowledge. other words, it is just a positive-orient constraint.
In ad-dition, since image harmonization aims to adjust the fore-ground, why not directly constrain the foreground feature?
Considering the above problems, Cong et al. propose a triplet loss [4]. However, it directly pulls the foreground domain code to the background domain code, which is too strong and will be interfered by content information. One more important problem is that only using the input com-posite image as the negative sample, leading to limited ex-ternal distortion knowledge [41, 42], and the learned fea-In sum-ture distribution easily becomes biased [25, 41]. mary, why not dynamically generate multiple negative sam-ples and jointly constrain from the foreground self-style and foreground-background style consistency to obtain more distortion knowledge and reduce the solution space?
Motivated by the observations and analyses above, we try to address these two issues. For the ﬁrst issue, inspired by [26, 29], we propose a