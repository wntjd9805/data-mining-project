Abstract
We revisit large kernel design in modern convolutional neural networks (CNNs). Inspired by recent advances in vi-sion transformers (ViTs), in this paper, we demonstrate that using a few large convolutional kernels instead of a stack of small kernels could be a more powerful paradigm. We suggested five guidelines, e.g., applying re-parameterized large depth-wise convolutions, to design efficient high-performance large-kernel CNNs. Following the guidelines, we propose RepLKNet, a pure CNN architecture whose ker-nel size is as large as 31×31, in contrast to commonly used 3×3. RepLKNet greatly closes the performance gap be-tween CNNs and ViTs, e.g., achieving comparable or supe-rior results than Swin Transformer on ImageNet and a few typical downstream tasks, with lower latency. RepLKNet also shows nice scalability to big data and large models, obtaining 87.8% top-1 accuracy on ImageNet and 56.0% mIoU on ADE20K, which is very competitive among the state-of-the-arts with similar model sizes. Our study fur-ther reveals that, in contrast to small-kernel CNNs, large-kernel CNNs have much larger effective receptive fields and higher shape bias rather than texture bias. Code & mod-els at https://github.com/megvii-research/
RepLKNet. 1.

Introduction
Convolutional neural networks (CNNs) [40, 53] used to be a common choice of visual encoders in modern computer vision systems. However, recently, CNNs [40, 53] have
*This work is supported by the National Natural Science Foundation of
China (Nos.61925107, U1936202, 62021002) and the Beijing Academy of
Artificial Intelligence (BAAI). This work is done during Xiaohan Ding’s internship at MEGVII Technology.
†Project leader.
‡Corresponding author.
Figure 1. The Effective Receptive Field (ERF) of ResNet-101/152 and RepLKNet-13/31 respectively. A more widely distributed dark area indicates a larger ERF. More layers (e.g., from ResNet-Instead, our 101 to ResNet-152) help little in enlarging ERFs. large kernel model RepLKNet effectively obtains large ERFs. been greatly challenged by Vision Transformers (ViTs) [34, 59, 86, 94], which have shown leading performances on many visual tasks – not only image classification [34, 104] and representation learning [4, 9, 16, 100], but also many downstream tasks such as object detection [24, 59], seman-tic segmentation [94, 98] and image restoration [10, 54].
Why are ViTs super powerful? Some works believed that multi-head self-attention (MHSA) mechanism in ViTs plays a key role. They provided empirical results to demon-strate that, MHSA is more flexible [50], capable (less in-ductive bias) [20], more robust to distortions [66, 98], or able to model long-range dependencies [69, 90]. But some works challenge the necessity of MHSA [115], attribut-ing the high performance of ViTs to the proper building blocks [33], and/or dynamic sparse weights [38,111]. More works [20, 38, 42, 95, 115] explained the superiority of ViTs from different point of views.
In this work, we focus on one view: the way of build-In ViTs, MHSA is usually ing up large receptive fields. designed to be either global [34, 78, 94] or local but with large kernels [59, 70, 89], thus each output from a single
MHSA layer is able to gather information from a large re-gion. However, large kernels are not popularly employed in CNNs (except for the first layer [40]).
Instead, a typ-ical fashion is to use a stack of many small spatial con-volutions1 [40, 44, 47, 68, 77, 82, 109] (e.g., 3×3) to en-large the receptive fields in state-of-the-art CNNs. Only some old-fashioned networks such as AlexNet [53], Incep-tions [79–81] and a few architectures derived from neural architecture search [37, 43, 56, 116] adopt large spatial con-volutions (whose size is greater than 5) as the main part.
The above view naturally lead to a question: what if we use a few large instead of many small kernels to conventional
CNNs? Is large kernel or the way of building large recep-tive fields the key to close the performance gap between
CNNs and ViTs?
To answer this question, we systematically explore the large kernel design of CNNs. We follow a very simple “phi-just introducing large depth-wise convolutions losophy”: into conventional networks, whose sizes range from 3×3 to 31×31, although there exist other alternatives to intro-duce large receptive fields via a single or a few layers, e.g. feature pyramids [93], dilated convolutions [13, 101, 102] and deformable convolutions [23]. Through a series of ex-periments, we summarize five empirical guidelines to ef-fectively employ large convolutions: 1) very large kernels can still be efficient in practice; 2) identity shortcut is vi-tal especially for networks with very large kernels; 3) re-parameterizing [30] with small kernels helps to make up the optimization issue; 4) large convolutions boost downstream tasks much more than ImageNet; 5) large kernel is useful even on small feature maps.
Based on the above guidelines, we propose a new architecture named RepLKNet, a pure2 CNN where re-parameterized large convolutions are employed to build up large receptive fields. Our network in general fol-lows the macro architecture of Swin Transformer [59] with a few modifications, while replacing the multi-head self-attentions with large depth-wise convolutions. We mainly benchmark middle-size and large-size models, since ViTs used to be believed to surpass CNNs on large data and mod-els. On ImageNet classification, our baseline (similar model size with Swin-B), whose kernel size is as large as 31×31, achieves 84.8% top-1 accuracy trained only on ImageNet-1K dataset, which is 0.3% better than Swin-B but much more efficient in latency.
More importantly, we find that the large kernel design is particularly powerful on downstream tasks. For exam-ple, our networks outperform ResNeXt-101 [99] or ResNet-101 [40] backbones by 4.4% on COCO detection [55] and 6.1% on ADE20K segmentation [114] under the similar complexity and parameter budget, which is also on par with or even better than the counterpart Swin Transformers but with higher inference speed. Given more pretraining data 1Convolutional kernels (including the variants such as depth-wise/group convolutions) whose spatial size is larger than 1×1. 2Namely CNNs free of any attention or dynamic mechanism, e.g., squeeze-and-excitation [46], multi-head self-attention, dynamic weights [38, 95], and etc. (e.g., 73M images) and more computational budget, our best model obtains very competitive results among the state-of-the-arts with similar model sizes, e.g. 87.8% top-1 ac-curacy on ImageNet and 56.0% on ADE20K, which shows excellent scalability towards large-scale applications.
We believe the high performance of RepLKNet is mainly because of the large effective receptive fields (ERFs) [63] built via large kernels, as compared in Fig. 1. Moreover, Re-pLKNet is shown to leverage more shape information than conventional CNNs, which partially agrees with human’s cognition. We hope our findings can help to understand the intrinsic mechanism of both CNNs and ViTs. 2.