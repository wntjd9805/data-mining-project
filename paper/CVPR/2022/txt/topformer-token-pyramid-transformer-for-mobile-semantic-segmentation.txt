Abstract
Although vision transformers (ViTs) have achieved great success in computer vision, the heavy computational cost hampers their applications to dense prediction tasks such as semantic segmentation on mobile devices.
In this pa-per, we present a mobile-friendly architecture named Token
Pyramid Vision Transformer (TopFormer). The proposed
TopFormer takes Tokens from various scales as input to produce scale-aware semantic features, which are then in-jected into the corresponding tokens to augment the rep-resentation. Experimental results demonstrate that our method significantly outperforms CNN- and ViT-based net-works across several semantic segmentation datasets and achieves a good trade-off between accuracy and latency.
On the ADE20K dataset, TopFormer achieves 5% higher accuracy in mIoU than MobileNetV3 with lower latency on an ARM-based mobile device. Furthermore, the tiny ver-sion of TopFormer achieves real-time inference on an ARM-based mobile device with competitive results. The code and models are available at: https://github.com/hustvl/TopFormer. 1.

Introduction
Vision Transformers (ViTs) have shown considerably stronger results for a few vision tasks, such as image clas-sification [11], object detection [28], and semantic segmen-tation [58]. Despite the success, the Transformer architec-ture with the full-attention mechanism [42] requires power-ful computational resources beyond the capabilities of many mobile and embedded devices.
In this paper, we aim to explore a mobile-friendly Vision Transformer specially de-signed for dense prediction tasks, e.g., semantic segmenta-tion.
*Huazhong University of Science and Technology, China
†Tencent PCG, China
‡Fudan University, China
§Zhejiang University, China
W. Zhang and Z. Huang contributed equally. This work was done when
W. Zhang was an intern at Tencent PCG. X. Wang and W. Liu are the corresponding authors, e-mail: {xgwang,liuwy}@hust.edu.cn
Figure 1 – The latency, mIoU performance versus model size on the ADE20K val. set. Previous models are marked as red points, and our models are shown in blue points. Our methods achieve a better latency/accuracy trade-off. The latency is measured on a single Qualcomm Snapdragon 865 with input size 512×512, and only an ARM CPU core is used for speed testing. No other means of acceleration, e.g., GPU or quantification, is used. * indicates the input size is 448×448.
To adapt Vision Transformers to various dense pre-diction tasks, most recent Vision Transformers such as
PVT [43], CvT [45], LeViT [12], MobileViT [31] adopt a hierarchical architecture, which is generally used in Con-volution Neural Networks (CNNs), e.g., AlexNet [23],
ResNet [15]. These Vision Transformers apply the global self-attention and its variants on the high-resolution tokens, which bring heavy computation cost due to the quadratic complexity in the number of tokens.
To improve the efficiency, some recent works, e.g.,
Swin Transformer [28], Shuffle Transformer [19], Twins [7] and HR-Former [51], compute self-attention within the lo-cal/windowed region. However, the window partition is sur-prisingly time-consuming on mobile devices. Besides, To-ken slimming [40] and Mobile-Former [6] decrease calcula-tion capacity by reducing the number of tokens, but sacrifice their recognition accuracy.
Among these Vision Transformers, MobileViT [31] and
Mobile-Former [6] are specially designed for mobile de-vices. They both combine the strengths of CNNs and ViTs.
For image classification, MobileViT achieves better perfor-mance than MobileNets [16, 36] with a similar number of parameters. Mobile-Former achieves better performance than MobileNets with a fewer number of FLOPs. However, they do not show advantages in actual latency on mobile de-vices compared to MobileNets, as reported in [31]. It raises a question: Is it possible to design mobile-friendly networks which could achieve better performance on mobile semantic segmentation tasks than MobileNets with lower latency?
Inspired by MobileViT and Mobile-Former, we also make use of the advantages of CNNs and ViTs. A CNN-based module, denoted as Token Pyramid Module, is used to process high-resolution images to produce local features1 pyramid quickly. Considering the very limited computing power on mobile devices, here we use a few stacked light-weight MobileNetV2 blocks with a fast down-sampling strategy to build a token pyramid. To obtain rich seman-tics and large receptive field, the ViT-based module, de-noted as Semantics Extractor, is adopted and takes the to-kens as input. To further reduce the computational cost, the average pooling operator is used to reduce tokens to an extremely small number, e.g., 1/(64×64) of the input size.
Different from ViT [11], T2T-ViT [49] and LeViT [12] use the last output of the embedding layer as input tokens, we pool the tokens from different scales (stages) into the very small numbers (resolution) and concatenate them along the channel dimension. Then the new tokens are fed into the
Transformer blocks to produce global semantics. Due to the residual connections in the Transformer block, the learned semantics are related to scales of tokens, denoted as scale-aware global semantics.
To obtain powerful hierarchical features for dense pre-diction tasks, scale-aware global semantics is split by the channels of tokens from different scales, then the scale-aware global semantics are fused with the corresponding tokens to augment the representation. The augmented to-kens are used as the input of the segmentation head.
To demonstrate the effectiveness of our approach, we conduct experiments on the challenging segmentation datasets: ADE20K [59], Pascal Context [33] and COCO-Stuff [1]. We examine the latency on hardware, i.e., an off-the-shelf ARM-based computing core. As shown in Fig-ure 1, our approach obtains better results than MobileNets with lower latency. To demonstrate the generalization of our approach, we also conduct experiments of object detection on the COCO [27] dataset. To summarize, our contributions are as follows.
• The proposed TopFormer takes tokens from different scales as input, and pools the tokens to the very small numbers, in order to obtain scale-aware semantics with 1We use ‘features’ and ‘tokens’ interchangeably here. very light computation cost.
• The proposed Semantics Injection Module can inject the scale-aware semantics into the corresponding to-kens to build powerful hierarchical features, which is critical to dense prediction tasks.
• The proposed base model can achieve 5% mIoU better than that of MobileNetV3, with lower latency on an
ARM-based mobile device on the ADE20K dataset.
The tiny version can perform real-time segmentation on an ARM-based mobile device, with competitive re-sults. 2.