Abstract
Deep Metric Learning (DML) aims to learn represen-tation spaces on which semantic relations can simply be expressed through predeﬁned distance metrics. Best per-forming approaches commonly leverage class proxies as sample stand-ins for better convergence and generalization.
However, these proxy-methods solely optimize for sample-proxy distances. Given the inherent non-bijectiveness of used distance functions, this can induce locally isotropic sample distributions, leading to crucial semantic context being missed due to difﬁculties resolving local structures and intraclass relations between samples. To alleviate this problem, we propose non-isotropy regularization (NIR) for proxy-based Deep Metric Learning. By leveraging Nor-malizing Flows, we enforce unique translatability of sam-ples from their respective class proxies. This allows us to explicitly induce a non-isotropic distribution of samples around a proxy to optimize for. In doing so, we equip proxy-based objectives to better learn local structures. Exten-sive experiments highlight consistent generalization bene-ﬁts of NIR while achieving competitive and state-of-the-art performance on the standard benchmarks CUB200-In addi-2011, Cars196 and Stanford Online Products. tion, we ﬁnd the superior convergence properties of proxy-based methods to still be retained or even improved, making
NIR very attractive for practical usage. Code available at github.com/ExplainableML/NonIsotropicProxyDML. 1.

Introduction
Visual similarity plays a crucial role for applications in image & video retrieval and clustering [4, 52, 63], face re-identiﬁcation [7, 19, 32] or general supervised [22] and un-supervised [5, 18] contrastive representation learning. A majority of approaches used in these ﬁelds employ or can be derived from Deep Metric Learning (DML). DML aims to learn highly nonlinear distance metrics parametrized by deep networks. These networks span a representation space in which semantic relations between images are expressed as distances between respective representations. In the ﬁeld of DML, methods utilizing proxies have shown to provide
Figure 1. Proxy-based Deep Metric Learning methods optimize for non-bijective similarity measures between proxies ((cid:78)) and sample representation (◦), which can introduce local isotropy around proxies, impeding local structures and non-discriminative features to be learned. We propose NIR to explicitly resolve this. among the most consistent and highest performances in ad-dition to fast convergence [23, 38, 56]. While other meth-ods introduce ranking tasks over samples for the network to solve, proxy-based methods require the network to con-trast samples against a proxy representation, commonly ap-proximating generic class prototypes. Their utilization ad-dresses sampling complexity issues [16, 47, 52, 63] inherent to purely sample-based approaches, resulting in improved convergence and benchmark performance.
However, there is no free lunch. Relying on sample-proxy relations, relations between samples within a class can not be explicitly captured. This is exacerbated by proxy-based objectives optimizing for distances between samples and proxies using non-bijective distance functions.
This means, for a particular proxy, that alignment to a sam-ple is non-unique - as long as the angle between sample and proxy is retained, i.e. samples being aligned isotrop-ically around a proxy (see Fig. 1), their distances and re-spective loss remain the same. This means that samples lie on a hypersphere centered around a proxy with same dis-tance and thus incurring the same training loss. This incor-porates an undesired prior over sample-proxy distributions which doesn’t allow local structures to be resolved well.
By incorporating multiple classes and proxies (which is au-tomatically done when applying proxy-based losses such as [23, 38, 43, 56] to training data with multiple classes), this is extended to a mixture of sample distributions around proxies. While this offers an implicit workaround to ad-dress isotropy around modes by incorporating relations of samples to proxies from different classes, relying only on other unrelated proxies potentially far away makes ﬁne-grained resolution of local structures difﬁcult. Furthermore, as training progresses and proxies move further apart. As a consequence, the distribution of samples around prox-ies, which proxy-based objectives optimize for, comprises modes with high afﬁnity towards local isotropy. This in-troduces semantic ambiguity, as semantic relations between samples within a class are not resolved well. However, a lot of recent work has shown that understanding and incorpo-rating these non-discriminative relations drives generaliza-tion performance [31, 34, 46, 49, 66].
To tackle this issue without resorting to sample-based objectives that impede the superior convergence and gen-eralization of proxy-based approaches, this work proposes non-isotropy regularisation (NIR) for proxy-based DML.
NIR extends proxy-based objectives to encourage explic-itly learning unique sample-proxy relations and eliminating semantic ambiguity. In detail, we introduce a novel unique-ness constraint, in which samples within a class must be uniquely and sufﬁciently described by a (non-linear) trans-lation from the respective class proxy. This explicitly in-duces a distribution for proxy-based objectives to match in which isotropy and ambiguity is heavily penalized. We achieve uniqueness by leveraging a bijective and thus in-vertible family of translations. As the proxy-sample trans-lations need to adapt to the speciﬁc domain at hand, we require both trainability and non-linearity of our transla-tion models. These functional constraints are naturally expressed through Normalizing Flows and Invertible Net-works [2, 8, 41]. Using conditional variants, we then for-malize NIR where sample relations are (uniquely) mapped by a Normalizing Flow given some residual conditioned on the respective class proxy.
Extensive experiments show that NIR indeed introduces higher feature diversity, reduces overclustering, increases uniformity in learned representation spaces and learns more diverse class-distributions than non-regularized counter-parts. Evaluating our approach on the standard DML bench-marks CUB200-2011 [57], CARS196 [30] and Stanford
Online Products [40] showcases improved generalization capabilities of NIR-equipped proxy DML, achieving com-petitive or state-of-the-art performance while retaining or even improving convergence speeds. 2.