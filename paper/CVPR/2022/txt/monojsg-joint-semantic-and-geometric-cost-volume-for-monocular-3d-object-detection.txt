Abstract
Due to the inherent ill-posed nature of 2D-3D projec-tion, monocular 3D object detection lacks accurate depth recovery ability. Although the deep neural network (DNN) enables monocular depth-sensing from high-level learned features, the pixel-level cues are usually omitted due to the deep convolution mechanism. To benefit from both the pow-erful feature representation in DNN and pixel-level geomet-ric constraints, we reformulate the monocular object depth estimation as a progressive refinement problem and propose a joint semantic and geometric cost volume to model the depth error. Specifically, we first leverage neural networks to learn the object position, dimension, and dense normal-ized 3D object coordinates. Based on the object depth, the dense coordinates patch together with the corresponding object features is reprojected to the image space to build a cost volume in a joint semantic and geometric error man-ner. The final depth is obtained by feeding the cost volume to a refinement network, where the distribution of semantic and geometric error is regularized by direct depth supervi-sion. Through effectively mitigating depth error by the re-finement framework, we achieve state-of-the-art results on both the KITTI and Waymo datasets.1 1.

Introduction
As a fundamental component in 3D perception, 3D ob-ject detection has drawn increasing attention from the area of autonomous driving, robotic navigation, etc. Recently, it has achieved remarkable progress based on lidar or stereo sensing solutions. However, the high cost of lidar sensors and the complicated online calibration in stereo cameras limit their mass applications in downstream tasks. There-fore, researchers start to focus on a cheaper alternative, monocular-based sensing solution. Yet due to the ill-posed 2D-3D projection, the localization accuracy of monocular 3D object detection is far behind the lidar and stereo-based 1Code available at https://github.com/lianqing11/MonoJSG
Figure 1. From top to bottom: Visualization of the estimated cor-ners in the image space, which can be used to constrain an initial 3D bounding box; Reproject the object patch to the original image using the initial bounding boxes; Initial (green) and our Mono-JSG refined (blue) bounding boxes in the BEV space respectively.
Compared with corners, photometric mismatch provides discrimi-native features for identifying localization error. approaches.
Driven by powerful neural networks, multiple ap-proaches [1, 16, 24, 35, 38, 39] are proposed to alleviate the challenging monocular depth recovery from different per-spectives. From the perspective of data formation, pseudo-lidar based approaches [22, 23, 35, 37] transform the input images to pseudo point cloud and directly adopt lidar de-tectors on it. Although they achieve better performance over traditional approaches [6,26], the heavy reliance on the depth prediction network leads to high latency and overfit-ting [29]. From the perspective of geometry reasoning, ge-ometric constraint based approaches [16, 38] leverage neu-ral networks to predict variant 2D cues then solve the ob-ject depth according to 2D-3D projection constraints.
In particular, the 2D-3D constraints are built from the object edges [20, 26, 28, 38], sparse keypoints [16, 17, 19], dense keypoints [4], etc. Although reasoning depth by 2D-3D constraints is interpretable and easy-to-trace, the optimiza-tion gap between the indirect 2D cue prediction and di-rect depth prediction limits the final depth solving accuracy, i.e., the overall minimum 2D loss on whole training data does not necessarily mean the best depth estimation perfor-mance. As visualized in Figure 1, although the estimated bounding boxes’ edges and corners look almost accurate in the image, the solved 3D bounding box has a non-trivial lo-calization error from the bird’s eye view. This localization error is agnostics from regressed 2D cues. However, if we reproject the object to the original image using the inaccu-rate location, a significant photometric misalignment can be observed in Figure 1, which inspires our joint semantic and geometric depth refinement approach.
In this work, we propose an approach called Joint Se-mantic and Geometric Cost Volume (MonoJSG), which uti-lizes pixel-level visual cues to refine bounding box propos-als. We first enrich the traditional 2D-3D constraint [16,19] by extra estimating the location of each pixel in the nor-malized object coordinate. Based on the estimated object depth, the normalized object coordinate is projected into the image space to build a pixel-level constraint for each bounding box. The pixel-level constraint measures the geo-metric error between each pixel’s 2D location and the pro-jection location of the normalized object coordinate. We further enrich the constraints with a semantic error, which measures the distance of the features queried by the ori-gin 2D location and the projection location. As Figure 1 shows, although the depth error can be obviously revealed by pixel-level raw photometric error, we found that simply extending this strategy to all instances cannot always refine accurate depth due to the variant textureless and irregular re-gions (e.g., the windshield or the rear window, etc.). We in-stead leverage neural networks to learn semantic features as a more robust and discriminative representation compared with the raw image intensity. Based on the designed joint geometric and semantic error manner, we construct a 4D cost volume to draw the error distribution around proposal depth for refinement. To make the cost volume adapt to variant depth error, its size is customized for each proposal based on a predicted depth uncertainty. Then a refinement network is designed to take the adaptive cost volume as in-put and output the final depth.
Our approach shows benefits from two perspectives.
From the view of explicit constraining, the exploited se-mantic features provide more dense cues to measure the lo-calization error than pure sparse keypoints. From the view of data-driven, we force the network to learn discriminative features that are suitable for refinement by end-to-end depth supervision. With the aforementioned advantages, the pro-posed framework achieves superior performance, leading to new state-of-the-arts on the KITTI and Waymo datasets.
We summarize our main contributions as follows:
• Based on pixel-level geometric and semantic visual cues, we present a novel joint semantic and geomet-ric error measuring approach for object depth.
• We design an adaptive 4D cost volume that models the error distribution for depth refinement.
• We demonstrate the effectiveness of the proposed ap-proach on both the KITTI and Waymo datasets, which achieve state-of-the-art results with real-time perfor-mance. 2.