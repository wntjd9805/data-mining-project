Abstract 1.

Introduction
Volumetric view synthesis methods with neural represen-tations, such as NeRF and NeX, have recently demonstrated high-quality novel view synthesis. However, optimizing these representations is slow, and even fully trained mod-els cannot reproduce all ﬁne details in the input views. We present a simple but effective technique to boost the render-ing quality, which can be easily integrated with most view synthesis methods. The core idea is to transfer color resid-uals (the difference between the input images and their re-construction) from training views to novel views. We blend the residuals from multiple views using a heuristic weight-ing scheme depending on ray visibility and angular differ-ences. We integrate our technique with several state-of-the-art view synthesis methods and evaluate the Real Forward-facing and the Shiny datasets. Our results show that at about 1/10th the number of training iterations, we achieve the same rendering quality as fully converged NeRF and
NeX models, and when applied to fully converged models, we signiﬁcantly improve their rendering quality.
State-of-the-art neural view synthesis methods such as
NeRF [25] and NeX [42], to name just two, have re-cently shown near-photorealistic novel view synthesis re-sults. These methods model a scene using a 5D radiance
ﬁeld (3 spatial and 2 view direction dimensions) of contin-uous volumetric density and color. Novel views are syn-thesized using numerical quadrature for approximating the volume rendering integral. The radiance ﬁeld is represented using neural methods, particularly a multilayer perceptron (MLP) in the case of NeRF and feature coefﬁcients on a multi-plane image in the case of NeX. Because volume ren-dering is differentiable, it is straightforward to optimize the parameters of these representations from a sparse set of posed training images by minimizing the difference be-tween these ground truth views and their reconstructions.
However, the training typically requires a long optimiza-tion time on the order of several hours to achieve good visual quality. Even a fully trained model struggles with capturing high-frequency details in the training images for complex scenes. Figure 2 shows an example highlighting
. n o c e
R l a u d i s e
R
#Iters: 10k 100k 1,000k
Figure 2. Reconstructing training images with NeRF. The training is computationally expensive. However, even a fully trained NeRF model (up to 1 million iterations) cannot recover all
ﬁne appearance details present in the training images. The bottom row shows the scaled residual map (difference between the ground truth and the rendered image). Results best viewed on a monitor with zoom-in. the challenges of reproducing ﬁne details in an input image.
In this paper we present a method to boost the render-ing quality of many view synthesis methods by injecting these missing details into their volume rendering proce-dure. Given a pre-trained model, we ﬁrst render the scene at the input training views and pre-compute residual color images, i.e., the difference between the training views and their reconstruction. We then transfer these residual col-ors from the training views to reﬁne the novel color pre-diction when synthesizing novel views. More speciﬁcally, we retrieve multiple residual colors for each sampled 3D point during volume rendering by projecting it onto the im-age planes of training views. We then blend them using a heuristic weighting function that takes visibility and angu-lar deviation into account and improves color prediction by adding blended residuals. This results in perfect zero-error reconstructions at training views and signiﬁcantly improved reconstructions at nearby unobserved views (See Figure 1).
We integrate our approach with several state-of-the-including NeRF and NeX. art view synthesis methods,
We evaluate the effectiveness of our method on the Real
Forward-facing [25] and Shiny [42] datasets. For most view synthesis methods, integrating our proposed approach re-quires only a few lines of code changes with and negli-gible runtime overhead compared to the baseline runtime.
Our results show that our approach improves the novel view rendering quality of fully converged baseline methods. Al-ternatively, it achieves the same quality as fully converged baseline models at signiﬁcantly reduced training times (e.g., about 1/10th number of iterations in the case of NeRF). 2.