Abstract
Human-Object Interaction Detection tackles the problem of joint localization and classiﬁcation of human object in-teractions. Existing HOI transformers either adopt a sin-gle decoder for triplet prediction, or utilize two parallel de-coders to detect individual objects and interactions sepa-rately, and compose triplets by a matching process. In con-trast, we decouple the triplet prediction into human-object pair detection and interaction classiﬁcation. Our main mo-tivation is that detecting the human-object instances and classifying interactions accurately needs to learn represen-tations that focus on different regions. To this end, we present Disentangled Transformer, where both encoder and decoder are disentangled to facilitate learning of two sub-tasks. To associate the predictions of disentangled de-coders, we ﬁrst generate a uniﬁed representation for HOI triplets with a base decoder, and then utilize it as input fea-ture of each disentangled decoder. Extensive experiments show that our method outperforms prior work on two pub-lic HOI benchmarks by a sizeable margin. Code will be available. 1.

Introduction
Human-object interaction(HOI) detection [11] aims at detecting all the <human, verb, object> triplets in an image.
It has attracted increasing attention in the computer vision community in recent years [8, 10]. Accurate estimation of human-object interactions can beneﬁt multiple downstream tasks, such as human action recognition [38], scene graph generation [25], and image caption [4].
Recent advances show that HOI detection can be for-mulated as set prediction problem [3, 17, 30, 44]. Existing
HOI transformers can be categorized into two types: single-branch transformer and parallel-branch transformer. Single-*Equal contribution.
†Work done when Zhichao and Leshan were interns at VIS, Baidu.
Figure 1. Architecture comparison of different HOI transformers. (a) Single-branch transformer [30, 44] adopts a single decoder to directly detect HOI triplets. (b) Parallel-branch transformer [3,17] utilizes separate decoders detect individual objects and interac-tions, and then compose triples by a matching process, which might introduce additional grouping errors. (c) Ours disentangles the task of triplet prediction into human-object pair detection and interaction classiﬁcation via an instance stream and an interaction stream, where both encoder and decoder are disentangled. branch transformer [30, 44] adopts multi-task strategy, in which one query is responsable for predicting a <human, verb, object> triplet within a single decoder. In contrast, parallel-branch transformer [3, 17] adopts parallel decoders for instance detection and interaction classiﬁcation sepa-rately. Speciﬁcally, one instance decoder follows DETR [1] and detects individual objects, and the other interaction de-coder estimates the interactions in the image. To compose
HOI triplets, it generates additional associative embeddings to match the interactions and instances. Since HOI detec-tion is a composition problem [13, 15], the latter decom-posing strategy has several advantages compared with uni-ﬁed multi-tasking strategy. First, two sub-task decoders
might attend to different regions via cross attention to facil-itate learning and also results in better interpretability. In addition, it has better generalizability, especially for rare categories due to long-tail distribution of triplet composi-tions. However, existing parallel-decoder transformers suf-fer from two crucial drawbacks under complex scenarios: i) the interaction predictions have to ﬁnd their correspond-ing human and object instances in instance decoder, which might introduce additional errors due to mis-grouping; ii) regardless of the shared encoder, the decoding sub-tasks are relatively independent and the joint conﬁgurations of in-stances and interactions are not considered.
To overcome above limitations, we present Disentan-gled Transformer(DisTR). We decouple the triplet predic-tion into human-object pair detection and interaction clas-siﬁcation via an instance stream and an interaction stream, where both encoder and decoder are disentangled. An illus-tration of architecture comparison between ours and prior
HOI transformers is shown in Fig.1. Our encoder mod-ule extracts different contextual information for two sub-tasks. During decoding process, the task decoder decodes its representation based on the corresponding task encoder.
Different from prior parallel-decoder transformers [3, 17] that the instance decoder predicts individual objects, our in-stance decoder predicts a set of interactive human-object pairs. To associate the predictions of task decoders, we adopt a base decoder to ﬁrst generate a uniﬁed representa-tion for HOI triplets, following QPIC [30], and then utilize it as input feature of each task decoder. The task decoder then reﬁnes its representation based on the uniﬁed represen-tation, resulting in a coarse-to-ﬁne process. We further de-sign an attentional fusion block to pass information between task decoders help them communicate with each other.
We evaluate our proposed method on two public bench-marks: V-COCO [11] and HICO-DET [2]. Our method out-performs current state-of-the-art by a sizeable margin. We further visualize the cross attentions in our task decoders, and observe that our task decoders indeed attend to differ-ent spatial regions, demonstrating the effectiveness of our proposed disentangled strategy.
The contributions of this paper are three folds:
• We propose a disentangled strategy for HOI detection, where the triplet prediction is decoupled into human-object pair detection and interaction classiﬁcation via an instance stream and an interaction stream.
• We develop a new transformer, where both encoder and decoder are disentangled. We also propose a coarse-to-ﬁne strategy to associate the predictions of instance decoder and interaction decoder, and an at-tentional fusion block for communication between task decoders.
• We achieve new state-of-the-art on both V-COCO and
HICO-DET benchmarks. 2.