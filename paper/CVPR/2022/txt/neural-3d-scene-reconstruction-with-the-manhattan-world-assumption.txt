Abstract
This paper addresses the challenge of reconstructing 3D indoor scenes from multi-view images. Many previ-ous works have shown impressive reconstruction results on textured objects, but they still have difficulty in handling low-textured planar regions, which are common in indoor scenes. An approach to solving this issue is to incorporate planer constraints into the depth map estimation in multi-view stereo-based methods, but the per-view plane estima-tion and depth optimization lack both efficiency and multi-view consistency.
In this work, we show that the planar constraints can be conveniently integrated into the recent implicit neural representation-based reconstruction meth-ods. Specifically, we use an MLP network to represent the signed distance function as the scene geometry. Based on the Manhattan-world assumption, planar constraints are employed to regularize the geometry in floor and wall re-gions predicted by a 2D semantic segmentation network.
To resolve the inaccurate segmentation, we encode the se-mantics of 3D points with another MLP and design a novel loss that jointly optimizes the scene geometry and seman-tics in 3D space. Experiments on ScanNet and 7-Scenes datasets show that the proposed method outperforms previ-ous methods by a large margin on 3D reconstruction qual-ity. The code and supplementary materials are available at https://zju3dv.github.io/manhattan_sdf. 1.

Introduction
Reconstructing 3D scenes from multi-view images is a cornerstone of many applications such as augmented re-ality, robotics, and autonomous driving. Given input im-ages, traditional methods [43, 44, 58] generally estimate the depth map for each image based on the multi-view stereo (MVS) algorithms and then fuse estimated depth maps into 3D models. Although these methods achieve successful re-The authors from Zhejiang University are affiliated with the State Key
Lab of CAD&CG and the ZJU-SenseTime Joint Lab of 3D Vision. ∗Equal contribution. †Corresponding author: Xiaowei Zhou.
Figure 1. Core idea. We represent the geometry and semantics of 3D scenes with implicit neural representations, which enables the joint optimization of geometry reconstruction and semantic seg-mentation in 3D space based on the Manhattan-world assumption. construction in most cases, they have difficulty in handling low-textured regions, e.g., floors and walls of indoor scenes, due to the unreliable stereo matching in these regions.
To improve the reconstruction of low-textured regions, a typical approach is leveraging the planar prior of man-made scenes, which has long been explored in literature
[8,10,11,41,48,51]. A renowned example is the Manhattan-world assumption [8], i.e., the surfaces of man-made scenes should be aligned with three dominant directions. These works either use plane estimation as a postprocessing step to inpaint the missing depth values in low-textured regions, or integrate planar constraints in stereo matching or depth optimization. However, all of them focus on optimizing per-view depth maps instead of the full scene models in 3D space. As a result, depth estimation and plane segmentation could still be inconsistent among views, yielding subopti-mal reconstruction quality as demonstrated by our experi-mental results in Section 5.3.
There is a recent trend to represent 3D scenes as im-plicit neural representations [32, 46, 55] and learn the rep-resentations from images with differentiable renderers. In particular, [49, 54, 55] use a signed distance field (SDF) to represent the scene and render it into images based on the sphere tracing or volume rendering. Thanks to the well-defined surfaces of SDFs, they recover high-quality 3D ge-ometries from images. However, these methods essentially rely on the multi-view photometric consistency to learn the
SDFs. So they still suffer from poor performance in low-textured planar regions, as shown in Figure 1, as many plausible solutions may satisfy the photometric constraint in low-textured planar regions.
In this work, we show that the Manhattan-world assump-tion [8] can be conveniently integrated into the learning of implicit neural representations of 3D indoor scenes and sig-nificantly improves the reconstruction quality. Unlike pre-vious MVS methods that perform per-view depth optimiza-tion, implicit neural representations allow the joint repre-sentation and optimization of scene geometry and semantics simultaneously in 3D space, yielding globally-consistent reconstruction and segmentation. Specifically, we use an
MLP network to predict signed distance, color and seman-tic logits for any point in 3D space. The semantic log-its indicate the probability of a point being floor, wall or background, initialized by a 2D semantic segmentation net-work [4]. Similar to [54], we learn the signed distance and color fields by comparing rendered images to input images based on volume rendering. For the surface points on floors and walls, we enforce their surface normals to respect the
Manhattan-world assumption. Considering the initial seg-mentation could be inaccurate, we design a loss that simul-taneously optimizes the semantic logits along with the SDF.
This loss effectively improves both the scene reconstruction and semantic segmentation, as illustrated in Figure 1.
We evaluate our method on the ScanNet [9] and 7-Scenes [45] datasets, which are widely-used datasets for 3D indoor scene reconstruction. The experiments show that the proposed approach outperforms the state-of-the-art meth-ods in terms of reconstruction quality by a large margin, especially in planar regions. Furthermore, the joint opti-mization of semantics and reconstruction improves the ini-tial semantic segmentation accuracy.
In summary, our contributions are as follows:
• A novel scene reconstruction approach that integrates the Manhattan-world constraint into the optimization of implicit neural representations.
• A novel loss function that optimizes semantic labels along with scene geometry.
• Significant gains of reconstruction quality compared to state-of-the-art methods on ScanNet and 7-Scenes. 2.