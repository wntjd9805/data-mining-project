Abstract
Template-based 3D object tracking still lacks a high-precision benchmark of real scenes due to the difficulty of annotating the accurate 3D poses of real moving video ob-In this paper, we present a jects without using markers. multi-view approach to estimate the accurate 3D poses of real moving objects, and then use binocular data to con-struct a new benchmark for monocular textureless 3D ob-ject tracking. The proposed method requires no markers, and the cameras only need to be synchronous, relatively fixed as cross-view and calibrated. Based on our object-centered model, we jointly optimize the object pose by mini-mizing shape re-projection constraints in all views, which greatly improves the accuracy compared with the single-view approach, and is even more accurate than the depth-based method. Our new benchmark dataset contains 20 tex-tureless objects, 22 scenes, 404 video sequences and 126K images captured in real scenes. The annotation error is guaranteed to be less than 2mm, according to both theo-retical analysis and validation experiments. We re-evaluate the state-of-the-art 3D object tracking methods with our dataset, reporting their performance ranking in real scenes.
Our BCOT benchmark and code can be found at https:
//ar3dv.github.io/BCOT-Benchmark/. 1.

Introduction
Template-based 3D object tracking aims to estimate the accurate 6DOF pose of moving objects with known 3D models. It is an essential task of computer vision [21], and is widely used in applications that desire high-precision 3D object pose, such as augmented reality, robotic grasping, etc. Despite the rapid development of single-frame 6DOF pose estimation methods [32, 41], for video analysis 3D tracking can be more accurate and more efficient, and thus is indispensable.
Because it is difficult to annotate the accurate 3D pose of a moving object in the real video, it is a great challenge
*Corresponding author: Xueying Qin (qxy@sdu.edu.cn) and Te Li (lite@zhejianglab.com)
Figure 1. The capturing camera set is composed of two approxi-mately orthogonal cameras. Based on the binocular data and pro-posed joint optimization framework, we can organize the bench-mark with precise annotated pose as rendered in red contour. to evaluate 3D tracking methods in real scenes. Previous works use only synthetic datasets or datasets with low pre-cision, the movement of object and camera is also limited.
Currently the main adopted datasets include RBOT [38],
OPT [45] and YCB-Video [47]. The RBOT dataset is semi-synthetic with rendered moving objects in the real back-ground image sequences, which may be different from real video in camera effects and object movement. The OPT dataset is real captured, but with numerous artificial markers around the object, and the objects are not allowed to move.
The YCB-Video is a real RGB-D dataset without markers; however, it contains only static objects, and the pose anno-tation contains a significant error that prevents it from being used in high-precision scenarios (as required by many AR applications). The above limitations are especially impor-tant for the learning-based tracking methods [43,44], due to the domain difference as well as the bias to the visual cues and object movements in the training dataset.
Table 1 lists and compares related datasets, including some datasets for single-frame pose estimation [1, 13, 14, 18, 47]. Note that even with the depth camera, it is still dif-ficult to annotate the accurate 3D pose, due to the depth er-Dataset
Linemod [13]
Linemod Occlusion [1]
T-LESS [14]
HomebrewedDB [18]
TOD [27]
StereOBJ-1M [26]
YCB-Video [47]
OPT [45]
RBOT [38]
BCOT (Ours) data type real real real real real real real real semi-synthetic real marker-less
×
×
×
×
×
×
✓
×
✓
✓ depth
✓
✓
✓
✓
✓
×
✓
✓
×
× outdoor
×
×
×
×
×
✓
×
×
×
✓ dynamic object
×
×
×
×
×
×
×
×
✓
✓ for tracking
×
×
×
×
×
×
✓
✓
✓
✓ objects 13 8 30 33 20 18 21 6 18 20 sequences
-------552 72 404 frames 15K 1.2K 49K 17K 64K 397K 134K 101K 72K 126K
Table 1. Dataset Comparisons. Our BCOT benchmark is the only real scene benchmark that provides dynamic objects. The benchmark doesn’t have invasive artificial markers and includes indoor and outdoor scenes. ror and mis-alignment with RGB image, especially around the object boundary [47]. On the other hand, the datasets with artificial markers [1, 13, 14, 18], will not only dam-age the naturalness of the scene, but also limit the object movement. Therefore, a high-precision markerless pose es-timation method is required in order to build a real-life 3D tracking benchmark, which according to our knowledge has not been addressed in previous works.
In this paper we propose a markerless multi-view ap-proach to address the above problem. Our method can es-timate the high-precision 3D pose of video objects from two orthogonal views of RGB images captured by the high-resolution, high-speed and synchronous cameras, as illus-trated in Fig. 1. To deal with textureless objects, we adopted a shape-based method, which does not rely on point corre-spondences between views. The object pose is solved by jointly optimizing the shape re-projection error of multiple views, in a novel object-centered pose estimation frame-work. Based on the proposed approach we contribute a new 3D tracking benchmark, namely BCOT (BinoCular Ob-ject Tracking) benchmark, which contains accurately an-notated real videos, with both camera and objects can move freely. The maximum annotation error of the BCOT bench-mark is 2mm, achieving the best annotation precision at present. The major contributions can be summarized as fol-lows.
• We propose a multi-view approach that can estimate the accurate 3D pose of real video objects. Our method is markerless and suitable for textureless and moving objects, and thus provides a way to annotate real-life tracking videos.
• We build a 3D tracking benchmark of real scenes with high-precision ground truth (GT) poses, with annota-tion accuracy guaranteed by both theoretical analysis and validation experiments.
• We comprehensively evaluate the existing SOTA 3D object tracking methods on the proposed BCOT bench-mark. 2.