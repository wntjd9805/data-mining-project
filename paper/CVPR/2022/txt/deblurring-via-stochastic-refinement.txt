Abstract
Image deblurring is an ill-posed problem with multiple plausible solutions for a given input image. However, most existing methods produce a deterministic estimate of the clean image and are trained to minimize pixel-level dis-tortion. These metrics are known to be poorly correlated with human perception, and often lead to unrealistic recon-structions. We present an alternative framework for blind deblurring based on conditional diffusion models. Unlike ex-isting techniques, we train a stochastic sampler that reﬁnes the output of a deterministic predictor and is capable of pro-ducing a diverse set of plausible reconstructions for a given input. This leads to a signiﬁcant improvement in perceptual quality over existing state-of-the-art methods across multiple standard benchmarks. Our predict-and-reﬁne approach also enables much more efﬁcient sampling compared to typical diffusion models. Combined with a carefully tuned network architecture and inference procedure, our method is com-petitive in terms of distortion metrics such as PSNR. These results show clear beneﬁts of our diffusion-based method for deblurring and challenge the widely used strategy of producing a single, deterministic reconstruction. 1.

Introduction
Image deblurring is a long-standing problem in computer vision. Various conditions such as moving objects, camera shakes, or an out-of-focus lens may contribute to blurring artifacts. Single image deblurring is a highly ill-posed in-verse problem where multiple plausible sharp images could lead to the very same blurry observation. Nonetheless, most existing methods produce a single deterministic estimate of the clean image.
Traditional methods formulate deblurring as a variational optimization problem and ﬁnd a solution that satisﬁes close-ness to certain image and/or blur kernel prior [9,18,28,38,58].
With the emergence of deep learning, convolutional neural networks (CNNs) have become the de-facto standard for
∗This work was done during an internship at Google Research.
Figure 1. Top: Perception-Distortion (P-D) trade-off [5] of current state-of-the-art deblurring methods (top). Our method sets a new
Pareto frontier in the P-D plot and allows us to traverse through the
P-D curve using a single model without retraining or ﬁnetuning.
Bottom: Samples from our method compared to other competitive methods. We include two extremes from our model – one optimized for perceptual quality (“Ours”) and one for distortion using Sample
Averaging (“Ours-SA”). These correspond to the two end points of the P-D curve. For the ease of interpretation, we used negative
Kernel Inception Distance [4] (C − KID for a constant C) deblurring models [14, 35, 40, 59, 63, 65, 66, 74]. Typically, these CNNs are trained with simulated sharp-blurry image pairs through supervised learning. Minimizing L1 or L2 pixel loss is perhaps the most widely adopted approach for training such models. These losses provide a straightfor-ward learning objective and optimize for the popular PSNR
(peak signal-to-noise-ratio) metric. Unfortunately, PSNR and other distortion metrics are well-known to only partially correspond to human perception [5, 17, 19] and can actually lead to algorithms with visibly lower quality in the recon-structed images. To alleviate this problem, recent works introduced additional loss terms [17, 21, 34, 44, 45] that seek to improve the quality of generated images under metrics that represent human perception more reliably. Training net-works to go from corrupted images to a known ground truth in a supervised way belongs in the family of end-to-end meth-ods [50]. These methods perform very well in-distribution, but can be quite fragile to distributional shifts or changes in the corruption process [25, 50].
A second body of work has focused on using deep gener-ative models to solve inverse problems [6]. For deblurring,
Generative Adversarial Networks (GANs) [22] have been successfully applied with competitive performance [3,34,35].
GAN-based restoration methods train the deblurring network with an adversarial loss to make the restored images more perceptually plausible. However the proposed methods so far have been deterministic, and adversarial losses often intro-duce artifacts not present in the original clean image, leading to large distortion (e.g. [42] for super-resolution).
In this work, we adopt a different perspective and view deblurring as a conditional generative modeling task, where we seek to generate diverse samples from the posterior dis-tribution. Speciﬁcally, we introduce a “predict-and-reﬁne” conditional diffusion model, where a deterministic data-adaptive predictor is jointly trained with a stochastic sampler that reﬁnes the output of the said predictor (see Fig. 2).
Our predict-and-reﬁne approach enables more efﬁcient sampling compared to the standard diffusion model. This for-mulation also naturally leads to a stochastic model capable of producing realistic images without sacriﬁcing pixel-level distortion. To the best of our knowledge, this is the ﬁrst blind deblurring technique that leverages a deep generative model and is capable of producing diverse samples.
Overall, our method produces a variety of plausible and photo-realistic results, while achieving state-of-the-art per-formance under many quantitative metrics in terms of both distortion and perceptual quality across multiple standard datasets. In addition, by aggregating a different number of generated deblurred samples, our framework allows us to conveniently traverse the Perception-Distortion curve [5, 19] as shown in Fig. 1, without any expensive retraining or
ﬁnetuning. These results show clear beneﬁts of stochas-tic diffusion-based methods for deblurring and challenge the currently dominant strategy of producing deterministic reconstructions. 2.