Abstract
Few-Shot Image Classiﬁcation (FSIC) aims to recognize novel image classes with limited data, which is signiﬁcant in practice. In this paper, we consider the FSIC problem in the case of adversarial examples. This is an extremely challenging issue because current deep learning methods are still vulnerable when handling adversarial examples, even with massive labeled training samples. For this prob-lem, existing works focus on training a network in the meta-learning fashion that depends on numerous sampled few-shot tasks. In comparison, we propose a simple but effec-tive baseline through directly learning generalizable repre-sentations without tedious task sampling, which is robust to unforeseen adversarial FSIC tasks. Speciﬁcally, we intro-duce an adversarial-aware mechanism to establish auxil-iary supervision via feature-level differences between legit-imate and adversarial examples. Furthermore, we design a novel adversarial-reweighted training manner to alleviate the imbalance among adversarial examples. The feature puriﬁer is also employed as post-processing for adversarial features. Moreover, our method can obtain generalizable representations to remain superior transferability, even fac-ing cross-domain adversarial examples. Extensive exper-iments show that our method can signiﬁcantly outperform state-of-the-art adversarially robust FSIC methods on two standard benchmarks. 1.

Introduction
Current deep learning methods have made signiﬁcant progress on several computer vision tasks [8, 11, 12, 14, 21].
However, these methods often depend on a high computa-tional budget and abundant data, which is costly to collect in the real-world setting. To address this issue, few-shot learn-ing aims at developing efﬁcient learning algorithms with
∗Corresponding Author
Figure 1. The adversarial example can be obtained by adding a nearly undetectable adversarial perturbation (magniﬁed for visibil-ity) to the legitimate example, which can cause a misclassiﬁcation with high conﬁdence. Compared with the standard few-shot clas-siﬁer, the adversarially robust classiﬁer learns a robust decision boundary to classify adversarial examples correctly. limited data [5, 22, 25–27]. In practice, there also exists a potential security threat to deep neural networks from ad-versarial examples [28]. Adversarial examples are some tailored examples with almost no differences from natural examples in human vision, but can strongly disturb the in-ference of neural networks [3,13]. Due to the scarce data for building the robust decision boundary, current deep learning methods even suffer more severe attacks from adversarial examples in the few-shot setting [7].
The primary purpose of adversarially robust Few-Shot
Image Classiﬁcation (FSIC) is to build models that perform well in standard few-shot classiﬁcation and are simultane-ously robust against adversarial examples, as shown in Fig-ure 1. Because of the limited data and the existence of ad-versarial examples, adversarially robust FSIC still remains a challenging problem. Nevertheless, existing researches
on this problem are still rare and mainly based on meta-learning [7,31,37], which aims to learn a model from abun-dant few-shot tasks and then generalize it to unforeseen few-shot tasks. Each task consists of limited training examples and query examples from the same distribution. The dataset is divided into the meta-training set and the meta-test set of disjoint categories. The meta-learning model is trained on the meta-training set to quickly adapt to new tasks, while the meta-test set is for evaluating few-shot accuracy. However, there still exists a considerable gap of label spaces between the meta-training and the meta-test set. Learning from ex-cessive few-shot tasks may induce overﬁtting on the source label space and thus aggravate the performance on adver-sarial examples of different label spaces, especially in the cross-domain scenario.
In this paper, beyond the meta-learning manner, we pro-pose a novel adversarially robust FSIC framework, which learns a robust embedding model and generalizes it to un-foreseen adversarial few-shot classiﬁcation tasks. Based on the observation that features extracted from adversarial ex-amples are non-robust to trained models [13], we design an auxiliary adversarial-aware module to learn the nuance between legitimate examples and corresponding adversar-ial examples. On account of the imbalance among adver-sarial examples, we propose a novel adversarial-reweighted method according to the loss variation. We also append a simple but effective postprocessing module to purify adver-sarial features.
To fully evaluate the effectiveness of our methods, we conduct extensive experiments on two standard few-shot classiﬁcation benchmarks: miniImageNet [30] and CIFAR-FS [1]. The evaluation contains both accuracy on legitimate examples and their corresponding adversarial examples in the few-shot setting. In addition, we validate the robustness of our methods, which is conducted under various attack strengths. We also conduct cross-domain transfer experi-ments to demonstrate the generalizability of our proposed methods against adversarial examples.
The main contributions of our work can be summarized as follows:
• We propose a new baseline on adversarially robust
FSIC by directly learning a robust embedding model, eliminating complicated meta-training steps.
• We design an adversarial-aware method for auxil-iary supervision between legitimate and adversarial examples. To address the imbalance among adver-sarial examples during training, we propose a novel adversarial-reweighted mechanism according to the loss variation. For postprocessing, we introduce a fea-ture puriﬁcation module to mitigate adversarial effects.
• Extensive experiments demonstrate that our algorithm achieves a new state-of-the-art performance on adver-sarially robust FSIC. Our method can also provide additional beneﬁts on robustness to different attack strengths and generalizability in the cross-domain sce-nario simultaneously. 2.