Abstract
Video captioning aims to generate natural language de-scriptions according to the content, where representation learning plays a crucial role. Existing methods are mainly developed within the supervised learning framework via word-by-word comparison of the generated caption against the ground-truth text without fully exploiting linguistic se-mantics. In this work, we propose a hierarchical modular network to bridge video representations and linguistic se-mantics from three levels before generating captions.
In particular, the hierarchy is composed of: (I) Entity level, which highlights objects that are most likely to be men-tioned in captions. (II) Predicate level, which learns the actions conditioned on highlighted objects and is super-vised by the predicate in captions. (III) Sentence level, which learns the global semantic representation and is su-pervised by the whole caption. Each level is implemented by one module. Extensive experimental results show that the proposed method performs favorably against the state-of-the-art models on the two widely-used benchmarks: MSVD 104.0% and MSR-VTT 51.5% in CIDEr score. Code will be made available at https://github.com/MarcusNerva/HMN. 1.

Introduction
Video captioning aims to automatically generate natu-ral language descriptions from videos, which plays an im-portant role in numerous applications, such as assisting visually-impaired people, human-computer interaction, and video retrieval [7, 27, 36, 42, 43, 45, 49]. Despite recent ad-vances in this field, it remains a challenging task as a video usually contains rich and diverse content, but only some in-formation is relevant to caption (e.g., two or three out of many objects are captured in a caption).
*Corresponding author.
Figure 1. To effectively bridge video content and linguistic cap-tion, we propose to supervise video representation learning in a three-level hierarchical structure, i.e., the entity level, the predi-cate level, and the sentence level.
Existing methods aim to learn effective video represen-tations to generate captions via recurrent decoders, which can be broadly categorized into two lines of work. The first one focuses on designing complex video encoders to learn better video representations [1,5,6,26,44,50,51]. For exam-ple, STG-KD [26] and ORG-TRL [51] build object relation graphs to reason the spatial and temporal relations between video objects. While GRU-EVE [1] applies Short Fourier
Transform [25] to embed temporal dynamics in visual fea-tures, POS+CG [44] develops a cross gating block to fuse appearance and motion features and make a comprehen-sive representation. However, the optimization objectives of these methods are computed word-by-word as captions generating, disregarding the relevance between video rep-resentations and their linguistic counterpart. The other one focuses on narrowing the semantic gap between video rep-resentation and linguistic captions ahead of generating cap-tions [28, 35, 52]. For example, Pan et al. [28] learn to align
the global representation of a video to the embedding of a whole caption. In contrast, Shen et al. [35] and Zheng et al. [52] associate nouns and verbs with visual features to ex-plore the video-language correspondence on a fine-grained level. These approaches are able to generate more accurate captions as more representative video embedding is learned.
However, they either focus on global sentence correspon-dence or local word correspondence, which disregard fine-grained details or global relevance.
In this work, we propose a hierarchical modular network to address the issues mentioned above. Our model aims to learn three kinds of video representations supervised by lan-guage semantics at different hierarchical levels as shown in
Figure 1: (I) Entity level, which highlights objects that are most likely to be mentioned in captions and is supervised by entities1 in the caption. (II) Predicate level, which learns the actions conditioned on highlighted objects and is super-vised by the predicate in the caption. (III) Sentence level, which learns the global video representation supervised by the whole caption. Each level is implemented by one mod-ule. The motivation of our design is that objects usually serve as the cornerstone of a video caption, which can be the subject or object of an action as well as modifiers of subject and/or object. Instead of learning the visual representation of verbs along, we propose to learn video representation of predicates (verb+noun). This helps reduce the correspon-dence errors from a multi-meaning verb to a specific video action embedding, such as the play in playing soccer and playing the piano. The global video content embedding su-pervised by the embedding of a whole caption enables the generated caption to have a reasonable meaning.
It is worth noting that we propose a novel entity module.
This module takes all the pre-extracted objects of a video as input and outputs a small set of principal objects that are most likely mentioned in a caption. Motivated by the success of DETR [3] for object detection, our entity mod-ule is designed with a transformer encoder-decoder archi-tecture. In contrast to DETR, our queries are enhanced by video content and supervised by entities in captions, which enables the model to select principal objects according to video scenarios.
The contributions of this paper are summarized below:
• We propose a hierarchical modular framework to learn multi-level visual representations at different granular-ity by associating them with their linguistic counter-parts: the entity, predicate, and sentence.
• We propose a transformer-based entity module to learn to select principal objects that are most likely to be mentioned in captions.
• Our method performs favorably against the state-1Note that entities are different from nouns. Nouns contain abstract nouns, such as happiness and hunger., while entities consist of object names, such as onion and car. of-the-art models on two widely-used benchmarks:
MSVD [4] and MSR-VTT [47]. 2.