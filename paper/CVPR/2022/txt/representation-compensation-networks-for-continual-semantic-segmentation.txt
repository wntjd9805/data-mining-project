Abstract
In this work, we study the continual semantic segmenta-tion problem, where the deep neural networks are required to incorporate new classes continually without catastrophic for-getting. We propose to use a structural re-parameterization mechanism, named representation compensation (RC) mod-ule, to decouple the representation learning of both old and new knowledge. The RC module consists of two dynami-cally evolved branches with one frozen and one trainable.
Besides, we design a pooled cube knowledge distillation strategy on both spatial and channel dimensions to further enhance the plasticity and stability of the model. We conduct experiments on two challenging continual semantic segmen-tation scenarios, continual class segmentation and contin-ual domain segmentation. Without any extra computational overhead and parameters during inference, our method out-performs state-of-the-art performance. The code is available at https://github.com/zhangchbin/RCIL. 1.

Introduction
Data-driven deep neural networks [64, 72, 96, 108] have made many milestones in semantic segmentation. However, these fully-supervised models [16, 23, 93] can only handle a fixed number of classes. In real-world applications, it is preferable that a model can be dynamically extended to iden-tify new classes. A straightforward solution is to rebuild the training set and retrain the model with all data available, known as Joint Training. However, considering the cost of re-training models, sustainable development of algorithms and privacy issues, it is particularly crucial to update the model with only current data to achieve the goal of recognizing both new and old classes. Nevertheless, naively fine-tuning a trained model with new data can result in catastrophic forgetting [48]. Therefore, in this paper, we seek continual learning, which can potentially enable a model to recognize new categories without catastrophic forgetting.
In the scenario of continual semantic segmentation [8, 27,
*The first two authors contribute equally.
†Corresponding author (xialei@nankai.edu.cn) ft−1 ft
RC Module
Figure 1. Illustration of our proposed training framework for con-tinual semantic segmentation to avoid catastrophic forgetting. We design two mechanisms in our method, representation compensa-tion (RC) module and pooled cube distillation (PCD). 62, 63], given the previously trained model and the training data of the new classes, the model is supposed to distinguish all seen classes, including previous classes (old classes) and new classes. However, to save the labeling cost, the new training data often only has labels for the new classes, treat-ing old classes as background. Learning with the new data directly without any additional designs is very challenging, which can easily lead to catastrophic forgetting [48].
As indicated in [28, 48, 51], fine-tuning the model on new data may lead to catastrophic forgetting, i.e., the model quickly fits the data distribution of the new classes, while losing the discrimination for the old classes. Some meth-ods [43,48,56,66,67,80,95] play regularization on model pa-rameters to improve its stability. However, all parameters are updated on the training data of the new classes. This is how-ever challenging, as new and old knowledge are entangled together in model parameters, making it extremely difficult to keep the fragile balance of learning new knowledge and keeping old ones. Some other methods [45, 57, 75, 76, 82, 91] increase the capacity of the model to have a better trade-off of stability and plasticity, but with the cost of growing memory of the network.
In this work, we propose an easy-to-use representation compensation module, aiming at remembering the old knowl-edge while allowing extra capacity for new knowledge. In-spired by structural re-parameterization [24, 25], we replace the convolution layers in the network with two parallel branches during training, which is named as representation compensation module. As shown in Fig. 1, during train-ing, the output of two parallel convolutions is fused before the non-linear activation layer. At the beginning of each continual learning step, we equivalently merge the param-eters of the two parallel convolutions into one convolution, which will be frozen to retain the old knowledge. Another branch is trainable and it inherits the parameters from the corresponding branch in the previous step. The represen-tation compensation strategy is supposed to remember the old knowledge using the frozen branch while allowing ex-tra capacity for new knowledge using the trainable branch.
Importantly, this module brings no extra parameters and computation cost during inference.
To further alleviate catastrophic forgetting [48], we in-troduce a knowledge distillation mechanism [70] between intermediate layers (shown in Fig. 1), named Pooled Cube
Distillation. It can suppress the negative impact of errors and noises in local feature maps. The main contributions of this paper are:
• We propose a representation compensation module with two branches during training, one for retaining the old knowledge and one for adapting to new data. It always keeps the same computation and memory cost during inference as the number of tasks grows.
• We conduct experiments on continual class segmenta-tion and continual domain segmentation, respectively.
Experimental results demonstrate that our method out-performs the state-of-the-art performance on three dif-ferent datasets. 2.