Abstract
Representational learning forms the backbone of most deep learning applications, and the value of a learned repre-sentation is intimately tied to its information content regard-ing different factors of variation. Finding good representa-tions depends on the nature of supervision and the learning algorithm. We propose a novel algorithm that utilizes a weak form of supervision where the data is partitioned into sets according to certain inactive (common) factors of variation which are invariant across elements of each set. Our key insight is that by seeking correspondence between elements of different sets, we learn strong representations that exclude the inactive factors of variation and isolate the active factors that vary within all sets. As a consequence of focusing on the active factors, our method can leverage a mix of set-supervised and wholly unsupervised data, which can even belong to a different domain. We tackle the challenging problem of synthetic-to-real object pose transfer, without pose annotations on anything, by isolating pose informa-tion which generalizes to the category level and across the synthetic/real domain gap. The method can also boost perfor-mance in supervised settings, by strengthening intermediate representations, as well as operate in practically attainable scenarios with set-supervised natural images, where quan-tity is limited and nuisance factors of variation are more plentiful. Accompanying code may be found on github. 1.

Introduction
A good representation is just as much about what it ex-cludes as what it includes, in terms of factors of variation across a dataset [52]. Control over the information content of learned representations depends on the nature of available su-pervision and the algorithm used to leverage it. For example, full supervision of desired factors of variation provides max-*Work done as part of the Google AI Residency program.
†seas.upenn.edu
‡google.com imum flexibility for fully disentangled representations, as an interpretable mapping is straightforward to obtain between elements and the factors [3, 19]. However, such supervision is often unrealistic since many common factors of variation, such as 3D pose or lighting in image data, are difficult to annotate at scale in real-world settings. On the other hand, unsupervised learning makes the fewest limiting assump-tions about the data but does not allow control over the discovered factors [30]. Neither extreme, fully supervised or unsupervised, is practical for many real-world tasks.
As an alternative, we consider weak supervision in the form of set membership [9, 24], used in prior works though often only informally defined. To be specific, we assume access to subsets of training data within which some inac-tive factors of variation have fixed values and the remaining active factors freely vary for different elements of the subset.
For example, consider the images of a synthetic car in set A of Fig. 1. All images in this set share common values for factors of variation relating to the specific car instance, and the only actively varying factor is the car’s orientation in the image. Set membership is the only information; there are no annotations on any factors of variation. In many complex tasks that are beyond the scope of categorical classification, set supervision serves as a more flexible source of informa-tion for operating on factors of variation across a dataset.
Many techniques designed to utilize set supervision ex-ploit correspondence across data that match in desired factors of variation [7, 54]. For instance, if images of cars with the same 3D pose are grouped together (i.e. the inactive factor in each set is pose), then a straightforward training objective that maps images within groups to similar embeddings and images from different groups to dissimilar embeddings will have successfully isolated pose. However, in this scenario and more generally, this variant of set supervision is often prohibitive to obtain: in our example it requires identifying images of different cars from exactly the same viewpoint.
A more readily available form of set supervision is where the desired factors are active in each set. Continuing the ex-ample, such supervision can be obtained by simply imaging each car from multiple viewpoints (as in set A in Fig. 1).
Figure 1. Approximate bijective correspondence (ABC). Leveraging weak set supervision—merely groupings of data within which certain factors of variation are invariant—ABC isolates factors of variation which actively vary across sets. The images in set A (left) actively vary by only the orientation of the rendered car. We claim that if one-to-one correspondence can be found between A and B, for all possible pairs
A and B, it must leverage orientation. We find this to be true even when only one of the sets in each pair is set-supervised. Importantly, this allows the incorporation of out-of-domain data with no supervision of any sort, such as the images of real cars in B. By training a neural network ϕ with a loss that measures correspondence in representation space by the degree to which the nearest neighbor in B of a point in A (black arrow) is paired up with the same point in A (green arrow) or a different point in A (red arrow, middle), the learned representations (right) isolate the active factor of variation, orientation.
This does not require correspondence in viewpoints across object instances, nor any pose values attached to the images.
However, isolating the active factors (pose in this example) from set supervision is much harder, as there is no explicit correspondence in the desired factor (i.e., no matching im-ages with identical pose information).
In this work, our goal is to operate in this more practical set-supervised setting, but the lack of correspondence in the desired active factors makes a solution difficult. To this end, we propose a novel approach, approximate bijective corre-spondence (ABC), which isolates the active factors through the process of finding correspondence between elements of different sets. To consistently yield correspondence across sets, learned representations must ignore invariant informa-tion within a set (inactive factors) and focus on active factors common to all sets. A powerful consequence is the capability to incorporate sets with extraneous active factors, including wholly unsupervised and even out-of-domain data (e.g., set
B in Fig. 1), as long as one of the sets is more constrained (set A in Fig. 1). In the example of Fig. 1, ABC-learned embeddings isolate orientation, the common active factor across every pair of sets during training.
In our approach, each element of a set is paired with a cooresponding proxy element of another set constructed with a differentiable form of nearest neighbors [10, 14, 34, 40, 46].
The two serve as a positive pair for use in a standard con-trastive (InfoNCE) loss [53]. We find that the same desir-able properties of learned representations that optimize In-foNCE on explicitly provided positive pairs—namely, align-ment, where differences within positive pairs are ignored, and uniformity, where maximal remaining information is retained [54, 57]—can be utilized to guide a network to find useful correspondences on its own. The key strengths of
ABC are the following:
• Isolates factors inaccessible to related methods. ABC isolates the active factors of variation in set-supervised data, and suppresses the inactive factors.
• Mixed-domain learning. The ability to incorporate unsu-pervised data with extraneous factors of variation allows
ABC to learn representations which bridge domain gaps with entirely unsupervised data from one domain.
• Faster training. ABC is much faster than alternative routes to isolating active factors from set-supervised data, all of which require learning the inactive factors as well.
We analyze the method and its strengths through experi-ments on a series of image datasets including Shapes3D [4] and MNIST [25]. In its fullest form, ABC addresses the challenging task of pose estimation in real images by mean-ingfully utilizing entirely unsupervised real images with set-supervised synthetic images, bridging the domain gap from synthetic to real. Our experiments show that ABC presents a viable path to learning 3D pose embeddings of real images of unseen objects without having access to any pose annotations during training. We conclude by training
ABC with set-supervised real images, including one scenario matching the hypothetical example of images of cars taken from multiple viewpoints. ABC successfully isolates active factors of variation out of the many nuisance factors of vari-ation common to natural images, all with access to only a limited quantity of training examples.
2.