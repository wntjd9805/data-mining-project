Abstract
Object proposal generation is an important and funda-In this paper, we pro-mental task in computer vision. pose ProposalCLIP, a method towards unsupervised open-category object proposal generation. Unlike previous works which require a large number of bounding box annota-tions and/or can only generate proposals for limited ob-ject categories, our ProposalCLIP is able to predict pro-posals for a large variety of object categories without an-notations, by exploiting CLIP (contrastive language-image pre-training) cues. Firstly, we analyze CLIP for unsuper-vised open-category proposal generation and design an ob-jectness score based on our empirical analysis on proposal selection. Secondly, a graph-based merging module is pro-posed to solve the limitations of CLIP cues and merge frag-mented proposals. Finally, we present a proposal regres-sion module that extracts pseudo labels based on CLIP cues and trains a lightweight network to further refine proposals.
Extensive experiments on PASCAL VOC, COCO and Vi-sual Genome datasets show that our ProposalCLIP can bet-ter generate proposals than previous state-of-the-art meth-ods. Our ProposalCLIP also shows benefits for downstream tasks, such as unsupervised object detection. 1.

Introduction
Object proposal generation aims to predict a number of category-agnostic bounding box proposals for all objects in an image.
It serves as a fundamental and crucial step towards many higher-level tasks, such as object detection
[11, 23], object segmentation [12, 34–36] and image cap-tioning [19, 24]. How to effectively generate as few as pos-sible proposals to cover all objects is the key challenge in object proposal generation.
Traditional proposal generation methods [1, 6, 41, 47, 50] often utilizes low-level cues (e.g., color, texture, gradient and/or edge) to select proposals from sliding window boxes.
In recent years, deep-learning-based methods [18,22,31,50]
Figure 1. Examples of CLIP [28] image-text matching results.
This pre-trained model well recognizes objects of open categories.
However, it can only highlight a single object in an image. For example, the “chair” and “laptop” are not recognized in the bottom image. take high-level semantics from CNNs or Transformers as cues to select or regress proposals. Although these deep-learning-based methods significantly improve the proposal generation performance, they require a huge number of bounding box annotations for training, which is very labor-intensive, especially for large-scale datasets. Meanwhile, due to the significant annotation effort required, only ob-jects of a limited number of categories can be labeled. Thus, these supervised methods can only generate proposals for limited object categories. However, real-world applications such as object retrieval [2, 13], image captioning [19, 24] and referring grounding [27, 45] usually require object pro-posals of diverse categories.
Some recent efforts [16, 37, 42, 46] aim to address these challenges. ORE [16] and OVR-CNN [46] leverage in-cremental learning and image caption supervisions to rec-ognize additional object categories. However, these ap-proaches also need a mass of bounding box and image caption annotations. Without the human-intensive annota-tions, these methods fail to perform well. LOST [37] and rOSD [42] propose unsupervised deep-learning-based pro-posal generation, which leverages off-the-shelf knowledge from other tasks to generate proposals. Specifically, they predict proposals based on class activation maps (CAMs) and attention maps from pre-trained classification networks.
These unsupervised methods avoid the box annotation, but they are only able to recognize limited object categories.
In addition, although CAMs/attention maps activate some salient regions, there are many objects in non-activated re-gions. As a result, proposals generated by these methods can only cover parts of objects, as shown in Tables 2 and 3.
In this paper, we propose a novel method, called Pro-posalCLIP, towards unsupervised and open-category object proposal generation. Our method can generate a variety of proposals of different object categories without requir-ing expensive bounding box annotations, by leveraging the off-the-shelf image-text matching model, CLIP (contrastive language-image pre-training) [28]. We exploit CLIP [28] features because it is trained on millions of image-language pairs from web, and thus have potential to generalize to var-ious object categories, as shown in Fig. 1. Nevertheless,
CLIP [28] cannot be directly used in object proposal gen-eration, because it is only trained to recognize single-object images and cannot well handle multi-object images. For ex-ample, in the second image in Fig. 1, it only ignores both the “chair” and “laptop” objects. Thus, it is non-trivial to apply CLIP for our task. In our ProposalCLIP, we first an-alyze CLIP features and build an objectness score based on our analysis for proposal generation. In addition, we de-sign a graph-based proposal merging model, which exploits
CLIP features to effectively combine different proposals.
We also extract pseudo labels based on CLIP cues to train a box regression model to further improve our proposals. We conduct experiments on three common datasets, PASCAL
VOC (20 classes), COCO (80 classes) and Visual Genome (1,600 classes). The experimental results demonstrate the effectiveness of our proposed method.
Our major contributions can be summarized as follows: (1) We propose a novel method that can effectively generate proposals for open categories in real world, without requir-ing annotations. (2) To the best of our knowledge, this is the first study to analyze and exploit CLIP cues as prior knowl-edge for object proposal generation. We analyze the CLIP for proposal generation and design a CLIP proposal selec-tion model, a graph-based proposal merging model as well as a proposal regression model to further refine and tailor
CLIP cues. (3) Extensive experiments show that our pro-posed framework obtains significant improvements on three popular datasets and has benefits for downstream tasks. 2.