Abstract
Despite plenty of efforts focusing on improving the do-main adaptation ability (DA) under unsupervised or few-shot semi-supervised settings, recently the solution of ac-tive learning started to attract more attention due to its suitability in transferring model in a more practical way with limited annotation resource on target data. Neverthe-less, most active learning methods are not inherently de-signed to handle domain gap between data distribution, on the other hand, some active domain adaptation meth-ods (ADA) usually requires complicated query functions, which is vulnerable to overfitting.
In this work, we pro-pose a concise but effective ADA method called Select-by-Distinctive-Margin (SDM), which consists of a maximum margin loss and a margin sampling algorithm for data se-lection. We provide theoretical analysis to show that SDM works like a Support Vector Machine, storing hard exam-ples around decision boundaries and exploiting them to find informative and transferable data.
In addition, we propose two variants of our method, one is designed to adaptively adjust the gradient from margin loss, the other boosts the selectivity of margin sampling by taking the gra-dient direction into account. We benchmark SDM with standard active learning setting, demonstrating our algo-rithm achieves competitive results with good data scala-bility. Code is available at https://github.com/
TencentYoutuResearch/ActiveLearning-SDM 1.

Introduction
The domain adaptation problem has been widely studied in transfer learning society, where adaptation algorithms are
*Both author contributed equally to this work. Work is completed dur-ing Ming Xie’s internship at Tencent Youtu Lab.
†Corresponding author
Figure 1. A simple conceptual illustration of our Select-by-Distinctive-Margin pipeline. Before each sampling step, a model is trained with a maximum margin objective, and unlabeled data lying in the margin with similar distance to different categorical centers are sampled to augment training data. designed to generalize a model trained on source domain to a target domain with different data distribution [4]. In most of studies, the semantic labels from target domain are assumed to be unavailable [4, 8, 12, 13, 24] (UDA) or only few-shot of target samples are labeled [21, 23, 29](SSDA).
However, in a more practical sense, although it is difficult to annotate all data in target domain, a moderate amount of labeled data should be acceptable given certain budget on annotations cost.
With this consideration, domain adaptation turns into an active learning problem (AL), which focusing on addition-ally labeling limited data to bring maximum improvement of machine learning algorithms [2,6,17,30,32,37,38]. How-ever, currently most active learning algorithms are derived from a pure semi-supervised scenario, where the unlabeled data are assumed to conform to the same distribution as la-beled data. These methods usually focus on designing a distinctive query function to depict how informative or rep-resentative an unlabeled data sample is, which highly relies on the uncertainty [6, 32] or structural distribution of data features [2, 30]. On the contrary, in a domain adaptation problem, the task model is initially trained with only source data and the query function is usually correlated to the pre-diction of task models, in this case, most of target data will be discriminated as uncertain regardless its location in fea-ture space. Consequently, the sampling methods are prone to sample some target samples that are easily classified and make less effect on the biased decision boundaries.
Recently, there exist some researches aimed at appro-priate data selection under the scenario of domain adapta-tion. However, these methods either design complicated and hand-crafted query function with deliberately designed architecture [11, 27], or select data in a tedious manner of high complexity [26]. These complicated design makes the query function and selection strategies easy to over-fit to a certain transferring scenario and hard to be ex-tended to more general cases.
In addition, most of these methods simply exploit all source data equally during train-ing [11, 26, 34], which is vulnerable to bias toward source domain and results unreliable query. Besides, few of studies above discuss the intrinsic relationship between their train-ing objective and query function, ignoring the potential cor-relation between data of two domains during selection.
With the consideration above, in this paper, we pro-pose to tackle domain adaptation problem with a simple but effective active learning strategy called Selective-by-Distinctive-Margin (SDM) by evaluating the distance from a data sample to different categorical clusters (as shown in
Figure 1). Different from most of previous efforts focus-ing on selecting data through the uncertainty or diversity of pure unlabeled target data [11, 26, 27, 30, 32], SDM makes attempt to select unlabeled data via their relation to some
“hard examples” from the source domain. However, in-stead of explicitly model such data relation, we implicitly depict the similarity between unlabeled samples and poten-tial hard source samples via a simple maximum margin loss function. Intuitively, the margin loss will guide the network to maximize the distance between close examples from dif-ferent categorical clusters in source domain, meanwhile ig-nore the affect from well classified source samples. This reversely helps detecting informative target samples still ly-ing near the trained decision boundaries through a simple margin sampling query function. By collecting these data into training set, the manifold of decision boundaries can be further refined and generalized to target distribution. The-oretically, by analyzing with a simplified linear model, we confirm that model trained with margin loss can act like a
Support Vector Machine [7], which collects only “hard ex-amples” in source domain, and take these examples to de-tect unlabeled target data via the similarity in feature space.
In addition, derived from the simple SDM baseline, we further extend the strategy into two variants. For the training phase, for the sake of dynamically adjust gradient of margin loss to adapt to samples of different difficulties, we propose to extend the original margin loss to a dynamic form with adaptive modulation factor and max-logit reglularizer. On the other hand, during sample selection, to boost the selec-tivity, we take the first-order gradient of margin sampling function as additional guidance in query function, leading to select target samples which decreases the sampling function in the fastest direction with its estimated gradient. Further, both variants can further be combined together to construct more effective active learning pipeline.
Our SDM algorithm is evaluated on different domain adaption benchmarks like Office-Home [28] and Office-31 [35] under a classical active learning setting, besides, we also extend our method to a general active learning task on CIFAR-10 [18], demonstrating our approach can achieve state-of-the-art results with less query complexity and good data-scalablility.
In a nutshell, our contributions can be summarized into three folds:
• We propose Select-by-Distinctive-Margin (SDM), a concise but effective active learning method for active domain adaptation, which consists of a maximum mar-gin loss and a margin sampling function as a complete active learning cycle. Theoretical analysis is provided to show this SDM framework work like a SVM to take hard examples to mine informative targets.
• Derived from the SDM baseline, two variants are de-veloped. One is designed for training phase to dynami-cally adjust margin loss gradient, the other is designed to enhance the selectivity with the help of first-order gradient of margin sampling function.
• Experiments conducted on several domain adaptation benchmarks show that our approach can achieve state-of-the-art results with limited annotation budget. 2.