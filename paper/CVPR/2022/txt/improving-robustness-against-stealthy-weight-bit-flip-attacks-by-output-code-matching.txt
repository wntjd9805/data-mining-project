Abstract
Deep neural networks (DNNs) have been shown to be vulnerable against adversarial weight bit-ﬂip attacks through hardware-induced fault-injection methods on the memory systems where network parameters are stored. Re-cent attacks pose the further concerning threat of ﬁnding minimal targeted and stealthy weight bit-ﬂips that preserve expected behavior for untargeted test samples. This renders the attack undetectable from a DNN operation perspective.
We propose a DNN defense mechanism to improve robust-ness in such realistic stealthy weight bit-ﬂip attack scenar-ios. Our output code matching networks use an output cod-ing scheme where the usual one-hot encoding of classes is replaced by partially overlapping bit strings. We show that this encoding signiﬁcantly reduces attack stealthiness. Im-portantly, our approach is compatible with existing defenses and DNN architectures. It can be efﬁciently implemented on pre-trained models by simply re-deﬁning the output classiﬁ-cation layer and ﬁnetuning. Experimental benchmark eval-uations show that output code matching is superior to exist-ing regularized weight quantization based defenses, and an effective defense against stealthy weight bit-ﬂip attacks. 1.

Introduction
While deep neural networks (DNNs) are becoming ubiq-uitous in artiﬁcial intelligence applications, they also have been proven to be highly vulnerable to a variety of ma-licious attack paradigms. One of the most widely stud-ied aspect is the adversarial input attack, where hardly-perceptible and intentionally crafted input perturbations can lead to conﬁdent incorrect decisions for DNNs [13, 33]. A recently emerged category of attacks exposes the parame-ter space vulnerability of DNNs by negatively inﬂuencing the inference process at the deployment stage. It has been shown that information stored in the form of bits on dy-namic random-access memory (DRAM) chips can be sim-ply manipulated by ﬂipping any bit precisely as desired via fault-injection techniques (e.g., row-hammer attacks [19]).
As the weight parameters of widely deployed DNNs are generally stored on the DRAM due to their high mem-ory demand, such hardware-induced attacks open malicious pathways to jeopardize DNN predictions by changing vul-nerable parameters [7, 17, 22, 41].
There has been growing interest in developing adversar-ial weight bit-ﬂip attack algorithms to identify vulnerable quantized DNN bits in simulations (cf. Section 2.1), in or-der to provide practical guidance for fault-injection attacks towards reaching malicious goals against expected DNN behavior. As physical bit-ﬂipping may become time con-suming and lead to abnormal background processes [14,36], constraining the number of malicious bit-ﬂips for efﬁcient attacks is essential for the adversary. Going forward, re-cently proposed algorithms also consider ﬁnding minimal bits for targeted and stealthy weight bit-ﬂip attacks, i.e., having a targeted negative impact on an attacked source (a single input sample [3] or samples belonging to a class [26]) while having almost no change in performance for the re-maining test samples. From a DNN operation perspective, such a scenario is far more concerning as it becomes impos-sible to suspect any unusual activity if the network shows expected behavior for untargeted test samples.
To date, relatively little guidance is available for how to improve network robustness against adversarial weight bit-ﬂip attacks (cf. Section 2.2). Our goal in this study is to improve robustness from a DNN architecture per-spective, which would also be naturally compatible to po-tential hardware-driven solutions against fault-injection at-tacks. We particularly focus on more realistic, targeted at-tack scenarios, where the existence of the attack also can not be easily detected via the usual DNN behavior, i.e., tar-geted bit-ﬂip attack algorithms with stealthiness [3,26]. We approach this problem using an alternative output coding scheme for multi-class classiﬁcation with DNNs, in com-parison to the usual one-hot encoded output representations.
The proposed output code matching networks predict class-speciﬁc partially overlapping bit strings, which constitutes an effective defense against stealthy weight bit-ﬂip attacks.
Contributions of this study are summarized as follows:
• We present for the ﬁrst time a DNN defense mech-anism, output code matching, to improve robustness against stealthy weight bit-ﬂip attacks in various tar-geted settings. Our approach is compatible with any
DNN backbone by re-deﬁning the output classiﬁcation layer and ﬁnetuning pre-trained model weights.
• The proposed output code matching networks outper-form state-of-the-art defenses and scale to large DNN architectures. Our ImageNet experiments show that targeted stealthy attacks on a ResNet-50 require up to 20× and 5× more bits to be attacked on our models with respect to vanilla networks, and networks trained with the state-of-the-art defense [16], respectively.
• We empirically demonstrate that the proposed frame-work is also applicable to networks trained with exist-ing defenses, such as DNNs trained with regularized weight quantization (i.e., piecewise clustering [16]). 2.