Abstract
In this paper, we propose HeadNeRF, a novel NeRF-based parametric head model that integrates the neural radiance field to the parametric representation of the hu-man head. It can render high fidelity head images in real-time on modern GPUs, and supports directly controlling the generated images’ rendering pose and various semantic at-tributes. Different from existing related parametric models, we use the neural radiance fields as a novel 3D proxy in-stead of the traditional 3D textured mesh, which makes that
HeadNeRF is able to generate high fidelity images. How-ever, the computationally expensive rendering process of the original NeRF hinders the construction of the parametric
NeRF model. To address this issue, we adopt the strategy of integrating 2D neural rendering to the rendering pro-cess of NeRF and design novel loss terms. As a result, the rendering speed of HeadNeRF can be significantly acceler-ated, and the rendering time of one frame is reduced from 5s to 25ms. The well designed loss terms also improve the rendering accuracy, and the fine-level details of the human head, such as the gaps between teeth, wrinkles, and beards,
*Corresponding Author can be represented and synthesized by HeadNeRF. Exten-sive experimental results and several applications demon-strate its effectiveness. The trained parametric model is available at https://github.com/CrisHY1995/headnerf. 1.

Introduction
The parametric face/head model, which encodes the human face/head in low-dimensional space, is a hot re-search topic in computer vision and computer graphics and widely used in many applications like identity recog-nition [34, 52], face analysis [11, 64] and film/game pro-duction [47], etc. Early works of the parametric face/head model [3, 6, 11, 30, 41, 57] mainly model 3D faces with the topologically uniformed face template mesh and usually ig-nore to represent the non-face parts, such as hair and teeth.
With the development of deep learning, 2D generative ad-versarial networks (GAN) [21,22] are able to directly render photo-realistic face images without the help of 3D model-ing. Some methods [5,9,14,26,36] further introduce seman-tically disentangled constraints to render the face images in a user-controlled way. However, their rendered results from different views often tend to be inconsistent as they do not
explicitly encode or model 3D geometry.
Recently, Mildenhall et al. [35] propose to represent 3D scenes using neural radiance fields (NeRF). This strat-egy can synthesis photorealistic images and has emerged as a compelling technique. Compared with the above-mentioned generative methods, the density field from NeRF actually implicitly encodes the 3D geometry of the scene.
Therefore, the results from NeRF have excellent multi-view
In another opinion, NeRF itself can be re-consistency. garded as a novel 3D representation equivalent to a textured mesh and naturally supports differentiable rendering.
Based on this observation, we apply the NeRF struc-ture to the representation of human heads and propose a novel NeRF-based parametric head model, HeadNeRF.
HeadNeRF inherits the excellent properties of NeRF, which can generate high fidelity head images and maintain re-markable multi-view consistency. Moreover, NeRF itself supports freely changing the camera perspective used for rendering, and thus HeadNeRF naturally supports the pose editing of rendered images. It is challenging for the above-mentioned 2D generative methods. In addition, HeadNeRF intrinsically supports differentiable rendering. In contrast, traditional 3D representations, such as mesh, point cloud, voxel, etc., need to design various approximation meth-ods [19, 23, 27, 28, 32] to alleviate the non-differentiable problems of their rendering process. Therefore, compared with the previous methods, which need to capture and pro-cess a large amount of high-quality 3D scan data, the con-struction of HeadNeRF only needs 2D images as input.
Specifically, we collect and process three large-scale human head image datasets and design novel loss terms to disentan-gle this parametric representation. With the well designed network structure and loss function, HeadNeRF can seman-tically disentangle the identity, expression, and appearance of the rendered images. Fig. 1 shows some results by freely exploring within the space of HeadNeRF.
We further integrate the volume rendering of NeRF with 2D neural rendering to achieve real-time rendering. Sim-ilar with GIRAFFE [38] and StyleNeRF [16], this coarse to fine strategy significantly accelerates the rendering speed of HeadNeRF, and it can exceed 40fps without sacrificing the rendering quality. Benefiting from its nice disentangled representation, real-time rendering of inference, and high fidelity generated results, we apply HeadNeRF to various applications, including novel view synthesis from a single face image, semantically editing face attributes, and even facial reenactment where the expressions of one person are transferred to another person. In summary, the main contri-butions of this paper include:
• We propose the first NeRF-based parametric human head model, which can directly efficiently control the rendering pose, identity, expression, and appearance.
• We propose an effective training strategy to train the model from general 2D image datasets, and the trained model can generate high fidelity rendered images.
• We design and implement several novel applications with HeadNeRF, and the results verify its effective-ness. We believe that more interesting applications can be explored with our HeadNeRF. 2.