Abstract
Designing better machine translation systems by consid-ering auxiliary inputs such as images has attracted much attention in recent years. While existing methods show promising performance over the conventional text-only trans-lation systems, they typically require paired text and image as input during inference, which limits their applicability to real-world scenarios. In this paper, we introduce a vi-sual hallucination framework, called VALHALLA, which requires only source sentences at inference time and in-stead uses hallucinated visual representations for multi-modal machine translation. In particular, given a source sentence an autoregressive hallucination transformer is used to predict a discrete visual representation from the input text, and the combined text and hallucinated repre-sentations are utilized to obtain the target translation. We train the hallucination transformer jointly with the trans-lation transformer using standard backpropagation with cross-entropy losses while being guided by an additional loss that encourages consistency between predictions us-ing either ground-truth or hallucinated visual representa-tions. Extensive experiments on three standard translation datasets with a diverse set of language pairs demonstrate the effectiveness of our approach over both text-only base-lines and state-of-the-art methods. Project page: http:
//www.svcl.ucsd.edu/projects/valhalla. 1.

Introduction
Machine Translation (MT) is a core task in natural lan-guage processing and has undergone several paradigm shifts over the past few decades, from early rules-based sys-tems [38] to pipelined statistical MT approaches [25, 33] to recent end-to-end neural network-based models [9, 58, 1, 62]. While such advances have led to impressive results on standard benchmarks, existing systems by and large uti-lize text-only information and lack any explicit grounding
*Work done during an internship at the MIT-IBM Watson AI Lab.
EN: A snowboarder  wearing a red coat is  going down a snow-covered slope. (a)
DE: Ein snowboarder in  einem roten anzug fährt  eine schneebedeckte  piste herunter.
Can visual  hallucination  improve machine  translation? (b) Visual Context (c) Visual Hallucination
Figure 1: Visual context such as images has been exploited in de-signing better machine translation systems. Different from most existing methods that require manually annotated sentence-image pairs as the input during inference, we introduce VALHALLA, that leverages hallucinated visual representation from the source sen-tences at test time for improved machine translation. to the real world. There has thus been a growing interest in developing multimodal MT systems that can incorporate rich external information into the modeling process.
Consider the example in Figure 1(a), where a source sen-tence in English (blue box) is to be translated to a target sentence in German (red box). Since both sentences depict the same visual scene, shown in Figure 1(b), there is com-mon grounding information across the two sentences. More generally, while there are many different ways to describe a situation in the physical world, the underlying visual percep-tion is shared among speakers of different languages. The addition of visual context in the form of images is thus likely to help the machine translation. In particular, grounding should improve the data-efﬁciency of translation methods and beneﬁt translation in low resource scenarios.
This has motivated much recent work on vision-based multimodal machine translation (MMT), which aims to im-prove machine translation systems by utilizing the visual modality [6, 30, 76, 20]. These methods typically require source sentences to be paired with the corresponding images during training and testing, which hinders their applicability
to settings where images are not available during inference.
In this work we consider the question of whether a system that has access to images only at training time can generalize to these settings. We hypothesize that “visual hallucination, i.e., the ability to imagine visual scenes, can be leveraged to improve machine translation systems”. Under this hypoth-esis, a translation system with access to images at training time could be taught to abstract an image or visual repre-sentation of the text sentence, as shown in Figure 1(c), in order to ground the translation process. At test time, this abstracted visual representation could be used in lieu of an actual image to perform multimodal translation. a
We introduce effective VisuAL yet simple
HALLucinAtion (VALHALLA) framework, which in-corporates images at training time to produce a more effective text-only model for machine translation. As is usual for machine translation, the goal is to train a model that only sees source sentences at test time. However, during training, the model is trained to complement the text representation extracted from the source sentence with a latent visual representation that mirrors the one extracted from a real image (paired with the source sentence) by an
MMT system. We achieve this by training an autoregressive hallucination transformer over a discrete codebook (learned using VQGAN-VAE [14]) to predict visual tokens from the input source sentences for multimodal translation.
VALHALLA consists of a pair of transformers: a visual hallucination transformer that maps the source sentence into a discrete image representation, and an MMT transformer that maps the source sentence paired with its discrete im-age representation into the target sentence. We train the transformer models end-to-end with a combination of hallu-cination, translation, and consistency losses. As sampling of the discrete image representations (i.e., visual hallucina-tions) is non-differentiable, we rely on a Gumbel-Softmax relaxation [21, 35] to effectively train the hallucination trans-former jointly with the translation transformer. To the best of our knowledge, ours is the ﬁrst work that successfully leverages an autoregressive image transformer jointly with the translation transformer to hallucinate discrete visual rep-resentations. We ﬁnd that discrete visual representations lead to improved performance compared to continuous visual em-beddings used in existing MMT methods [66, 30, 68, 74, 32].
Extensive experiments on three standard MT datasets (Multi30K [13], WIT [54] and WMT [2]) with a diverse set of language pairs and different scales of training data (in total 13 pairs) demonstrate the superiority of VALHALLA over strong translation baselines. VALHALLA yields an aver-age 2 3 BLEU improvement over the text-only translation baseline, while consistently outperforming the most relevant state-of-the-art MMT methods that make use of continuous image representations [74, 32]. The gains over the text-only baseline are as large as +3.1 BLEU on under-resourced trans-⇠
RO and EN
AF tasks from lation settings, such as the EN
!
WIT, conﬁrming the hypothesis that visual hallucinations can have signiﬁcant practical value in these settings. This is also conﬁrmed by additional analysis suggesting that, un-der limited textual context, VALHALLA models do leverage visual hallucination to generate better translations.
! 2.