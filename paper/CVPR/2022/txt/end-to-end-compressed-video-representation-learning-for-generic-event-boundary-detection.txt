Abstract
Generic event boundary detection aims to localize the taxonomy-free event boundaries that segment generic, videos into chunks. Existing methods typically require video frames to be decoded before feeding into the network, which demands considerable computational power and storage space. To that end, we propose a new end-to-end com-pressed video representation learning for event boundary detection that leverages the rich information in the com-pressed domain, i.e., RGB, motion vectors, residuals, and the internal group of pictures (GOP) structure, without fully decoding the video. Specifically, we first use the Con-vNets to extract features of the I-frames in the GOPs. Af-ter that, a light-weight spatial-channel compressed encoder is designed to compute the feature representations of the P-frames based on the motion vectors, residuals and represen-tations of their dependent I-frames. A temporal contrastive module is proposed to determine the event boundaries of video sequences. To remedy the ambiguities of annotations and speed up the training process, we use the Gaussian ker-nel to preprocess the ground-truth event boundaries. Exten-sive experiments conducted on the Kinetics-GEBD dataset demonstrate that the proposed method achieves compara-ble results to the state-of-the-art methods with 4.5× faster running speed. 1.

Introduction
Video traffic will account for 82% percent of all internet traffic by 2022, up from 75% in 2017 [11]. Understanding video content using AI technology is an active area of re-search in recent years. However, it is still a challenging task due to the complex temporal evolution in the enormous size
*This work was done during internships at ByteDance Inc.
†Corresponding author (libo@iscas.ac.cn)
Figure 1. The inference time vs. F1 score of different methods on the Kinetics-GEBD dataset [28]. (a) The previous method [28]
PC is relatively fast with inferior results. (b) After integrating the optical flow (OF) module, the accuracy is improved with much (c, d) Our method achieves competitive slower running speed.
F1 score with extremely fast running speed by directly leveraging (e, f) motion vectors and residuals in the compressed domain.
CLA [18] and CASTANET [14] take fully decoded RGB frames as input, which are much slower than the methods conducted in compressed domain. The green region indicates the methods run in real-time. of raw video streams with high temporal redundancy.
Video understanding is one of the most fundamental problems in computer vision, which includes video tagging, action recognition, and video boundary detection, etc. In contrast to static images, videos provide rich information in-volving temporal consistency in consecutive frames which can be additionally utilized. Currently, the two-stream net-work [8,9,30] and 3D convolutional network [17,33,34,37] are two popular network architectures in the video under-standing field. The two-stream network incorporates both the decoded RGB video frames and optical flow to exploit temporal information. However, extracting optical flow is very slow, which dominates the overall pre-processing time in the video understanding tasks. 3D convolutional net-work is another choice to model temporal information us-ing the spatio-temporal filters. The drawbacks of 3D con-volutional network is the massive parameters contained in 3D convolution operations, which slows down the inference speed. Besides the aforementioned methods, the new trend in video understanding is using the transformers, includ-ing [1, 5, 6, 24, 51], achieving competitive results.
In recent years, several methods [16, 29, 42, 45, 47, 49] demonstrate the advantages of directly taking videos in compressed domain as input for video understanding.
These methods use motion vectors and residuals in the com-pressed representation that developed for storage and trans-mission of videos rather than operating on the decoded RGB frames, which run in two orders of magnitude faster than the methods using optical flow while achieving competi-tive results [29]. Specifically, these methods use the al-most compute-free motion vectors and residuals encoded in P-frames as an alternative to the compute-intensive opti-cal flow. For example, CoViAR [45] directly feeds motion vectors and residuals into 2D CNNs for action recognition, and DMC-Net [29] improves the CoViAR method by re-constructing the optical flow based on motion vectors and residuals. Although the aforementioned method achieves promising results, they are still far from satisfactory, which lack effective fusion strategies between different modalities, such as decoded I-frames, motion vectors, and residuals.
In this paper, we focus on the generic event bound-ary detection (GEBD [28]) task that aims to localize the moments where humans naturally perceive taxonomy-free event boundaries that segment a longer event into shorter temporal segments. The ability to divide a long form video into small meaningful clips makes this task demanding for several downstream video understanding tasks and industry applications that requires high accuracy and low latency.
The previous attempt [28] formulate it as a classification task by considering the context information of the candidate boundaries. However, it neglects the temporal relations be-tween consecutive frames and operates inefficiently during feature extraction stage. Inspired by [16, 29, 42, 45, 47, 49], we design an end-to-end trained network to exploit the dis-criminative features for GEBD in compressed domain, i.e.,
MPEG-4, which is able to save decoding cost and improve feature extraction efficiency. Specifically, most modern codecs split a video into several group of pictures (GOP), where each GOP is formed by one I-frames and T P-frames.
To solve difficulty arised from the long chain of dependency of the P-frames, inspired by [45], we use the back-tracing technique to compute the accumulated motion vectors and
In this way, the consecutive P-residuals in linear time. frames in each GOP are only depending on the reference
I-frame, which can be processed in parallel.
In contrast to the I-frame, it is difficult to learn the dis-criminative features of the P-frames. Refining the features of the reference I-frame based on the motion vectors and residuals becomes an intuitive option. Motion vectors and residuals provide information to reconstruct P-frames by re-ferring the dependent I-frames. In addition to that, they also provide motion information that obtained from the video encoding process. To that end, we design a light-weight spatial-channel compressed encoder to refine the features of the reference I-frame with the guidance of the motion vectors and residuals. In this way, the features of P-frames and I-frames are converted to the same feature space, which benefits the subsequent processing. After that, a temporal contrastive module is proposed to capture the context in-formation in temporal domain to predict the event bound-aries of videos. Notably, our temporal contrastive module imitates humans, i.e., look back and forth around the can-didate frames to determine event boundaries, by comparing the extracted features before and after the candidate frames.
In addition, to remedy the ambiguities of annotations and speed up the training process, we use the Gaussian kernel to preprocess the ground-truth event boundaries instead of using the “hard lables” of boundaries. Extensive experi-ments conducted on the Kinetics-GEBD dataset to demon-strate the effectiveness of the proposed method. Specifi-cally, the proposed method achieves comparable results to the state-of-the-art method at the CVPR’21 LOVEU Chal-lenge [18] with 4.5× faster running speed, see Figure 1.
The main contributions of this paper are listed as follows. (1) We propose an end-to-end compressed video represen-tation learning method to solve the challenging GEBD task. (2) We design the spatial-channel compressed encoder to project the features of reference I-frame with the guidance of motion vectors and residuals to compute the features of
P-frames with low cost. (3) A temporal contrastive mod-ule is proposed to determine the event boundaries of videos by exploiting the context information in temporal domain. (4) The proposed method achieves comparable results to the state-of-the-art methods at the CVPR’21 LOVEU Chal-lenge [18] with 4.5× faster running speed, demonstrating its effectiveness. 2.