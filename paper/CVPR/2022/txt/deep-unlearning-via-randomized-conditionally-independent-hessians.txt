Abstract
Recent legislation has led to interest in machine un-learning, i.e., removing specific training samples from a predictive model as if they never existed in the training dataset. Unlearning may also be required due to cor-rupted/adversarial data or simply a user’s updated pri-vacy requirement. For models which require no train-ing (k-NN), simply deleting the closest original sample can be effective. But this idea is inapplicable to models which learn richer representations. Recent ideas leveraging optimization-based updates scale poorly with the model di-mension d, due to inverting the Hessian of the loss function.
We use a variant of a new conditional independence coeffi-cient, L-CODEC, to identify a subset of the model parame-ters with the most semantic overlap on an individual sample level. Our approach completely avoids the need to invert a (possibly) huge matrix. By utilizing a Markov blanket selec-tion, we premise that L-CODEC is also suitable for deep un-learning, as well as other applications in vision. Compared to alternatives, L-CODEC makes approximate unlearning possible in settings that would otherwise be infeasible, in-cluding vision models used for face recognition, person re-identification and NLP models that may require unlearn-ing samples identified for exclusion. Code is available at https://github.com/vsingh-group/LCODEC-deep-unlearning 1.

Introduction
As personal data becomes a valuable commodity, legisla-tive efforts have begun to push back on its widespread col-lection/use particularly for training ML models. Recently, a focus is the “right to be forgotten” (RTBF), i.e., the right of an individual’s data to be deleted from a database (and derived products). Despite existing legal frameworks on fair use, industry scraping has led to personal images being used without consent, e.g. [20]. Large datasets are not only stored for descriptive statistics, but used in training large
*Joint First Authors. models. While regulation (GDPR, CCPA) has not specified the extent to which data must be forgotten, it poses a clear question: is deletion of the data enough, or does a model trained on that data also needs to be updated?
Recent work by [6, 7] has identified scenarios where trained models are vulnerable to attacks that can reconstruct input training data. More directly, recent rulings by the Fed-eral Trade Commission [12, 24] have ordered companies to fully delete and destroy not only data, but also any model trained using those data. While deletion and (subsequent) full model retraining without the deleted samples is possi-ble, most in-production models require weeks of training and review, with extensive computational/human resource cost. With additional deletions, it is infeasible to retrain each time a new delete request comes in. So, how to update a model ensuring the data is deleted without retraining?
: zi
{
Task. Given a set of input data of
S size n, training simply identifies a hypothesis ˆw via g( ˆw, z′) until conver-an iterative scheme wt+1 = wt
, z′) is a stochastic gradient of a fixed loss gence, where g(
· function. Once a model at convergence is found, machine unlearning aims to identify an update to ˆw through an anal-ogous one-shot unlearning update: n i=1 ∼ D
∈ W
−
} w′ = ˆw + g ˆw (z′) , (1)
∈ S that is to be unlearned. for a given sample z′
Contributions. We address several computational issues with existing approximate formulations for unlearning by taking advantage of a new statistical scheme for sufficient parameter selection. First, in order to ensure that a sample’s impact on the model predictions is minimized, we propose a measure for computing conditional independence called
L-CODEC which identifies the Markov Blanket of param-eters to be updated. Second, we show that the L-CODEC identified Markov Blanket enables unlearning in previously infeasible deep models, scaling to networks with hundreds of millions of parameters. Finally, we demonstrate the abil-ity of L-CODEC to unlearn samples and entire classes on networks, from CNNs/ResNets to transformers, including face recognition and person re-identification models.
Figure 1. Large deep learning networks typically associate specific subsets of network parameters, blocks (blue), to specific samples in the input space. Traditional forward or backward passes may not reveal these blocks: high correlations among features may not distinguish important ones. Input perturbations can be used to identify them in a probabilistic, distribution-free manner. These blocks can then be unlearned together in an efficient block-coordinate style update (right, blue lines), approximating an update to the full network which requires a costly/infeasible full Hessian inverse (red line). 2. Problem Setup for Unlearning
A
∈ W
Let and outputs a hypothesis w be an algorithm that takes as input a training set
, defined by a set of
S d parameters Θ. An unlearning scheme takes as input a sample z′
, and ideally, outputs where z′ has been deleted an updated hypothesis w′
∈ W from the model. An unlearning algorithm should output a hypothesis that is close or equivalent to one that would have z′. A framework been learned had the input to for this goal was given by [13] as, used as input to been
∈ S
S \
A
A
U with additive Gaussian noise w′ = w′ + N (0, σ2) scaling as a function of n, ϵ, δ, and the Lipschitz and (strong) con-vexity parameters of the loss f . We can interpret the update using (4) from the optimization perspective as a trajectory
“reversal”: starting at a random initialization, the first or-der (stochastic gradient) trajectory of w (possibly) with z′ is reversed using residual second order curvature informa-tion (Hessian) at the optimal ˆw in (4), achieving unlearning.
This is shown to satisfy Def. 1, and only incurs an additive error that scales by O(√d/n2) in the gap between F (w′) and the global minimizer F (w∗) over the ERM F ( ˆw).
Definition 1 ((ϵ, δ) with a “delete request” z′ is (ϵ, δ)
P( forgetting if (S), z′)
− (
−
)
U
A
∈ W
≤
∈ S forgetting). For all sets of size n,
, an unlearning algorithm
S
U eϵP( (
S \
A z′)
∈ W
) + δ (2)
In essence, for an existing model w, a good unlearning will output a model ˆw close to
∈ S z′) with high probability. algorithm for request z′ the output of (S
A
\
Remark 1. Definition 1 is similar to the standard defini-tions of differential privacy. The connection to unlearning is: if an algorithm is (ϵ, δ) forgetting for unlearning, then it is also differentially private.
− is an empirical risk minimizer for the loss f , let
If
A
: (
, f )
A
S
→
ˆw (3) (cid:80)n
ˆw = arg min F (w) and F (w) = 1 i=1 f (w, zi). Re-n call g(z′) from (1): our unlearning task essentially involves identifying the form of g(z′) for which the update in (1) is (ϵ, δ)-forgetting. If an oracle provides this information, we have accomplished the unlearning task.
The difficulty, as expected, tends to depend on f and
.
A
Recent unlearning results have identified forms of f and where such a g(z′) exists. The authors in [30] define
A g(z′) = 1 f ( ˆw, z′), where (cid:0)n
∇ 2F ( ˆw)
− ∇ 2f ( ˆw, z′)(cid:1) , (4) n−1 H ′−1
∇ 1
H ′ = n 1
−
∇
Rationale for approximate schemes. From the reversal of w optimization perspective, it is clear that there may be other choices to achieve unlearning. For a practitioner in-terested in unlearning, the aforementioned algorithm (as in (4)) can be directly instantiated if one has extensive com-putational resources. Indeed, in settings where it is not di-rectly possible to compute the Hessian inverse necessary for
H ′−1 f ( ˆw, z′), we must consider alternatives.
∇
A potential idea. Our goal is to identify a form of g(z′) that approximates H ′−1 f ( ˆw, z′). Let us consider the
Newton-style update suggested by (4) as a smoothing of a traditional first order gradient step. The inverse Hessian is a weighting matrix, appropriately scaling the gradients based on the second order difference between the training set mean point F ( ˆw) and at the sample of interest f ( ˆw, z′).
This smoothing can also be seen from an information per-spective: the Hessian in this case corresponds to a Fisher-style information matrix, and its inverse as a conditional co-variance matrix [14, 16]. It is not hard to imagine that from this perspective, if there are specific set of parameters that have small gradients at f ( ˆw, z′) or if the information matrix is zero or small, then we need not consider their effect.
Examples of this intuition in vision. [3,11,32] and oth-ers have shown that models trained on complex tasks tend to delegate subnetworks to specific regions of the input space.
That is, parameters and functions within networks tend to (or can be encouraged to) act in blocks. For example, activa-tion maps for different filters in a trained (converged) CNN model show differences for different classes, especially for filters closer to the output layer. We formalize this observa-tion as an assumption for samples in the training set.
Assumption 1. For all subsets of training samples S there exists a subset of trained model parameters P ∗ such that
,
⊂ S
Θ
⊂ f (S) w∗ w∗
P
⊥
Θ\P |
Due to the computational issues discussed above, if we could make such a simple/principled selection scheme prac-tical, it may offer significant benefits. (5) 3.