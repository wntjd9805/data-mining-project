Abstract 1.

Introduction
We present an approach for recommending a music track for a given video, and vice versa, based on both their tempo-ral alignment and their correspondence at an artistic level.
We propose a self-supervised approach that learns this cor-respondence directly from data, without any need of human annotations.
In order to capture the high-level concepts that are required to solve the task, we propose modeling the long-term temporal context of both the video and the music signals, using Transformer networks for each modal-ity. Experiments show that this approach strongly outper-forms alternatives that do not exploit the temporal context.
The combination of our contributions improve retrieval ac-curacy up to 10× over prior state of the art. This strong improvement allows us to introduce a wide range of analy-ses and applications. For instance, we can condition music retrieval based on visually defined attributes.
Work partly done during an internship at Adobe Research.
Music is a crucial component of video creation, for example soundtracks in feature film, music for advertise-ments, background music in video blogs, or creative uses of music in social media. However, choosing the right mu-sic for a video is difficult—the video creator needs to de-termine what kind of music to use for different moments in the video and then search for this music. Each of these tasks presents difficulties: choosing the right music to set the mood of a video can be hard for non-professionals and, even when you know what type of music you want, it can be hard to search for it using conventional text-based methods.
It is very hard to describe the “feel” of a song in words and metadata-based search engines are not well suited for this task. An automated tool to suggest relevant music given video footage as input could be of great value to a range of creators, from beginners and amateurs in need of a sim-ple solution, through to communicators and professionals in search of inspiration. The inverse problem, matching
ˇ
“ (
ˇ
“ (
video footage to a given song, similarly presents notable challenges, and a solution has the potential to unlock new creative applications. As such, an automated tool to per-form retrieval in both directions, from video to music and vice versa, is of great interest.
While other audio-visual tasks aim to establish physi-cal correspondences for discrete events between the two modalities (e.g., the sound of a person clapping with the visual motion of the person performing the clapping ac-tion) [5–7], such correspondences are predominantly not the deciding factor for pairing music with video. The determin-ing factors for the pairing task are instead often “artistic” and non-physical, and may comprise the overall visual style or aesthetics of the video, and the genre, mood or “feel” of the music. Additionally, a system may pair musical gen-res with visual attributes (e.g., depicted scene type or mu-sical instrument played) or populations presenting a partic-ular gender or race. Studying the interplay of these factors is important for understanding and exposing how a system makes its decisions and mitigating potential bias [26].
To address these tasks, we seek to train an audio-visual model to determine how well a paired video and music au-dio clip “go together”, or correspond, where we learn this correspondence directly from video data without requiring any manual labeling. Once trained, the model can be used to retrieve music that would pair well with a given input video, and to retrieve video clips that would pair well with a given input music track (see Fig. 1 for some examples).
Moreover, we seek to understand how a trained model as-sociates the aforementioned musical genres and visual at-tributes. As it is difficult to manually collect annotated data at large scale describing the mood of video and musical audio, we leverage self-supervision, i.e., learning from the structure inherent to the data. Since we have access to large video collections where music and video have already been paired together by human creators, we leverage these data to learn what makes for a good pairing. The model is pre-sented with both the large collection paired by human cre-ators and randomly paired audio/video tracks, and is trained to distinguish between the two collections.
Previous approaches for this task typically rely on cor-responding short video and music segments or aggregat-ing features over multiple segments [51]. However, as the correspondence between video and music is often an artis-tic one, it often depends on long-range temporal context, which is hard to capture in a short segment or by aggregat-ing multiple segment features. For example, a given scene in a movie conditions the “mood” of the music in the next scene, and the proximity of the climax of a song conditions how the video clip is edited [33, 42, 52]. Furthermore, these prior approaches optimize metric losses that do not weight hard examples during training [15], and leverage modality-specific visual base features trained on a fixed-vocabulary classification task [21] or audio base features that are not specific for music [17]. Finally, while these approaches evaluate retrieval accuracy, they do not study how a model associates musical genre and visual attributes.
To address these challenges, we make the following con-tributions. First, we show for the first time that temporal context is important for this artistic correspondence learn-ing task. We do so by leveraging a Transformer architec-ture [63] to model long-range temporal context and em-ploying other best practices for video-music retrieval (e.g., optimize a contrastive loss during training, build on strong base features for each modality), leading to a dramatic 10× improvement in retrieval accuracy. Second, we conduct a detailed analysis of our model, shedding light onto what visual attributes present in the video, such as scene type and musical instruments, are used by the model to establish artistic correspondence with different musical genres. This analysis includes “attributes” whose over-simplistic defini-tion or representation such as gender and race can lead to potentially concerning biases. Third, we demonstrate the usefulness of the learned audiovisual representation through several applications, including novel ones such as combin-ing a music query with visual attributes to retrieve music of the same genre where the visual attributes are musically represented in the audio signal. Finally, we study and dis-cuss potential issues with our model related to bias. Since our task is concerned with learning artistic correspondence based on video-music pairings made by humans, rather than audio-visual correspondence grounded in physics, it presents new and important challenges and considerations concerning bias, cultural awareness and appropriation. 2.