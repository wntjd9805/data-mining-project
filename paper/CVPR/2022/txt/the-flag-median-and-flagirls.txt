Abstract as a prototype y via solving
Finding prototypes (e.g., mean and median) for a dataset is central to a number of common machine learn-ing algorithms. Subspaces have been shown to provide use-ful, robust representations for datasets of images, videos and more. Since subspaces correspond to points on a
Grassmann manifold, one is led to consider the idea of a subspace prototype for a Grassmann-valued dataset.
While a number of different subspace prototypes have been described, the calculation of some of these prototypes has proven to be computationally expensive while other proto-types are affected by outliers and produce highly imperfect clustering on noisy data. This work proposes a new sub-space prototype, the flag median, and introduces the Fla-gIRLS algorithm for its calculation. We provide evidence that the flag median is robust to outliers and can be used effectively in algorithms like Linde-Buzo-Grey (LBG) to produce improved clusterings on Grassmannians. Numer-ical experiments include a synthetic dataset, the MNIST handwritten digits dataset, the Mind’s Eye video dataset and the UCF YouTube action dataset. The flag median is compared the other leading algorithms for computing pro-totypes on the Grassmannian, namely, the ℓ2-median and to the flag mean. We find that using FlagIRLS to compute the flag median converges in 4 iterations on a synthetic dataset. We also see that Grassmannian LBG with a code-book size of 20 and using the flag median produces at least a 10% improvement in cluster purity over Grassmannian
LBG using the flag mean or ℓ2-median on the Mind’s Eye dataset. 1.

Introduction
The mean and median are basic methods for calcu-lating central prototypes from a probability distribution.
The median is commonly more robust to outliers than the mean. Generalizations of such prototypes to Euclidean space can be formulated as a solution to an optimization problem. Suppose we have a set of points in Euclidean
⊂ Rn which we would like to represent space, X = {xi } p i =1 arg min y∈A p (cid:88) i =1
∥xi − y∥q 2 . (1)
The solution to (1) for A = Rn and q = 2 is called the cen-troid, which may be viewed as the generalization of the mean. In fact, the centroid is the component-wise mean of the vectors in X , (cid:80)p i =1 xi /p. Generalizations of the me-dian involve solving (1) when q = 1. When A = X , the solution is called the medoid, while when A = Rn, the so-lution is called the geometric median. The geometric me-dian inherits the robustness to outliers from the median without being required to be a point in the dataset; how-ever, it is not as straightforward to compute as a centroid since that calculation is not simply a least squares prob-lem. An iterative algorithm for approximating a geometric median is the Weiszfeld algorithm [21]; each iteration of this algorithm is a weighted centroid problem. Thus, the
Weiszfeld algorithm falls into a class known as Iteratively
Reweighted Least Squares algorithms (IRLS). These Eu-clidean prototypes are used as a statistic for a dataset and in common machine learning algorithms like k-means and nearest centroid classification.
Not all datasets are best represented using points in
Euclidean space. Specifically, image or video datasets are sometimes better represented using subspaces, i.e., as points on a Grassmannian. For example, the smallest principal angle between two subspaces has proven pow-erful for modeling illumination spaces [3]. Hyperspectral data may fail to be linearly separable in Euclidean space but separate linearly on the Grassmannian [5]. Hence, it is potentially useful to find versions of the Euclidean prototypes on the Grassmannian. A logical generalization of prototypes from Euclidean space to the Grassmannian is to replace the Euclidean 2-norm in the optimization problems for the centroid, medoid and geometric median with a distance or dissimilarity between subspaces. The centroid is generalized using the geodesic distance in [13] and the chordal distance in [9]. To the extent of our re-search, we have have not found a generalization of the medoid. However, the geometric median has been gen-eralized using the geodesic distance and is called the ℓ2-median in [1, 11]. Prototypes like these have been used alone as a method to classify emotion in images [24], as a step in a k-means type algorithm [18] and in feature ex-traction [11].
Popular machine learning techniques, like dictionary learning, have been adapted to Riemannian manifolds
[22]. Jayasumana et. al. consider learning on the Grass-mannian (and Riemannian manifolds in general) with
RBF kernels and advocate for chordal distance (some-times referred to as the projection norm) kernels on the
Grassmannian because chordal distance generates a pos-itive definite Gaussian kernel [12]. More recently, Cherian et. al. use kernalized Grassmannian pooling for activity recognition [6]. Methods for Riemannian optimization like Riemannian SVRG have gained popularity alongside this surge of interest in Riemannian learning [23]. Even more uses for subspaces in computer vision and machine learning can be found in, e.g., [1, 2, 9, 10, 17, 18, 24].
In this paper we propose the flag median, a prototype which is a generalization of the geometric median to the
Grassmannian using the chordal distance. We solve the flag median optimization problem using the novel Fla-gIRLS algorithm. The FlagIRLS is an IRLS algorithm on the Grassmannian that solves a weighted flag mean prob-lem at each iteration similar to the way an iteration of the
Weiszfeld algorithm solves a weighted centroid problem.
We conduct experiments with the flag median, ℓ2-median and the flag mean on synthetic datasets, the MNIST hand-written digits dataset [8], the DARPA (Defense Advanced
Research Projects Agency) Mind’s Eye dataset used in [18] and the UCF YouTube action dataset [16]. In these exam-ples we find that the FlagIRLS algorithm tends to converge quickly. We show that the flag median appears to be more robust to outliers than the flag mean and ℓ2-median, and produces the highest cluster purities in the LBG algorithm
[15]. 2.