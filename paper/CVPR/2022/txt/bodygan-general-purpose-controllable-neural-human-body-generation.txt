Abstract
Recent advances in generative adversarial networks (GANs) have provided potential solutions for photo-realistic human image synthesis. However, the explicit and individual control of synthesis over multiple factors, such as poses, body shapes, and skin colors, remains difﬁcult for ex-isting methods. This is because current methods mainly rely on a single pose/appearance model, which is limited in dis-entangling various poses and appearance in human images.
In addition, such a unimodal strategy is prone to causing severe artifacts in the generated images like color distor-tions and unrealistic textures. To tackle these issues, this paper proposes a multi-factor conditioned method dubbed
BodyGAN. Speciﬁcally, given a source image, our Body-GAN aims at capturing the characteristics of the human body from multiple aspects: (i) A pose encoding branch consisting of three hybrid subnetworks is adopted, to gener-ate the semantic segmentation based representation, the 3D surface based representation, and the key point based rep-resentation of the human body, respectively. (ii) Based on the segmentation results, an appearance encoding branch is used to obtain the appearance information of the human body parts. (iii) The outputs of these two branches are rep-resented by user-editable condition maps, which are then processed by a generator to predict the synthesized image.
In this way, our BodyGAN can achieve the ﬁne-grained dis-entanglement of pose, body shape, and appearance, and consequently enable the explicit and effective control of syn-thesis with diverse conditions. Extensive experiments on multiple datasets and a comprehensive user study show that our BodyGAN achieves the state-of-the-art performance. 1.

Introduction
Realistic human body image generation is a challeng-ing problem, due to the complex textures, diverse poses
∗: These authors contribute equally.
†: Corresponding authors. (cid:1)(cid:3)(cid:2) (cid:1)(cid:4)(cid:2) (cid:1)(cid:5)(cid:2) (cid:1)(cid:6)(cid:2)
Figure 1. We propose BodyGAN, i.e., a general approach for syn-thesizing photo-realistic human body with explicit control over multiple factors: (a) Body pose and shape. (b) Skin colors. (c)
Hidden face and cartoon. (d) Marble and bronze sculptures. The source pictures are shown in the top right corners. and illumination distributions. As there are a large number of application scenarios that rely on the high-quality and controllable human image generation, such as virtual try-on and virtual reality, extensive research has been conducted to tackle this problem. For instance, in recent research on vir-tual try-on [33, 47], both mask based methods [35, 37] and mask-free methods [25, 42] have been explored to generate images of the human body with target clothes. However, these methods consider the human body and clothes togeth-er as the whole target, and adopt the end-to-end generation manner, which leads to poor results and artifacts on human body parts, such as unnatural texture patterns, loss of detail-s and color distortions. Meanwhile, a few skin completion methods [23,43] target at the generation of face skin. There are also algorithms [5, 18, 28, 30] generating faces based on contour sketches. Nevertheless, considering that the human
body is a ﬂexible articulated structure with a higher degree of freedom, the above solutions are not suitable for generat-ing human body images.
Recently, a few GAN based methods have been proposed for human body image synthesis, such as StyleGAN [15],
StylePoseGAN [34] and StyleRig [36]. The key idea of these methods is to utilize a single model to disentangle pose and appearance, so that different synthesized images can be obtained via changing the pose or the appearance.
However, in real-life applications, a single model is not robust enough to disentangle images with various poses and appearances. Furthermore, existing methods may need paired training images (e.g., StylePoseGAN) to ensure the performance of their models. These issues of existing meth-ods make the explicit control of the synthesis difﬁcult and restrict their applications.
To tackle the above limitations of existing methods, we propose a highly controllable framework for human image synthesis called BodyGAN. Unlike previous methods aim-ing at a particular synthesis task with a single end-to-end network, our BodyGAN focuses on the generation of pure human body images with controllable poses, body shapes and skin colors. The generated human body images are then modiﬁed based on the requirements of the downstream task (e.g., merged with a garment to complete try-on). Speciﬁ-cally, given a source image, our BodyGAN adopts a pose encoding branch and an appearance encoding branch, to generate seven types of condition maps for image synthe-sis. In the pose encoding branch, we utilize three subnet-works to obtain the semantic segmentation based condition map, the 3D surface based condition map, and the key point based condition map of the human body. While in the ap-pearance encoding branch, four appearance condition maps are used to encode the head, the hand, the upper body, and the lower body of the human body. In this way, we achieve the disentanglement of pose and appearance that is more
ﬁne-grained, compared with existing methods. Besides, our condition maps encode pose information via both 2D repre-sentations (semantic segmentation and key points) and 3D surface faces, which help to generate more natural poses.
The condition maps are fed into a generator that is opti-mized via adversarial learning (without the need for paired training images), to produce the synthesized images.
The contributions of this paper are summarized as fol-lows:
• We propose BodyGAN, which is a general model for human body image generation with explicit controls over multiple factors. Our BodyGAN can disentangle pose and appearance from a single source image, and adopt condition maps, which are a convenient and ed-itable representation of pose and appearance informa-tion, to generate realistic human images.
• The proposed condition maps are generated by three subnetworks that model the person from both 2D and 3D perspectives, which enhance the robustness of our
BodyGAN effectively. In addition, utilizing pose and appearance condition maps allows us to train and apply the BodyGAN with unpaired images.
• Compared with the existing methods, our BodyGAN can generate more realistic and visually-pleasant re-sults, even with extremely difﬁcult poses such as crossed arms/legs. Extensive experiments on three datasets validate the effectiveness of our BodyGAN.
• We present the downstream applications of BodyGAN in image manipulation, such as virtual try-on and dig-ital humans. In summary, BodyGAN provides a novel diagram for human image generation in computer ani-mation. 2.