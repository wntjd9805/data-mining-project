Abstract
Many adaptations of transformers have emerged to ad-dress the single-modal vision tasks, where self-attention modules are stacked to handle input sources like images.
Intuitively, feeding multiple modalities of data to vision transformers could improve the performance, yet the inner-modal attentive weights may be diluted, which could thus greatly undermine the final performance. In this paper, we propose a multimodal token fusion method (TokenFusion), tailored for transformer-based vision tasks. To effectively fuse multiple modalities, TokenFusion dynamically detects uninformative tokens and substitute these tokens with pro-jected and aggregated inter-modal features. Residual posi-tional alignment is also adopted to enable explicit utiliza-tion of the inter-modal alignments after fusion. The design of TokenFusion allows the transformer to learn correlations among multimodal features, while the single-modal trans-former architecture remains largely intact. Extensive exper-iments are conducted on a variety of homogeneous and het-erogeneous modalities and demonstrate that TokenFusion surpasses state-of-the-art methods in three typical vision tasks: multimodal image-to-image translation, RGB-depth semantic segmentation, and 3D object detection with point cloud and images. Code will be released 1 2. 1.

Introduction
Transformer is initially widely studied in the natural lan-guage community as a non-recurrent sequence model [37] and it is soon extended to benefit vision-language tasks. Re-cently, numerous studies have further adopted transform-ers for computer vision tasks with well-adapted architec-(cid:66) Corresponding author: Fuchun Sun. 1https://github.com/huawei-noah/noah-research 2https://gitee.com/mindspore/models/tree/master/research/cv/TokenFusion tures and optimization schedules. As a result, vision trans-former variants have shown great potential in many single-modal vision tasks, such as classification [5, 19], segmenta-tion [41, 44], detection [2, 7, 20, 45], image generation [14].
Yet up until the date of this work, the attempt of extend-ing vision transformers to handle multimodal data remains scarce. When multimodal data with complicated alignment relations are introduced, it poses great challenges in design-ing the fusion scheme for model architectures. The key question to answer is how and where the interaction of fea-tures from different modalities should take place. There have been a few methods for transformer-based vision-language fusion, e.g., VL-BERT [35] and ViLT [15].
In these methods, vision and language tokens are directly con-catenated before each transformer layer, making the overall architecture very similar to the original transformer. Such fusion is actually alignment-agnostic, which means the inter-modal alignments are not explicitly utilized. We also try to apply such intuitive fusion methods on multimodal vision tasks (Sec. 4). Unfortunately, this intuitive trans-former fusion cannot bring promising gains or may even result in worse performance than the single-modal counter-part, which is mainly due to the fact that the inter-modal interaction is not fully exploited. There are also several at-tempts for fusing multiple vision modalities. For example,
TransFuser [24] leverages transformer modules to connect
CNN backbones of images and LiDAR points. However, these methods still neglect to find an effective and general method to insert inter-modal alignments into transformers.
We aim to benefit the learning process by multimodal data while also leveraging inter-modal alignments, which are naturally available in many vision tasks, e.g., with cam-era intrinsics/extrinsics, a world-space point can be pro-jected and correspond to a pixel on the camera plane. Un-like the alignment-agnostic fusion (will be described in
Sec. 3.1), the alignment-aware fusion explicitly involves
the alignment relations of different modalities. Yet, since inter-modal projections are introduced to the transformer, alignment-aware fusion may greatly alter the original model structure and data flow, which potentially undermines the success of single-modal architecture designs or learned at-tention during pretraining. Thus, one may have to determine the “correct” layers/tokens/channels for multimodal projec-tion and fusion, and also re-design the architecture or re-tune optimization settings for the new model. To avoid deal-ing with these challenging matters and inherit the majority of the original single-modal design, we propose multimodal token fusion, termed TokenFusion, which adaptively and effectively fuses multiple single-modal transformers.
The basic idea of our TokenFusion is to prune multiple single-modal transformers and then re-utilize pruned units for multimodal fusion. We apply individual pruning to each single-modal transformer and each pruned unit is substi-tuted by projected alignment features from other modalities.
This fusion scheme is assumed to have a limited impact on the original single-modal transformers, as it maintains the relative attention relations of the important units. TokenFu-sion also turns out to be superior in allowing multimodal transformers to inherit the parameters from single-modal pretraining, e.g., on ImageNet.
To demonstrate the advantage of the proposed method, we consider extensive tasks including multimodal image translation, RGB-depth semantic segmentation, and 3D ob-ject detection based on images and point clouds, covering up to four public datasets and seven different modalities.
TokenFusion obtains state-of-the-art performance on these extensive tasks, demonstrating its great effectiveness and generality. Specifically, TokenFusion achieves 64.9% and 70.8% mAP@0.25 for 3D object detection on the challeng-ing SUN RGB-D and ScanNetV2 benchmarks, respectively. 2.