Abstract
Despite the large augmentation family, only a few cherry-picked robust augmentation policies are beneficial to self-supervised image representation learning.
In this paper, we propose a directional self-supervised learning paradigm (DSSL), which is compatible with significantly more augmentations. Specifically, we adapt heavy augmen-tation policies after the views lightly augmented by stan-dard augmentations, to generate harder view (HV). HV usu-ally has a higher deviation from the original image than the lightly augmented standard view (SV). Unlike previous methods equally pairing all augmented views to symmet-rically maximize their similarities, DSSL treats augmented views of the same instance as a partially ordered set (with directions as SV↔SV, SV←HV), and then equips a direc-tional objective function respecting to the derived relation-ships among views. DSSL can be easily implemented with a few lines of codes and is highly flexible to popular self-supervised learning frameworks, including SimCLR, Sim-Siam, BYOL. Extensive experimental results on CIFAR and
ImageNet demonstrated that DSSL can stably improve var-ious baselines with compatibility to a wider range of aug-mentations. Code is available at: https://github. com/Yif-Yang/DSSL. 1.

Introduction
Unsupervised visual representation learning aims at learning image features without using manual semantic an-notations. Recently, self-supervised learning driven by in-stance discrimination tasks [3, 13, 22, 27, 29] or Siamese architecture [5, 12] has achieved great success in learn-ing high-quality visual features and closing the perfor-mance gap with supervised pretraining on various computer tasks. The visual embedding space of self-supervised learn-ing method is constructed by minimizing the dissimilarity among representations of variations derived from the same image, and/or increasing the distance between the represen-*Equal contribution.
†Corresponding author.
Augment
Standard Aug. w/ JigSaw(2) w/ JigSaw(4) w/ RA(1,1) w/ RA(2,1) w/ RA(2,5) w/ UA
SimSiam w/ DSSL 92.17 92.79 89.72 88.00 82.80 9.78 91.11 93.56 91.95 92.29 93.09 94.17 93.27
Figure 1. Left: Linear evaluation accuracy of SimSiam [5] on
CIFAR-10 by adding extra heavy augmentations besides the orig-inal standard augmentations. The number of grids for JigSaw(n) is n × n. RA(m, n) is the RandAugment [8] with n augmentation transformation of m magnitude. UA denotes the UniformAum-gent [19]. Right: validation accuracy of kNN classification during pre-training. Incorporating heavy augmentations on SimSiam re-sults in unstable performance even collapsing (linear evaluation on collapsed model tends to random guess). Our DSSL consistently benefits from heavy augmentations for higher performances. tations of augmented view from different images (negative pair). Image transformation, which aims to generate varia-tions from the same image [3, 5, 12, 13], plays a crucial role in the self-supervised visual representations learning.
However, these self-supervised learning methods share a common fundamental weakness: only a few carefully se-lected augmentation policies with proper settings are ben-eficial to the model training. The combination of random crop, color distortion, Gaussian blur and grayscale is cru-cial to achieving good performance, applied as the basic augmentation setting for many popular instance-wise self-supervised learning methods [2, 5, 12, 13, 17]. Here we de-fine these augmentations as the standard augmentations.
We denote the images augmented by standard augmenta-tions as the standard views. Recent works find that a few other augmentations (e.g., RandAugment, JigSaw) can fur-ther improve the performance of self-supervised learning methods rely on negative pairs [23]. However, for neg-ative pair free self-supervised learning methods like Sim-Siam [5], introducing such augmentations usually results in
Figure 2. Overview of standard self-supervised learning and our DSSL. Original standard image transformations generate the standard views, and the harder view is derived from the the standard view by applying heavy augmentation RandAugment. (a) Standard instance-wise learning with standard views. (b) Instance-wise self-supervised learning after introducing heavily augmented (harder) views. Applying symmetric loss to maximize the similarity between standard and heavily augmented views roughly expands the feature cluster in the visual embedding space. The model may confuse the instance-level identity. (c) DSSL: To prevent the adverse effect from missing information of heavily augmented views, DSSL avoids arbitrarily maximizing their visual agreement. To tighten the feature cluster, DSSL applies an asymmetric loss for only gathering each heavily augmented view to its relevant standard view. a lousy performance, even model collapsing during train-ing, as shown in Fig. 1. We name these unstable and risky data augmentation policies as heavy augmentations, since they usually largely alters the image appearance.
Inspired by previous works [5, 17] that formulating the instance-wise self-supervised learning as K-means cluster-ing of all augmented views from the same instance, we hypothesize a gold standard feature cluster for all views of one given image instance existing in the visual fea-ture embedding space, and define d as the deviation of augmented view’s feature from the core-point of its rele-vant gold standard feature cluster. Standard self-supervised learning methods treat all augmented views of the same im-age fairly to construct training pairs. As shown in Fig. 2 (a), such a strategy works well, and the model can converge sta-bly for the standard image transformations. However, after incorporating the views augmented from heavy image trans-formations (Fig. 2 (b)), two obvious risks arise. 1) Closing the representation of standard views to heavily augmented views would roughly expand the feature cluster in the em-bedding space. This would increase the difficulty of con-structing an embedding space where all instances are well-separated and also may cause unexpected confusion with other instance distributions [1, 26]. 2) Maximizing the vi-sual agreement among heavily augmented views disaccords with the “InfoMin principle” [23]. Since the mutual infor-mation among views with large d is usually low, contrasting these views leads to missing information and results in poor performance in downstream tasks.
To address this, we propose Directional Self-supervised
Learning (DSSL), a new training method for unsupervised representation learning that could stably improve the per-formance of instance-wise self-supervised learning by com-pletely applying more heavy image transformations. Fig. 2 (c) shows an illustration of DSSL. For each standard view (SV) augmented from the original robust image transforma-tions, we can generate various harder views (HV) derived from it by applying additional heavy augmentation poli-cies. These heavily augmented view has a larger d than its relevant standard view. In this way, we can treat all aug-mented views of the same image as a partially ordered set (SV↔SV, SV←HV) in terms of d. An asymmetric loss is introduced to encourage the representation of each heav-ily augmented view (HV) to be close to its relevant source standard view (SV). In this way, the feature cluster for all augmented views can be presented as non-convex, rather than the K-means convex clustering, the whole cluster is tightened. Moreover, DSSL discards the instance-wise self-supervised learning among RVs to bypass the issue of low mutual information among HVs. As a result, more augmen-tation policies can be introduced to enrich the information of the whole embedding space but keep the instances still well-separated.
DSSL is a straightforward algorithm that can be easily implemented in a few line of Pseudo code. Also, there are no additional hyper-parameter needs to be adjusted in
DSSL. We validate the effectiveness of DSSL by evaluat-ing it on several self-supervised benchmarks. In particular, on the ImageNet linear evaluation protocol, DSSL achieved stable performance improvements. All DSSL based pre-training models surpass the supervised pre-training model on the CIFAR-10 linear evaluation. Moreover, the trans-fer performance on detection and segmentation task further demonstrate the efficiency of DSSL with heavy augmenta-tions.
The main contributions are summarized as follows:
• A novel Directional Self-supervised Learning (DSSL) paradigm is proposed for unsupervised visual repre-sentation learning. We introduce a partially ordered set to organize the augmented views, and introduce an asymmetric loss for harnessing rich information from heavy augmented views.
• DSSL is easy-to-implement and applicable to various standard instance-wise self-supervised learning frame-works by introducing minor modifications without any hyper-parameters.
• DSSL stably improves over various self-supervised learning methods on standard benchmarks, even when thoroughly applying heavy image transformations that show adverse effect to previous methods. 2.