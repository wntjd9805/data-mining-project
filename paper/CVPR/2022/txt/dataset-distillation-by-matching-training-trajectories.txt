Abstract
Dataset distillation is the task of synthesizing a small dataset such that a model trained on the synthetic set will match the test accuracy of the model trained on the full dataset. In this paper, we propose a new formulation that optimizes our distilled data to guide networks to a similar state as those trained on real data across many training steps. Given a network, we train it for several iterations on our distilled data and optimize the distilled data with respect to the distance between the synthetically trained parameters and the parameters trained on real data. To efﬁciently obtain the initial and target network parameters for large-scale datasets, we pre-compute and store training trajectories of expert networks trained on the real dataset. Our method handily outperforms existing methods and also allows us to distill higher-resolution visual data. 1.

Introduction
In the seminal 2015 paper, Hinton et al. [15] proposed model distillation, which aims to distill the knowledge of a complex model into a simpler one. Dataset distillation, proposed by Wang et al. [44], is a related but orthogonal task: rather than distilling the model, the idea is to distill the dataset. As shown in Figure 2, the goal is to distill the knowledge from a large training dataset into a very small set of synthetic training images (as low as one image per class) such that training a model on the distilled data would give a similar test performance as training one on the origi-nal dataset. Dataset distillation has become a lively research topic in machine learning [2, 25, 26, 38, 45, 46, 47] with various applications, such as continual learning, neural archi-tecture search, and privacy-preserving ML. Still, the prob-lem has so far been of mainly theoretical interest, since most prior methods focus on toy datasets, like MNIST and CIFAR, while struggling on real, higher-resolution images. In this work, we present a new approach to dataset distillation that not only outperforms previous work in performance, but is also applicable to large-scale datasets, as shown in Figure 1.
Unlike classical data compression, dataset distillation aims for a small synthetic dataset that still retains adequate task-related information so that models trained on it can gen-eralize to unseen test data, as shown in Figure 2. Thus, the distilling algorithm must strike a delicate balance by heavily compressing information without completely obliterating
distilled dataset to guide the network optimization along a similar trajectory (Figure 3). We ﬁrst train a set of mod-els from scratch on the real dataset and record their expert training trajectories. We then initialize a new model with a random time step from a randomly chosen expert trajectory and train for several iterations on the synthetic dataset. Fi-nally, we penalize the distilled data based on how far this synthetically trained network deviated from the expert tra-jectory and back-propagate through the training iterations.
Essentially, we transfer the knowledge from many expert training trajectories to the distilled images.
Extensive experiments show that our method handily out-performs existing dataset distillation methods as well as coreset selection methods on standard datasets, including
CIFAR-10, CIFAR-100, and Tiny ImageNet. For example, we achieve 46.3% with a single image per class and 71.5% with 50 images per class on CIFAR-10, compared to the pre-vious state of the art (28.8% / 63.0% from [45, 46] and 36.1%
/ 46.5% from [26]). Furthermore, our method also general-izes well to larger data, allowing us to see high 128 128-resolution images distilled from ImageNet [6] for the ﬁrst time. Finally, we analyze our method through additional ab-lation studies and visualizations. Code and models are also available on our webpage.
× 2.