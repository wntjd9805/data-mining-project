Abstract
Transformer has demonstrated promising performance in many 2D vision tasks. However, it is cumbersome to compute the self-attention on large-scale point cloud data because point cloud is a long sequence and unevenly dis-tributed in 3D space. To solve this issue, existing meth-ods usually compute self-attention locally by grouping the points into clusters of the same size, or perform convolu-tional self-attention on a discretized representation. How-ever, the former results in stochastic point dropout, while the latter typically has narrow attention fields. In this pa-per, we propose a novel voxel-based architecture, namely
Voxel Set Transformer (VoxSeT), to detect 3D objects from point clouds by means of set-to-set translation. VoxSeT is built upon a voxel-based set attention (VSA) module, which reduces the self-attention in each voxel by two cross-attentions and models features in a hidden space induced by a group of latent codes. With the VSA module, VoxSeT can manage voxelized point clusters with arbitrary size in a wide range, and process them in parallel with lin-ear complexity. The proposed VoxSeT integrates the high performance of transformer with the efficiency of voxel-based model, which can be used as a good alternative to the convolutional and point-based backbones. VoxSeT reports competitive results on the KITTI and Waymo de-tection benchmarks. The source codes can be found at https://github.com/skyhehe123/VoxSeT. 1.

Introduction
Object detection from 3D point cloud has been receiving extensive attention as it empowers many applications like autonomous driving, robotics and virtual reality. Unlike 2D images, 3D point clouds are naturally sparse and unevenly distributed in continuous space, impeding the CNN layers from being directly applied. To resolve this issue, some ap-proaches [5, 10, 42, 49, 51] first transform the point cloud
*Corresponding author.
The illustrations of
Figure 1. (b) convolutional-based and (c) our proposed induced set-based atten-tion mechanisms. (a) grouping-based, into a discrete representation and then apply CNN models to extract high dimensional features. Another class of ap-proaches [27, 33, 41, 44, 45] model the point cloud in con-tinuous space, where the multi-scale features are extracted through interleaved grouping and aggregation steps.
Beyond the above two schemes, transformer-based mod-els [9, 21–23, 31, 47] have recently attracted great interest in processing point cloud data as the self-attention used in transformers is invariant to permutation and cardinality of the input components, which makes transformer an appro-priate choice for point cloud processing. The main limita-tion of transformer models, however, lies in that the self-attention computation is quadratic. Each token has to be updated by using all the other tokens from previous layers, making self-attention intractable for long sequence point clouds. Point Transformer [47] builds transformers upon a
PointNet [28] architecture, which hierarchically groups the point cloud data into different clusters and computes self-attention in each cluster. CT3D [31] presents a two-stage point cloud detector, where 3D RoIs are extracted to group the raw points in the first stage and transformers are applied to the grouped points in the second stage.
However, since the distribution of point clouds is ex-tremely uneven, the number of points in each cluster varies a lot. To enable the self-attention to run in parallel, cur-rent approaches [23, 31, 47] balance the token number in each cluster by stochastically dropping points or padding dummy points (see Figure 1(a)). This results in unsta-ble detection results and redundant computations. Besides, each operation of grouping n points to m clusters will cost
O(nm) complexity, which is relatively intensive. Alterna-tively, Voxel Transformer [21] performs self-attention on a discrete voxel grid, as depicted in Figure 1(b). It com-putes self-attention in a convolutional manner and hence is as efficient as sparse convolution with O(n) complex-ity. However, since convolutional attention is a point-wise operation, to save the memory, the attention field of the con-volutional kernel is typically small, thus hindering the voxel transformer to model long-range dependencies. It is worth mentioning that though Group-free [20] and 3DETR [22] present a promising solution by computing self-attention on a reduced set of seed points, this solution is only applicable to indoor scenes, where the point clouds are relatively dense and concentrated. Considering that the point clouds of out-door scenes are typically sparse, large-scale (e.g., > 20k), and unevenly distributed, the scale and coverage of seed points remain an issue.
To address the above issues, we introduce a voxel-based set attention (VSA) module. For each VSA, we divide the whole scene into non-overlapping 3D voxels and compute the voxel indices of the input point with instant efficiency.
We use these voxels to determine the attentive region which is analogous to the window attention in SwinTransformer
[19]. Unlike image, LiDAR has irregular structures, and the resulting attention groups have different lengths, which hinders the parallelization of the model.
Inspired by the induced set transformer [13], we assign a group of trainable “latent codes” to each voxel. These latent codes build a fixed-length bottleneck for the point cloud, through which the information from input points within the voxel can be compressed to a static hidden space. This formulation is based on the key observation that the self-attention matrix is typically low-rank, and hence we can decompose an intensive full self-attention into two consecu-tive cross-attention modules. As shown in Figure 1(c), VSA first transforms the latent codes, which serve as queries, to a hidden space by attending to the projected features, i.e., keys and values, from the input points. The transformed hidden features, which encode the context information of the input points in each voxel, are enriched by a convolu-tional feed-forward network, in which the features across voxels exchange their information in spatial domain. Af-ter that, the hidden features are attentively fused with in-put, producing output features of the input resolution. By leveraging the latent codes, the cross-attention performed in all voxels can be vectorized, making VSA a highly par-allel module. Given n d-dimensional input features and k latent codes, VSA has a complexity of O(nkd) and it can be implemented with general matrix multiplications.
With VSA, we propose a Voxel Set Transformer (VoxSeT) to detect 3D objects by learning point cloud fea-tures in a set-to-set translation process. VoxSeT is com-posed of VSA modules, MLP layers and a shallow CNN for
Birds-Eye-View (BEV) feature extraction. To verify the ef-fectiveness of the proposed model, we conduct experiments on two 3D detection benchmarks, KITTI and Waymo open dataset. VoxSeT achieves competitive performance with current state-of-the-arts.
In addition, the proposed VSA module can be seamlessly adopted into point-based detec-tors such as PointRCNN [33], and demonstrates advantages over the set abstraction module.
In summary, in this work we first invent a voxel-based set attention module, which can model long-range dependen-cies from the token cluster of arbitrary size, bypassing the limitation of current grouped-based and convolution-based attention modules. We then present a Voxel Set Transformer to learn point cloud features effectively by leveraging the superiority of transformer on large-scale sequential data.
Our work provides a novel alternative to the current con-volutional and point-based backbones for 3D point cloud data processing. 2.