Abstract
Decoupling spatiotemporal representation refers to features into decomposing the spatial and temporal dimension-independent factors. Although previous RGB-D-based motion recognition methods have achieved promis-ing performance through the tightly coupled multi-modal spatiotemporal representation, they still suffer from (i) op-timization difﬁculty under small data setting due to the tightly spatiotemporal-entangled modeling; (ii) informa-tion redundancy as it usually contains lots of marginal information that is weakly relevant to classiﬁcation; and (iii) low interaction between multi-modal spatiotemporal information caused by insufﬁcient late fusion. To allevi-ate these drawbacks, we propose to decouple and recou-ple spatiotemporal representation for RGB-D-based mo-tion recognition. Speciﬁcally, we disentangle the task of learning spatiotemporal representation into 3 sub-tasks: (1)
Learning high-quality and dimension independent features through a decoupled spatial and temporal modeling net-work. (2) Recoupling the decoupled representation to es-tablish stronger space-time dependency. (3) Introducing a Cross-modal Adaptive Posterior Fusion (CAPF) mech-anism to capture cross-modal spatiotemporal information from RGB-D data. Seamless combination of these novel designs forms a robust spatiotemporal representation and achieves better performance than state-of-the-art methods on four public motion datasets. Our code is available at https://github.com/damo-cv/MotionRGBD. 1.

Introduction
The RGB-D-based motion recognition has attracted much attention in computer vision due to its broad applica-tion scenarios such as video surveillance and human-object
*Work done during an internship at Alibaba Group.
†Corresponding author.
Spatial 
Domain (cid:28614)(cid:28633)(cid:28631)(cid:28643)(cid:28649)(cid:28644)(cid:28640)(cid:28637)(cid:28642)(cid:28635)
Temporal 
Domain (cid:28600)(cid:28615)(cid:28610) (cid:28600)(cid:28615)(cid:28610) (cid:28600)(cid:28616)(cid:28610) (cid:28600)(cid:28616)(cid:28610) (cid:28599) (cid:28597) (cid:28612) (cid:28602) (cid:28617)(cid:28642)(cid:28637)(cid:28641)(cid:28643)(cid:28632)(cid:28629)(cid:28640)(cid:28564)(cid:28602)(cid:28633)(cid:28629)(cid:28648)(cid:28649)(cid:28646)(cid:28633)(cid:28564)(cid:28615)(cid:28648)(cid:28646)(cid:28633)(cid:28629)(cid:28641) (cid:28599)(cid:28646)(cid:28643)(cid:28647)(cid:28647)(cid:28577)(cid:28641)(cid:28643)(cid:28632)(cid:28629)(cid:28640)(cid:28564)(cid:28602)(cid:28633)(cid:28629)(cid:28648)(cid:28649)(cid:28646)(cid:28633)(cid:28564)(cid:28615)(cid:28648)(cid:28646)(cid:28633)(cid:28629)(cid:28641) (cid:28615)(cid:28644)(cid:28629)(cid:28648)(cid:28637)(cid:28629)(cid:28640) (cid:28602)(cid:28633)(cid:28629)(cid:28648)(cid:28649)(cid:28646)(cid:28633) (cid:28614)(cid:28633)(cid:28631)(cid:28643)(cid:28649)(cid:28644)(cid:28640)(cid:28637)(cid:28642)(cid:28635) (cid:28611)(cid:28644)(cid:28633)(cid:28646)(cid:28629)(cid:28648)(cid:28637)(cid:28643)(cid:28642) (cid:28616)(cid:28633)(cid:28641)(cid:28644)(cid:28643)(cid:28646)(cid:28629)(cid:28640)(cid:28564)(cid:28602)(cid:28633)(cid:28629)(cid:28648)(cid:28649)(cid:28646)(cid:28633) (cid:28605)(cid:28642)(cid:28634)(cid:28643)(cid:28646)(cid:28641)(cid:28629)(cid:28648)(cid:28637)(cid:28643)(cid:28642)(cid:28564)(cid:28605)(cid:28642)(cid:28648)(cid:28633)(cid:28646)(cid:28629)(cid:28631)(cid:28648)(cid:28637)(cid:28643)(cid:28642)
Figure 1. Illustration of the proposed multi-modal spatiotempo-ral representation learning framework. The RGB-D-based motion recognition can be described as cross-modal representation inter-active learning based on decoupled and recoupled spatiotemporal information. Wherein DSN and DTN represent decoupled spatial and decoupled temporal feature learning networks, respectively;
And represents the element-wise add operation. (cid:2) interfaces. Recently, the CNN and RNN based methods greatly improve the performance of recognition on both gesture [1, 27, 37, 43, 45] and action [7, 17, 38, 41] through fully exploring the color and depth cues. Meanwhile, in-spired by the transformer scaling success in vision tasks,
Transformer-based methods [10,21] also achieve surprising results on RGB-D-based motion recognition by introducing the cross-attention module for multi-modality fusion.
Although these works make great progress, we ﬁnd they are still problematic in the following three aspects. (i) Op-timization difﬁculty exists in the case of limited RGB-D data due to the tightly spatiotmporal entangled modelling (e.g., C3D [33] and I3D [3]). (ii) Redundant informa-tion is hard to deal with in the entangled space-time space.
To address the above two issues, some decoupled networks (i.e., 2D CNN+LSTM/Transformer [19,34]) are proposed to learn the spatiotemporal independent representation. How-ever, we argue that these methods are not conducive to com-pact representation as they somewhat weaken or even de-DSN (cid:28606) (cid:28606) (cid:28602) (cid:28614) (cid:28612) (cid:28609)(cid:28629)(cid:28652)(cid:28564)(cid:28644)(cid:28643)(cid:28643)(cid:28640)(cid:28637)(cid:28642)(cid:28635) (cid:28615) (cid:28609) (cid:28615)
Input (cid:28614)(cid:28599)(cid:28609)
Recoupling
Sampling  (cid:28581) (cid:28582) (cid:28583) (cid:28584) (cid:28585) (cid:28586) (cid:28587) (cid:28588) (cid:28616) (cid:28609) (cid:28615) (cid:28616) (cid:28609) (cid:28615) (cid:28616) (cid:28609) (cid:28615) (cid:28581) (cid:28587) (cid:28582) (cid:28584) (cid:28586) (cid:28581) (cid:28583) (cid:28585) (cid:28588) (cid:28615)(cid:28641)(cid:28629)(cid:28640)(cid:28640)(cid:28564) (cid:28616)(cid:28646)(cid:28629)(cid:28642)(cid:28647)(cid:28634)(cid:28643)(cid:28646)(cid:28641)(cid:28633)(cid:28646)(cid:28564) (cid:28663) (cid:28609)(cid:28633)(cid:28632)(cid:28637)(cid:28649)(cid:28641)(cid:28564) (cid:28616)(cid:28646)(cid:28629)(cid:28642)(cid:28647)(cid:28634)(cid:28643)(cid:28646)(cid:28641)(cid:28633)(cid:28646)(cid:28564) (cid:28663) (cid:28608)(cid:28629)(cid:28646)(cid:28635)(cid:28633)(cid:28564) (cid:28616)(cid:28646)(cid:28629)(cid:28642)(cid:28647)(cid:28634)(cid:28643)(cid:28646)(cid:28641)(cid:28633)(cid:28646) (cid:28663)
DTN (cid:28640)(cid:28639)(cid:28643) (cid:28640)(cid:28639)(cid:28643) (cid:28630)(cid:28671)(cid:28660)(cid:28678)(cid:28678) (cid:28640)(cid:28639)(cid:28643) (cid:28599)(cid:28640)(cid:28629)(cid:28647)(cid:28647)(cid:28564)(cid:28616)(cid:28643)(cid:28639)(cid:28633)(cid:28642) (cid:28612)(cid:28629)(cid:28648)(cid:28631)(cid:28636)(cid:28564)(cid:28616)(cid:28643)(cid:28639)(cid:28633)(cid:28642)
Figure 2. Illustration of proposed decoupling and recoupling saptiotemporal representation learning network. The whole network mainly consists of a decoupled spatial and temporal representation learning networks namely DSN and DTN, as well as a spatiotemporal recoupling module (RCM). The FRP indicates a fast regional positioning module; SMS and TMS indicate the space- and time-centric multi-scale indicate element-wise product, 1D convolution and element-wise add operation respectively.
Inception Module respectively. (cid:3)
, and (cid:2) (cid:4) stroy the original spatiotemporal coupling structure. Con-sidering that a certain number of human action classes have strong correlations between time and space, the recoupling process after spatiotemporal decoupling is still necessary. (iii) Insufﬁcient interaction occurs between multi-modal spatiotemporal information. Several works [45, 47] adopt independent branches for unimodal spatiotemporal repre-sentations learning followed by late fusion, resulting in in-sufﬁcient cross-modal information communication. Thus, it is still a challenge to explore high quality multi-modal spatiotemporal features.
Given the aforementioned concerns, as illustrated in Fig-ure 1, we introduce a new method of multi-modal spa-tiotemporal representation learning for RGB-D-based mo-tion recognition. It mainly consists of a decoupled spatial representation learning network (DSN), a decoupled tem-poral representation learning network (DTN) and a cross-modal adaptive posterior fusion module (CAPF). For each unimodal branch, as shown in Figure 2, we propose a decoupling and recoupling spatiotemporal feature learn-ing method, wherein a spatiotemporal recoupling module (RCM) is designed as a bonding of DSN and DTN. RCM acts as feature selection for DSN and knowledge integra-tion for DTN. The entire framework can be decomposed into 3 steps: (1) Spatiotemporal decoupling learning. In the DSN, the video clips are ﬁrst fed into stack of inception-based spatial multi-scale features learning (SMS) modules to extract hierarchical spatial features. Meanwhile, they are also input into a bypath network, called fast regional po-sitioning module (FRP), to generate visual guidance map, which guides the network to focus on local important ar-eas in the video frame. Then the integrated spatial features from SMS and FRP are fed into RCM for feature selec-tion. After that, we sample several sub-sequences at dif-ferent frame rates from the enhanced spatial features as in-put to the DTN. The DTN is conﬁgured as a multi-branch structure with an inception-based temporal multi-scale layer (TMS) and multiple Transformer blocks for hierarchical lo-cal ﬁne-grained and global coarse-grained temporal feature learning. (2) Spatiotemporal recoupling learning. To rebuild the space-time interdependence, a self-distillation-based recoupling strategy is developed. As shown by the red dashed line in Figure 2, the recoupling method is designed as an inner loop optimization mechanism to distill the inter-frame correlations from time domain into the space domain, to enhance the quality of the spatial features via RCM. (3)
Cross-modal interactive learning. For multi-modal repre-sentation learning from RGB-D data, as shown in Figure 1, we propose an interactive cross-modal spatiotemporal rep-resentation learning method. Speciﬁcally, the cross-modal spatial features derived from unimodal branches ﬁrstly in-teract at the spatial level and are mapped to a joint spatial representation. Then it is separately integrated with the two unimodal spatial feature streams through the residual struc-ture. After that, the two spatial feature streams are input into their respective temporal modeling networks to capture temporal features. Similar to the spatial feature interaction, a joint temporal representation can also be obtained through interaction at the temporal level. Combined with the joint temporal representation, the two temporal feature streams are fed into the CAPF, which is based on a multi-loss joint optimization mechanism, to conduct deep multi-modal rep-resentation fusion.
Through the above design, our method not only effec-tively achieves the spatiotemporal information decoupling and recoupling learning within each modality, but also re-alizes the deep communication and fusion of multi-modal spatiotemporal information. The proposed method achieves state-of-the-art performance on four public RGB+D ges-ture/action datasets, namely NvGesture [26], Chalearn
IsoGD [36], THU-READ [32], and NTU-RGBD [28]. 2.