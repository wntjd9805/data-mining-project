Abstract
Humans can easily recognize where and how the sound is produced via watching a scene and listening to correspond-ing audio cues. To achieve such cross-modal perception on machines, existing methods only use the maps generated by interpolation operations to localize the sound source.
As semantic object-level localization is more attractive for potential practical applications, we argue that these exist-ing map-based approaches only provide a coarse-grained and indirect description of the sound source.
In this pa-per, we advocate a novel proposal-based paradigm that can directly perform semantic object-level localization, without any manual annotations. We incorporate the global re-sponse map as an unsupervised spatial constraint to weight the proposals according to how well they cover the esti-mated global shape of the sound source. As a result, our proposal-based sound source localization can be cast into a simpler Multiple Instance Learning (MIL) problem by filter-ing those instances corresponding to large sound-unrelated regions. Our method achieves state-of-the-art (SOTA) per-formance when compared to several baselines on multiple datasets. 1.

Introduction
In human multi-modal perception [35], the natural co-occurrence of audio-visual events in time provides potential cues for better perception [4]. Such temporal co-occurrence comes from the fact that “Sound is produced by the vibra-tion of objects”. By such inherent and universal correspon-dence, we can distinguish and associate diverse visual ap-pearances from their produced sounds, i.e., the sound source can be visually localized and detected.
Machine intelligence should also have the human-like ability of sound source localization in visual scenes, i.e., the
*Corresponding author
†This research was supported by ANR ML3RI (19-CE33-0008-01).
Figure 1. Map-based paradigm uses the maps (Fig.(a) and Fig.(c)) generated by interpolation operations to perform pseudo pixel-level localization or indirect detection by a post-processing step.
The typical issues of such maps: 1) tend to highlight the most discriminative regions; 2) may encounter a pathological bias, e.g., rail for “train” and 3) it is non-trivial to choose an optimal thresh-old since different thresholds T can result in the bounding boxes with different sizes and locations, which are marked in green and pink.
In this paper, we advocate a novel proposal-based paradigm, which directly performs semantic object-level localiza-tion of class-agnostic sound source (Fig.(b) and Fig.(d)). sound source can be localized once they appear in the im-ages. Such ability can play an important role in many prac-tical applications. For example, the rescue robots can find people who are calling for help after a disaster [42], and the sound can help photographers focus better, since the sound source is usually more interesting subjects in photos [19].
For this purpose, some works [29, 38, 44] exploit the attention map to localize the sound source in a weakly-supervised manner. Other methods modify the CNN frame-works to generate Class Activation Maps [48] for sound source localization. Nevertheless, these methods can only perform class-specific, rather than class-agnostic localiza-tion, owing to the usage of image-level annotations. Col-lecting these manual annotations is not only labor-intensive and time-consuming, but also often subjective and error-prone. As a promising approach to solve this problem, self-supervised learning [25] was introduced into this task for its excellent data efficiency and generalization ability.
For example, some works [1, 6, 20, 21, 31, 47] localize the class-agnostic sound source through the confidence score map learnt from massive unlabeled videos.
Such maps are not only an effective way to explain the network’s decision, but also are utilized to localize the sound source. As shown in Fig.1(a), the long-standing weakness for these map-based paradigm is that it often results in highlighting the most discriminative parts, and thereby covering partial regions rather than the entire extent of objects. Moreover, map-based paradigm may encounter a pathological bias in the training samples when the fore-ground objects incidentally always correlate with the same background object, e.g., the rail for “train” in Fig.1(c).
Furthermore, an up-sampling operation (i.e., interpola-tion function) is required to generate such maps, and in-troduces numerous uncertainties. We argue that the maps generated in map-based paradigm not only perform coarse-grained (pseudo pixel-level) localization, but also are too smooth to indicate the accurate boundaries of the sound source. For practical applications, the semantic object-level localization of the sound source is more attractive. Al-though the bounding box of the sound source can also be inferred from such maps by a post-processing step with fixed thresholds, as shown in the green and pink bounding boxes in Fig.1(a) and Fig.1(c), it is non-trivial to choose an optimal threshold, since different thresholds can result in bounding boxes with different sizes and locations.
Despite the fact that visual object detection [10, 22, 24, 50] has been widely studied and has achieved promising re-sults, these current prevailing object detection frameworks cannot be utilized for object-level sound source localization.
On the one hand, this task involves both audio and vision rather than only vision. On the other hand, all objects in videos could produce sound, including the objects whose annotations are difficult to obtain due to their large varia-tions. In other words, sound source localization must be ad-dressed in a class-agnostic fashion, and therefore location-, frame- or even video-level annotations are extremely diffi-cult to obtain, if even possible at all.
In this paper, we advocate for a paradigm shift on sound source localization and propose a novel proposal-based method, which performs semantic object-level localization of class-agnostic sound source. Mainly, we aspire to make the next logical step in this task. Motivated by the great success of Weakly-Supervised Object Detection (WSOD)
[28,36,46], we pose the proposal-based sound source local-ization problem as a Multiple Instance Learning (MIL) [11] problem, where the sound source detectors (instance clas-sifiers) are put into the network as hidden nodes to learn in an end-to-end manner. The audio information, viewed as a “free” source of weak annotations, is introduced into the traditional WSOD framework to establish associations be-tween visible objects and corresponding audio contents.
In our setting, the temporal co-occurrence of audio-visual signals is employed as the only available supervision.
In such weak supervision, only considering the features of local instances without looking at a global scope of all in-stances may make our model difficult to train. Furthermore, there are a large number of sound-unrelated objects in the instances, which introduces distractions during training. To this end, we incorporate the Global Response Map (GRM), which reveals the coarse location of the sound source and provides information about its global shape, as an unsu-pervised spatial constraint to weight the local instances ac-cording to how well they cover the estimated GRM. Our motivation is that, although some instances only capture the sound source partially, the instances having high spatial overlap with the GRM may cover the entire sound source, or at least contain a larger portion.
Intuitively, such pro-posals only cover a partial region of the entire frame and ignore the backgrounds where no objects exist, making our model pay more attention to the regions where the sound source maybe exists since the sound source cannot exist in the backgrounds.
Our main contributions can be summarized as follows:
• A novel proposal-based solution for sound source lo-calization, which directly performs semantic object-level localization;
• An unsupervised spatial constraint is introduced to build an effective three-stream framework, which sim-plifies our problem and improves training efficiency;
• Alleviating the shortcomings of map-based methods, resulting in SOTA performance. 2.