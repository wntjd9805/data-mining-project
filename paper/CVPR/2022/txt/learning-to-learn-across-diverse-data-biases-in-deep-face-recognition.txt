Abstract
Head class Tailed class
Long-Tail
Convolutional Neural Networks have achieved remark-able success in face recognition, in part due to the abun-dant availability of data. However, the data used for train-ing CNNs is often imbalanced. Prior works largely focus on the long-tailed nature of face datasets in data volume per identity, or focus on single bias variation. In this pa-per, we show that many bias variations such as ethnicity, head pose, occlusion and blur can jointly affect the accu-racy signiﬁcantly. We propose a sample level weighting approach termed Multi-variation Cosine Margin (MvCoM), to simultaneously consider the multiple variation factors, which orthogonally enhances the face recognition losses to incorporate the importance of training samples. Further, we leverage a learning to learn approach, guided by a held-out meta learning set and use an additive modeling to predict the
MvCoM. Extensive experiments on challenging face recogni-tion benchmarks demonstrate the advantages of our method in jointly handling imbalances due to multiple variations. 1.

Introduction
Deep face recognition has achieved remarkable progress
[4, 6, 27, 39, 42, 50, 56], with strong results on public bench-marks [19, 57]. However, real-world data distributions are usually long-tailed, whereby a method trained with uniform sampling over the imbalanced training data leads to degraded accuracy. Since it is impractical to collect data that sufﬁ-ciently covers a wide variety of the imbalance factors, there is a pressing need to develop training methods that can miti-gate dataset bias along multiple factors of variations.
In current literature, long-tailed or imbalanced data distri-bution is usually analyzed in terms of per-class data volume, or a single bias factor such as ethnicity [10, 11, 44, 52] or head pose [29, 35, 60, 64]. Previous approaches distinguish long-tailed classes (minority in samples) from head classes
*This work was conducted as part of a summer internship at NEC Labs
America.
ID 1
Per-class Data Volume
ID 3
ID 2
Head Pose
Ethnicity (cid:2)(cid:1) (cid:1)(cid:1) (cid:3)(cid:1)
Figure 1. While traditional methods only consider per-class data volume or single bias factor for long-tailed effects, multiple bias factors such as head pose and ethnicity jointly manifest as long-tailed effects in MS-Celeb-1M [14]. Further, samples from the same identity can show different variations – for example, images from ID1 show both frontal and proﬁle poses – indicating that accounting for identity or class-level variation is not sufﬁcient.
Hence, our MvCoM aims at explicitly modeling the sample-level multiple long-tailed variations jointly for face recognition. (majority in samples) to mitigate the bias. However, we observe that there usually exist more than one bias variation factors. As shown in Fig. 1, several bias factors jointly in-ﬂuence the overall data distribution. We hypothesize that dealing with such multiple factors of imbalance results in a feature space that allows better test-time generalization.
Moreover, recent methods focus on class-level imbalance, where samples within the same class share the same impor-tance [2, 21, 37]. This is limited in practice, as different images from the same person would likely differ in their importance (e.g., frontal and proﬁle views). Some other methods [20, 54] compensate the loss with the sample hard-ness, which is general but cannot attribute the hardness to any of the concrete variation factor. Thus, we hypothesize considering sample-level variation instead of class-level, and explicitly model each of the variation factor into the loss design.
To handle data imbalance, classical methods [2, 16, 41] introduce re-weighted loss functions by assigning higher
loss weights to long-tailed classes and lower weights to head classes. Cao et al. [2] mentioned that “label-distribution-aware” re-weighting approaches are advantageous in com-putational efﬁciency. However, the assigned weights are usually either ﬁxed based on prior statistics or obtained by sophisticated design choices [5]. We seek a more adaptive re-weighting method that can count for per-sample variation regarding multiple variation factors while potentially sacri-ﬁcing certain training efﬁciency. Meta-learning [9] is such an adaptive differentiable mechanism to iteratively learn sample-level importance and further contribute to recogni-tion model update. It allows a plug-in mechanism to many recognition losses such as Cosine Loss [50].
Speciﬁcally, our proposed framework deals with head pose, ethnicity, blur and occlusion as multiple factors of variation that cause data imbalances, besides per-class data volume. First, we show that the weighted identiﬁcation loss which is commonly used in re-weighting methods [21, 37], is equivalent to a learnable margin built into the cosine loss (Sec. 3.1). Thereby, we represent each imbalance factor through a corresponding learnable margin. Second, we pro-pose an additive framework to indicate a sample’s variation importance using the class volume margin as prior, together with residuals as other variations (Sec. 3.2). With the care-fully designed sample-level margin, we orthogonally equip it with cosine loss or its variants, termed Multi-variation
Cosine Margin (MvCoM). During training, the proposed
MvCoM controls the contribution of each instance in the loss function by assigning its dedicated margin considering all imbalance factors. To realize a meta-learning framework for MvCoM update, we introduce four variation classiﬁers corresponding to the four variation factors. By hard sample mining over a held-out meta-learning set (no identity over-lap with the training set, and its identity data is not used in updating recognition model) to select the samples that are most variation-different from the current training batch, we meta-learn the MvCoM and feedback to the recognition loss update (Sec. 3.2.2). Fig. 2 summarizes our approach.
In the experiments, our method empirically achieve con-sistently better performance across ﬁve challenging datasets highlighting all the long-tailed variations such as occlusion, head pose, blur and ethnicity. Moreover, we ﬁnd that the proposed MvCoM can be equipped with many backbones such as CosFace and URFace (see Table 3), demonstrating the wide applicability to face recognition platforms. We also visualize the learned sampling importance alongside all those long-tailed variations in Fig. 3 and verify that our Mv-CoM indeed assigns signiﬁcant weights to those long-tailed factors which leads to overall smaller loss.
Our technical contributions are thus concluded:
• To our best knowledge, we are the primary several to ex-plicitly model multiple long-tailed variation factors, such as ethnicity, pose and occlusion, in an additive formulation within a single framework for face recognition.
• We move beyond class-level imbalance to propose a novel sample-level Multi-variation Cosine Margin (MvCoM) that better compensates distribution imbalance from mul-tiple factors.
• We propose a meta-learning based differentiable mecha-nism to adaptively learn the proposed MvCoM, enabling an end-to-end uniﬁed recognition training scheme.
• Extensive experiments on both controlled and challenging benchmarks show that our method can better mitigate distribution imbalances to outperform prior methods. 2.