Abstract
Federated learning (FL) has attracted growing atten-tions via data-private collaborative training on decentral-ized clients. However, most existing methods unrealistically assume object classes of the overall framework are fixed over time. It makes the global model suffer from significant catastrophic forgetting on old classes in real-world scenar-ios, where local clients often collect new classes continu-ously and have very limited storage memory to store old classes. Moreover, new clients with unseen new classes may participate in the FL training, further aggravating the catastrophic forgetting of global model. To address these challenges, we develop a novel Global-Local Forgetting to learn a global class-Compensation (GLFC) model, incremental model for alleviating the catastrophic forget-ting from both local and global perspectives. Specifically, to address local forgetting caused by class imbalance at the local clients, we design a class-aware gradient com-pensation loss and a class-semantic relation distillation loss to balance the forgetting of old classes and distill consistent inter-class relations across tasks. To tackle the global forgetting brought by the non-i.i.d class imbalance across clients, we propose a proxy server that selects the best old global model to assist the local relation distilla-tion. Moreover, a prototype gradient-based communication mechanism is developed to protect the privacy. Our model outperforms state-of-the-art methods by 4.4%∼15.1% in terms of average accuracy on representative benchmark datasets. The code is available at https://github. com/conditionWang/FCIL. 1.

Introduction
Federated learning (FL) [4, 18, 42, 46] enables multiple local clients to collaboratively learn a global model while
*Equal contributions (ordered alphabetically). †Corresponding authors. providing secure privacy protection for local clients. It suc-cessfully addresses the data island challenge without com-pletely compromising clients’ privacy [12, 22]. Recently, it has attracted significant interests in academia and achieved remarkable successes in various industrial applications, e.g., autonomous driving [39], wearable devices [33], medical diagnosis [10, 52] and mobile phones [36].
Generally, most existing FL methods [16, 42, 46, 52] are modeled in a static application scenario, where data classes of the overall FL framework are fixed and known in ad-vance. However, real-world applications are often dynamic, where local clients receive the data of new classes in an on-line manner. To handle such a setting, existing FL methods typically require storing all training data of old classes at the local clients’ side so that a global model can be obtained via FL, however the high storage and computation overhead may render the FL unrealistic when new classes arrive dy-namically [33, 36, 47, 52]. And if these methods [42, 52] are required to learn new classes continuously with very limited storage memory, they may suffer from significant perfor-mance degradation (i.e., catastrophic forgetting [20,37,40]) on old classes. Moreover, in real-world scenarios, new lo-cal clients that collect the data of new classes in a streaming manner may want to participate in the FL training, which could further exacerbate the catastrophic forgetting on old classes in the global model training.
To address these practical scenarios, we consider a chal-lenging FL problem named Federated Class-Incremental
Learning (FCIL) in this work.
In the FCIL setting, each local client collects the training data continuously with its own preference, while new clients with unseen new classes could join in the FL training at any time. More specifically, the data distributions of the collected classes across the cur-rent and newly-added clients are non-independent and iden-tically distributed (non-i.i.d.). FCIL requires these local clients to collaboratively train a global model to learn new classes continuously, with constraints on privacy preserva-tion and limited memory storage [37, 49]. To better com-prehend the FCIL problem, we here use COVID-19 diag-nosis among different hospitals as a possible example [6].
Imagine that before the pandemic, there could be hundreds of hospitals working collaboratively to train a global infec-tious disease diagnosis model via FL. Due to the sudden emergence of COVID-19, these hospitals will collect a large amount of new data related to COVID-19 and add them into the FL training as new classes. Moreover, new hospitals whose main focus is not infectious diseases may join the fight against COVID-19, where they have little data of the old infectious diseases, and all hospitals should learn to di-agnose the old diseases and new COVID-19 variants.
In such scenarios, most existing FL methods will likely suffer from catastrophic forgetting on old diseases diagnosis under the sudden emergence of new COVID-19 variants data.
An intuitive way to address new classes (e.g., learning new COVID-19 variants) continuously in the FCIL setting is to simply integrate FL [4, 32, 42] and class-incremental learning (CIL) [17, 37, 50] together. However, such strat-egy needs the central server to know when and where the data of new classes arrives (privacy-sensitive information), which violates the requirement of privacy preservation in
FL. In addition, although local clients can utilize conven-tional CIL [17, 37] to address their local catastrophic for-getting, the non-i.i.d. class imbalance across clients may still cause heterogeneous forgetting on different clients, and this simple integration strategy could further exacerbate lo-cal catastrophic forgetting due to the heterogeneous global catastrophic forgetting on old classes across clients.
To tackle these challenges in FCIL, we propose a novel
Global-Local Forgetting Compensation (GLFC) model in this paper, which effectively addresses local catastrophic forgetting occurred on local clients and global catastrophic forgetting across clients. Specifically, we design a class-aware gradient compensation loss to alleviate the local for-getting brought by the class imbalance at the local clients via balancing the forgetting of different old classes, and propose a class-semantic relation distillation loss to distill consistent inter-class relations across different incremen-tal tasks. To overcome the global catastrophic forgetting caused by the non-i.i.d. class imbalance across clients, we design a proxy server to select the best old global model for the class-semantic relation distillation at the local side.
Considering the privacy preservation, the proxy server col-lects perturbed prototype samples of new classes from lo-cal clients via a prototype gradient-based communication mechanism, and then utilizes them to monitor the perfor-mance of the global model for selecting the best one. Our model achieves 4.4%∼15.1% improvement in terms of av-erage accuracy on several benchmark datasets, when com-pared with a variety of baseline methods. The major contri-butions of this paper are summarized as follows:
• We address a practical FL problem, namely Federated
Class-Incremental Learning (FCIL), in which the main challenges are to alleviate the catastrophic forgetting on old classes brought by the class imbalance at the local clients and the non-i.i.d class imbalance across clients.
• We develop a novel Global-Local Forgetting Compensa-tion (GLFC) model to tackle the FCIL problem, alleviat-ing both local and global catastrophic forgetting. To our best knowledge, this is the first attempt to learn a global class-incremental model in the FL settings.
• We design a class-aware gradient compensation loss and a class-semantic relation distillation loss to address local forgetting, by balancing the forgetting of old classes and capturing consistent inter-class relations across tasks.
• We design a proxy server to select the best old model for class-semantic relation distillation on the local clients to compensate global forgetting, and we protect the com-munication between this proxy server and clients with a prototype gradient-based mechanism for privacy. 2.