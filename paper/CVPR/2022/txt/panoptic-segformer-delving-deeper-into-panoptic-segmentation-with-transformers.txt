Abstract
Panoptic segmentation involves a combination of joint semantic segmentation and instance segmentation, where image contents are divided into two types: things and stuff. We present Panoptic SegFormer, a general framework for panoptic segmentation with transformers.
It contains three innovative components: an efficient deeply-supervised mask decoder, a query decoupling strategy, and an im-proved post-processing method. We also use Deformable
DETR to efficiently process multi-scale features, which is a fast and efficient version of DETR. Specifically, we su-pervise the attention modules in the mask decoder in a layer-wise manner. This deep supervision strategy lets the attention modules quickly focus on meaningful semantic regions.
It improves performance and reduces the num-ber of required training epochs by half compared to De-formable DETR. Our query decoupling strategy decouples the responsibilities of the query set and avoids mutual in-terference between things and stuff. In addition, our post-processing strategy improves performance without addi-tional costs by jointly considering classification and seg-mentation qualities to resolve conflicting mask overlaps.
Our approach increases the accuracy 6.2% PQ over the baseline DETR model. Panoptic SegFormer achieves state-of-the-art results on COCO test-dev with 56.2% PQ. It also shows stronger zero-shot robustness over existing methods. 1.

Introduction
Semantic segmentation and instance segmentation are two important and related vision tasks. Their underlying connections recently motivated panoptic segmentation as a unification of both the tasks [6]. In panoptic segmentation, image contents are divided into two types: things and stuff.
Things refer to countable instances (e.g., person, car) and each instance has a unique id to distinguish it from the other
PQ (%) #Param (M)
DETR-R50 [1]
Max-Deeplab-S [2]
Max-Deeplab-L [2]
MaskFormer-T [3]
MaskFormer-B [3]
MaskFormer-L [3]
K-Net-L [4]
Panoptic SegFormer-B0
Panoptic SegFormer-B2
Panoptic SegFormer-B5 43.5 48.4 51.1 47.4 51.1 52.7 54.6 49.5 52.5 55.4 42.8 62.0 451.0 42.0 102.0 212.0 208.9 24.2 43.6 104.9
Figure 1. Comparison to the prior arts in panoptic segmentation methods on the COCO val2017 split. Panoptic SegFormer models outperform the other counterparts among different models. Panop-tic SegFormer (PVTv2-B5 [5]) achieves 55.4% PQ, surpassing previous methods with significantly fewer parameters. instances. Stuff refers to the amorphous and uncountable regions (e.g., sky, grassland) and has no instance id [6].
Recent works [1–3] attempt to employ transformers to handle both things and stuff through a query set. For exam-ple, DETR [1] simplifies the workflow of panoptic segmen-tation by adding a panoptic head on top of an end-to-end object detector. Unlike previous methods [6,7], DETR does not require additional handcrafted pipelines [8, 9]. While being simple, DETR also causes some issues: (1) It requires a lengthy training process to converge; (2) Because the com-putational complexity of self-attention is squared with the length of the input sequence, the feature resolution of DETR is limited. So that it uses an FPN-style [1,10] panoptic head to generate masks, which always suffer low-fidelity bound-aries; (3) It handles things and stuff equally, yet represent-ing them with bounding boxes, which may be suboptimal for stuff [2, 3]. Although DETR achieves excellent perfor-mance on the object detection task, its superiority on panop-tic segmentation has not been well demonstrated. In order
Figure 2. Overview of Panoptic SegFormer. Panoptic SegFormer is composed of backbone, encoder, and decoder. The backbone and the encoder output and refine multi-scale features. Inputs of the location decoder are Nth thing queries and the multi-scale features. We feed Nth thing queries from the location decoder and Nst stuff queries to the mask decoder. The location decoder aims to learn reference points of queries, and the mask decoder predicts the final category and mask. Details of the decoder will be introduced below. We use a mask-wise merging method instead of the commonly used pixel-wise argmax method to perform inference. to overcome the defects of DETR on panoptic segmenta-tion, we propose a series of novel and effective strategies that improve the performance of transformer-based panop-tic segmentation models by a large margin.
Our approach. In this work, we propose Panoptic Seg-Former, a concise and effective framework for panoptic seg-mentation with transformers. Our framework design is mo-tivated by the following observations: 1) Deep supervision matters in learning high-qualities discriminative attention representations in the mask decoder. 2) Treating things and stuff with the same recipe [1] is suboptimal due to the differ-ent properties between things and stuff [6]. 3) Commonly used post-processing such as pixel-wise argmax [1–3] tends to generate false-positive results due to extreme anoma-lies. We overcome these challenges in Panoptic SegFormer framework as follows:
• We propose a mask decoder that utilizes multi-scale at-tention maps to generate high-fidelity masks. The mask decoder is deeply-supervised, promoting discriminative attention representations in the intermediate layers with better mask qualities and faster convergence.
• We propose a query decoupling strategy that decomposes the query set into a thing query set to match things via bipartite matching and another stuff query set to process stuff with class-fixed assign. This strategy avoids mutual interference between things and stuff within each query and significantly improves the qualities of stuff segmenta-tion. Kindly refer to Sec. 3.3.1 and Fig. 3 for more details.
• We propose an improved post-processing method to gen-erate results in panoptic format. Besides being more effi-cient than the widely used pixel-wise argmax method, our method contains a mask-wise merging strategy that con-siders both classification probability and predicted mask qualities. Our post-processing method alone renders a 1.3% PQ improvement to DETR [1].
We conduct extensive experiments on COCO [11] dataset. As shown in Fig. 1, Panoptic SegFormer signif-icantly surpasses priors arts such as MaskFormer [3] and
K-Net [4] with much fewer parameters. With deformable attention [12] and our deeply-supervised mask decoder, our method requires much fewer training epochs than previous transformer-based methods (24 vs. 300+) [1, 3]. In addi-tion, our approach also achieves competitive performance with current methods [13, 14] on the instance segmentation task. 2.