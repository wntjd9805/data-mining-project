Abstract
Biological intelligence systems of animals perceive the world by integrating information in different modalities and
In contrast, processing simultaneously for various tasks. current machine learning research follows a task-specific paradigm, leading to inefficient collaboration between tasks and high marginal costs of developing perception models for new tasks. In this paper, we present a generic perception architecture named Uni-Perceiver, which processes a vari-ety of modalities and tasks with unified modeling and shared parameters. Specifically, Uni-Perceiver encodes different task inputs and targets from arbitrary modalities into a uni-fied representation space with a modality-agnostic Trans-former encoder and lightweight modality-specific tokeniz-ers. Different perception tasks are modeled as the same formulation, that is, finding the maximum likelihood target for each input through the similarity of their representa-tions. The model is pre-trained on several uni-modal and multi-modal tasks, and evaluated on a variety of down-stream tasks, including novel tasks that did not appear in the pre-training stage. Results show that our pre-trained model without any tuning can achieve reasonable perfor-mance even on novel tasks. The performance can be im-proved to a level close to state-of-the-art methods by con-ducting prompt tuning on 1% of downstream task data.
Full-data fine-tuning further delivers results on par with or better than state-of-the-art results. Code and pre-trained weights shall be released. 1.

Introduction
Biological intelligence systems of animals perceive the world by receiving information in different modalities, inte-*Equal contribution. â€ This work is done when Jinguo Zhu, Hao Li, and
Xiaoshi Wu are interns at SenseTime Research. (cid:66)Corresponding author.
Figure 1. Comparing previous task-specific perception models with our proposed Uni-Perceiver, which processes various modal-ities and tasks with a single siamese model and shared parameters. grating with the complex central nervous system, and pro-cessing simultaneously for different tasks. However, de-signing a generic artificial perception model that handles multiple modalities and numerous tasks has always been considered too difficult. To simplify this problem, pre-vious machine learning research has focused on develop-ing specialized models for inputs from certain restricted modality, e.g., Convolutional Neural Networks [45] for vi-sual recognition and Transformers [80] for natural language processing. Recently, Transformers have been proved to have competitive performance in more scenarios such as image [10, 20, 51, 76, 78, 82, 84, 90] and video [4, 6, 87] recognition, which triggers a new paradigm of designing unified architectures for different modalities. Following this paradigm, recent works [1, 27, 33, 64] adopt Transform-ers as the backbone for multi-modal applications such as
visual-linguistic recognition. They convert the inputs from different modalities into unified input token sequences with modality-specific tokenizers. Models are pre-trained with large-scale multi-modal datasets, and then adapted to down-stream tasks with fine-tuning.
We argue that
Despite the ability of processing multi-modal informa-tion with unified architectures, current methods still require specific design and training for different tasks. This limita-tion is caused by two reasons. First, the input of a particular model is the combination of specific modalities required by its target task. Second, previous works require prediction heads specifically designed and trained for the target tasks. this task-specific paradigm conflicts with the objective of designing generic perceptual mod-els. Specifically, during pre-training, the specialised de-signs for different tasks hinder the collaboration between tasks, which may hurt the representational capacity. Mean-while, when a pre-trained model is applied to a new task, the input format and the prediction head need to be re-designed and fine-tuned on sufficient downstream data. Considerable effort in collecting and annotating data is required. Also, all parameters need to be copied and maintained for each downstream task, which becomes inefficient and inconve-nient as the number of tasks and the model size grow. On the other hand, when fine-tuning is performed with insufficient training data, it may forget the pre-trained knowledge that is beneficial to the downstream task, thereby hurting gen-eralization performance [14]. All of these issues increase the marginal cost of developing perception models for new tasks and limit the capability to meet the rapidly grow-ing demands of diverse scenarios, indicating task-specific paradigm is not suitable for generic perceptual modeling.
Our core idea is to replace task-specific designs by encoding different task inputs and targets from arbitrary modalities into a unified representation space, and model the joint probability of inputs and targets through the sim-ilarity of their representations. This design eliminates the gap between the formulations of different perception tasks, and therefore encourages the collaboration between differ-ent modalities and tasks in representation learning. More-over, by aligning the formulations of pre-training and down-stream tasks, the knowledge can be better transferred when applying the pre-trained model to the target tasks. The model can even conduct zero-shot inference on novel tasks that do not appear in the pre-training stage.
In this paper, we propose a unified architecture named
Uni-Perceiver, which processes various modalities and tasks with a single siamese model and shared parameters.
Specifically, the task inputs and targets from arbitrary com-binations of modalities are first converted into unified to-ken sequences with lightweight modality-specific tokeniz-ers. The sequences are then encoded by a modality-agnostic
Transformer encoder into a unified representation space.
Different perception tasks are modeled as the same formu-lation, finding the maximum likelihood target for each input through the similarity of their representations, so as to facil-itate the generic perceptual modeling.
Uni-Perceiver is pre-trained on various uni-modal tasks such as image / video classification and language model-ing, and multi-modal tasks such as image-text retrieval and language modeling with image clues. When applied to downstream tasks, thanks to the generic modeling of per-ception tasks, the pre-trained model shows the ability of zero-shot inference on novel tasks that did not appear in the pre-training stage. Moreover, the performance can be fur-ther boosted with additional task-specific data. For the few-shot scenario, we adapt the model to downstream tasks with prompt tuning [47], where only a small amount of addi-tional parameters are optimized for specific tasks. The per-formance of our model can be further improved with full-model fine-tuning on sufficient downstream training data.
We pre-train our model on several uni-modal and multi-modal tasks, and evaluate its performance on a variety of downstream tasks, including novel tasks that did not ap-pear in the pre-training stage. Results show that our pre-trained model without any tuning can achieve reasonable performance even on novel tasks. Its performance can be boosted to a level close to state-of-the-art methods by con-ducting prompt tuning with 1% of the downstream task data.
When fine-tuning the pre-trained model with 100% of the target data, our model achieves result on par with or better than state-of-the-art methods on almost all the tasks, which demonstrates the strong representation ability. 2.