Abstract 1.

Introduction
We present PlanarRecon – a novel framework for glob-ally coherent detection and reconstruction of 3D planes from a posed monocular video. Unlike previous works that detect planes in 2D from a single image, PlanarRecon in-crementally detects planes in 3D for each video fragment, which consists of a set of key frames, from a volumet-ric representation of the scene using neural networks. A learning-based tracking and fusion module is designed to merge planes from previous fragments to form a coherent global plane reconstruction. Such design allows Planar-Recon to integrate observations from multiple views within each fragment and temporal information across different ones, resulting in an accurate and coherent reconstruc-tion of the scene abstraction with low-polygonal geome-try. Experiments show that the proposed approach achieves state-of-the-art performances on the ScanNet dataset while being real-time. Code is available at the project page: https://neu-vi.github.io/planarrecon/.
This work was partially done when Yiming Xie was a research assis-tant at Zhejiang University. † Corresponding authors: Xiaowei Zhou and
Huaizu Jiang.
Recovering 3D planar surfaces from a posed monocular video is a critical task for many downstream applications in 3D vision, such as Augmented and Virtual Reality (AR and VR), interior modeling, and human-robot interaction.
Planar surfaces provide a compact representation and im-portant geometric cues of the 3D scene. AR, for example, to enable realistic and immersive interactions between AR effects and the surrounding physical scene, 3D plane de-tection needs to be accurate, consistent, and performed in real-time. While camera poses can be tracked accurately with state-of-the-art visual-inertial SLAM systems [1,5,30], real-time image-based 3D plane detection remains to be a challenging problem due to the low detection quality and high computation demands.
Most recent deep learning-based plane recovery works
[22, 23, 38, 42, 48] focus on the single-view case. Their pipeline typically aims to jointly segment the plane in-stances and regress the plane parameters (i.e., surface nor-mals and offsets). Despite the significant progress made in this direction using deep neural networks, because of the single-view scale ambiguity, these methods cannot deliver absolute depth estimation in the unknown scenes. More-over, fusing the plane detections from multiple views is not trivial. Jin et al. [19] extend the single-view approach [22] to sparse views (mostly 2 views), where time-consuming
energy minimization is used to fuse single-view detections.
As their design does not consider the temporal consistency, however, it is still unclear how to extend this work to video inputs, which are more natural and common vision sources for applications like AR and VR.
In the traditional 3D vision, a few attempts [2, 3, 7, 15, 34, 44, 45] have been made to recover planes from multi-view images and videos. But they usually rely on hand-crafted features [2, 3, 34] and/or strong geometric priors of the scene [7, 15, 44, 45]. In real-world scenarios, these fea-tures may be unreliable and such priors may not always hold due to the scene complexity, such as lighting condition change, textureless regions, fixtures violating the Manhat-tan world assumption [8], etc. An efficient plane recovery method that is robust to the aforementioned challenges and makes no strong assumptions of the scene is desired.
To meet this demand, we propose a novel learning-based framework, called PlanarRecon, to perform 3D plane de-tection and reconstruction in real-time from a posed monoc-ular video. The main idea of our proposed PlanarRecon is illustrated in Fig. 2.
It consists of two major compo-nents. The first component is fragment-based plane detec-tion. Given a video input, we sequentially split it into mul-tiple non-overlapping fragments. For each fragment, Pla-narRecon constructs 3D feature volumes by back-projection of the image features, which fuses information from multi-ple views. Based on the occupancy classification, for each occupied voxel, we estimate the plane parameters as well as its displacement to shift it to the geometric centroid of the plane it belongs to. Mean-shift clustering [48] is then performed to group voxels that have similar plane parame-ters and shifted positions to get plane detections in the frag-ment. The second component is plane tracking and fusion.
PlaneRecon maintains a global reconstruction of planes us-ing plane detections from all previous fragments. When a new fragment is processed, we resort to the attention mech-anism [39] to compute the similarities between the global reconstructed planes and the current detections. A differen-tiable Hungarian matching algorithm is then used to obtain the correspondences of planes. The global reconstruction is updated accordingly to ensure temporal coherence.
Our model incrementally obtains 3D plane reconstruc-tions from the input video. Thanks to its fast inference speed, the system can run in real-time, enabling more au-thentic interaction experiences with the scene for a down-stream AR application, for instance. Compared with single-image based approaches [22, 23, 38, 42, 48], PlanarRecon directly regresses planes from 3D feature volumes. It not only fuses information from multiple views, but also offers a coherent reconstruction of the scene without the scale am-biguity. On the other hand, compared with traditional multi-view reconstruction approaches, our learning-based model is more robust to scene complexity [2, 3, 34] and does not rely on the existence of certain scene priors [7, 15, 44, 45].
Experimental results on the ScanNet benchmark [10] show that PlanarRecon achieves state-of-the-art accuracy.
To summarize, our main contributions are twofold: i) We propose PlanarRecon to detect and reconstruct 3D planes from a posed monocular video. To our best knowledge,
PlanarRecon is the first learning-based approach in this di-rection. ii) We propose a novel volume-based plane recon-struction approach that can detect, track, and fuse plane in-stances, directly in 3D. Our model integrates observations from multiple frames and temporal information from the video, leading to globally coherent plane detection and re-construction. Compared with existing approaches, our ap-proach is more robust and runs significantly faster. 2.