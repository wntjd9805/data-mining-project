Abstract 1.

Introduction
In this paper, we present a general-purpose solution to cartoon image synthesis with unpaired training data.
In contrast to previous works learning pre-defined cartoon styles for specified usage scenarios (portrait or scene), we aim to train a common cartoon translator which can not only simultaneously render exaggerated anime faces and realistic cartoon scenes, but also provide flexible user con-trols for desired cartoon styles. It is challenging due to the complexity of the task and the absence of paired data. The core idea of the proposed method is to introduce gated cy-cle mapping, that utilizes a novel gated mapping unit to produce the category-specific style code and embeds this code into cycle networks to control the translation process.
For the concept of category, we classify images into differ-ent categories (e.g., 4 types: photo/cartoon portrait/scene) and learn finer-grained category translations rather than overall mappings between two domains (e.g., photo and cartoon). Furthermore, the proposed method can be eas-ily extended to cartoon video generation with an auxiliary dataset and a new adaptive style loss. Experimental results demonstrate the superiority of the proposed method over the state of the art and validate its effectiveness in the brand-new task of general cartoon image synthesis.
Cartoon is a popular art form that can be widely used in diverse scenes such as advertising, animation production, and the creation of virtual characters. Artists aim to build a vivid cartoon world in a simplified or exaggerated way based on real-world persons and scenarios. However, man-ually recreating the real world in cartoon styles is labor in-tensive and requires substantial professional skills.
Recently, inspired by the power of Generative Adver-sarial Networks (GANs) [10] in image-to-image transla-tion tasks, a series of GAN-based methods have been pro-posed to achieve photo-to-cartoon (P2C) translation. These methods can be roughly categorized as scene cartooniza-tion [6, 7, 31] and portrait cartoonization [26, 28, 33, 34], which are tailored to different use cases. For the former, the main idea is to introduce specialized losses or pre-extracted representations to sharpen edges and smooth sur-faces, thus learning an abstract conversion between photo and cartoon images. However, they are incapable of gener-ating vivid cartoon faces with exaggerated geometry trans-form, such as delicate big eyes and simplified mouths.
Portrait cartoonization methods are proposed to produce manga [28, 33, 34] or caricature [5, 26] faces with large ge-ometric changes. Yet, they heavily depend on facial char-acteristics (e.g., decomposed facial components or guided facial landmarks) and are not suitable for common scenes.
There also exist some unsupervised image-to-image trans-lation (UIT) models [17, 23, 35] or StyleGAN-based meth-ods [25, 27] that aim to handle the challenging selfie2anime task, while they either produce unsatisfactory results with missing contents or require training a model for each spe-cific style. Overall, neither P2C nor UIT is capable of pro-viding flexible user controls on cartoon styles, i.e., generat-ing cartoon images in the style of an arbitrary input exem-plar, and the portraits and scenes need to be processed via specifically designed models.
The goal of this paper is to design a general framework of cartoon image synthesis that is capable of rendering di-verse source photos with controllable cartoon styles. As shown in Figure 1, with a single trained generator, exag-gerated cartoon faces and realistic cartoon scenarios in de-sired styles (specified by input exemplars) can be simulta-neously synthesized. The challenge of this task lies in three aspects. First, no paired training data is available and the model needs to be trained in an unsupervised way. Exist-ing methods [7, 17, 31] typically utilize the cycle consis-tency to exploit unpaired data. But it is difficult to generate high-quality results due to the significant geometry changes along with texture style variation. Second, in contrast to pre-defined styles that can be straightforwardly learned by training on large-scale databases, we only have a style-mixed cartoon collection and aim to render images in an ar-bitrary style with the trained model. Third, due to different conversion requirements for portraits and scenes, multiple generators that are trained respectively for them are needed, making it a heavy architecture and thus limiting its practical usage.
To address the aforementioned challenges, we propose a simple yet effective cartoon image synthesis model with gated cycle mapping. In contrast to previous works [7, 17, 37] that forcedly learn bidirectional mappings between two domains using multiple generators (GA→B and GB→A), we design a simplified cycle network with a single generator equipped with the gated style encoder Egs. Egs utilizes a novel gated mapping unit (GMU) consisting of domain and group specific layers to produce the category-specific style code, which can be directly injected into the genera-tor to provide a target style guidance, making it easier to learn the texture style, meanwhile, enabling the network to transfer the corresponding style into a given image. For the concepts of group and category, considering the huge se-mantic discrepancy and different conversion requirements between portrait and scene images, we introduce a fine-grained category translation mechanism. All images in each domain (photo or cartoon) are further classified into two groups (portrait and scene), and only category translations within each group will be learned, aiming to ignore unrea-sonable mappings with mismatched structures. Cooperat-ing with the gated cycle networks mentioned above, we can simply use a single generator for image translation in all directions, where only the decoder part is modulated by the corresponding style codes. The proposed strategy not only achieves a common cartoon translator with signif-icantly lighter architecture, but also provide a flexible user control for desired cartoon styles. In summary, major con-tributions of this paper are threefold:
• We propose a brand-new task of synthesizing style-controllable cartoon images with a common translator for both portraits and scenes, and solve it by designing a novel gated cycle mapping network.
• We develop a gated mapping unit which utilizes the gating mechanism to learn category-specific style rep-resentations via domain and group specific layers.
• We extend the proposed method to video synthesis of cartoon portraits, leveraging an auxiliary dataset and a new adaptive style loss, which achieves stable results with the precise control of facial expressions. 2.