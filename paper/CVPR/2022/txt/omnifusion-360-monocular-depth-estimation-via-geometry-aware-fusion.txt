Abstract
A well-known challenge in applying deep-learning meth-ods to omnidirectional images is spherical distortion.
In dense regression tasks such as depth estimation, where structural details are required, using a vanilla CNN layer on the distorted 360 image results in undesired information loss. In this paper, we propose a 360 monocular depth esti-mation pipeline, OmniFusion, to tackle the spherical dis-tortion issue. Our pipeline transforms a 360 image into less-distorted perspective patches (i.e. tangent images) to obtain patch-wise predictions via CNN, and then merge the patch-wise results for final output. To handle the dis-crepancy between patch-wise predictions which is a ma-jor issue affecting the merging quality, we propose a new framework with the following key components. First, we propose a geometry-aware feature fusion mechanism that combines 3D geometric features with 2D image features to compensate for the patch-wise discrepancy. Second, we employ the self-attention-based transformer architecture to conduct a global aggregation of patch-wise information, which further improves the consistency. Last, we intro-duce an iterative depth refinement mechanism, to further refine the estimated depth based on the more accurate geo-metric features. Experiments show that our method greatly mitigates the distortion issue, and achieves state-of-the-art performances on several 360 monocular depth estimation benchmark datasets. Our code is available at https:
//github.com/yuyanli0831/OmniFusion. 1.

Introduction
A 360 image provides a comprehensive view of the scene with its wide field of view (FoV), which is beneficial in understanding the scene holistically. However, commonly used 360 image representation format such as the equirect-angular projection (ERP) image can introduce geometric distortions. The distortion factor varies in the vertical direc-tion and may degrade the performance of regular convolu-tional layers designed for non-distorted perspective images.
*Equal contribution
Figure 1. Our method, Omnifusion, produces high-quality dense depth (shown as the image on the right) from a monocular ERP in-put (shown as an image wrapped on a unit sphere on the left). Our method uses a set of N perspective patches (i.e. tangent images) to represent the ERP image (top branch), and fuse the image fea-tures with 3D geometric features (bottom branch) to improve the estimation of the merged depth map. The corresponding camera poses of the tangent images are shown in the middle row.
Many studies have been proposed to address the distortion issue. [4, 29, 39] proposed distortion-aware convolutions or spherical customized kernels. However, it remains unclear how effective such spherical convolutions are, especially in deeper layers [29,35]. Some spherical CNNs [8,37] defined convolution in the spectral domain, with potentially heavier computation overhead. Attempts have also been made to tackle the ERP distortion via other less-distorted formats.
BiFuse [35] and UniFuse [20] took complementary proper-ties from ERP and cubemap. Several works [7, 30] applied regular CNNs repeatedly to multiple perspective projections of the 360 image. Recently, Eder et al. [13] proposed to use a set of subdivided icosahedron tangent images, and demon-strated that using tangent image representation can facilitate the network transfer between perspective and 360 images.
It is advantageous to use tangent images [13] as it has less distortion, and can make good use of the large pool of pre-trained CNNs developed for perspective imaging. Ad-ditionally, the tangent image representation inherits a supe-rior scalability to handle high resolution inputs compared to those holistically method. However, the vanilla pipeline
[13] has some limitations. First, severe discrepancies occur between perspective views since the same object may ap-pear differently from multiple views (an example is shown
in Figure 3). This issue is especially problematic for the depth regression task, since the inconsistent depth scale es-timated from individual tangent images creates undesired artifacts during merging. Second, the advantage of estimat-ing depth from holistic 360 image is unfortunately lost, be-cause of the decomposition of the global scene into local tangent images. The predictions from the tangent images are independent of each other and there is no information exchange between tangent images.
In this paper, we present OmniFusion, a 360 monocu-lar depth estimation framework with geometry-aware fu-sion (see Figure 1). We proposed the following three key components to solve the aforementioned discrepancy issue and merge the depth results of tangent images seamlessly.
First, we use a geometric embedding module to provide additional features to compensate for the discrepancy be-tween 2D features from patch to patch. For each patch, we calculate the 3D points located on the spherical surface that correspond to the patch pixels, encode them and the patch center coordinate through shared Multi-layer Percep-tron (MLP), and add the geometric features to the corre-sponding 2D features. Second, to regain the holistic power in understanding the entire scene, we incorporate a self-attention-based transformer in our pipeline. With the trans-former, patch-wise information is globally aggregated to en-hance the estimation of the global scale of depth, and to improve the consistency between patch-wise results. Third, we introduce an iterative refining mechanism, where more accurate 3D information from the predicted depth maps is fed back to the geometric embedding module to further im-prove the depth quality in an iterative manner.
We test OmniFusion on three benchmark datasets: Stan-ford2D3D [1], Matterport3D [3], and 360D [43]. Exper-imental results show that our method outperforms state-of-the-art methods by a significant margin on all of these datasets.
Our contributions can be summarized as follows:
• We present a 360 monocular depth prediction pipeline that addresses the distortion issue via geometry-aware fusion and achieves the state-of-the-art performance.
• We introduce a geometric embedding network to pro-vide 3D geometric features to mitigate the discrepancy in patch-wise image features.
• We incorporate a self-attention-based transformer to globally aggregate patch-wise information which en-hances the estimation of the physical scale of depth.
• We propose an iterative mechanism to further improve the depth estimation with structural details. 2.