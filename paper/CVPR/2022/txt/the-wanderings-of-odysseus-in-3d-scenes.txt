Abstract
Our goal is to populate digital environments, in which digital humans have diverse body shapes, move perpetu-ally, and have plausible body-scene contact. The core chal-lenge is to generate realistic, controllable, and inﬁnitely long motions for diverse 3D bodies. To this end, we pro-pose generative motion primitives via body surface mark-ers, or GAMMA in short. In our solution, we decompose the long-term motion into a time sequence of motion prim-itives. We exploit body surface markers and conditional variational autoencoder to model each motion primitive, and generate long-term motion by implementing the gen-erative model recursively. To control the motion to reach a goal, we apply a policy network to explore the genera-tive model’s latent space and use a tree-based search to preserve the motion quality during testing. Experiments show that our method can produce more realistic and con-trollable motion than state-of-the-art data-driven methods.
With conventional path-ﬁnding algorithms, the generated human bodies can realistically move long distances for a long period of time in the scene. Code is released for re-search purposes at: https://yz-cnsdqz.github. io/eigenmotion/GAMMA/ 1.

Introduction
In recent years, the rapid development of 3D technolo-gies has accelerated the creation of a digital replica of the real world and initiated new ways that people interact with the world and communicate with each other. However, there is no existing solution to automatically populate the digital world with realistic virtual humans, which move and act like real ones. This work aims to enable virtual humans to cruise within a 3D digital environment, similar to Odysseus, who arrived home after wandering and hazards. The vir-tual humans follow randomized routes, pass individual way-points, and reach the destination, while retaining realistic body shape, pose, and body-scene contact. Such technology can considerably enrich AR/VR user experiences and has many downstream applications. For example, having vir-tual humans strolling inside the digital model of a medieval city can make the experience more vivid, which allows real users to follow their guidance for better sightseeing. Be-yond AR/VR, virtual humans can provide architects with a blueprint, enabling better foresight into design functionali-ties and defects.
This is particularly relevant to character animation in graphics, foremost in the gaming industry. Conventionally, a set of 3D characters are pre-designed, and a motion dataset is pre-recorded. To let characters respond to user inputs or background events, motions from the dataset are created via motion graph [34] or motion matching [13]. Although current AAA games demonstrate highly realistic character motion, this conventional technology cannot easily handle a massive number of characters with different behaviors [26].
Motion generalization across diverse characters usually re-quires extra motion re-targeting procedures [5]. Moreover, the generated motions are often deterministic, close to the
pre-recorded clips, and hence have limited diversity.
The availability of large-scale motion capture datasets (e.g. AMASS [44]) facilitates the learning of generative mo-tion models. They can effectively produce motions based on the motion in the past [80], action labels [58], scene con-text [25] and music [40], without limit on a speciﬁc char-acter. Although the motion realism is improved by replac-ing 3D skeletons with expressive body meshes, e.g., SMPL-X [52], the generated motion is limited to a few seconds. It often has jittering, foot-skating, and other artifacts. To pop-ulate the digital environment, we need a fully automated way to generate long-term (potentially inﬁnite) and realistic motions for a large variety of human shapes.
This is a considerably challenging task and far beyond the scope of existing solutions to our knowledge. The ﬁrst obstacle we encounter is how to generate inﬁnitely long, diverse, and stochastic human motion sequences. Existing methods regard motion as a standard time sequence of high-dimensional feature vectors and propose to model it with a single deep neural network. However, the uncertainty of hu-man motion grows as time progresses. It is unclear whether a deep neural network has sufﬁcient power to represent a perpetual motion.
To overcome this issue, we decompose a long-term mo-tion into a time sequence of motion primitives, model each primitive, and compose them to obtain a long-term motion.
Our insight comes from psychological studies. Human re-action time to visual stimuli is about 0.25 seconds [1, 68].
Namely, humans cannot control their body motion immedi-ately after seeing a signal but have to wait for 0.25 sec-onds to give a response due to body inertia. Therefore, we let a motion primitive span 0.25 seconds. In this case, it mainly contains unconscious body dynamics, which are shorter, more deterministic, and easier to model. Speciﬁ-cally, we exploit the body surface markers to represent the body in motion [80] and use conditional variational autoen-coder (CVAE) [33, 64] to model body dynamics. To efﬁ-ciently recover the 3D body from markers, we design a body regressor with recursions. By blending the marker predictor and body regressor, our model can synthesize realistic long-term motion, which is perceptually similar to high-quality mocap sequences, e.g. from AMASS [44]. Of note, our marker-based motion primitive is generalizable to various body shapes, which initiates populating the 3D scene with a massive amount of virtual humans of different identities.
The second challenge is how to let the virtual humans move naturally within 3D scenes, towards a designated des-tination, while considering the geometric constraints of the environment.
Inspired by Ling et al. [41], we propose a novel motion synthesis pipeline with control, which con-sists of a policy network and a tree-based search mech-anism. We formulate long-term motion generation as a
Markov decision process, and use a policy network to ex-plore the CVAE latent space. By sampling from the policy, the body can gradually move to the goal, while keeping the foot-ground contact plausible. Simultaneously, we organize the motion generation process into a tree structure, which searches best motion primitives at each generation step and rejects unrealistic ones. We perform experiments to eval-uate motion realism and controllability. Results show that our method can produce realistic long-term motions in 3D scenes and outperform state-of-the-art methods. Combing with conventional path-ﬁnding algorithms, e.g., navigation mesh baking and A* search [24,63], we can populate large-scale 3D scenes with a massive number of virtual humans, which have diverse body shapes, cruise following paths, and
ﬁnally reach their destinations.
We name our method GAMMA, for GenerAtive Motion primitive via body surface MArkers. Code and model are released for research purposes. 2.