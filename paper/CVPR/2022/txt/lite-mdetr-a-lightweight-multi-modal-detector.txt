Abstract
Recent multi-modal detectors based on transformers and modality encoders have successfully achieved impressive results on end-to-end visual object detection conditioned on a raw text query. However, they require a large model size and an enormous amount of computations to achieve high performance, which makes it difficult to deploy mobile applications that are limited by tight hardware resources.
In this paper, we present a Lightweight modulated detector,
Lite-MDETR, to facilitate efficient end-to-end multi-modal understanding on mobile devices. The key primitive is that
Dictionary-Lookup-Transformormations (DLT) is proposed to replace Linear Transformation (LT) in multi-modal de-tectors where each weight in Linear Transformation (LT) is approximately factorized into a smaller dictionary, in-dex, and coefficient. This way, the enormous linear pro-jection with weights is converted into efficient linear pro-jection with dictionaries, a few lookups and scalings with indices and coefficients. DLT can be applied to any pre-trained multi-modal detectors, removing the need to per-form expensive training from scratch. To tackle the chal-lenging training of DLT due to non-differentiable index, we convert the index and coefficient into a sparse matrix, train this sparse matrix during the fine-tuning phase, and recover it back to index and coefficient during the inference phase.
Our experiments on phrase grounding, referring expression comprehension and segmentation, and VQA show that our
Lite-MDETR achieves similar accuracy as the prior multi-modal detectors with up to ∼ 4.1× model size reduction. 1.

Introduction
Recently, multi-modal models on vision and language data have shown increased performance on vision and lan-guage tasks thanks to the adoption of transformers and large-scale pre-training [3, 7, 12, 21, 32]. These multi-modal transformers substantially benefit from large model com-plexity and computationally costly pre-training on large-scale datasets. Unfortunately, their large model complex-Figure 1. Our lightweight multi-modal Lite-MDETR using pro-posed DLT layers. We replace the linear transformation layers in text encoder and multi-modal transformer with more efficient DLT layers to reduce the model size. ity prohibits us from deploying them on resource-limited platforms, i.e., mobile devices [19, 31]. For this reason, im-proving the efficiency of recent multi-modal models is crit-ical for real-world applications. However, there exists no method that can leverage already pre-trained multi-modal transformers while reducing their size as the existing meth-ods [25, 26, 29] require pre-training the compressed model on large-scale datasets.
Diving deeper into multi-modal models, we observe that linear transformer layer (LT) in individual transformers dominate large share of the overall model size. For exam-ple, a recently published state-of-the-art vision and languge model, called MDETR [12], uses a vision and language transformer based detector together with a language trans-former and a CNN as backbones to process text and image inputs. We find out that the LT layers in the text encoder, a
RoBERTa transformer [20], and vision and language trans-former in MDETR occupy ∼ 90% of the model size. As a result, it is reasonable to replace the traditional LT lay-ers with more efficient layers to reduce the model size of
MDETR to achieve high accuracy while being able to de-ploy it on resource-limited real-world platforms.
Inspired by this finding, we propose Lightweight Dictio-nary Lookup Transform (DLT) layer that can replace the costly LT layers in transformers in MDETR model. We show integration of our DLT layers in MDETR in Figure 1.
As shown in the figure, we use the DLT layers in the text
Figure 2. (a) The state-of-the-art multi-modal detector architecture used in MDETR [12]. (b) The model size breakdown of MDETR using
ENB-3 as backbone and the effects of our lightweight architecture. Ours-Q means we further use 16-bit quantization technique on our lightweight architecture. encoder and multi-modal transformer in MDETR. Our DLT layer compresses the representation of LT as a dictionary, scales and sums the sparse lookups on dictionary to re-cover the compressed representation. To further reduce the model size, we use the orthogonal quantization method al-ready implemented in PyTorch [14, 23]. Our experiments show that our Lite-MDETR using DLT achieves similar accuracy with MDETR using LT on several tasks such as phrase grounding, referring expression comprehension and segmentation, and visual question answering. More spef-ically, DLT alone can reduce the model size of MDETR by 2.03× with slight loss in accuracy whereas the orthogo-nal quantization further reduces the model size by 1.94× as shown in Figure 2(b). We want to highlight that we achieve these results without pre-training Lite-MDETR on a large-scale dataset. In conclusion, these results increase the pos-sibility of the application of MDETR on resource-limited platforms, i.e, mobile devices. 2.