Abstract
Face-swapping models have been drawing attention for their compelling generation quality, but their complex ar-chitectures and loss functions often require careful tun-ing for successful training. We propose a new face-swapping model called ‘Smooth-Swap’, which excludes complex handcrafted designs and allows fast and stable training. The main idea of Smooth-Swap is to build smooth identity embedding that can provide stable gradients for identity change. Unlike the one used in previous mod-els trained for a purely discriminative task, the proposed embedding is trained with a supervised contrastive loss promoting a smoother space. With improved smoothness,
Smooth-Swap suffices to be composed of a generic U-Net-based generator and three basic loss functions, a far sim-pler design compared with the previous models. Extensive experiments on face-swapping benchmarks (FFHQ, Face-Forensics++) and face images in the wild show that our model is also quantitatively and qualitatively comparable or even superior to the existing methods. 1.

Introduction
Face swapping is a task to switch the person-identity of a given face image with another, preserving other attributes like facial expressions, head poses, and backgrounds. The task has been highlighted for its wide use of real-world ap-plications, such as anonymization in privacy protection and the creation of new characters in the entertainment indus-try. With progress made over years [3, 6, 16, 21, 22, 28, 31, 33], state-of-the-art face-swapping models can generate a swapped image of decent quality using a single shot of a new source identity.
Despite the performance improvement, however, exist-ing models usually adopt complex model architectures and numerous loss functions to change face shape. Face shape is a crucial component of identity, but changing it is a nontriv-ial task; it incurs a dramatic change of pixels, but no guid-ance can be given due to the inherent absence of the ground-truth swapped images. Thus, previous studies have focused on using handcrafted components such as mask-based mix-ing [6] or 3D face-shape modeling [16, 31]. Although such components are effective for changing shape and improving the swapped-image quality, the models have added com-plexity of hyperparameters and loss functions that require careful tuning for successful training.
In this study, we postulate that the approaches based on handcrafted components are not the best way to resolve the difficulty of face-swapping. We propose instead a new iden-tity embedding model having improved smoothness, which we assume to be related most to the gist of the problem. An identity embedding model, or an embedder, plays a key role during the training of the swapping model. It gives gradi-ents for the generator, to which direction it has to tune to change the identity. It is thus important the embedder has a smooth space, since the gradients can be erroneous or noisy otherwise. In our proposed model, Smooth-Swap, we con-sider a new embedder trained with supervised constrastive loss [14]. [30]. We find it has a smoother space than the
ArcFace embedder [7], one used in the most of the existing models, and helps faster and stable training.
Through the smooth embedder, Smooth-Swap works without any handcrafted components. It adopts a simple U-Net [24]-based generator, and we train it using only three basic loss functions—identity change, target preserving, and adversarial (Fig. 2). While this set-up is simpler than the existing models, we find that our model can still achieve comparable or superior performance by taking a data-driven approach and minimizing inductive bias.
The advantages of Smooth-Swap can be summarized as follows. 1) Simple architecture: Smooth-Swap uses a sim-ple U-Net [24]-based generator, which does not involve any handcrafted components as the existing models. 2) Simple loss functions: The Smooth-Swap generator can be trained using minimal loss functions for face-swapping—identity, pixel-level change, and adversarial loss. 3) Fast training:
The smooth identity embedder allows faster training of the generator by providing more stable gradient information. 2.