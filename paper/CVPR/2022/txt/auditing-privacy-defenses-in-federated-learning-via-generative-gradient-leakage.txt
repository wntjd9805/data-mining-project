Abstract
Federated Learning (FL) framework brings privacy ben-efits to distributed learning systems by allowing multiple clients to participate in a learning task under the coordi-nation of a central server without exchanging their private data. However, recent studies have revealed that private in-formation can still be leaked through shared gradient infor-mation. To further protect user’s privacy, several defense mechanisms have been proposed to prevent privacy leak-age via gradient information degradation methods, such as using additive noise or gradient compression before shar-ing it with the server.
In this work, we validate that the private training data can still be leaked under certain de-fense settings with a new type of leakage, i.e., Genera-tive Gradient Leakage (GGL). Unlike existing methods that only rely on gradient information to reconstruct data, our method leverages the latent space of generative adversarial networks (GAN) learned from public image datasets as a prior to compensate for the informational loss during gradi-ent degradation. To address the nonlinearity caused by the gradient operator and the GAN model, we explore various gradient-free optimization methods (e.g., evolution strate-gies and Bayesian optimization) and empirically show their superiority in reconstructing high-quality images from gra-dients compared to gradient-based optimizers. We hope the proposed method can serve as a tool for empirically mea-suring the amount of privacy leakage to facilitate the design of more robust defense mechanisms. 1.

Introduction
Federated Learning (FL)
[26, 29, 34] has recently emerged as a new machine learning paradigm that enables multiple clients to collaboratively train a global learning model under the orchestration of a central server. Instead of directly exchanging their private data, each client learns on its local dataset and shares the computed model update or gradient to update the global model. FL places a heavy em-phasis on user’s data privacy, which has made it particularly suitable for developing machine learning models in privacy-sensitive scenarios such as typing prediction [21], spoken
Figure 1. Illustration of data leakage via gradient: 1⃝ Client com-putes gradients on its private data; 2⃝ Client applies defense to degrade the computed gradients y; 3⃝ Adversary attempts to re-construct the private image from the shared gradients y′. language understanding [16,20], medical research [4,8,41], and financial services [32, 50].
Although FL is designed to structurally encode data min-imization principles to protect privacy, recent studies have revealed that, in certain cases, sensitive information can still be leaked through the shared gradients [13, 35, 51, 54, 56].
To further strengthen FL’s privacy properties in these cases, several defense strategies have been proposed to degrade the gradient information before sharing it with the server, such as differential privacy [14, 48], gradient compres-sion/sparsification [56], and perturbing gradients via data representations [44]. These state-of-the-art privacy de-fenses have been shown to be effective against existing at-tacks through modifying the gradient information to de-grade its fidelity prior to sharing.
A natural question is: Can the aforementioned defenses provide sufficient privacy guarantees to prevent the leakage of sensitive information from the client’s private data? To investigate this, we model the gradient leakage process as an inverse problem, where the goal is to reconstruct the pri-vate training data from the client’s shared low-fidelity and noisy gradients. Existing methods seek to solve this inverse problem by iteratively solving for the optimal set of data samples that best match the client’s shared gradients via an optimization process (e.g., gradient descent [13, 51] or L-BFGS [54, 56]). However, such a problem is ill-posed as there are infinite sets of feasible solutions in the image space and the outcome of the reconstruction may not be a decent natural image. To solve this, existing attacks [13, 51] uti-lize handcrafted image priors such as total variation [33] to regularize the reconstruction process. Although such prior constraint is relatively effective when there is no defense, we find that it is still not sufficiently tight (i.e., many non-image signals can satisfy this constraint) for reconstructing from low-fidelity and noisy gradient observations, causing existing attacks to falsely return unrealistic images when a defense mechanism is applied (e.g., differential privacy), as illustrated in Figure 1.
In this work, we demonstrate on two image datasets that recovering high-fidelity images from shared gradients is still feasible even under certain defense settings by intro-ducing a new type of leakage, namely Generative Gradient
Leakage (GGL). As shown in Figure 1, our method lever-ages the manifold of the generative adversarial network (GAN) [6, 15, 27] learned from a large public image dataset as prior information, which provides a good proximation of the natural image space. By minimizing the gradient match-ing loss in the GAN image manifold, our method can find images that are highly similar to the client’s private training data with high quality. However, solving such an optimiza-tion problem is not trivial as both the gradient operator and the GAN latent space are highly non-linear and non-convex, and the defense methods applied at the client’s side also in-ject noises into the objective function. To resolve this, we design an adaptive loss function against common defenses by considering the underlying gradient transformation and resort to gradient-free optimization methods (e.g., evolution strategies [19] and Bayesian optimization [10]) to search for the global minima within the GAN latent space. We empir-ically demonstrate that compared with gradient-based op-timizers, doing so significantly reduces the chance of con-verging to a local minimum, leading to a higher quality of reconstructed images as well as improved similarity to the client’s private image. We note that the findings made from the chosen defense settings and datasets may not be general in scope. Nevertheless, we expect the proposed method can serve as a means for privacy auditing in FL by showing how much an adversary can learn under a specific defense setting to assist the future design of privacy mechanisms.
Our main contributions are summarized as follows:
• We propose to solve the inverse problem of gradient leak-age in FL under noises and defensive transformations by leveraging the prior information learned from deep gen-erative models.
• We systematically study 4 types of gradient-degradation-based defenses, including additive noise, gradient clip-ping, gradient compression, and representation perturba-tion, and design adaptive loss functions by accounting for the underlying gradient transformation.
• To avoid sub-optimal solutions and reveal more private in-formation, we compare different gradient-free optimizers with conventional gradient-based optimizers (e.g., Adam) and experimentally show their superiority for gradient leakage attack in terms of reconstructed image quality and its similarity to the client’s private image.
• We demonstrate on two image datasets (i.e., CelebA [31] and ImageNet [9]) that with the proposed GGL, high-resolution images can still be recovered from the shared gradients even with the considered defenses, while exist-ing gradient leakage attacks all fail. 2.