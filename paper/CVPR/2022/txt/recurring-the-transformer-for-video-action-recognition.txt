Abstract 1.

Introduction
Existing video understanding approaches, such as 3D convolutional neural networks and Transformer-Based methods, usually process the videos in a clip-wise man-ner; hence huge GPU memory is needed and fixed-length video clips are usually required. To alleviate those issues, we introduce a novel Recurrent Vision Transformer (RViT) framework based on spatial-temporal representation learn-ing to achieve the video action recognition task. Specifi-cally, the proposed RViT is equipped with an attention gate to build interaction between current frame input and pre-vious hidden state, thus aggregating the global level inter-frame features through the hidden state temporally. RViT is executed recurrently to process a video by giving the cur-rent frame and previous hidden state. The RViT can capture both spatial and temporal features because of the attention gate and recurrent execution. Besides, the proposed RViT can work on variant-length video clips properly without re-quiring large GPU memory thanks to the frame by frame processing flow. Our experiment results demonstrate that
RViT can achieve state-of-the-art performance on various datasets for the video recognition task. Specifically, RViT can achieve a top-1 accuracy of 81.5% on Kinetics-400, 92.31% on Jester, 67.9% on Something-Something-V2, and an mAP accuracy of 66.1% on Charades.
*Work done while interning at TCL Corporate Research (HK) Co., Ltd. and equal contribution
Existing video understanding works, such as [6, 17, 18, 21, 27, 57], usually utilize the 3D-CNNs network to achieve the spatial-temporal features extraction. With the successful adaptation of the Visual Transformer [11] for vision tasks,
Transformer-based methods become a hot topic for video understanding tasks. TimeSformer [3], ViViT [1], VTN
[40], Mformer [41] and MViT [15] are typical representa-tives. Though Transformer-based methods on computer vi-sion task can achieve significant performance, tremendous computing memory is needed for those methods, which hin-der the deployment of such schemes.
On the other hand, some studies [23, 42, 59] show that human visual attention in a video is driven by prior knowl-edge. For example, [59] points out that human often focuses on user-interested regions of videos, and prominent actions draw more attention than their surrounding neighbours at the initial sight. In the video understanding pipeline of hu-mans, information from the previous frame usually can help determine the attention in the subsequent frame of the video in a recurrent manner.
From the human attention perspective, we reason that there are two categories of information contained in a video: (i) spatial (single frame), (ii) temporal (inter-frames). Both the spatial feature in the current frame and the temporal feature aggregated from prior frames play a crucial role in video understanding tasks. Meanwhile, the temporal fea-tures from adjacent frames usually show high similarity.
However, existing clip-wise approaches generally extract the temporal features at each processing batch, which leads to non-interest information included in the temporal fea-tures. Additionally, temporal features are usually extracted from a fix-length clip instead of a length-adaptive clip.
Motivated by the above discussion, we proposed a novel recurrent processing pipeline, namely Recurrent Vision-Transformer (RViT), to achieve the video action recognition task in this work. Specifically, the proposed RViT frame-work is based on an attention gate enabled RViT unit, given the current input frame x(t) and hidden state h(tâˆ’1) from the previous frame, an output O(t) and a hidden state h(t) will be generated from the current RViT unit. To achieve an length-adaptive temporal feature extraction, an attention gate is designed to transfer the temporal (inter-frames) fea-ture through the hidden state instead of extracting temporal feature from every frame by batch.The aggregated tempo-ral features through the hidden state is utilized to attend the spatial features in the subsequent processing flow.
Our contributions are: (i) An end-to-end Recurrent
Vision-Transformer is proposed to process video sequences for action recognition. The proposed model consumes less GPU memory thanks to the frame-flow processing and achieves state-of-the-art performance simultaneously; (ii)
A novel attention gate is incorporated into the RViT unit to preserve inter-frame attention information through the hidden state; Thus, an interaction between the aggregated temporal features and the current spatial feature can be (iii) Our extensive experiments demonstrate established. that state-of-the-art performance can be achieved for action recognition task. Our method can achieve a top-1 accu-racy of 81.5% on Kinetics-400, 92.31% on Jester, 67.9% on
Something-Something-V2, and an mAP accuracy of 66.1% on Charades. Additionally, temporal attention has been demonstrated visually. 2.