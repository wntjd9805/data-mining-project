Abstract
Dense 3D reconstruction from a stream of depth images is the key to many mixed reality and robotic applications.
Although methods based on Truncated Signed Distance
Function (TSDF) Fusion have advanced the ﬁeld over the years, the TSDF volume representation is confronted with striking a balance between the robustness to noisy mea-surements and maintaining the level of detail. We present
Bi-level Neural Volume Fusion (BNV-Fusion), which lever-ages recent advances in neural implicit representations and neural rendering for dense 3D reconstruction. In order to incrementally integrate new depth maps into a global neu-ral implicit representation, we propose a novel bi-level fu-sion strategy that considers both efﬁciency and reconstruc-tion quality by design. We evaluate the proposed method on multiple datasets quantitatively and qualitatively, demon-strating a signiﬁcant improvement over existing methods. 1.

Introduction
Dense 3D reconstruction from images is one of the most long-standing tasks in the computer vision commu-nity. While there is a large body of research focusing on reconstruction using RGB-only images [13,15], the increas-ing popularity of depth sensors in commodity devices (e.g.
Microsoft Kinect [45], Apple LiDAR scanner [1]) has en-abled researchers to develop reconstruction algorithms tak-ing advantage of depth maps [9, 26, 33].
However, the representation used in these methods –
Truncated Signed Distance Function (TSDF) Volume – is known to lose ﬁne details at sub-voxel scale (e.g. thin sur-faces) [3, 41] because it discretizes the scene geometry at a pre-deﬁned resolution. In addition to the limitation of the representation, each depth measurement is integrated into the volume independently using voxel-wise weighted aver-aging without any local context, which makes the fusion process vulnerable to noisy depth measurements.
In contrast, emerging neural implicit representations, which show promising results in novel view synthesis [25], and shape modelling [24,29,31], have the potential of being a better alternative to TSDF volume-based reconstruction in an online setting. In essence, these representations are deep neural networks that map continuous 3D coordinates to a task-dependent scene property, such as the color or the dis-tance to the nearest surface. As a result, a surface can be extracted at any resolution given the implicit function rep-resented by the network, without any increase in memory
usage. Another advantage of neural implicit representations is that the network can be trained as a generative model to capture prior knowledge of a family of surfaces. These ap-pealing aspects of the neural implicit representations have motivated recent works [2, 3, 17] to develop surface recon-struction methods in an ofﬂine setting. Nonetheless, while
TSDF volume-based methods [26, 33] have demonstrated voxel-wise weighted averaging is real-time capable, how to incrementally integrate new depth measurements using neu-ral implicit representations is still an open question.
Inspired by traditional volumetric fusion approaches, we present Bi-level Neural Volume Fusion (BNV-Fusion) for high-quality and online 3D reconstruction in this paper.
Given a sequence of depth maps and the associated poses,
BNV-Fusion incrementally integrates depth measurements into a global neural volume. The novelty of BNV-Fusion is the combination of a local-level fusion and a global-level fusion. At the local level, a new depth map is ﬁrst mapped to latent codes, each representing local geometry in the latent space. They are then fused into the global neural volume by weighted averaging, which resembles the efﬁcient up-date in traditional volumetric fusion methods. However, the local-level fusion is susceptible to depth outliers as it only integrates the measured surfaces and their surroundings to the volume. Furthermore, although several works on shape modelling [12, 43] suggest that arithmetic operations in the latent space correspond to the actual geometry change to some degree, updating the global representation in the la-tent space using an additive scheme does not always lead to correct geometry. To this end, we propose to optimize the global volume using neural rendering, where we penal-ize the discrepancies between the SDFs extracted from the global volume and those of depth measurements. This opti-mization is coined as global-level fusion as it encourages a coherent reconstruction globally.
Overall, the key realization in BNV-Fusion is that the local- and global-level fusions are complementary. While the local-level fusion efﬁciently integrates new information and initializes the global-level fusion, the reconstruction quality is improved signiﬁcantly by the global-level fusion.
To summarize, our contributions are threefold:
• We propose BNV-Fusion, a novel and state-of-the-art dense 3D reconstruction pipeline that represents the geometry of a scene by an implicit neural volume.
• We design a novel bi-level fusion algorithm that efﬁ-ciently and effectively updates the neural volume given new depth measurements.
• We conduct extensive experiments, including an evalu-ation on 312 sequences of various indoor environments in ScanNet [8], to validate that BNV-Fusion improves existing approaches signiﬁcantly and is truly general-izable to arbitrary scenes. 2.