Abstract
The referring video object segmentation task (RVOS) in-volves segmentation of a text-referred object instance in the frames of a given video. Due to the complex nature of this multimodal task, which combines text reasoning, video understanding, instance segmentation and tracking, exist-ing approaches typically rely on sophisticated pipelines
In this paper, we propose a sim-in order to tackle it. ple Transformer-based approach to RVOS. Our framework, termed Multimodal Tracking Transformer (MTTR), mod-els the RVOS task as a sequence prediction problem. Fol-lowing recent advancements in computer vision and nat-ural language processing, MTTR is based on the realiza-tion that video and text can be processed together effec-tively and elegantly by a single multimodal Transformer model. MTTR is end-to-end trainable, free of text-related inductive bias components and requires no additional mask-reﬁnement post-processing steps. As such, it simpliﬁes the
RVOS pipeline considerably compared to existing meth-ods. Evaluation on standard benchmarks reveals that MTTR signiﬁcantly outperforms previous art across multiple met-rics. In particular, MTTR shows impressive +5.7 and +5.0 mAP gains on the A2D-Sentences and JHMDB-Sentences datasets respectively, while processing 76 frames per sec-ond.
In addition, we report strong results on the public validation set of Refer-YouTube-VOS, a more challenging
RVOS dataset that has yet to receive the attention of re-searchers. The code to reproduce our experiments is avail-able at https://github.com/mttr2021/MTTR. 1.

Introduction
Attention-based [41] deep neural networks exhibit im-pressive performance on various tasks across different ﬁelds, from computer vision [10, 27] to natural language processing
[3, 8]. These advancements make networks of this sort, such as the Transformer [41], particularly interesting candidates for solving multimodal problems. By relying on the self-Figure 1. Given a text query and a sequence of video frames, the proposed model outputs prediction sequences for all object instances in the video prior to determining the referred instance.
Here predictions with the same color and shape belong to the same sequence and attend to the same object instance in different frames.
Note that the order of instance predictions for different frames remains the same. Best viewed in color. attention mechanism, which allows each token in a sequence to globally aggregate information from every other token,
Transformers excel at modeling global dependencies and have become the cornerstone in most NLP tasks [3, 8, 35, 50].
Transformers have also started showing promise in solving computer vision tasks, from recognition [10] to object de-tection [4] and even outperforming the long-used CNNs as general-purpose vision backbones [27].
The referring video object segmentation task (RVOS) involves the segmentation of a text-referred object instance in the frames of a given video. Compared with the referring image segmentation task (RIS) [29, 52], in which objects are mainly referred to by their appearance, in RVOS objects can
also be referred to by the actions they are performing or in which they are involved. This renders RVOS signiﬁcantly harder than RIS, as text expressions that refer to actions often cannot be properly deduced from a single static frame.
Furthermore, unlike their image-based counterparts, RVOS methods may be required to establish data association of the referred object across multiple frames (tracking) in order to deal with disturbances such as occlusions or motion blur.
To solve these challenges and effectively align video with text, existing RVOS approaches [14, 25, 32] typically rely on complicated pipelines. In contrast, here we propose a simple, end-to-end Transformer-based approach to RVOS. Using recent advancements in Transformers for textual feature ex-traction [26, 41], visual feature extraction [10, 27, 28] and object detection [4, 45], we develop a framework that sig-niﬁcantly outperforms existing approaches. To accomplish this, we employ a single multimodal Transformer and model the task as a sequence prediction problem. Given a video and a text query, our model generates prediction sequences for all objects in the video before determining the one the text refers to. Additionally, our method is free of text-related inductive bias modules and utilizes a simple cross-entropy loss to align the video and the text. As such, it is much less complicated than previous approaches to the task.
The proposed pipeline is schematically depicted in Fig. 1.
First, we extract linguistic features from the text query using a standard Transformer-based text encoder, and visual fea-tures from the video frames using a spatio-temporal encoder.
The features are then passed into a multimodal Transformer, which outputs several sequences of object predictions [45].
Next, to determine which of the predicted sequences best cor-responds to the referred object, we compute a text-reference score for each sequence. For this we propose a temporal segment voting scheme that allows our model to focus on more relevant parts of the video when making the decision.
Our main contributions are as follows:
• We present a Transformer-based RVOS framework, dubbed Multimodal Tracking Transformer (MTTR), which models the task as a parallel sequence prediction problem and outputs predictions for all objects in the video prior to selecting the one referred to by the text.
• Our sequence selection strategy is based on a temporal segment voting scheme, a novel reasoning scheme that allows our model to focus on more relevant parts of the video with regards to the text.
• The proposed method is end-to-end trainable, free of text-related inductive bias modules, and requires no ad-ditional mask reﬁnement. As such, it greatly simpliﬁes the RVOS pipeline compared to existing approaches.
• We thoroughly evaluate our method. On the A2D-Sentences and JHMDB-Sentences [12], MTTR signiﬁ-cantly outperforms all existing methods across all met-rics. We also show strong results on the public vali-dation set of Refer-YouTube-VOS [39], a challenging dataset that has yet to receive attention in the literature. 2.