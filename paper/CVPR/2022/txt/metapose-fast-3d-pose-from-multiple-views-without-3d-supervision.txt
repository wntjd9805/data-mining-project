Abstract
In the era of deep learning, human pose estimation from multiple cameras with unknown calibration has received little attention to date. We show how to train a neural model to perform this task with high precision and mini-mal latency overhead. The proposed model takes into ac-count joint location uncertainty due to occlusion from mul-tiple views, and requires only 2D keypoint data for train-ing. Our method outperforms both classical bundle adjust-ment and weakly-supervised monocular 3D baselines on the well-established Human3.6M dataset, as well as the more challenging in-the-wild Ski-Pose PTZ dataset. 1.

Introduction
We tackle the problem of estimating 3D coordinates of human joints from RGB images captured using synchro-nized (potentially moving) cameras with unknown posi-tions, orientations, and intrinsic parameters. We addition-ally assume having access to a training set with only 2D positions of joints labeled on captured images.
Historically, real-time capture of the human 3D pose has been undertaken only by large enterprises that could af-ford expensive specialized motion capture equipment [18].
In principle, if camera calibrations are available [3], hu-man body joints can be triangulated directly from camera-space observations [26, 33]. One scenario in which cam-era calibration cannot easily be estimated is sports capture, in which close-ups of players are captured in front of low-texture backgrounds, with wide-baseline, moving cameras.
Plain backgrounds preclude calibration via classical multi-camera SfM [21], as not sufﬁciently many feature corre-spondences can be detected across views; see Figure 1.
In this work, we propose a neural network to simulta-neously predict 3D human and relative camera poses from multiple views; see Figure 1. Our approach uses human body joints as a source of information for camera calibra-tion. As joints often become occluded, uncertainty must be carefully accounted for, to avoid bad calibration and con-Figure 1. We show how to train a neural network that can ag-gregate outputs of multiple single-view methods, takes prediction uncertainty into consideration, has minimal latency overhead, and requires only 2D supervision for training. Our method mimics the structure of bundle-adjustment solvers, but using the joints of the human body to drive camera calibration, and by implementing a bundle-like solver with a simple feed-forward neural network. sequent erroneous 3D pose predictions. As we assume a synchronized multi-camera setup at test-time, our algorithm should also be able to effectively aggregate information from different viewpoints. Finally, our approach supervised by 2D annotations alone, as ground-truth annotation of 3D data is unwieldy. As summarized in Figure 2, and detailed in what follows, none of the existing approaches fully satis-ﬁes these fundamental requirements.
Fully-supervised 3D pose estimation approaches yield the lowest estimation error, but make use of known 3D cam-era speciﬁcation during either training [65] or both training and inference [26]. However, the prohibitively high cost of 3D joint annotation and full camera calibration in-the-wild makes it difﬁcult to acquire large enough labeled datasets representative of speciﬁc environments [30, 53], therefore rendering supervised methods not applicable in this setup.
Monocular 3D methods [25, 37, 62] and 2D-to-3D lifting networks [10, 61], relax data constraints to enable 3D pose inference using just multi-view 2D data without calibration at train time. Unfortunately, at inference time, these meth-ods can only be applied to a single view at a time, therefore unable to leverage cross-view information and uncertainty.
Classical SfM (structure from motion) approaches to 3D pose estimation [33] iteratively reﬁne both the camera and
ments. Martinez et al. [46] use pre-trained 2D pose net-works [49] to take advantage of existing datasets with 2D pose annotations. Epipolar transformers [22] use only 2D keypoint supervision, but require camera calibration to in-corporate 3D information in the 2D feature extractors.
Weak and self-supervision. Some approaches do not use full 3D GT poses for training. Many augment limited 3D annotations with 2D labels [32, 48, 66, 69]. Fitting-based methods [32, 38, 40, 66] jointly ﬁt a statistical 3D hu-man body model and 3D human pose to monocular images.
Analysis-by-synthesis methods [27, 41, 52] learn to predict 3D human pose by estimating appearance in a novel view.
Most related to our work are approaches that exploit the structure of multi-view image capture. EpipolarPose [37] uses epipolar geometry to obtain 3D pose estimates from multi-view 2D predictions, and subsequently uses them to directly supervise 3D pose regression. Iqbal et al. [25] pro-poses a weakly-supervised baseline to predict pixel coordi-nates of joints and their depth in each view and penalized the discrepancy between rigidly aligned predictions for dif-ferent views during training. The self-supervised Canon-Pose [62] further advances state-of-the-art by decoupling 3D pose estimation in “canonical” frame. Drover et al. [15] learn a “dictionary” mapping 2D pose projections into cor-responding realistic 3D poses, using a large collection of simulated 3D-to-2D projections. RepNet [61] and Chen et al. [10] train similar “2D-to-3D lifting networks” with more realistic data constraints. While all the aforemen-tioned methods use multi-view consistency for training, they do not allow pose inference from multiple images.
Iterative reﬁnement. Estimating camera and pose simulta-neously is a long-standing problem in vision [54]. One of the more recent successful attempts is the work of Bridge-man et al. [8] that proposed an end-to-end network that re-ﬁnes the initial calibration guess using center points of mul-tiple players in the ﬁeld. In the absence of such external calibration signals, Takahashi et al. [57] performs bundle adjustment with bone length constraints, but do not report results on a public benchmark. AniPose [33] performs joint 3D pose and camera reﬁnement using a modiﬁed version of the robust 3D registration algorithm of Zhou et al. [68].
Such methods ignore predicted uncertainty for faster infer-ence, but robustly iteratively estimate outlier 2D observa-tions and ignores them during reﬁnement. In Section 5, we show that these classical approaches struggle in ill-deﬁned settings, such as when we have a small number of cameras.
More recently, SPIN [40], HUND [67] and Holopose [19] incorporate iterative pose reﬁnement for monocular inputs, however, the reﬁnement is tightly integrated into the pose estimation network. MetaPose effectively regularizes the multi-view pose estimation problem with a ﬁnite-capacity neural network resulting in both faster inference and higher precision than the classical reﬁnement.
Figure 2. Prior work — Existing solutions either require 3D an-notations [26], perform inference on a single view at a time [62], or ignore uncertainty in joint coordinates due to occlusions [33]. the 3D pose from noisy 2D observations. However, these methods are often much slower than their neural counter-parts, since they have to perform several optimization steps during inference. Further, most of them do not consider un-certainty estimates, resulting in sub-par performance.
To overcome these limitation we propose MetaPose; see Figure 1. Our method for 3D pose estimation aggregates pose predictions and uncertainty estimates across multiple views, requires no 3D joint annotations or camera param-eters at both train and inference time, and adds very little latency to the resulting pipeline.
Overall, we propose the feed-forward neural architecture that can accurately estimate the 3D human pose and the relative cameras conﬁguration from multiple views, taking into account joint occlusions and prediction uncertainties, and uses only 2D joint annotations for training. We employ an off-the-shelf weakly-supervised 3D network to form an initial guess about the pose and the camera setup, and a neural meta-optimizer that iteratively reﬁnes this guess us-ing 2D joint location probability heatmaps generated by an off-the-shelf 2D pose estimation network. This modular approach not only yields low estimation error, leading to state-of-the-art results on Human3.6M [24] and Ski-Pose
PTZ [53], but also has low latency, as inference within our framework executes as a feed-forward neural network. 2.