Abstract
In this paper, we propose self-supervised training for video transformers using unlabeled video data. From a given video, we create local and global spatiotemporal views with varying spatial sizes and frame rates. Our self-supervised objective seeks to match the features of these dif-ferent views representing the same video, to be invariant to spatiotemporal variations in actions. To the best of our knowledge, the proposed approach is the ﬁrst to alleviate the dependency on negative samples or dedicated memory banks in Self-supervised Video Transformer (SVT). Further, owing to the ﬂexibility of Transformer models, SVT sup-ports slow-fast video processing within a single architecture using dynamically adjusted positional encoding and sup-ports long-term relationship modeling along spatiotempo-ral dimensions. Our approach performs well on four action recognition benchmarks (Kinetics-400, UCF-101, HMDB-51, and SSv2) and converges faster with small batch sizes.
Code is available at: https://git.io/J1juJ 1.

Introduction
Self-supervised learning enables extraction of meaning-ful representations from unlabeled data, alleviating the need for expensive annotations. Recent self-supervised methods perform on-par with supervised learning for certain vision tasks [11,12,16,37]. The necessity of self-supervised learn-ing is even greater in domains such as video analysis where annotations are more expensive [40, 44, 63, 65].
At the same time, the emergence of vision transform-ers (ViTs) [26] and their successful adoption to differ-ent computer vision tasks including video understanding
[4,9,29,53,67,68] within the supervised setting shows their promise in the video domain. In fact, recent works using simple ViT backbones [9] surpass convolutional neural net-works (CNN) for supervised video analysis with reduced compute. Motivated by the ability of self-attention to model
Figure 1. Our Self-supervised Video Transformer (SVT) learns cross-view and motion correspondences by jointly matching video clips sampled with varying ﬁeld of view and temporal resolutions.
Speciﬁcally, Global views (top and bottom right) with different temporal resolutions as well as Local views (bottom left) from dif-ferent spatiotemporal windows are sampled. The representations of these multiple views are matched in a student-teacher frame-work to learn cross-view and motion correspondences (middle block). The proposed self-supervised framework can learn high-quality spatiotemporal features while converging faster. long-range dependencies, we propose a simple yet effective method to train video transformers [9] in a self-supervised manner. This process uses spatial and temporal context as a supervisory signal (from unlabelled videos) to learn motion, scale, and viewpoint invariant features.
Many existing self-supervised representation learning methods on videos [64, 65, 88] use contrastive learning ob-jectives which can require larger batch sizes, longer train-ing regimes, careful negative mining and dedicated mem-ory banks. Further, the contrastive objectives require care-ful temporal sampling [64] and multiple networks looking at similar/different clips to develop attract/repel loss formu-lations [65]. In contrast, we propose to learn self-supervised features from unlabelled videos via self-distillation [72] by a twin network strategy (student-teacher models) [13, 34].
⇡
Our proposed approach, Self-supervised Video Trans-former (SVT), trains student and teacher models with a sim-ilarity objective [13] that matches the representations along spatial and temporal dimensions by space and time atten-tion [9]. We achieve this by creating spatiotemporal pos-itive views that differ in spatial sizes and are sampled at different time frames from a single video (Fig. 1). During training, teacher video transformer parameters are updated as an exponential moving average of the student video trans-former. Both of these networks process different spatiotem-poral views of the same video and our objective function is designed to predict one view from the other in the fea-ture space. This allows SVT to learn robust features that are invariant to spatiotemporal changes in videos while gener-ating discriminative features across videos [34]. SVT does not depend on negative mining or large batch sizes and re-mains computationally efﬁcient as it converges within only 20 on Kinetics-400 [14]). a few epochs (
In addition to the above advantages, our design allows the ﬂexibility to model varying time-resolutions and spatial scales within a uniﬁed architecture. This is a much desired feature for video processing since real-world actions can oc-cur with varying temporal and spatial details. Remarkably, current self-supervision based video frameworks [64, 87] operate on ﬁxed spatial and temporal scales which can pose difﬁculties in modeling the expressivity and dynamic na-ture of actions. We note that convolutional backbones used in these approaches lack the adaptability to varying tempo-ral resolutions (due to ﬁxed number of channels) and thus require dedicated networks for each resolution [30, 45]. To address this challenge, the proposed SVT uses dynamically adjusted positional encodings to handle varying temporal resolutions within the same architecture. Further, the self-attention mechanism in SVT can capture both local and global long-range dependencies across both space and time, offering much larger receptive ﬁelds as compared to tradi-tional convolutional kernels [57].
The main contributions in this work are as follows:
• We introduce a novel mechanism for self-supervised training of video transformers by exploiting spatiotem-poral correspondences between varying ﬁelds of view (global and local) across space and time (Sec. 3.2).
• Self-supervision in SVT is performed via a joint mo-tion and crossview correspondence learning objective.
Speciﬁcally, global and local spatiotemporal views with varying frame rates and spatial characteristics (Sec. 3.2.1
Sec. 3.2.2) are matched by our motion and crossview cor-respondences in the latent space.
• A unique property of our architecture is that it allows slow-fast training and inference using a single video transformer. To this end, we propose to use dynamic posi-tional encoding within SVT to handle variable frame rate inputs generated from our sampling strategy (Sec. 3.2.3).
Our extensive experiments and results on various video datasets including Kinetics-400 [14], UCF-101 [69],
HMDB-51 [49], and SSv2 [33] show state-of-the-art trans-fer of our self-supervised features using only RGB data.
Further, our method shows a rapid convergence rate. 2.