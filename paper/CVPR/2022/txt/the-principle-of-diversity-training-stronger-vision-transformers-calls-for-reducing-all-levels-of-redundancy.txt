Abstract
Vision transformers (ViTs) have gained increasing popu-larity as they are commonly believed to own higher mod-eling capacity and representation flexibility, than tradi-tional convolutional networks. However, it is questionable whether such potential has been fully unleashed in prac-tice, as the learned ViTs often suffer from over-smoothening, yielding likely redundant models. Recent works made pre-liminary attempts to identify and alleviate such redundancy, e.g., via regularizing embedding similarity or re-injecting convolution-like structures. However, a “head-to-toe as-sessment” regarding the extent of redundancy in ViTs, and how much we could gain by thoroughly mitigating such, has been absent for this field. This paper, for the first time, systematically studies the ubiquitous existence of re-dundancy at all three levels: patch embedding, attention map, and weight space.
In view of them, we advocate a principle of diversity for training ViTs, by presenting cor-responding regularizers that encourage the representation diversity and coverage at each of those levels, that enabling capturing more discriminative information. Extensive ex-periments on ImageNet with a number of ViT backbones validate the effectiveness of our proposals, largely eliminat-ing the observed ViT redundancy and significantly boosting the model generalization. For example, our diversified DeiT obtains 0.70% ∼ 1.76% accuracy boosts on ImageNet with highly reduced similarity. Our codes are fully available in https://github.com/VITA-Group/Diverse-ViT. 1.

Introduction
Transformer [57], as the de facto neural architecture in natural language processing (NLP) [4, 19], recently revolu-tionizes modern computer vision applications such as image classification [21, 26, 53], object detection [5, 17, 74, 80], and image generation [10, 31, 46]. Rather than relying on convolution-like inductive bias, vision transformers [21] (ViTs) leverage the self-attention [57] to aggregate image patches across all spatial positions and model their global-range relationships, which are believed to improve model
Figure 1. Relative similarity comparisons in embedding, attention, and weight spaces of DeiT-Small on ImageNet. The larger num-ber indicates severer correlation/redundancy. B1∼B5 donate the blocks in the DeiT-Small model. Cosine, (normalized) MSE, 1 -(normalized) reconstruction loss are adopted to measure embed-ding, attention, and weight similarity. The former two are com-puted with 10, 000 images from the ImageNet training set without data augmentation, following the standard in [23]. expressiveness and representation flexibility.
Despite their promising potentials, the ViT training still suffers from considerable instability, especially when going deeper [23, 55]. One of the major reasons [23] is that the global information aggregation among all patches encour-ages their representations to become overly similar, caus-ing substantially degraded discrimination ability. This phe-nomenon, known as over-smoothening, suggests a high de-gree of “redundancy” or ineffective usage of the ViT ex-pressiveness and flexibility, and has been studied by a few prior arts [23,55,75,76]. Several initial attempts strive to fill the gap from different aspects. For example [23] proposes contrastive-based regularization to diversity patch embed-dings, and [76] directly refines the self-attention maps via convolution-like aggregation to augment local patterns.
This paper aims to comprehensively study and mitigate the ViT redundancy issue. We first systematically demon-strate the ubiquitous existence of redundancy at all three levels: patch embedding, attention map, and weight space, for current state-of-the-art (SOTA) ViTs. That is even the case for those equipped with strong data augmentations (i.e., DeiT [54]) or sophisticated attention mechanisms (i.e.,
In view of such
Swin [43]), e.g, as shown in Figure 1. collapse, we advocate a principle of diversity for train-ing ViTs, by proposing corresponding regularizers that en-courage the representation diversity and coverage at each of those levels, that unleashes the true discriminative power and representation flexibility of ViTs. We find each level’s regularizers to provide generalization gains, and applying them altogether consistently yields superior performance.
Our contributions lie in the following aspects:
• We provide the first comprehensive investigation of re-dundancy in ViTs by demonstrating its ubiquitous ex-istence in all three levels of patch embeddings, atten-tions, and weights, across SOTA ViT models.
• For each of the three levels, we present diversity regu-larizers for training ViTs, which demonstrate comple-mentary effects in eliminating redundancy, encourag-ing diversity, and enhancing generalization.
• We conduct extensive experiments with vanilla ViT,
DeiT, and Swin transformer backbones on the Ima-geNet datasets, showing consistent and significant per-formance boost gains by addressing the tri-level redun-dancy issues with our proposed regularizers. Specif-ically, our proposals improve DeiT and Swin, by 0.70% ∼ 1.76% and 0.15% ∼ 0.32% accuracy. 2.