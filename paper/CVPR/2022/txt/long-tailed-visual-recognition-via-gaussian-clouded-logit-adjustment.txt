Abstract
Long-tailed data is still a big challenge for deep neural networks, even though they have achieved great success on balanced data. We observe that vanilla training on long-tailed data with cross-entropy loss makes the instance-rich head classes severely squeeze the spatial distribution of the tail classes, which leads to difficulty in classifying tail class samples. Furthermore, the original cross-entropy loss can only propagate gradient short-lively because the gradient in softmax form rapidly approaches zero as the logit dif-ference increases. This phenomenon is called softmax sat-It is unfavorable for training on balanced data, uration. but can be utilized to adjust the validity of the samples in long-tailed data, thereby solving the distorted embedding space of long-tailed problems. To this end, this paper pro-poses the Gaussian clouded logit adjustment by Gaussian perturbation of different class logits with varied amplitude.
We define the amplitude of perturbation as cloud size and set relatively large cloud sizes to tail classes. The large cloud size can reduce the softmax saturation and thereby making tail class samples more active as well as enlarging the embedding space. To alleviate the bias in a classifier, we therefore propose the class-based effective number sam-pling strategy with classifier re-training. Extensive experi-ments on benchmark datasets validate the superior perfor-mance of the proposed method. Source code is available at https://github.com/Keke921/GCLLoss. 1.

Introduction
Deep neural networks (DNNs) have been widely utilized in a variety of visual recognition problems [6, 7, 21, 28] by virtue of the large-scale, high-quality, and annotated datasets. DNNs usually require the training dataset to be artificially balanced and have sufficient samples of each class. Unfortunately, from a practical perspective, object frequency usually follows a power law and typically ex-*Yiu-ming Cheung is the Corresponding Author.
Figure 1. t-SNE visualization of the distorted embedding space. (Color for the best view.) The embeddings are calculated with
ResNet-32 on a subset with four classes of CIFAR-10-LT. We ran-domly select four classes with the training numbers 500, 200, 100, and 50, respectively. The gray areas show the obscure regions be-tween different classes. hibits a long-tailed distribution. Naive learning on such data is prone to undesirable bias towards the head classes which occupy the majority of the training samples [37]. Since tail classes have few training samples that cannot cover the real distribution in embedding space, their spatial span is severely compressed by head classes.
In addition, a vast number of head class samples generate overwhelming dis-couraging gradients for tail classes. Thus, the learning of a classifier is biased towards the head classes. As a result, directly training on long-tailed data brings two key prob-lems: 1) the distorted embedding space, and 2) the biased classifier.
In the literature, most of the recently proposed ap-proaches focus on addressing the second problem only, i.e., the biased classifier. For example, Menon et al. [17] and
Hong et al. [8] applied post-adjust strategy to the trained model to calibrate the class boundary. Nevertheless, the distorted embedding cannot be adjusted with the post-hoc calibration, which is not conducive to further improving the
model performance. Most recently, the two-stage decou-pling methods [2, 10, 31, 35, 40] have been proposed to ob-tain good embeddings in the first stage and then re-balance the classifier in the second stage. These methods obtain the representation by cross-entropy (CE) loss, which, however, leads to a severely uneven distributed embedding space. We implement a toy experiment to illustrate the distortion of the embedding space as shown in Fig. 1, where t-SNE [25] is utilized to visualize the features of a long-tailed subset from
CIFAR-10 dataset. We can observe that the tail class occu-pies a much small spatial span than the head class. This is because the tail class with fewer samples cannot cover the ground truth distribution. Moreover, Fig. 1 also shows that there are obscure regions (i.e., the grey area) between dif-ferent classes. Softmax saturation [3] is one of the factors of these obscure regions because it leads to insufficient train-ing. These obscure regions have a severe effect on the tail classes but little on the head classes. Since tail class samples clustered around the class boundary aggravate their spatial squeezing, while the head class samples with enough vari-ety can already cover the true distribution.
Softmax saturation refers to the inopportune early gra-dients vanishing produced by the softmax [3, 36], which weakens the validity of training samples and impedes model training. However, from another perspective, the seemingly harmful softmax saturation has the ability to balance the valid samples of different classes and thus help calibrate the distortion of embedding space. Specifically, we disturb the logit of different classes with different amplitudes. We name the disturbed logit as Gaussian clouded logit (GCL) and the amplitude of the disturbance as cloud size, because we set the disturbance to a Gaussian distribution. The tail classes have few training samples and thus the training sam-ples of them should be more valid. We therefore disturb the logit of tail classes with large relative cloud sizes to re-duce the softmax saturation. In this way, tail class samples can provide more gradients without overfitting and thus in-directly affect their embedding space. In addition, a large cloud size of the tail class logit corresponds to the large cloud size on feature in the direction of the class anchor.
Therefore, tail classes can have large margins towards the class boundary, so as to alleviate the severe uneven distribu-tion between the head and tail classes. Conversely, the head classes are set to small cloud sizes, so that they can be auto-matically filtered out during training. Eventually, as shown in Fig. 2, the tail class samples can be pushed more away from the class boundary so as the distortion of the embed-ding space can be calibrated.
To address the biased classifier, we re-balance the train-ing data with a class-wise sampling strategy. As training with GCL makes the validity of different classes vary, the so-called “effectiveness” [4] of them are different. Existing class-wise balanced sampling strategies will lead to exces-Figure 2. An overview of GCL. (Color for the best view.) The tail class logit is assigned to a larger sample cloud size than the head class, which corresponds to a large relative cloud size of the feature in the direction of the tail class anchor. In this way, the distortion of the embedding space can be calibrated well. sive training of tail classes for GCL. We thereby propose the class-based effective number (CBEN) sampling strat-egy, which is based on sample validity and label frequen-cies to re-balance the classifier. This simple but effective sampling strategy helps mitigate the classifier bias towards the head classes and further boost the performance of GCL.
Extensive experiments on multiple commonly used long-tailed recognition benchmark datasets demonstrate that the proposed GCL surpasses the recently proposed counter-parts. In summary, the key contributions of our work are three-fold:
• We propose the GCL adjustment loss function, which utilizes softmax saturation to balance the sample valid-ity of different classes. An evenly distributed embed-ding can be obtained with the proposed GCL.
• We propose a simple but effective class-based effective number (CBEN) sampling strategy for re-balancing the classifier to avoid repeat training of tail classes.
This sampling strategy can further boost the perfor-mance of GCL.
• Extensive experiments on popular long-tailed datasets demonstrate that the proposed method outperforms the state-of-the-art counterparts. 2.