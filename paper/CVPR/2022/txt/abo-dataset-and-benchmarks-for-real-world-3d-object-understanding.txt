Abstract
We introduce Amazon Berkeley Objects (ABO), a new large-scale dataset designed to help bridge the gap between real and virtual 3D worlds. ABO contains product catalog images, metadata, and artist-created 3D models with com-plex geometries and physically-based materials that cor-respond to real, household objects. We derive challeng-ing benchmarks that exploit the unique properties of ABO and measure the current limits of the state-of-the-art on three open problems for real-world 3D object understand-ing: single-view 3D reconstruction, material estimation, and cross-domain multi-view object retrieval. 1.

Introduction
Progress in 2D image recognition has been driven by large-scale datasets [15, 26, 37, 43, 56]. The ease of collect-ing 2D annotations (such as class labels or segmentation masks) has led to the large scale of these diverse, in-the-wild datasets, which in turn has enabled the development of 2D computer vision systems that work in the real world.
Theoretically, progress in 3D computer vision should fol-low from equally large-scale datasets of 3D objects. How-ever, collecting large amounts of high-quality 3D annota-tions (such as voxels or meshes) for individual real-world objects poses a challenge. One way around the challeng-ing problem of getting 3D annotations for real images is to focus only on synthetic, computer-aided design (CAD) models [10, 35, 70]. This has the advantage that the data is large in scale (as there are many 3D CAD models avail-able for download online) but many of the models are low quality or untextured and do not exist in the real world.
This has led to a variety of 3D reconstruction methods that work well on clear-background renderings of synthetic ob-jects [13, 24, 46, 65] but do not necessarily generalize to real images, new categories, or more complex object ge-ometries [5, 6, 58].
To enable better real-world transfer, another class of 3D datasets aims to link existing 3D models with real-world images [63, 64]. These datasets find the closest matching
CAD models for the objects in an image and have human annotators align the pose of each model to best match the
image. While this has enabled the evaluation of 3D re-construction methods in-the-wild, the shape (and thus pose) matches are approximate. Further, because this approach relies on matching CAD models to images, it inherits the limitations of the existing CAD model datasets (i.e. poor coverage of real-world objects, basic geometries and tex-tures).
The IKEA [41] and Pix3D [57] datasets sought to im-prove upon this by annotating real images with exact, pixel-aligned 3D models. The exact nature of such datasets has allowed them to be used as training data for single-view re-construction [21] and has bridged some of the synthetic-to-real domain gap. However, the size of the datasets are relatively small (90 and 395 unique 3D models, respec-tively), likely due to the difficulty of finding images that exactly match 3D models. Further, the larger of the two datasets [57] only contains 9 categories of objects. The pro-vided 3D models are also untextured, thus the annotations in these datasets are typically used for shape or pose-based tasks, rather than tasks such as material prediction.
Rather than trying to match images to synthetic 3D mod-els, another approach to collecting 3D datasets is to start with real images (or video) and reconstruct the scene by classical reconstruction techniques such as structure from motion, multi-view stereo and texture mapping [12, 54, 55].
The benefit of these methods is that the reconstructed geom-etry faithfully represents an object of the real world. How-ever, the collection process requires a great deal of manual effort and thus datasets of this nature tend to also be quite small (398, 125, and 1032 unique 3D models, respectively).
The objects are also typically imaged in a controlled lab setting and do not have corresponding real images of the object “in context”. Further, included textured surfaces are assumed to be Lambertian and thus do not display realistic reflectance properties.
Motivated by the lack of large-scale datasets with re-alistic 3D objects from a diverse set of categories and corresponding real-world multi-view images, we introduce
Amazon Berkeley Objects (ABO). This dataset is derived from Amazon.com product listings, and as a result, con-tains imagery and 3D models that correspond to mod-ern, real-world, household items. Overall, ABO contains 147,702 product listings associated with 398,212 unique catalog images, and up to 18 unique metadata attributes (category, color, material, weight, dimensions, etc.) per product. ABO also includes “360º View” turntable-style images for 8, 222 products and 7,953 products with corre-sponding artist-designed 3D meshes. In contrast to exist-ing 3D computer vision datasets, the 3D models in ABO have complex geometries and high-resolution, physically-based materials that allow for photorealistic rendering. A sample of the kinds of real-world images associated with a 3D model from ABO can be found in Figure 1, and sam-Dataset
# Models # Classes Real images Full 3D PBR
ShapeNet [10] 3D-Future [19]
Google Scans [54]
CO3D [53]
IKEA [42]
Pix3D [57]
PhotoShape [51]
ABO (Ours) 51.3K 16.6K 1K 18.6K 219 395 5.8K 8K 55 8
-50 11 9 1 63
✗
✗
✗
✓
✓
✓
✗
✓
✓
✓
✓
✗
✓
✓
✓
✓
✗
✗
✗
✗
✗
✗
✓
✓
Table 1. A comparison of the 3D models in ABO and other commonly used object-centric 3D datasets. ABO contains nearly 8K 3D models with physically-based rendering (PBR) ma-terials and corresponding real-world catalog images.
Figure 2. Posed 3D models in catalog images. We use instance masks to automatically generate 6-DOF pose annotations. ple metadata attributes are shown in Figure 3. The dataset is released under CC BY-NC 4.0 license and can be down-loaded at https://amazon- berkeley- objects.s3. amazonaws.com/index.html.
To facilitate future research, we benchmark the perfor-mance of various methods on three computer vision tasks that can benefit from more realistic 3D datasets: (i) single-view shape reconstruction, where we measure the domain gap for networks trained on synthetic objects, (ii) mate-rial estimation, where we introduce a baseline for spatially-varying BRDF from single- and multi-view images of com-plex real world objects, and (iii) image-based multi-view object retrieval, where we leverage the 3D nature of ABO to evaluate the robustness of deep metric learning algorithms to object viewpoint and scenes. 2.