Abstract
Recent years have witnessed substantial progress in se-mantic image synthesis, it is still challenging in synthesiz-ing photo-realistic images with rich details. Most previ-ous methods focus on exploiting the given semantic map, which just captures an object-level layout for an image.
Obviously, a fine-grained part-level semantic layout will benefit object details generation, and it can be roughly in-ferred from an object’s shape. In order to exploit the part-level layouts, we propose a Shape-aware Position Descrip-tor (SPD) to describe each pixel’s positional feature, where object shape is explicitly encoded into the SPD feature. Fur-thermore, a Semantic-shape Adaptive Feature Modulation (SAFM) block is proposed to combine the given semantic map and our positional features to produce adaptively mod-ulated features. Extensive experiments demonstrate that the proposed SPD and SAFM significantly improve the gener-ation of objects with rich details. Moreover, our method performs favorably against the SOTA methods in terms of quantitative and qualitative evaluation. The source code and model are available at SAFM. 1.

Introduction
Semantic image synthesis is a kind of conditional im-age generation task, which aims to generate semantically aligned and photo-realistic images with the given seman-tic maps. Compared to unconditional image generation, it has significant flexibility in image generation since we can flexibly control the generated image content by drawing or editing the input semantic maps. Semantic image synthe-sis has been widely used in many practical scenarios, e.g., content creation and image editing [7, 28, 33, 42].
Recently, Generative Adversarial Networks (GANs) [10] are broadly adopted to solve this problem and achieve im-pressive results. Most works attempt to model the map-ping between different semantic classes and visual appear-ances. Park et al. [28] propose to use spatially-adaptive transformations (SPADE) learned from the input seman-tic layouts to modulate the activations in the generator.
Figure 1. The given semantic map only provides an object-level layout, which is too coarse for generating images with rich details.
The part-level semantic layout is implied in the shape/contour of an object instance. By encoding object shape into the proposed
SPD feature, we can effectively exploit such part-level layouts for better image details generation.
CC-FPSE [21] subsequently extends SPADE by predicting spatially-varying conditional convolution kernels from the semantic layouts. Most recently, SC-GAN [36] exploits the learned semantic vectors to get spatially-variant and appearance-correlated convolution kernels and normaliza-tion parameters for the semantic stylization.
A semantic map has not only semantic labels but also a spatial layout. Such a spatial layout can be used to reg-ularize the semantic image synthesis. Generally, one ob-ject instance is composed of some object parts, and pixels from the same object part should have a similar appearance while pixels from distinct object parts should not. For in-stance, an object ‘car’ is composed of ‘window’, ‘wheel’, etc. Thus, the pixels from the ‘window’ should look differ-ent from those from the ‘wheel’. In contrast, two pixels both from the ‘window’ should look similar to each other. By ex-ploiting such a spatial layout, we can suppress artifacts and generate coherent image details.
Semantic layouts have been effectively exploited to im-prove image synthesis in previous methods. However, the given semantic map just captures an object-level layout for an image, which describes whether two pixels belong to the same object instance or not. It is too coarse to capture the
fine-grained structure of an object instance. If we could sub-tly exploit a part-level semantic layout, it will benefit the generation of image high-frequency details.
Obviously, the shape/contour of each object instance can be easily identified from the object-level semantic layout.
On the other hand, given the shape/contour of an object (e.g., a car), its part-level layout (e.g., the position of ‘win-dow’ or ‘wheel’) can be roughly inferred according to the prior knowledge of an object’s structure, as shown in Fig. 1.
Therefore, there is a strong connection between an object’s shape and its part-level layout, i.e., an object’s shape im-plies its part-level layout. Thus, the exploitation of an ob-ject’s part-level layout can be implicitly achieved by mod-eling its shape.
In this paper, we propose a Shape-aware Position De-scriptor (SPD) to describe each pixel’s positional feature.
Our SPD describes the relative relations (distance and an-gle) between each pixel inside an object instance and pixels on its contour, as shown in Figure. 2 (a). Thus, the informa-tion of object shape has been encoded into each pixel’s SPD feature. In other words, the clue of an object’s part-level layout has been implicitly encoded into the SPD feature.
Next, we design the Semantic-shape Adaptive Feature
Modulation (SAFM) block to combine the given semantic map and our SPD features together, and modulate the in-put features adaptively. Specifically, our SAFM block first conditionally produces semantic-specific convolution ker-nels, and then performs semantic-specific convolution on the SPD features. At last, the SAFM block accepts in-put feature maps, adaptively modulates them, and forwards them to the next block, as shown in Figure. 2 (b).
Note that our SPD is inspired by the shape context de-scriptor [2] which describes the relations of pixels just on the shape contour, but our SPD describes the relations be-tween pixels inside an object and pixels on the contour.
Our main contributions can be summarized as follows:
• We propose a Shape-aware Position Descriptor (SPD) to describe the pixel’s positional feature, where the ob-ject’s part-level layout can be exploited and leveraged.
• We design a Semantic-shape Adaptive Feature Mod-ulation (SAFM) block, which combines the semantic maps and SPD features to produce adaptively modu-lated feature maps.
• Experimental results show that our method performs favorably on Cityscapes, COCO-stuff, and ADE20K datasets against SOTA methods and can generate more photo-realistic results with rich details. 2.