Abstract
The diversity and complexity of degradations in real-world video super-resolution (VSR) pose non-trivial chal-lenges in inference and training. First, while long-term propagation leads to improved performance in cases of mild degradations, severe in-the-wild degradations could be ex-aggerated through propagation, impairing output quality.
To balance the tradeoff between detail synthesis and arti-fact suppression, we found an image pre-cleaning stage in-dispensable to reduce noises and artifacts prior to propa-gation. Equipped with a carefully designed cleaning mod-ule, our RealBasicVSR outperforms existing methods in both quality and efficiency (Fig. 1). Second, real-world
VSR models are often trained with diverse degradations to improve generalizability, requiring increased batch size to produce a stable gradient. Inevitably, the increased com-putational burden results in various problems, including 1) speed-performance tradeoff and 2) batch-length trade-off. To alleviate the first tradeoff, we propose a stochas-tic degradation scheme that reduces up to 40% of training time without sacrificing performance. We then analyze dif-ferent training settings and suggest that employing longer sequences rather than larger batches during training al-lows more effective uses of temporal information, leading to more stable performance during inference. To facilitate fair comparisons, we propose the new VideoLQ dataset, which contains a large variety of real-world low-quality video se-quences containing rich textures and patterns. Our dataset can serve as a common ground for benchmarking. Code, models, and the dataset are publicly available at https:
//github.com/ckkelvinchan/RealBasicVSR. 1
1.

Introduction
In real-world video super-resolution (VSR), we aim at increasing the resolution of videos containing unknown degradations. The diversity of degradations in this task poses significant challenges in designing benchmarks and training settings, and hence earlier works assume either syn-thetic [4, 6, 35] or camera-specific [41] degradations and fo-cus on network designs. Although these works achieve re-markable success in restricted settings, the designs for these over-simplified scenarios cannot generalize well to the com-plex degradations in the wild. In addition, the complexity and diversity of degradations in real-world VSR introduce extra obstacles in both inference and training, including ar-tifact amplification and increased computational budgets.
This paper dives into the problems and tradeoffs in real-world VSR to share useful experiences in addressing the task.
It is shown by Chan et al. [4] that long-term information is beneficial to restoration. However, in real-world VSR, such information could also result in exaggerated artifacts, owing to error accumulation during propagation. This phe-nomenon leads to a tradeoff between enhancing details and suppressing artifacts, since the synthesizing power of a net-work comes at the cost of amplifying noises and artifacts.
In this work, we show that a simple solution can sufficiently remedy this tradeoff. In particular, we place an image clean-ing module prior to propagation for removing degradations in the input images. The resulting model RealBasicVSR avoids amplification of artifacts and achieves improved out-put quality while maintaining simplicity. We further de-velop a dynamic refinement scheme that repeatedly applies the cleaning module to remove excessive degradations in the inputs. Our scheme allows a flexible tradeoff between smoothness and detailedness, which can be adjusted based on a pre-defined threshold or user preference. A system-atic analysis of different combinations of losses and archi-tectures is conducted to demonstrate the significance of our designs.
Real-world VSR models are generally trained with di-verse degradations to improve generalizability, and hence they are often trained with increased batch size to ensure stable gradient. As a result, real-world VSR usually re-quires a longer training time and more immense computa-tional resources than the non-blind counterpart. This work inspects two tradeoffs in real-world VSR to improve train-ing efficiency, hence shortening research cycles.
First, with increased batch size, training with long se-quences is prohibitive owing to the I/O bottleneck induced by hardware limitations. The bottleneck is often alleviated by reducing either the batch size or sequence length, which results in degraded performance. To ameliorate the prob-lem, we propose a stochastic degradation scheme that ef-fectively reduces the I/O bottleneck without sacrificing the output quality. Notably, our degradation scheme yields up to 40% reduction of training time in comparison to the con-ventional training scheme.
Second, with a fixed computational budget, the increased batch size in real-world VSR inevitably decreases sequence length during training. We are interested in the tradeoff be-tween them with an aim to search for a more effective set-ting. To this end, we compare models trained with differ-ent combinations of batch sizes and sequence lengths. We conclude that networks trained with longer sequences rather than larger batches could more effectively employ the long-term information in the input sequence, improving stability.
In addition to the studies above, we introduce a new benchmark for real-world VSR. Most existing bench-marks [25, 32, 40, 42] are constructed by contaminating the high-resolution (HR) videos with pre-defined degradations.
The most recent RealVSR dataset [41] exploits the dual-camera system in iPhone to capture paired data. Yet, the
RealVSR dataset consists of only degradations specifically for the iPhone camera. With only pre-defined degradations, the benchmarks mentioned above cannot accurately reflect the generalizability of the models on real-world videos. In this work, we propose VideoLQ, a real-world video dataset consisting of diverse LR videos to cover various contents, resolutions, and degradations. Our dataset could serve as a common benchmark for future methods. We test existing methods on our datasets. Their quantitative and qualitative results and our dataset will be released for ease of future research. 2.