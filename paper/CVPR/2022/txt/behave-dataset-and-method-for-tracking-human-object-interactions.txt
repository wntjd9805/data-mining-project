Abstract
Modelling interactions between humans and objects in natural environments is central to many applications in-cluding gaming, virtual and mixed reality, as well as human behavior analysis and human-robot collaboration. This challenging operation scenario requires generalization to vast number of objects, scenes, and human actions. Un-fortunately, there exist no such dataset. Moreover, this data needs to be acquired in diverse natural environments, which rules out 4D scanners and marker based capture systems.
We present BEHAVE dataset, the ﬁrst full body human-object interaction dataset with multi-view RGBD frames and corresponding 3D SMPL and object ﬁts along with the annotated contacts between them. We record (cid:24)15k frames at 5 locations with 8 subjects performing a wide range of interactions with 20 common objects. We use this data to learn a model that can jointly track humans and ob-jects in natural environments with an easy-to-use portable multi-camera setup. Our key insight is to predict corre-spondences from the human and the object to a statistical body model to obtain human-object contacts during inter-actions. Our approach can record and track not just the humans and objects but also their interactions, modeled as surface contacts, in 3D. Our code and data can be found at: http://virtualhumans.mpi-inf.mpg.de/behave. 1.

Introduction
The last decade has seen rapid progress in modelling the appearance of humans ranging from body pose, shape [52, 58, 60, 61, 81], faces [74] and even detailed clothing [5, 7, 11, 57, 65]. With various practical use cases like virtual try-on, personalised avatar creation, and several applications
Figure 1. Given a multi-view RGBD sequence, our method tracks the human, the object and their contacts in 3D. in augmented and mixed reality, or human-robot collabora-tion, the focus on humans is justiﬁed. Beyond modelling ap-pearance, few methods have focused on capturing and syn-thesizing human interactions (human-object/scene interac-tion). There exists work to capture humans in a static 3D scene [33], even without using external cameras [30], and work to synthesize static poses [34, 49], or full body move-ment [32, 32, 50, 68] in a 3D scene.
These methods show growing interest in modelling hu-man behavior, highlighting a need to capture real hu-man interactions. Existing methods [32, 68] however are learned from high quality curated data captured using opti-cal marker based motion capture systems or wearable sen-sors. Unfortunately, such commercial systems are expen-sive, drastically limit the interactions that can be captured, and often fail when tracking humans and objects under oc-clusion. In addition, the recording volume is spatially con-ﬁned and difﬁcult to re-locate, thus limiting the activities, scenes, and objects that can be captured. Wearable sen-sors [30] are not restricted in volume, but close range in-teraction can not be accurately captured. Altogether, the
lack of diverse 3D interaction data, and the lack of accu-rate and ﬂexible capture methods both constitute barriers in modelling human behavior.
With the goal of simplifying the data capture process and hence allowing faster progress in the ﬁeld, we propose
BEHAVE, a method to capture diverse 3D human interac-tions in natural environments, using a setup comprising of portable, cheap, and easy to use RGBD cameras. Track-ing human interactions from sparse consumer grade cam-eras is however extremely challenging. Depth data is inher-ently noisy and incomplete. Moreover, the person and ob-ject occlude each other frequently during interactions. Fur-thermore, capturing interactions requires estimating human-object contacts accurately, which is difﬁcult because con-tacts represent small regions in the image, close to the ob-servable (resolution) limit. This requires innovation that goes signiﬁcantly beyond the current state of the art track-ers. We propose to track the human using a parametric hu-man model (such as SMPL [52]) and track objects using template meshes. Naively ﬁtting the human model and an object 3D template to the point-cloud completely fails due to the aforementioned challenges. Our key idea is to train a neural model which jointly completes the human and object shape, represented with implicit surfaces, while predicting a correspondence ﬁeld to the human, as well as an object orientation ﬁeld. These rich outputs allow us to formulate a powerful human-object ﬁtting objective which is robust to missing data, noise and occlusion.
To train and evaluate BEHAVE, we capture the largest dataset of human-object interactions in natural environ-ments. The BEHAVE dataset contains 20 3D objects, 8 subjects (5 male, 3 female), 5 different locations and totals around 15.2k frames of recording. We provide ground truth
SMPL and 3D object meshes as well as contacts.
Our contributions can be summarized as follows:
• We propose the ﬁrst approach that can accurately 3D track humans, objects and contacts in natural environ-ments using multi-view RGBD images.
• We collect the largest dataset of multi-view RGBD se-quences and corresponding human models, object and contact annotations. See Sec. 3 for details regarding its usefulness to the community.
• Since there exists no publicly available code and datasets to accurately track human-object interactions in natural environments, we will release our code and data for further research in this direction. 2.