Abstract
Time-of-Flight (ToF) cameras are subject to high lev-els of noise and distortions due to Multi-Path-Interference (MPI). While recent research showed that 2D neural net-works are able to outperform previous traditional State-of-the-Art (SOTA) methods on correcting ToF-Data, little research on learning-based approaches has been done to make direct use of the 3D information present in depth im-ages. In this paper, we propose an iterative correcting ap-proach operating in 3D space, that is designed to learn on 2.5D data by enabling 3D point convolutions to correct the points’ positions along the view direction. As labeled real world data is scarce for this task, we further train our net-work with a self-training approach on unlabeled real world data to account for real world statistics. We demonstrate that our method is able to outperform SOTA methods on several datasets, including two real world datasets and a new large-scale synthetic data set introduced in this paper. 1.

Introduction
Time of Flight (ToF) cameras are devices that are able to capture depth information of a scene by measuring the time the light emitted by the device needs to travel back once intersecting with an object. In practice, timing the re-ception of a light impulse requires precise and costly hard-ware and, as a result, consumer-level ToF cameras perform indirect measurements. The most common types are so-called Amplitude-Modulated Continuous-Wave (AMCW)
ToF cameras, as they are for example used by the Kinect.
The working principle of these AMCW cameras is to emit a periodically modulated light signal and retrieve the phase shift of the received signal, through which the travel time of the light and, consequently, the distance of the object to the camera is given [12]. However, the continuous illumi-nation of the scene inevitably leads to Multi-Path Interfer-ence (MPI), as not only the direct reflection is received but also indirectly illumination which significantly impairs the depth estimation. Furthermore, these ToF cameras suffer
Figure 1. Given an initial ToF depth reconstruction our method projects the problem into a latent 3D space. The 3D point positions are updated iteratively along the camera rays using RADU point convolutions, in order to optimize the final 2D depth prediction. from low Signal to Noise Ratios (SNR) on dark surfaces and the mixed pixel effect along sharp object edges [12].
Since deep learning approaches have shown great suc-cess in visual computing problems, many works have inves-tigated the capabilities of 2D neural networks to correct ToF depth images [1, 3, 4, 10, 11, 29]. However, existing meth-ods treat the task of ToF correction as a 2D image problem and do not take into account the explicit 3D information in their computations. In these works, the depth information is usually used as an input to standard Convolutional Neu-ral Networks (CNN) for images [1, 3, 19, 27, 28], while the underlying 3D structure is not analyzed. In this work in-stead, we propose a new neural network architecture that projects the problem into the 3D domain and makes use of point convolutional neural networks [14] to analyze the er-roneous reconstruction and adjust the point positions along the view direction, see Fig. 1. This iterative process makes small changes to the point positions to reduce the error level in between the convolutional layers and improve the final depth reconstruction. Further, we propose a novel fine-tuning procedure for Unsupervised Domain Adaptation (U-DA) based on self-training methods, to transfer the knowl-edge acquired by our network from synthetic to real world
ToF data. The effectiveness of this approach is evaluated on both synthetic and real datasets and proves to be able to outperform existing methods. Moreover, we introduce a large-scale high-resolution dataset consisting of challenging scenes, containing high MPI levels, materials which pro-duce low SNR captures and objects with high frequency de-tails. In summary, our contributions are:
• A novel architecture for correcting of ToF images in a latent 3D space.
• A two-staged training procedure with a cyclic self-training approach designed to bridge the gap between synthetic and real world ToF images.
• A large-scale high-resolution synthetic ToF dataset containing measurements for scenes with high MPI.
Our synthetic data set, code and trained networks are avail-able at https://github.com/schellmi42/RADU. 2.