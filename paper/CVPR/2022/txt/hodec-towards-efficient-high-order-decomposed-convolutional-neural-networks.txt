Abstract
High-order decomposition is a widely used model com-pression approach towards compact convolutional neural networks (CNNs). However, many of the existing solutions, though can efficiently reduce CNN model sizes, are very dif-ficult to bring considerable saving for computational costs, especially when the compression ratio is not huge, thereby causing the severe computation inefficiency problem. To overcome this challenge, in this paper we propose efficient
High-Order DEcomposed Convolution (HODEC). By per-forming systematic explorations on the underlying reason and mitigation strategy for the computation inefficiency, we develop a new decomposition and computation-efficient ex-ecution scheme, enabling simultaneous reductions in com-putational and storage costs.
To demonstrate the effectiveness of HODEC, we per-form empirical evaluations for various CNN models on dif-ferent datasets. HODEC shows consistently outstanding compression and acceleration performance. For compress-ing ResNet-56 on CIFAR-10 dataset, HODEC brings 67% fewer parameters and 62% fewer FLOPs with 1.17% ac-curacy increase than the baseline model. For compress-ing ResNet-50 on ImageNet dataset, HODEC achieves 63%
FLOPs reduction with 0.31% accuracy increase than the uncompressed model. 1.

Introduction
Deep neural networks (DNNs) have been widely adopted in various fundamental and downstream computer vision tasks, such as image classification [11], object detection
[30], action recognition [44], super-resolution [15], seismic signal analysis [28] and channel decoding [18]. Consid-ering the inherent high computational and storage costs of modern large-scale neural networks, in recent years many model compression approaches have been proposed and developed. For instance, by exploring and removing the model redundancy at the neuron level and bit level, respec-tively, pruning [1, 9, 10, 13, 16, 21, 22, 35, 43] and quanti-zation [4, 14, 23, 29, 33] are the popular methods that can efficiently reduce DNN model sizes while preserving high task accuracy.
DNN Compression via Tensor Decomposition. Alter-natively, the redundancy of a DNN model can also exhibit and be further reduced at the network topology level. Moti-vated by this philosophy and observation, high-order tensor decomposition, a technique that explores multi-dimensional low-rankness of DNN models, has been proposed and stud-ied in the recent years. By decomposing the original large-scale weight matrices and/or weight tensors to a set of small tensor cores, tensor decomposition-based compression can bring significant reduction in neural network sizes. Notably, as reported in many prior works [2, 3, 24, 26, 37–42], the state-of-the-art tensor decomposition approaches, e.g., ten-sor train (TT) and its variant tensor ring (TR) [25, 45], can achieve ultra-high compression ratio (more than 1,000ˆ) for various recurrent neural networks (RNNs). Evidently, such outstanding compression performance is very attrac-tive for designing and producing compact DNN models.
Limitations on Computation Efficiency. Despite their very promising potentials in model size reduction, the ad-vanced TT and TR decomposition approaches suffer severe computation inefficiency problem, especially when aiming to accelerate the computation-intensive convolutional neu-ral networks (CNNs). To be specific, when a CNN model is decomposed to the TT or TR format, though its total num-ber of weight parameters is indeed reduced significantly, the corresponding computational cost (a.k.a., floating point op-erations per second (FLOPs)) is not saved accordingly, and sometimes it even increases if only moderate compression ratio is allowed to preserve accuracy. For instance, when using the classical TT decomposition [7,24] to factorize the convolutional layer, with 2.02ˆ compression ratio setting a
TT-format CNN model will even suffer around five times higher computational cost than the original uncompressed network. Consequently, consider 1) CNNs are very funda-mental neural network models that have been very popu-larly deployed in practice; and 2) tremendous computer vi-sion tasks, e.g., object detection and motion prediction, are very time-sensitive applications and highly demand real-time processing and acceleration; such computation ineffi-ciency problem of the tensor decomposition approaches, if cannot be addressed properly, will severely hinder their fur-ther widespread deployment in many practical applications.
Technical Preview and Contributions. To overcome this severe computation inefficiency challenge for the ad-vanced tensor decomposed CNNs, in this paper we pro-pose to study and develop efficient High-Order tensor DE-composed Convolution (HODEC). By performing system-atic analysis and exploration on the underlying reason and mitigation strategy for the inefficiency of the classical TT-format convolution, we further propose and develop the new decomposition and execution scheme towards computation-efficient TT-format convolutional layer 1. As a type of plug-in component, this new tensor decomposed layer can be di-rectly used in the CNN models and bring simultaneous re-duction in computational and storage costs. Overall, the contributions of this paper are summarized as follows:
• We systematically study and analyze the underlying reason for the computation inefficiency of the conven-tional TT-format convolution. We then identify and develop a series of strategies that can further mitigate this problem and reduce the computational costs.
• Based upon the obtained understanding from our anal-ysis, we further propose and develop the new decom-position and execution scheme for the computation-efficient TT-format convolutional layer, which enjoys very significant reduction in computational costs and memory consumption.
• We perform empirical evaluations for various CNN models on different datasets, and the experimental re-sults show that HODEC can consistently outperform the existing pruning and low-rank matrix/tensor de-composition approaches. For ResNet-56 on CIFAR-10 dataset, using HODEC can bring 67% fewer model parameters and 62% fewer FLOPs with 1.17% accu-racy increase than the baseline. For ResNet-50 on Im-ageNet dataset, HODEC can achieve 63% FLOPs re-duction with 0.31% accuracy increase than the uncom-pressed model. 2.