Abstract
While methods that regress 3D human meshes from im-ages have progressed rapidly, the estimated body shapes of-ten do not capture the true human shape. This is problem-atic since, for many applications, accurate body shape is as important as pose. The key reason that body shape accuracy lags pose accuracy is the lack of data. While humans can la-bel 2D joints, and these constrain 3D pose, it is not so easy to “label” 3D body shape. Since paired data with images and 3D body shape are rare, we exploit two sources of infor-mation: (1) we collect internet images of diverse “fashion” models together with a small set of anthropometric mea-surements; (2) we collect linguistic shape attributes for a wide range of 3D body meshes and the model images. Taken together, these datasets provide sufficient constraints to in-fer dense 3D shape. We exploit the anthropometric mea-surements and linguistic shape attributes in several novel ways to train a neural network, called SHAPY, that re-gresses 3D human pose and shape from an RGB image.
We evaluate SHAPY on public benchmarks, but note that they either lack significant body shape variation, ground-truth shape, or clothing variation. Thus, we collect a new dataset for evaluating 3D human shape estimation, called
HBW, containing photos of “Human Bodies in the Wild” for which we have ground-truth 3D body scans. On this new benchmark, SHAPY significantly outperforms state-of-the-art methods on the task of 3D body shape estimation.
This is the first demonstration that 3D body shape regres-sion from images can be trained from easy-to-obtain an-thropometric measurements and linguistic shape attributes.
Our model and data are available at: shapy.is.tue.mpg.de 1.

Introduction
The field of 3D human pose and shape (HPS) estimation is progressing rapidly and methods now regress accurate 3D pose from a single image [7, 26, 28, 30–33, 43, 65, 67]. Un-fortunately, less attention has been paid to body shape and many methods produce body shapes that clearly do not rep-resent the person in the image (Fig. 1, top right). There are several reasons behind this. Current evaluation datasets fo-cus on pose and not shape. Training datasets of images with 3D ground-truth shape are lacking. Additionally, humans appear in images wearing clothing that obscures the body, making the problem challenging. Finally, the fundamental scale ambiguity in 2D images, makes 3D shape difficult to estimate. For many applications, however, realistic body shape is critical. These include AR/VR, apparel design, vir-tual try-on, and fitness. To democratize avatars, it is impor-tant to represent and estimate all possible 3D body shapes; we make a step in that direction.
Note that commercial solutions to this problem require users to wear tight fitting clothing and capture multiple images or a video sequence using constrained poses.
In contrast, we tackle the unconstrained problem of 3D body shape estimation in the wild from a single RGB image of a person in an arbitrary pose and standard clothing.
Most current approaches to HPS estimation learn to regress a parametric 3D body model like SMPL [37] from images using 2D joint locations as training data. Such joint locations are easy for human annotators to label in images.
Supervising the training with joints, however, is not suffi-cient to learn shape since an infinite number of body shapes can share the same joints. For example, consider someone who puts on weight. Their body shape changes but their joints stay the same. Several recent methods employ addi-tional 2D cues, such as the silhouette, to provide additional shape cues [51, 52]. Silhouettes, however, are influenced by clothing and do not provide explicit 3D supervision. Syn-thetic approaches [35], on the other hand, drape SMPL 3D bodies in virtual clothing and render them in images. While this provides ground-truth 3D shape, realistic synthesis of clothed humans is challenging, resulting in a domain gap.
To address these issues, we present SHAPY, a new deep neural network that accurately regresses 3D body shape and pose from a single RGB image. To train SHAPY, we first need to address the lack of paired training data with real im-ages and ground-truth shape. Without access to such data, we need alternatives that are easier to acquire, analogous to 2D joints used in pose estimation. To do so, we introduce two novel datasets and corresponding training methods.
First, in lieu of full 3D body scans, we use images of peo-ple with diverse body shapes for which we have anthropo-metric measurements such as height as well as chest, waist, and hip circumference. While many 3D human shapes can share the same measurements, they do constrain the space of possible shapes. Additionally, these are important mea-surements for applications in clothing and health. Accurate anthropometric measurements like these are difficult for in-dividuals to take themselves but they are often captured for
Figure 2. Model-agency websites contain multiple images of mod-els together with anthropometric measurements. A wide range of body shapes are represented; example from pexels.com.
Figure 3. We crowd-source scores for linguistic body-shape attributes [57] and compute anthropometric measurements for
CAESAR [47] body meshes. We also crowd-source linguistic shape attribute scores for model images, like those in Fig. 2 different applications. Specifically, modeling agencies pro-vide such information about their models; accuracy is a re-quirement for modeling clothing. Thus, we collect a diverse set of such model images (with varied ethnicity, clothing, and body shape) with associated measurements; see Fig. 2.
Since sparse anthropometric measurements do not fully constrain body shape, we exploit a novel approach and also use linguistic shape attributes. Prior work has shown that people can rate images of others according to shape at-tributes such as “short/tall”, “long legs” or “pear shaped”
[57]; see Fig. 3. Using the average scores from several raters, Streuber et al. [57] (BodyTalk) regress metrically ac-curate 3D body shape. This approach gives us a way to eas-ily label images of people and use these labels to constrain 3D shape. To our knowledge, this sort of linguistic shape attribute data has not previously been exploited to train a neural network to infer 3D body shape from images.
We exploit these new datasets to train SHAPY with three novel losses, which can be exploited by any 3D human body reconstruction method: (1) We define functions of the
SMPL body mesh that return a sparse set of anthropomet-ric measurements. When measurements are available for an image we use a loss that penalizes mesh measurements that differ from the ground-truth (GT). (2) We learn a “Shape to
Attribute” (S2A) function that maps 3D bodies to linguis-tic attribute scores. During training, we map meshes to at-tribute scores and penalize differences from the GT scores. (3) We similarly learn a function that maps “Attributes to
Shape” (A2S). We then penalize body shape parameters that deviate from the prediction.
We study each term in detail to arrive at the final method.
Evaluation is challenging because existing benchmarks with
GT shape either contain too few subjects [61] or have lim-ited clothing complexity and only pseudo-GT shape [51].
We fill this gap with a new dataset, named “Human Bodies in the Wild” (HBW), that contains a ground-truth 3D body scan and several in-the-wild photos of 35 subjects, for a to-tal of 2543 photos. Evaluation on this shows that SHAPY estimates much more accurate 3D shape.
Models, data and code are available at shapy.is.tue.mpg.de. 2.