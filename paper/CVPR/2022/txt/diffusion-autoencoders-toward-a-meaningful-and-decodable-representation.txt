Abstract
Diffusion probabilistic models (DPMs) have achieved re-markable quality in image generation that rivals GANs’.
But unlike GANs, DPMs use a set of latent variables that lack semantic meaning and cannot serve as a useful rep-resentation for other tasks. This paper explores the possi-bility of using DPMs for representation learning and seeks to extract a meaningful and decodable representation of an input image via autoencoding. Our key idea is to use a learnable encoder for discovering the high-level seman-tics, and a DPM as the decoder for modeling the remaining stochastic variations. Our method can encode any image into a two-part latent code where the ﬁrst part is semanti-cally meaningful and linear, and the second part captures stochastic details, allowing near-exact reconstruction. This capability enables challenging applications that currently foil GAN-based methods, such as attribute manipulation on real images. We also show that this two-level encoding im-proves denoising efﬁciency and naturally facilitates various downstream tasks including few-shot conditional sampling.
Please visit our page: https://Diff-AE.github.io/ 1.

Introduction
Diffusion-based (DPMs) [22, 46] and score-based [49] generative models have recently succeeded in synthesiz-ing realistic and high-resolution images, rivaling those from
GANs [11, 15, 23]. These two models are closely related and, in practice, optimize similar objectives. Numerous applications have emerged notably in the image domain, such as image manipulation, translation, super-resolution
[8,32,35,43], in speech and text domains [5,6], or 3D point cloud [34]. Recent studies have improved DPMs further in both theory and practice [25,29,31]. In this paper, however, we question whether DPMs can serve as a good represen-tation learner. Speciﬁcally, we seek to extract a meaning-ful and decodable representation of an image that contains high-level semantics yet allows near-exact reconstruction of the image. Our exploration focuses on diffusion models, but the contributions are applicable also to score-based models.
One way to learn a representation is through an autoen-coder. There exists a certain kind of DPM [47] that can act as an encoder-decoder that converts any input image x0 into a spatial latent variable xT by running the generative pro-cess backward. However, the resulting latent variable lacks high-level semantics and other desirable properties, such as disentanglement, compactness, or the ability to perform meaningful linear interpolation in the latent space. Alter-natively, one can use a trained GAN for extracting a repre-sentation using the so-called GAN inversion [28,58], which optimizes for a latent code that reproduces the given input.
Even though the resulting code carries rich semantics, this technique struggles to faithfully reconstruct the input im-age. To overcome these challenges, we propose a diffusion-based autoencoder that leverages the powerful DPMs for decodable representation learning.
Finding a meaningful representation that is decodable re-quires capturing both the high-level semantics and low-level stochastic variations. Our key idea is to learn both levels of representation by utilizing a learnable encoder for discov-ering high-level semantics and utilizing a DPM for decod-ing and modeling stochastic variations.
In particular, we use our conditional variant of the Denoising Diffusion Im-plicit Model (DDIM) [47] as the decoder and separate the latent code into two subcodes. The ﬁrst “semantic” sub-code is compact and inferred with a CNN encoder, whereas the second “stochastic” subcode is inferred by reversing the generative process of our DDIM variant conditioned on the semantic subcode. In contrast to other DPMs, DDIM modi-ﬁes the forward process to be non-Markovian while preserv-ing the training objectives of DPMs. This modiﬁcation al-lows deterministically encoding an image to its correspond-ing initial noise, which represents our stochastic subcode.
The implication of this framework is two-fold. First, by conditioning DDIM on the semantic information of the target output, denoising becomes easier and faster. Sec-ond, this design produces a representation that is linear, semantically meaningful, and decodable—a novel property for DPMs’ latent variables. This crucial property allows harnessing DPMs for many tasks including those that are highly challenging for any GAN-based methods, such as in-terpolation and attribute manipulation on real images. Un-like GANs, which rely on error-prone inversion before op-erating on real images, our method requires no optimization to encode the input and produces high-quality output with original details preserved.
Despite being an autoencoder, which is generally not de-signed for unconditional generation, our framework can be used to generate image samples by ﬁtting another DPM to the semantic subcode distribution. This combination achieves competitive FID scores on unconditional genera-tion compared to a vanilla DPM. Moreover, the ability to sample from our compact and meaningful latent space also enables few-shot conditional generation (i.e., generate im-ages with similar semantics to those of a few examples).
Compared to other DPM-based techniques for the few-shot setup, our method produces convincing results with only a handful labeled examples without additional contrastive learning used in prior work [45]. 2.