Abstract
One major goal of the AI security community is to securely and reliably produce and deploy deep learning models for real-world applications. To this end, data poisoning based backdoor attacks on deep neural networks (DNNs) in the production stage (or training stage) and corresponding defenses are extensively explored in recent years.
Ironi-cally, backdoor attacks in the deployment stage, which can often happen in unprofessional users’ devices and are thus arguably far more threatening in real-world scenarios, draw much less attention of the community. We attribute this imbalance of vigilance to the weak practicality of existing deployment-stage backdoor attack algorithms and the insufﬁciency of real-world attack demonstrations. To
ﬁll the blank, in this work, we study the realistic threat of deployment-stage backdoor attacks on DNNs. We base our study on a commonly used deployment-stage attack paradigm — adversarial weight attack, where adversaries selectively modify model weights to embed backdoor into deployed DNNs. To approach realistic practicality, we propose the ﬁrst gray-box and physically realizable weights attack algorithm for backdoor injection, namely subnet replacement attack (SRA), which only requires architecture information of the victim model and can support physical triggers in the real world. Extensive real-world simulations and system-level experimental attack demonstrations are conducted. Our results not only suggest the effectiveness and practicality of the proposed attack algorithm, but also reveal the practical risk of a novel type of computer virus that may widely spread and stealthily inject backdoor into DNN models in user devices. By our study, we call for more attention to the vulnerability of DNNs in the deployment stage. 1.

Introduction
While deep learning models are marching ambitiously towards human-level performance and increasingly de-* Equal Contribution ployed in real-world applications [9, 20, 47, 54, 57], their vulnerability issues [13,21,23,24,55,59,65,75] have raised great concerns. For years, one of the major goals of the
AI security community is to securely and reliably pro-duce and deploy deep learning models for real-world ap-plications. To this end, data poisoning based backdoor at-tacks [13,23,55,75] on deep neural networks (DNNs) in the production stage (or training stage) and corresponding de-fenses [12, 14, 77] are extensively explored in recent years.
Commonly studied backdoor attack methods rely on ad-versaries’ involvement in the model production stage (train-ing stage) — attackers either inject multiple poisoned sam-ples into the training set [13,25] or provide pre-trained mod-els with backdoors for downstream applications [32, 60].
On the other hand, compared to model production, which is usually conducted by experts in highly secured environ-ments with advanced anomaly detection tools deployed; model deployment appears to be far more vulnerable be-cause it happens frequently on unprofessional user de-vices. Ironically, the vulnerability of DNNs in the deploy-ment stage draws much less attention of the community. We attribute this imbalance of vigilance to the weak practical-ity of existing deployment-stage attack algorithms and the insufﬁciency of real-world attack demonstrations.
To be speciﬁc, we highlight the most commonly used paradigm by existing deployment-stage backdoor attacks — adversarial weight attack [7, 41], where adversaries se-lectively modify model parameters to embed backdoor into deployed DNNs. Existing work under this paradigm [4, 7, 40, 41, 50–52, 80] heavily relies on gradient-based tech-niques (white-box settings) to identify a set of weights to overwrite. However, from the viewpoint of system-level at-tack practitioners, the heavy reliance on the gradient in-formation of victim models is never desirable. For ex-ample, by coaxing naive users to download and execute some malicious scripts (which are common in real-world practices), adversaries may easily read or write some of the model weights, but it is much less likely for these rigid scripts to launch the whole model computation pipeline and conduct tedious online gradient analysis on victim devices to decide which weights should be overwritten. Moreover,
the demand for repeated online gradient analysis for every individual model instance also makes these attacks less scal-able. On the other hand, the real-world attack demon-strations for this paradigm are neither sufﬁcient. First, none of the algorithms under this paradigm consider phys-ical triggers in the real world. Second, existing studies ei-ther only consider simple simulations (directly modifying weights in python scripts) [4, 80] or conduct complex hard-ware practice (using laser beam to physically ﬂip memory bits in embedded systems) [7], which are both far from realistic scenarios for attacking ordinary users. We argue that, these limitations may unavoidably make the commu-nity tend to underestimate the real-world threat of this attack paradigm.
To ﬁll the blank, in this work, we take designing and demonstrating practical deployment-stage backdoor attacks as our main focus.
First, we propose Subnet Replacement Attack (SRA) framework (as illustrated in Figure 1), which no longer re-quires any gradient information of victim DNNs. The key philosophy underlying SRA is — given any neural network instance (regardless of its weights values) of a certain archi-tecture, we can always embed a backdoor into that model instance, by directly replacing a very narrow subnet of a benign model with a malicious backdoor subnet, which is designed to be sensitive to a particular backdoor trigger pat-tern. Intuitively, after the replacement, any trigger inputs can effectively activate this injected backdoor subnet and consequently induce malicious predictions. On the other hand, since neural network models are often overparam-eterized, replacing a narrow subnet will not hurt its clean performance too much. To show its theoretic feasibility, we
ﬁrst simulate SRA via directly modifying model weights in
Python scripts. Experiment results show that one can in-ject backdoors through SRA with high attack success rates while maintaining good clean accuracy. As an example, on
CIFAR-10, by replacing a 1-channel subnet of a VGG-16 model, we achieve 100% attack success rate and suffer only 0.02% clean accuracy drop. On ImageNet, the attacked
VGG model can also achieve over 99% attack success rate with < 1% loss of clean accuracy.
Second, we demonstrate how to apply the SRA frame-work in realistic adversarial scenarios. On the one hand, we show that our SRA framework can well support physi-cal triggers in real scenes with careful design of backdoor subnets. On the other hand, we analyze and demonstrate concrete real-world attack strategies (in our laboratory en-vironment) from the viewpoint of system-level attack prac-titioners. Our study shows that the proposed SRA frame-work is highly compatible with traditional system-level at-tack [6, 43, 44, 64, 78] practices (e.g. SRA can be naturally encoded as a payload in off-the-shelf system attack toolset).
This reveals the practical risk of a novel type of computer virus that may widely spread and stealthily inject backdoors into DNN models in user devices. Our code is publicly available for reproducibility 1.
Technical Contributions. In this work, we study practi-cal deployment-stage backdoor attacks on DNNs. Our main contributions are three-fold:
• We point out that backdoor attacks in the deployment stage, which can often happen in devices of unprofes-sional users and are thus arguably far more threatening in real-world scenarios, draw much less attention of the community. We attribute this imbalance of vigilance to two problems: 1) the weak practicality of existing deployment-stage attack algorithms and 2) the insufﬁ-ciency of real-world attack demonstrations.
• We alleviate the ﬁrst problem by proposing the Sub-net Replacement Attack (SRA) framework, which does not require any gradient information of victim
DNNs and thus greatly improves the practicality of the deployment-stage adversarial weight attack paradigm.
Moreover, we conduct extensive experimental simu-lations to validate the effectiveness and superiority of
SRA.
• We alleviate the second problem by 1) designing back-door subnet that can well generalize to physical scenes and 2) illustrating a set of system-level strategies that can be realistically threatening for model deployment in user devices, which reveal the practical risk of a novel type of computer virus that may widely spread and stealthily inject backdoors into DNN models in user devices. 2.