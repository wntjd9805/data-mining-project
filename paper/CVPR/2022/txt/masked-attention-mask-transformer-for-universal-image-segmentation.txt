Abstract
Image segmentation groups pixels with different seman-tics, e.g., category or instance membership. Each choice of semantics deﬁnes a task. While only the semantics of each task differ, current research focuses on designing spe-cialized architectures for each task. We present Masked-attention Mask Transformer (Mask2Former), a new archi-tecture capable of addressing any image segmentation task
Its key components in-(panoptic, instance or semantic). clude masked attention, which extracts localized features by constraining cross-attention within predicted mask regions.
In addition to reducing the research effort by at least three times, it outperforms the best specialized architectures by a signiﬁcant margin on four popular datasets. Most no-tably, Mask2Former sets a new state-of-the-art for panoptic segmentation (57.8 PQ on COCO), instance segmentation (50.1 AP on COCO) and semantic segmentation (57.7 mIoU on ADE20K). 1.

Introduction
Image segmentation studies the problem of grouping pixels. Different semantics for grouping pixels, e.g., cat-egory or instance membership, have led to different types of segmentation tasks, such as panoptic, instance or seman-tic segmentation. While these tasks differ only in semantics, current methods develop specialized architectures for each task. Per-pixel classiﬁcation architectures based on Fully
Convolutional Networks (FCNs) [37] are used for semantic segmentation, while mask classiﬁcation architectures [5,24] that predict a set of binary masks each associated with a single category, dominate instance-level segmentation. Al-though such specialized architectures [6, 10, 24, 37] have advanced each individual task, they lack the ﬂexibility to generalize to the other tasks. For example, FCN-based ar-chitectures struggle at instance segmentation, leading to the evolution of different architectures for instance segmenta-tion compared to semantic segmentation. Thus, duplicate research and (hardware) optimization effort is spent on each
*Work done during an internship at Facebook AI Research.
Figure 1. State-of-the-art segmentation architectures are typically specialized for each image segmentation task. Although recent work has proposed universal architectures that attempt all tasks and are competitive on semantic and panoptic segmentation, they struggle with segmenting instances. We propose Mask2Former, which, for the ﬁrst time, outperforms the best specialized architec-tures on three studied segmentation tasks on multiple datasets. specialized architecture for every task.
To address this fragmentation, recent work [14, 62] has attempted to design universal architectures, that are capable of addressing all segmentation tasks with the same archi-tecture (i.e., universal image segmentation). These archi-tectures are typically based on an end-to-end set prediction objective (e.g., DETR [5]), and successfully tackle multiple tasks without modifying the architecture, loss, or the train-ing procedure. Note, universal architectures are still trained separately for different tasks and datasets, albeit having the same architecture. In addition to being ﬂexible, universal architectures have recently shown state-of-the-art results on semantic and panoptic segmentation [14]. However, re-cent work still focuses on advancing specialized architec-tures [20, 39, 45], which raises the question: why haven’t universal architectures replaced specialized ones?
Although existing universal architectures are ﬂexible enough to tackle any segmentation task, as shown in Fig-ure 1, in practice their performance lags behind the best specialized architectures. For instance, the best reported
performance of universal architectures [14, 62], is currently lower (> 9 AP) than the SOTA specialized architecture for instance segmentation [6]. Beyond the inferior per-formance, universal architectures are also harder to train.
They typically require more advanced hardware and a much longer training schedule. For example, training Mask-Former [14] takes 300 epochs to reach 40.1 AP and it can only ﬁt a single image in a GPU with 32G memory. In con-trast, the specialized Swin-HTC++ [6] obtains better perfor-mance in only 72 epochs. Both the performance and train-ing efﬁciency issues hamper the deployment of universal architectures.
In this work, we propose a universal image segmen-tation architecture named Masked-attention Mask Trans-former (Mask2Former) that outperforms specialized ar-chitectures across different segmentation tasks, while still being easy to train on every task. We build upon a sim-ple meta architecture [14] consisting of a backbone fea-ture extractor [25, 36], a pixel decoder [33] and a Trans-former decoder [51]. We propose key improvements that enable better results and efﬁcient training. First, we use masked attention in the Transformer decoder which restricts the attention to localized features centered around predicted segments, which can be either objects or regions depend-ing on the speciﬁc semantic for grouping. Compared to the cross-attention used in a standard Transformer decoder which attends to all locations in an image, our masked atten-tion leads to faster convergence and improved performance.
Second, we use multi-scale high-resolution features which help the model to segment small objects/regions. Third, we propose optimization improvements such as switching the order of self and cross-attention, making query features learnable, and removing dropout; all of which improve per-formance without additional compute. Finally, we save 3
⇥ training memory without affecting the performance by cal-culating mask loss on few randomly sampled points. These improvements not only boost the model performance, but also make training signiﬁcantly easier, making universal ar-chitectures more accessible to users with limited compute.
We evaluate Mask2Former on three image segmenta-tion tasks (panoptic, instance and semantic segmentation) using four popular datasets (COCO [35], Cityscapes [16],
ADE20K [65] and Mapillary Vistas [42]). For the ﬁrst time, on all these benchmarks, our single architecture performs on par or better than specialized architectures.
Mask2Former sets the new state-of-the-art of 57.8 PQ on
COCO panoptic segmentation [28], 50.1 AP on COCO in-stance segmentation [35] and 57.7 mIoU on ADE20K se-mantic segmentation [65] using the exact same architecture. 2.