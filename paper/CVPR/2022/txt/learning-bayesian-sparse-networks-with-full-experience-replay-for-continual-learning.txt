Abstract
Continual Learning (CL) methods aim to enable ma-chine learning models to learn new tasks without catas-trophic forgetting of those that have been previously mas-tered. Existing CL approaches often keep a buffer of previously-seen samples, perform knowledge distillation, or use regularization techniques towards this goal. Despite their performance, they still suffer from interference across tasks which leads to catastrophic forgetting. To amelio-rate this problem, we propose to only activate and select sparse neurons for learning current and past tasks at any stage. More parameters space and model capacity can thus be reserved for the future tasks. This minimizes the inter-ference between parameters for different tasks. To do so, we propose a Sparse neural Network for Continual Learn-ing (SNCL), which employs variational Bayesian sparsity priors on the activations of the neurons in all layers. Full
Experience Replay (FER) provides effective supervision in learning the sparse activations of the neurons in different layers. A loss-aware reservoir-sampling strategy is devel-oped to maintain the memory buffer. The proposed method is agnostic as to the network structures and the task bound-aries. Experiments on different datasets show that SNCL achieves state-of-the-art result for mitigating forgetting. 1.

Introduction
Humans continually acquire new skills and knowledge while maintaining the ability to perform tasks they learned in childhood. Continual Learning (CL) [26, 28] aims to mimic this process, enabling learning of new tasks sequen-tially without storing all of the associated data. However, even with the help of CL, deep neural networks often suf-fer from serious performance degradation on previously learned tasks when learning new ones. This phenomenon
*† indicates equal contribution. D. Gong (edgong01@gmail.com) is the corresponding author. This work was partly supported by the Centre for
Augmented Reasoning at the Australian Institute for Machine Learning.
Figure 1. Without continual learning, training on a new task in-terferes with the knowledge learned in mastering previous tasks, thus introducing the risk of catastrophic forgetting. Simple ex-perience replay approaches can help preserve the learned knowl-edge. However, the regularizations work on the network param-eters as a whole, invariably leading to forgetting. The proposed
SNCL learns sparse networks at all stages, enabling more intrinsic and common representations with less model capacity. It reserves more capacity/parameter space for future tasks, resulting in less interference and forgetting. is known as catastrophic forgetting [23].
The above issues motivate more effective CL algorithms
[6, 8, 24, 36] to mitigate catastrophic forgetting when the number of tasks and associated knowledge increases over time. For this purpose, existing approaches modify the the network learning strategies in different ways, such as the regularization methods [17, 20, 39], memory-based experi-ence replay [4, 7, 8, 22], and dynamic modular approaches
[1, 29, 37]. Specifically, Experience Replay (ER) meth-ods [4, 9, 22] mitigate catastrophic forgetting by replaying a selection of previously seen data maintained in a small episodic memory along with the new task data. Knowledge
Distillation (KD) [12,14] based methods [4,20,26] alleviate forgetting and encourage knowledge transfer by applying the old models as teachers. All these approaches learn all sequential tasks with the same whole parameter space. The learner potentially uses all neurons in the over-complete network to decrease the loss on current data. These char-acteristics inhibit the effectiveness in reducing the interfer-ence and invariably lead to forgetting (See Fig. 1).
In this work, we propose to learn Sparse neural Networks for Continual Learning (SNCL). We enforce the sparsity of the network neurons in the learning stages to reserve more parameters for future tasks. This prevents learning a new task from interfering with the parameters critical to achiev-ing previously learned tasks (See Fig. 1). Specifically, we introduce to use variational Bayesian sparsity (VBS) pri-ors on activations to induce a sparse network in a princi-pled manner. The sparse prior helps the model to learn a given task in the most compact set of neurons. This allows far greater control over the process of learning, and forget-ting. It particularly enables preserving of resources for fu-ture learning, and alleviates potential interference when new tasks are learned. A small replay buffer allows the common-ality between new and previous tasks to be exploited, thus increasing the network’s total learning capacity and reduc-ing the risk of forgetting.
The variational Bayesian framework flexibly allows learning the sparse networks by only relying on the back-propagation of the supervision signal (e.g., the classifica-tion loss). As a result, instead of performing experience replay with only data labels, we propose a more effective experience replay strategy by enforcing the stability of the intermediate representations of old samples. We label this approach Full Experience Replay (FER). To achieve this we store not only the past data in the memory but also the cor-responding logits and intermediate layer features. FER thus helps activate the appropriate neurons in each layer more ef-fectively, which helps avoid representation drift. Note that, in contrast to many similar knowledge distillation-based methods [20, 26], the proposed approach is not limited to the case where the task boundaries are known. We also in-troduce an effective Loss-aware Reservoir Sampling (LRS) strategy to maintain the memory, which selects samples on the basis of their ability to improve performance. In sum-mary, our contributions are as follows:
• We propose to learn sparse networks for continual learning with variational Bayesian sparsity priors on the neurons (SNCL). The proposed method alleviates the catastrophic forgetting and interference by enforc-ing sparsity of the networks to reserve parameters for future tasks. The memory based experience reply al-lows the commonality between new and previous tasks to be exploited, thus increasing the network’s total learning capacity and reducing the risk of forgetting.
• We propose Full Experience Replay (FER) to store and replay with old samples’ intermediate layer features generated when observed in data steam, unlike pre-vious methods with only labels. It provides more ef-fective supervision on learning the sparse activation of the neurons at different layers. A loss-aware reservoir sampling strategy is proposed to maintain the memory.
• The proposed approaches can generally improve the performance of the ER based methods, achieving state-of-the-art results under the same setting. The proposed method is agnostic and general to the network struc-tures and task boundaries. 2.