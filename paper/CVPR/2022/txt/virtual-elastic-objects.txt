Abstract 1.

Introduction
We present Virtual Elastic Objects (VEOs): virtual ob-jects that not only look like their real-world counterparts but also behave like them, even when subject to novel in-teractions. Achieving this presents multiple challenges: not only do objects have to be captured including the physi-cal forces acting on them, then faithfully reconstructed and rendered, but also plausible material parameters found and simulated. To create VEOs, we built a multi-view capture system that captures objects under the influence of a com-pressed air stream. Building on recent advances in model-free, dynamic Neural Radiance Fields, we reconstruct the objects and corresponding deformation fields. We propose to use a differentiable, particle-based simulator to use these deformation fields to find representative material parame-ters, which enable us to run new simulations. To render simulated objects, we devise a method for integrating the simulation results with Neural Radiance Fields. The result-ing method is applicable to a wide range of scenarios: it can handle objects composed of inhomogeneous material, with very different shapes, and it can simulate interactions with other virtual objects. We present our results using a newly collected dataset of 12 objects under a variety of force fields, which will be made available upon publication.
†Work was done while Hsiao-yu (partially) and Edith were interning at Reality Labs Research. 3D reconstruction is one of the fundamental problems of computer vision and a cornerstone of augmented and vir-tual reality. A recent achievement in this direction is the discovery of a fairly general formulation for representing ra-diance fields [5, 29, 33, 35, 39, 49, 50, 53, 57, 66, 68]. Neural radiance fields are remarkably versatile for reconstructing real-world objects with high-fidelity geometry and appear-ance. But static appearance is only the first step: it ignores how an object moves and interacts with its environment. 4D reconstruction tackles this problem in part by incorporat-ing the time dimension: with more intricate capture setups and more data, we can reconstruct objects over time—but can only re-play the captured sequences. Today, in the age of mixed reality, a photo-realistically reconstructed object might still destroy immersion if it is not “physically real-istic” because the object cannot be interacted with. (For example, if a soft object appears as rigid as the rocks next to it when stepped on.)
By building on advances in computer vision and physics simulation, we begin to tackle the problem of physically-realistic reconstruction and create Virtual Elastic Objects: virtual objects that not only look like their real-world coun-terparts but also behave like them, even when subject to novel interactions. For the first time, this allows for full-loop reconstruction of deforming elastic objects: from cap-ture, to reconstruction, to simulation, to interaction, to re-rendering.
Our core observation is that with the latest advances in 4D reconstruction using neural radiance fields, we can both capture radiance and deformation fields of a moving object over time, and re-render the object given novel deformation fields. The remaining challenge is to capture an object’s physics from observations of its interactions with the envi-ronment. With the right representation that jointly encodes an object’s geometry, deformation, and material behavior, compatible with both differentiable physical simulation and the deformation fields provided by 4D reconstruction algo-rithms, we can use these deformation fields to provide the necessary supervision to learn the material parameters.
But even with this insight, multiple challenges remain to create Virtual Elastic Objects. We list them together with our technical contributions: 1) Capture. To create VEOs, we need to collect data that not only contains visual information but also information about physical forces. We present the new PLUSH dataset1 containing occlusion-free 4D recordings of elastic objects deforming under known controlled force fields. To create this dataset, we built a multi-camera capture rig that incor-porates an air compressor with a movable, tracked nozzle.
More details can be found in Sec. 3.1. 2) Reconstruction. VEOs do not require any prior knowl-edge about the geometry of the object to be reconstructed; the reconstruction thus must be template-free and provide full 4D information (i.e., a 3D reconstruction and deforma-tion information over time). We extend Non-rigid Neural
Radiance Fields [56] with novel losses, and export point clouds and point correspondences to create the data required to supervise learning material behavior using physical sim-ulation. We provide further details in Sec. 3.2. 3) Simulation. Crucially for creating realistic interactive objects, a physical simulation is required, both to optimize for an unknown object’s physical parameters and to gener-ate deformations of that object in response to novel interac-tions. We implement a differentiable quasi-static simulator that is particle-based and is compatible with the deforma-tion field data provided by our 4D reconstruction algorithm.
We present the differentiable simulator and explain how we use it to obtain physical parameters in Sec. 3.3, and describe simulations of novel interactions in Sec. 3.4. 4) Rendering. Since we convert from a neural represen-tation of the captured object’s geometry to a point cloud reconstructing the object’s physical properties, we require a function that allows rendering the object given new simu-lated deformations of the point cloud. We introduce a map-ping function that enables us to use deformed point clouds instead of continuous deformation fields to alter the ray casting for the Neural Radiance Fields we used for the orig-inal reconstruction. Further details on re-rendering can be found in Sec. 3.5. 1https://hsiaoyu.github.io/VEO/ 2.