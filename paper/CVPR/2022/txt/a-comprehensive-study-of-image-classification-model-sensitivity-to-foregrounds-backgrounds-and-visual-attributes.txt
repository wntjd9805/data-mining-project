Abstract
While datasets with single-label supervision have pro-pelled rapid advances in image classification, additional annotations are necessary in order to quantitatively as-sess how models make predictions. To this end, for a sub-set of ImageNet samples, we collect segmentation masks for the entire object and 18 informative attributes. We call this dataset RIVAL10 (RIch Visual Attributes with
Localization), consisting of roughly 26k instances over 10 classes. Using RIVAL10, we evaluate the sensitivity of a broad set of models to noise corruptions in foregrounds, backgrounds and attributes. In our analysis, we consider diverse state-of-the-art architectures (ResNets, Transform-ers) and training procedures (CLIP, SimCLR, DeiT, Adver-sarial Training). We find that, somewhat surprisingly, in
ResNets, adversarial training makes models more sensitive to the background compared to foreground than standard training. Similarly, contrastively-trained models also have lower relative foreground sensitivity in both transformers and ResNets. Lastly, we observe intriguing adaptive abil-ities of transformers to increase relative foreground sensi-tivity as corruption level increases. Using saliency meth-ods, we automatically discover spurious features that drive the background sensitivity of models and assess alignment of saliency maps with foregrounds. Finally, we quantita-tively study the attribution problem for neural features by comparing feature saliency with ground-truth localization of semantic attributes. 1.

Introduction
Figure 1. Examples where background noise degrades perfor-mance of highly accurate models more than foreground noise.
Gaussian ℓ∞ noise with standard deviation σ = 0.24 shown.
Probabilities are averaged over ten trials. While these examples are cherry picked, we observe that they are surprisingly prevalent, and model design can affect the degree to which such cases arise.
Large scale benchmark datasets like ImageNet [9] that were constructed with single class label annotation have propelled rapid advances in the image classification task
[18, 21, 50, 58]. Over the last decade, several network ar-chitectures and training procedures were proposed to yield very high classification accuracies [10, 18, 45, 50]. How-ever, methods to interpret these model predictions and to diagnose undesirable behaviors are fairly limited. One of the most popular class of approaches are saliency meth-ods [43, 48, 49, 59] that use model gradients to produce a
To this end, we conduct a noise analysis that leverages object segmentation masks to quantitatively assess model sensitivity to foregrounds relative to backgrounds. We proxy sensitivity to a region by observing model perfor-mance under corruption of that region. We propose a nor-malized metric, relative foreground sensitivity (RF S), to compare models with various general noise robustness. A high RF S value indicates that the model uses foreground features in its inferences more than background ones since corrupting them result in higher performance degradation.
In Figure 2, we see different architectures and train-ing procedures lead to variations in both general noise ro-bustness (projection onto the main diagonal) and relative foreground sensitivity (normalized distance orthogonal to the diagonal). Notably, we find that adversarially training
ResNets significantly reduces RF S, surprisingly suggest-ing that robust ResNet models make greater use of back-ground information. We also observe contrastive training to reduce RF S, and transformers to uniquely be able to adjust
RF S across noise levels, reducing their sensitivity to back-grounds as corruption level increases. Lastly, we find object classes strongly affect RF S across models.
We couple our noise analysis with saliency methods to add a second perspective of model sensitivity to different in-put regions. Using RIVAL10 segmentations, we can quan-titatively assess the alignment of saliency maps to fore-grounds. We also show how we can discover spurious back-ground features by sorting images based on the saliency alignment scores. We observe that performance trends that our noise analysis reveals are not captured using qualitative saliency methods alone, suggesting our noise analysis can provide new insights on model sensitivity to foregrounds and backgrounds.
Lastly, we utilize RIVAL10 attribute segmentations to systematically investigate the generalizability of neural fea-ture attribution: for a neural feature (i.e., a neuron in the penultimate layer of the network) that achieves the highest intersection-over-union (IOU) score with a specific attribute mask on top-k images within a class, how the IOU scores of that neural feature behave on other samples in that class.
For some class-attribute pairs (e.g. dog, floppy-ears), we in-deed observe generalizability of neural feature attributions, in the sense that test set IOUs are also high.
In summary, we present a novel dataset with rich anno-tations of object and attribute segmentation masks that can be used for a myriad of applications including model inter-pretability. We then present a study involving three quan-titative methods to analyze the sensitivity of models to dif-ferent regions in inputs. We hope the RIVAL10 dataset will help study failure modes of current deep classifiers and pave the way for building more reliable models in the future.
Figure 2. Accuracy under noise in foregrounds and backgrounds, averaged over multiple noise levels. Marker size is proportional to parameter count. Models with higher relative foreground sensitiv-ity lie further from the diagonal. saliency map corresponding to the most influential input re-gions that yielded the resulting prediction. However, these methods are qualitative, require human supervision, and can be noisy, thus making their judgements potentially unreli-able when made in isolation of other supporting analysis.
In this paper, we argue that to obtain a proper under-standing of how specific input regions impact the predic-tion, we need additional ground truth annotations beyond a single class label. To this end, we introduce a novel dataset,
RIVAL10, whose samples include RIch Visual Attributions with Localization. RIVAL10 consists of images from 20 categories of ImageNet-1k [9], with a total of 26k high res-olution images organized into 10 classes, matching those of CIFAR10 [26]. The main contribution of our dataset is instance wise labels for 18 informative visual attributes, as well as segmentation masks for each attribute and the en-tire object. We present our dataset as a general resource for understanding models trained on ImageNet. We then pro-vide a study of the sensitivity of a diverse set of models to foregrounds, backgrounds, and attributes.
Our study of background and foreground model sensitiv-ity is motivated by some counter-intuitive model behaviors on images whose background and foreground regions were corrupted with Gaussian noise: Figure 1 shows instances where highly accurate models have performance degraded much more due to the background noise than the foreground noise. While this is not the norm (i.e. models are more sen-sitive to foregrounds on average), the existence of these ex-amples warrants greater investigation, as they expose a stark difference in how deep models and humans perform object recognition. Quantifying the degree to which different ar-chitectures and training procedures admit these examples can shed new insight on how models incorporate foreground and background information.
2. Review of Literature 2.1. Related Datasets
Prior to the rise of deep learning, a number of works studied attribute classification, leading to the construction of datasets such as Animals with Attributes [27] and aPAS-CAL VOC 2008 [14] (adding annotations to [13]). [54] pub-lished CUB 200, a fine-grained classification dataset of bird species with object segmentations and part localizations in the form of single coordinates, as opposed to segmenta-tion masks like in RIVAL10. Finally, [41] collected ob-ject attributes on a small-scale subset of ImageNet. More recently, [36] publish a large-scale object attribute dataset on a subset of ImageNet. The Celeb-A dataset [29] con-tains attribution with applications to generative modeling, but limited utility for general representation learning since it only contains face images. The broader dataset Visual At-tributes in the Wild (VAW) [38] provides large-scale in the wild attribute annotations for 250k object instances.
Many datasets aim to stress test models to reveal limita-tions. [19] introduces ImageNet variants under diverse cor-ruption types, including Gaussian noise. [20] adds two more
ImageNet variants that include challenging natural samples and out of distribution samples, on which top models see massive accuracy drops. Models evaluated on [2] simi-larly see large drops, though this dataset differs in that it is strictly a test set. Other works introduce synthetic datasets to assess spatial biases [57] or background reliance of clas-sifiers, such as [56] and [42], which perform some varia-tion of swapping or altering foregrounds and backgrounds.
Though similar, these works differ in objective and techni-cal contribution to ours. [42] focuses on developing a novel distributionally robust optimization procedure. [56] empha-sizes designing a multitude of test datasets through creative editing of foreground and background regions to serve as a general benchmark to evaluate models. In contrast, our work presents a novel method to analyze foreground sensi-tivity, and demonstrates its utility by applying it to a breadth of cutting-edge architectures and training paradigms, lead-ing to model-specific observations. Further, our RIVAL10 dataset is significantly larger and richer in annotation.
Recently, [46] uses saliency maps and feature visualiza-tion in a semi-automated process to identify deep neural nodes corresponding to core or spurious features for an ob-ject of a given class, resulting in a large-scale dataset with segmentations corresponding to salient features. However, annotation of the segmented regions are limited to just la-beling them as ‘core’ or ‘spurious’. 2.2. Interpretability Methods
A number of methods have been proposed to interpret model predictions, such as saliency or class activation maps influence functions [25], and surrogate white box
[43], models [40, 55]. However, saliency maps have been found to be noisy and influence functions are fragile [3,16]. Some methods seek to interpret the function of a neural node via synthesizing inputs that maximize its activation [33,35,47], though these methods are limited when non-adversarially robust models are used [34], and offer qualitative insights.
A motivation behind the development of interpretability methods is to work towards addressing the ‘shortcut learn-ing’ issue, where models rely on easy-to-learn features that lead to high performance on training sets, but poor gen-eralization in other settings. [15] discusses this at length, recommending the development and usage of challenging datasets whose inputs are out-of-distribution with respect to standard benchmarks. RIVAL10’s rich annotations open the door to the construction of many challenge datasets, in which shortcuts are broken via swapping backgrounds, foregrounds, and attributes (examples in Appendix).
Other constructive works aimed to reduce the reliance of deep models on spurious features appeal to counterfactual data generation [1, 6, 17], often appealing to disentangled representations or explicit annotations to break correlations of texture, shapes, colors, and backgrounds. Further, [23] found that removing spurious features can in fact hurt accu-racy and disproportionately affect groups. Thus, the notion that spurious features are always harmful is incomplete, and a closer look is required to ground discussions regarding the shortcut learning issue. Lastly, [52] provides theoretical context for stress testing models to discern causal factors. 3. RIVAL10 3.1. Overview
RIVAL10 differs from previous attributed datasets in that it provides attribute-specific localizations. That is, for every positive instance of an attribute, a binary segmentation mask identifies the image region in which the attribute occurs.
Perhaps, the most similar dataset in this regard is the recent Fashionpedia [22], a dataset providing attributes and localizations of 27 apparel categories. However, the dataset is proposed for the fashion domain which limits its utility for general purpose object recognition task. To the best of our knowledge, RIVAL10 is the first general domain dataset to provide both rich semantic attributes and localization, the combination of which we envision to aid in analyzing the robustness and interpretability of deep networks. While other datasets used for semantic segmentation and object detection go beyond single label annotations [8,12,28], they are not designed with classifiers specifically in mind, like RIVAL10.
Classes were chosen to be aligned with CIFAR-10 to en-able analyzing the existing architectures and training tech-niques developed for the object recognition task. In partic-ular, the classes we provide are: bird, car, cat, deer, dog,
Figure 3. (Left): Correlations between attributes in the training split. (Right): Class-wise means of attribute vectors in the training split. equine, frog, plane, ship, truck. We collected the following attributes for these object categories: beak, colored-eyes, ears, floppy-ears, hairy, horns, long, long-snout, mane, metallic, patterned, rectangular, tail, tall, text, wet, wheels, wings. Some attributes were inspired from [41].
We chose attributes to be intuitively informative, captur-ing semantic concepts that humans may allude to in classi-fying RIVAL10 objects. While the attributes contain some redundant information, they are nonetheless discriminative in the sense that a linear classifier on attributes achieves 93.3% test accuracy. We visualize attribute correlations and class-wise frequencies in Figure 3. 3.2. Data Collection
All images were sourced from ImageNet [9]. The images used in each RIVAL10 class were derived from pairs of re-lated ImageNet classes. In other words, 20 classes from Im-agenet were used to build the 10 RIVAL10 classes (details in appendix). To collect our attributes and localizations, we hired workers from Amazon Mechanical Turk (AMT). Data collected through AMT without careful control may be of low quality. To encourage quality annotations, we utilize strategies recommended by the HCI community [31]: pro-viding detailed instructions, screening workers for aptitude, and monitoring worker performance with attention checks.
Binary attributions were collected first. Workers were required to pass a qualification test of 20 images with known ground truth attributes: only workers who achieved a minimal overall precision and recall of 0.75 were hired for full data collection. Because the task of segmenta-tion is more involved than indicating whether or not an at-tribute is present, we required a second qualification test, as-sessing annotation quality by computing intersection-over-union (IOU) of the submitted attribute masks with ground truth masks. Workers were required to complete five seg-mentations with an average IOU of at least 0.7.
To ensure that quality is maintained in both the attribu-tion and segmentation phases, roughly 5% of images pro-vided to workers to annotate already had ground truth la-bels. These so-called attention checks allowed for the mon-itoring of annotation quality during the collection process.
In the first stage of collecting binary attribute labels, the av-erage precision and recall scores were 0.81 and 0.84 respec-tively. For each positive instance of an attribute marked in the first phase of data collection, an attribute segmentation was collected in the second phase. Completeing attribute segmentations in a second pass allowed for the review of the binary attributions and the removal of any false positives.
Average IOU of attention checks completed during the sec-ond phase of data collection was 0.745. Further details on our data collection pipeline, including images of instruc-tions shown to workers, payments, and quality-assurance metrics, can be found in the appendix. 4. Models
In our analyses, we focus on ResNets and Vision Trans-formers [10, 18]. We inspect ResNets trained (i) in a stan-dard supervised fashion, (ii) adversarially via ℓ2 projected gradient descent [30], and (iii) contrastively (i.e. no direct label supervision), with SimCLR and CLIP [7, 39]. We also consider CLIP Vision Transformers, as well as standard Vi-sion Transformers (ViT) and Data efficient Image Trans-formers (DeiT) [51]. DeiTs differ from ViTs primarily in their training set, solely using ImageNet-1k while ViTs used
ImageNet-21k. To make up for not having the inductive bi-ases of ResNets, ViTs increased the amount of training data, while DeiTs instead rely upon extensive augmentation. All other models, with the exception of those trained with CLIP, use ImageNet-1k as the pretraining set. CLIP, on the other hand, uses a much larger dataset of images and associated text. A full discussion on models is offered in the appendix.
Figure 4. Accuracy under noise in foreground (left) and background (middle) at various noise levels. Models are grouped by architecture and training procedure, with a curve corresponding to the average over all models in a group. (Right): RF S by group.
To perform classification on RIVAL10 dataset, we ap-pend a linear head to the penultimate layer of each base model. Only the linear head is fine-tuned on RIVAL10’s train split (i.e. other weights are frozen), so to preserve the feature space learned in the original pretraining. All models achieve upwards of 90% accuracy on the RIVAL10 test set, essentially controlling for classification ability. We note that while there is leakage between ImageNet-1k and the RI-VAL10 test set, the purpose of this study is not to improve model’s predictive accuracy directly, but instead to better understand the information used in making predictions.
Recently, a number of works compare the robustness of
ViTs to ResNets. While there are mixed findings on adver-sarial robustness [4, 44], there is agreement that ViTs have stronger out-of-distribution generalization, likely due to self attention [5, 37]. In contrast, our work focuses on relative robustness to noise in foreground and background regions. 5. Foreground and