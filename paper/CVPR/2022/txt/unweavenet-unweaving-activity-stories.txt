Abstract
Our lives can be seen as a complex weaving of activi-ties; we switch from one activity to another, to maximise our achievements or in reaction to demands placed upon us. Observing a video of unscripted daily activities, we parse the video into its constituent activity threads through a process we call unweaving. To accomplish this, we in-troduce a video representation explicitly capturing activity threads called a thread bank, along with a neural controller capable of detecting goal changes and resuming of past activ-ities, together forming UnweaveNet. We train and evaluate
UnweaveNet on sequences from the unscripted egocentric dataset EPIC-KITCHENS. We propose and showcase the efﬁ-cacy of pretraining UnweaveNet in a self-supervised manner. 1.

Introduction
“It’s the morning and you’ve just walked into the kitchen: you’re hungry, sleepy, the kitchen is a mess, but you have a paper to review for CVPR. You put some bread into the toaster, turn the kettle on to make coffee, and in between waiting for the kettle to boil and bread to toast, you clean the dishes. The toast pops up and you put it on a plate, then the kettle boils and you resume making your coffee, switching back and forth as necessary until your breakfast is ready.”
As in the storyline described above and depicted in Fig. 1, activities need not be completed over one continuous block of time. Instead they are often paused and interleaved with other activities. This observation gives rise to a new interpre-tation of video as a weaving of activities. Such a perspective supports the distinction between two instances of an activity when the activity is paused and later resumed. This dis-tinction can be important for downstream applications, like assistive technologies which need to differentiate between starting a new task vs. resuming a previously paused one.
This novel view of video leads to the task proposed and tackled in this paper: unweaving a video into its constituent activity threads. Like a person reading a story mentally un-weaves the story’s narrative threads as they unfold, a model unweaving a video does so similarly, processing video online, detecting new threads of activity as they appear and updating its representation of previously discovered threads as they are resumed. Following this analogy, videos of activities as referred to as activity stories.
This proposed task is related to two previously proposed tasks: event boundary detection and unsupervised activity segmentation. The relationship between unweaving and these other related tasks is summarised in Fig. 2. Event boundary detection [37, 1] aims to detect points in the video where a transition between two events occurs. This task aims to model the experimental observation that humans can consistently detect transitions between events as they watch video online [50, 51, 16]. Typically, these methods are performed online [37, 1], predicting the future video repre-sentation, comparing this against the true representation, and measuring the prediction error in order to decide whether a boundary can be detected. Compared to unweaving, event boundary detection focuses on ﬁnding the transitions be-tween activities and doesn’t support the association between
Figure 1. In our daily lives, one switches between activities (e.g. making toast, preparing coffee, washing up) to minimize idle time.
Such behaviour results in video demonstrating multiple activities woven together. This paper introduces a model that learns to undo this, unweaving video into threads of activity without the need for semantic labels.
propose a self-supervised method for detecting event bound-aries, by predicting upcoming features. A boundary is de-tected when the prediction error of the future frame exceeds a dynamically-set threshold. Shou et al. [37] introduce a new dataset for supervised event-boundary detection. They explore detecting event boundaries using both supervised and unsupervised approaches. One of the unsupervised ap-proaches, PredictAbility, measures the change in features about a point in time to detect boundaries.
Action segmentation and detection In action segmenta-tion [17, 25, 10, 45, 22, 36] the goal is to assign an action label to every frame. In contrast, action detection [41, 39, 38, 31] predicts segments of video that possibly overlap. Most efforts for these tasks are supervised.
Kukleva et al. [22] propose an unsupervised method for segmenting video by learning a temporal embedding of frames. First, they train an MLP to regress the position of a frame in the video from which it originates. Interme-diate features are extracted and act as the embedding of the frame. The embeddings are then clustered using a con-strained optimisation that prevents non-adjacent frames from being assigned to the same cluster. An extension was pro-posed in [43] that uses the embeddings from a model trained for future feature prediction. Sarfraz et al. [36] also clusters frames, in an unsupervised manner, to form a temporally-weighted distance graph where nodes represent frames and edge weights are determined by the feature dissimilarity and temporal distance. Frames are then clustered iteratively until the desired number of clusters is reached.
Movie scene segmentation A variety of works tackle the problem of segmenting a movie into scenes. All existing methods are ofﬂine and require specifying the number of scenes into which the movie will be split. Early work by Ye-ung et al. [48] introduced the concept of a hierarchical scene transition graph which splits a movie into acts, scenes, and shots. Cour et al. [7] use the screenplay and closed captions associated with a movie and introduce the problem of shot threading to undo the common scenario in which shots from 2 or more cameras are interleaved together. Tapaswi, Bäuml, and Stiefelhagen [42] introduce a method for building a ‘Sto-ryGraph’, a type of visualisation, originally proposed by the web-comic xkcd [29], where each character in a TV episode is represented as a line on a 2D chart.
More recently, Rao et al. [33] collect a large dataset Movie-Scenes containing 21k scene segments from 150 movies that are used to supervise their model. These approaches are spe-ciﬁc to movies which are made up of scenes and shots. The notion of characters, multiple-cameras, shots, and scenes are not present in daily-activity videos.
Online clustering Unweaving videos can be viewed as a type of online clustering, where the number of clusters is not known ahead of time, nor the number of elements to be
Figure 2. Task comparison: In unweaving, the model has to de-cide, online, whether the current part of the video is a continuation of the last-seen activity, a resumption of previously paused activity, or a completely new instance of an activity. The ﬁgure compares unweaving to two previously studied tasks. events depicting a paused-and-resumed activity. Also re-lated, unsupervised activity segmentation [22, 36] clusters visual features to produce a segmentation of the video. This task doesn’t distinguish between different instances of the same activity, e.g. the act of making two cups of tea, one after the other, is the same as making just one. Unweaving is signiﬁcantly more challenging as it is performed online, without specifying the number of activities, nor the duration of the video. Unweaving thus combines the challenges of the two aforementioned tasks.
In addition to introducing the problem of unweaving, this paper proposes a model that learns to unweave video into activity threads. Different threads of activity are modelled in an explicit manner by a thread bank that is manipulated by a neural controller as subsequent video is processed. To train UnweaveNet, a self-supervised approach is introduced that leverages within-thread temporal-order consistency to construct synthetic visual stories from unlabelled videos for pretraining. The model is then ﬁnetuned using a small set of manually annotated stories. The efﬁcacy of this approach is shown experimentally using the unscripted egocentric dataset EPIC-KITCHENS-100 [9].
Our contributions are summarised as: (i) The novel task of unweaving video into its activity threads, online. (ii) A new video representation explicitly modelling video as a set of ac-tivity threads operated by a neural controller, which together form UnweaveNet. (iii) A self-supervised pretraining ap-proach for UnweaveNet that samples threads from different parts of a long video and synthetically forms woven activity stories. (iv) Labelled annotations of activity threads from videos of the egocentric dataset: EPIC-KITCHENS1. (v) An empirical evaluation and ablation study of UnweaveNet. 2.