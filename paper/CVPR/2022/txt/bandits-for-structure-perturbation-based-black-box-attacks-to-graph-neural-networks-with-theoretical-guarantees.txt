Abstract
Graph neural networks (GNNs) have achieved state-of-the-art performance in many graph-based tasks such as node classification and graph classification. However, many re-cent works have demonstrated that an attacker can mislead
GNN models by slightly perturbing the graph structure. Ex-isting attacks to GNNs are either under the less practical threat model where the attacker is assumed to access the
GNN model parameters, or under the practical black-box threat model but consider perturbing node features that are shown to be not enough effective. In this paper, we aim to bridge this gap and consider black-box attacks to GNNs with structure perturbation as well as with theoretical guaran-tees. We propose to address this challenge through bandit techniques. Specifically, we formulate our attack as an on-line optimization with bandit feedback. This original prob-lem is essentially NP-hard due to the fact that perturbing the graph structure is a binary optimization problem. We then propose an online attack based on bandit optimization which is proven to be sublinear to the query number T , i.e.,
N T 3/4) where N is the number of nodes in the graph.
O(
Finally, we evaluate our proposed attack by conducting ex-periments over multiple datasets and GNN models. The experimental results on various citation graphs and image graphs show that our attack is both effective and efficient.
√ 1.

Introduction
Graph neural networks (GNNs) have been emerging as the most prominent methodology for learning with graphs, such as social networks, chemical networks, superpixel graphs, etc. GNNs have also advanced many graph-related applications including but not limited to drug discovery [24], fake news detection on social media [21], traffic forecast-ing [34], and superpixel graph classification [11]. However, recent works have shown that GNNs are vulnerable to ad-‡Corresponding author. versarial attacks [10, 37, 31, 38, 26, 32, 25, 20]. That is, an attacker can easily fool a GNN model by slightly perturb-ing the graph structure (e.g., injecting new fake edges into the graph or deleting the existing edges from the graph) or perturbing the node features. Most of the existing attacks to GNNs essentially rely on white-box or gray-box threat model [10, 37, 31, 38, 26, 32, 25]. An attacker can not only obtain the predictions generated by the targeted GNN model, but also know the whole (i.e., in white-box) or partial (i.e., in gray-box) GNNs’ inner parameters and network struc-ture. These threat models enable the attacker to derive the true gradients that can be used to construct an (almost) op-timal edge/feature perturbation via first-order optimization approaches, e.g., projected gradient descent (PDG).
In practice, however, an attacker often has limited knowl-edge about the GNN model. For instance, many models are deployed as an API due to the commercial value. In these practical scenarios, an attacker can only obtain the model pre-dictions by querying the API, while not knowing the model’s other information. An attack based on such a realistic threat model is called a black-box attack, which significantly raises the bar for the attacker as he cannot obtain the gradient in-formation. A recent work [20] performs black-box attacks against GNNs. However, this work has two key drawbacks.
First, it assumes that the attacker can only perturb the (con-tinuous) node features. Existing works (e.g., [37]) have shown that feature perturbation-based attacks to GNNs are significantly less effective than structure perturbation-based attacks. Second, the attack is implemented via a heuristic greedy algorithm, which has no theoretically guaranteed at-tack performance. Note that black-box attacks are classified as soft-label black-box attacks [14, 18] and hard-label [8] black-box attacks. The former means an attacker knows the confidence scores when querying a target model, while the latter means an attacker only knows the predicted label.
In this paper, we consider soft-label black-box attacks to
GNNs with structure perturbation. However, such a new attack setting is much more challenging, as finding the opti-mal structure perturbation is essentially an NP-hard problem
(i.e., a binary optimization problem) and the attacker only obtains the predictions via querying the model. We take the first step to solve the structure perturbation-based black-box attacks to GNNs with theoretical guarantees. Specifically, we first reformulate our attack as a bandit optimization (i.e., online optimization with bandit feedback) problem, which characterizes the attacker’s query process on the black-box
GNN model and captures the unknown gradients. Then, we handle the binary constraint of the discrete structure pertur-bation and integrate it into our bandit-based attack objective.
Next, we design an efficient and effective online attack to
GNNs. Finally, we theoretically analyze our attack. Our key contributions are summarized as follows:
• We design the first theoretically guaranteed structure perturbation-based black-box attacks to GNNs via ban-dit optimization.
• We prove that our bandit-based attack algorithm theoreti-N T 3/4) within cally yields a sublinear regret bound O(
T queries for attacking a graph with N nodes.
√ u (cid:16) d−1/2 v h(k−1) u
For instance,
W (k)(cid:0) (cid:80)
Different GNNs use different AGG and UPDATE functions. in Graph Convolutional Net-work (GCN) [16], AGG is the element-wise mean pool-ing function and UPDATE is the ReLU activation func-it has the following form: tion. More specifically, (cid:1)(cid:17) h(k) u∈N (v) d−1/2
= ReLU
, v where W (k) is the parameters for the k-th layer. A node v’s final representation h(K) v ∈ R|Y| can capture the struc-tural information of all nodes within v’s K-hop neigh-bors. Moreover, the final node representations of training nodes are used to train the node classifier. Specifically, let
Θ = {W (1), · · · , W (K)} be the model parameters and v’s output be fΘ(av) = softmax(h(K)
) ∈ R|Y|, where fΘ(av)y indicates the probability of node v being class y2.
Then, Θ are learnt by minimizing the cross-entropy loss on the outputs of the training nodes VL, i.e., v
Θ∗ = arg min
Θ
− (cid:88) v∈VL ln fΘ(av)y. (1)
• We conduct extensive experiments to evaluate our attack over multiple graph datasets and GNN models and demon-strate the effectiveness and efficiency of our attack.
With the learnt Θ∗, we can predict the label for each unla-beled nodes u ∈ V \ VL as ˆyu = arg maxy fΘ∗ (au)y. 2. Preliminaries and Problem Formulation 2.1. Graph neural networks
Let G = (V, E, X) be a graph, where u ∈ V is a node, (u, v) ∈ E is an edge between u and v, and X =
[x1; x2; · · · ; xN ] ∈ RN ×d is the node feature matrix. We denote av = [av 1; av 2; · · · ; av N ] ∈ {0, 1}N as the adja-cency vector of node v. N = |V| and M = |E| are the number of nodes and edges, respectively. We denote du and
N (u) as u’s node degree and the neighborhood set of u. We consider GNNs for node classification in this paper1. In this context, each node u ∈ V has a label yu from a label set
Y = {1, 2, · · · , LC}. Given a set of VL ⊂ V labeled nodes
{(xu, yu)}u∈VL as the training set, GNN for node classifi-cation is to learn a node classifier that maps each unlabeled node u ∈ V \ VL to a class y ∈ Y based on the graph G.
Generally speaking, GNN consists of two main steps: neighborhood aggregation and node representation update.
Suppose a GNN has K layers. We denote v’s representation in the k-th layer as h(k) v , with h(0) v = xv. In the neighbor-hood aggregation, GNN obtains the representation l(k) v by aggregating representations of v’s neighbors in the (k −1)-th
: u ∈ N (v)(cid:9)(cid:1). In the node layer as l(k) representation update, GNN updates v’s representation at the k-th layer via combining v’s previous layer’s representation h(k−1) with the aggregated neighborhood’s representations v v = UPDATE(cid:0)h(k−1)
, l(k) v : h(k) l(k) v v = AGG(cid:0)(cid:8)h(k−1) (cid:1). u v 2.2. Threat model
Attacker’s knowledge. The considered black-box attack setting in this paper implies that an attacker does not know the internal configurations (i.e., the learned parameters) of the targeted GNN model. For a target node v ∈ V, the only information the attacker knows about the GNN model is the predictions fΘ∗ (av) (i.e., output logits) via querying the
GNN model fΘ∗ . Moreover, we also reasonably assume that the attacker knows her neighbors, i.e., the adjacency vector 3. In practice, the attacker naturally knows the neighbors av of his controlled node. Taking social network as an instance, an attacker controls a malicious user, and this malicious user definitely knows his (non)neighbors in the social network.
Note that the compared black-box RL-S2V attack [10] also requires that an attacker’s target node knows his neighbors.
Attacker’s capability. We consider that the attacker can modify the connection status (e.g., injecting new fake edges or removing the existing edges) between the target node v and other nodes in the graph. In practice, it also incurs different costs for the attacker to manipulate different edges.
The attacker’s budget of manipulating edges is often limited, and we denote by C the cost budget. We also constrain that the number of edges to be manipulated is bounded by B.
Attacker’s goal. Based on the attacker’s knowledge and capability, an attacker’s goal is to fool a targeted GNN, i.e., making her target node v’s predicted label different from the true label yv, by perturbing her adjacency vector av with the cost budget C and allowed number of perturbed edges B. 2Note that the prediction also depends on v’s node feature xv and the 1Our attack can be naturally generalized to GNNs for graph classifica-whole graph G. We omit xv and G for notation simplicity. tion. We discuss This in Section 3.1. 3For graph classification, we assume attackers know the input graph.
Our threat model requires that an attacker knows the confidence scores (as many existing attacks to DNN models).
Although it is stronger than the threat model that an attacker only needs to know the hard label, we also highlight that this is the first optimization-based attack that targets discrete graph structure perturbation, where this problem itself is rather challenging. We will leave addressing the attack with hard labels as the query feedback in future work. 2.3. Problem formulation
Given the target node v, label yv, and adjacency vector av, an attacker aims to modify the connection status related to the target node v such that the targeted GNN misclassifies v. Let sv ∈ {0, 1}N be the adversarial structure perturbation on v, where svu = 1 means the connection status between the nodes v and u is changed, and svu = 0, otherwise. Then, we define the perturbed adjacency vector for v as av ⊕ sv, where ⊕ is the XOR operator between two binary vectors.
Moreover, we denote cv ∈ RN as the cost vector associated with v, i.e., cvu is the cost to modify the connection status between v and u. In the focused black-box setting, we con-sider the untargeted attack. Let L(av) be the loss function for the targeted node v without attack. With the adversarial perturbation sv, we have the attack loss as L(av ⊕ sv). In this paper, we use the CW attack loss function [3] with κ attack confidence. Specifically, it is defined as follows:
L(x) = max{fΘ∗ (x)y − max
ˆy̸=y
{fΘ∗ (x)ˆy}, −κ}. (2)
Finally, our problem of the structure perturbation-based black-box attack to GNN can be formulated as
L(av ⊕ sv), s.t., 1T sv ≤ B, cT sv ≤ C, sv ∈ {0, 1}N , (3) min sv where the first constraint means the number of edges to be perturbed is no more than B and the second constraint means the total costs of the perturbation are no more than C.
Lemma 1. Our problem in Eq. (3) is NP-hard.
Proof. Our problem in Eq. (3) is a combinatorial optimiza-tion problem, actually a type of knapsack problem, which is a classical NP-hard problem.
Lemma 1 implies that it is difficult to calculate the optimal perturbation vector s∗ v within polynomial time under large graphs (i.e., sv has large dimension). To this end, we aim to design an approximation algorithm to derive sub-optimal solution. One algorithm is to relax the combinatorial binary constraint sv ∈ {0, 1}N into convex hull sv ∈ [0, 1]N and obtain a continuous optimization problem. Let ˆsv be the solution of the continuous optimization problem. We can derive the sub-optimal solution for the original problem in
Eq. (3) by rounding ˆsv into combinatorial space {0, 1}N using randomization sampling like Bernoulli sampling [32].
Then, we have the following lemma to characterize the rela-tion between sv and ˆsv in expectation:
Lemma 2. When sampling sv element-wise in Bernoulli distribution using the probability from the relaxed vector
ˆsv ∈ [0, 1]N , then the expectation of sv is ˆsv, i.e., the condi-tion E[sv] = ˆsv holds.
Proof. This lemma holds due to the fact that a random vari-able X subject to Bernoulli distribution on support {0, 1} takes its probability as expectation, i.e., E[X] = P[X]. Ap-plying this fact elements in sv, we can prove this lemma.
Conventionally, to solve our relaxed continuous optimiza-tion problem, we can apply the PGD approach by running gradient updates projected onto the feasible domain within several steps. However, PGD requires gradient information to be available. In our black-box attack setting, only the prediction result is available (by querying the GNN) instead of the exact gradient. Thus, the attack problem becomes how to estimate the gradient such that the PGD method can still be applied. It is shown that zeroth-order optimization (short for ZOO4) can be used to estimate the gradient [6, 15, 19].
However, ZOO suffers from a low convergence rate and high query overhead due to necessarily exploring all edges to estimate the gradient per round. We aim to estimate the un-known gradient by controlling the exploration-exploitation tradeoff via bandit methods. Reinforcement learning (RL) can also control the exploration-exploitation tradeoff. How-ever, RL-based attack, i.e., RL-S2V [10], is naturally heuris-tic. Our attack can address both issues in ZOO-based and
RL-based attacks. Specifically, our attack has theoretical guarantees and better attack performance than ZOO-based and RL-based attacks (See Section 6). Next, we reformulate our attack problem as a bandit optimization problem and then propose a solution to it. 2.4. Reformulating our attack as a bandit problem
When the attacker selects a perturbation vector sv and uses the perturbed adjacent vector av ⊕sv to query the GNN, the GNN returns the prediction fΘ∗ (av ⊕ sv). Thus, the objective L(av ⊕ sv) is revealed based on Eq. (2), which can be seen as a bandit feedback (i.e., reward) for the se-lected perturbation vector sv (i.e., an arm). Under the bandit feedback, the attacker wants to maximize the cumulative rewards. Note that since the attacker does not know the optimal arm s∗ v in each round, it will incur a regret, i.e., the difference of the maximum reward under the optimal arm s∗ v in hindsight and the reward of the attacker’s attack algorithm. Then, the attacker’s goal is to minimize the cu-mulative regrets. Let Reg(T ) be the cumulative regrets in T rounds, and st v be the perturbation vector selected at round t, then the cumulative regrets Reg(T ) can be calculated as 4Note that ZOO is a perfect benchmark in our setting. First, only ZOO approximates the gradient directly through queries. Second, ZOO is the only method that also has a regret bound. Hence, we can compare with
ZOO in terms of both theoretical results and empirical attack performance.
t=1 L(st
Reg(T ) = E[(cid:80)T v)] − T L(s∗ v). In bandit optimiza-tion, it is important to design an arm selection algorithm with sublinear regret (i.e., Reg(T ) = o(T )). This is because the selected arm sv at round T is asymptotically optimal when
T is sufficiently large (i.e., limT →∞
Reg(T )
T = 0). 3. Structure Perturbation-based Black-Box At-tacks to GNNs via Bandits
Here, we design an online attack to GNN based on bandits optimization and show its sublinear regret in next section.
First, we relax the binary perturbation vector sv ∈
{0, 1}N into a continuous convex hull ˆsv ∈ [0, 1]N .
In this case, we can define the arm set W as: W = {ˆsv ∈
[0, 1]N | 1T ˆsv ≤ B, cT ˆsv ≤ C}, where W is convex. Note that our bandit for the black-box setting is different from the traditional multi-armed bandits (MAB), as arm set W contains infinite perturbation vectors. Thus, the approaches like upper confidence bound (UCB) [2] and Thompson Sam-pling [22] in MAB fail to solve our problem. Moreover, it is impossible to use the combinatorial bandits [7, 29] to derive the optimal perturbation because they have to collect enough historical samples to calculate the mean of each perturbed edge. In particular, in the exploitation phase, they behave as
ZOO [6] and estimate a gradient with N + 1 queries. In the exploration phase, they require an approximation algorithm to derive the suboptimal perturbation with the UCB values.
However, to the best of knowledge, there is no such an ef-ficient approximation algorithm in the context of structural perturbation attacks with theoretical guarantees. v v
Next, we need to address how to efficiently decide the next arm (i.e., ˆst+1
) at the end of round t such that the regret is minimized. We leverage online convex optimization (OCO) technique to derive the next arm ˆst+1 at round t + 1.
We note that the loss function L(·) is often non-convex.
However, we emphasize that the gradient descent used in
OCO is still useful as it is challenging to derive the closed-form solution for non-convex functions. OCO approach requires gradient information at the selected arm to conduct gradient descent. In our black-box attack setting, the attacker only receives the bandit feedback for the selected arm ˆst v. To estimate the gradient for the black-box attack at the selected arm ˆst v, the attacker can only query the GNN to compute the gradient. We use one point gradient estimation (OPGE) [13] technique to estimate the gradient due to its simplicity and effectiveness. The idea of OPGE is to find a vector in unit sphere S = {u ∈ RN | ||u||2 = 1} such that its direction has small intersection angle with the gradient (i.e., u is a good approximation of the gradient). To this end, we uniformly sample a unit vector from S and derive an approximate gradient in the following lemma.
Lemma 3. For a unit vector u uniformly sampled from the unit sphere S and a sufficient small δ > 0, we can estimate gradient as ∇ ˆL(ˆsv) = Eu∈S [ N
δ L(ˆsv + δu)u].
Algorithm 1 Black-box attack to GNN for node classifica-tion via OCO with bandit
Input: Target GNN model f ∗
Θ, target node v, maximal #perturbed edges
B, cost budget C, PGD step η, δ > 0, α ∈ [0, 1], query number T
Output: Perturbed vector sv 1: Initialize: v1 = 0 ∈ W = {ˆsv ∈ [0, 1]N | 1T ˆsv ≤ B, cT ˆsv ≤ C} 2: for t = 1 to T do 3: 4: 5: v = vt + δut. v by setting top-B values in ˆst v
Attacker randomly chooses a unit vector ut from the unit sphere S.
Attacker determines a perturbation ˆst
Attacker converts ˆst v to be binary st to be 1 and others to be 0.
Attacker queries the GNN model f ∗ tions and CW loss L(st
Attacker conducts PGD and updates: vt+1 = (cid:81)
ηˆg), ˆg = N v to obtain the predic-(1−α)W (vt − v) using Eq. (2).
Θ with st
δ L(st v)ut. 6: 7: 8: end for 9: return sT v
The details of our attack algorithm are shown in Algo-rithm 1. The inputs of our algorithm include the targeted
GNN model f ∗
Θ, target node v, maximal number of perturbed edges B, cost budget C, PGD step η, a small δ > 0, projec-tion scale α ∈ [0, 1], and query number T . The output is a perturbed vector sv for the target node v after T rounds.
In line 1, we set 0 as the initial prior vector v1. In line 2–8, we calculate a sub-optimal perturbed vector to attack the targeted GNN based on OCO with bandit feedback. At round t, we randomly select a unit vector ut from the unit sphere S as a stochastic gradient in line 3. In line 4, we derive a relaxed perturbed vector ˆst v by updating the prior vector vt according to the selected stochastic gradient ut. v to binary st
In line 5, we convert ˆst v by setting its top-B nonzero values (which corresponds to the entries in ˆst v with the B largest nonzero probabilities) to be 1 (thus perturbing at most B edges) and the remaining values to be 0. In line 6, we query the GNN model f ∗ v and obtain a loss feed-back L(st v). In line 7, we conduct PGD on the arm set W to update the vt+1 for round t + 1. Finally, after T queries, we obtain the perturbed vector sT v for the target node v.
Θ with st 3.1. Extending our attack for graph classification
Our proposed attack against node classification can be naturally extended to attack GNN models for graph clas-sification with a small effort of modifications. In a graph classification model, its input is an adjacent matrix of a graph and its output is the label of the graph. In node classifica-tion, we aim to perturb the adjacent vector of a target node, while in graph classification, we perturb the adjacent matrix.
Let A ∈ {0, 1}N ×N be the adjacent matrix of a graph with
N nodes. Moreover, let S ∈ {0, 1}N ×N be a perturbation matrix, where Sij = 1 if the connection status between the edge (i, j) is modified, and Sij = 0, otherwise. To perform our attack against graph classification models, we only need to flatten the matrix S into a vector s and feed it as an input to our attack algorithm. After obtaining the perturbed s, we can reshape it to a perturbed adjacent matrix.
4. Main Results
In this section, we analyze the regret bound of our attack algorithm where we assume the loss function is convex. We note that it is an interesting future work to generalize our analysis to non-convex loss functions. We first present the following assumptions and lemmas.
Assumption 1. There exists a Lipschitz constant CL such that the following inequality holds for any u and v,
|L(u) − L(v)| ≤ CL||u − v||2. (4)
Lemma 4. For continuous ˆst the instant regret is bounded as: v and the rounded binary sv,
E[|L(sv) − L(ˆst v)|] ≤ CLN 3/4 (cid:114) 1 +
η
δ
. (5)
Line 7 in Algorithm 1 can be seen as the stochastic gra-dient decent on L(ˆsv + δu). Thus, we have the following lemma to characterize its regret bound.
Lemma 5 ([12]). Suppose that the arm set W satisfies W ⊆
RB for given radius R > 0, where B is unit ball, i.e., B =
{u ∈ RN : ||u||2 ≤ 1}. When loss function L(·) is convex, the cumulative regret for relaxed continuous variables are bounded as follows:
L(ˆst v)] − T L(ˆs∗ v) ≤ CLR
√
T , (6)
E[
T (cid:88) t=1 where ˆst v is continuous variable at round t and ˆs∗ continuous solution of our relaxed optimization problem. v is optimal
Note that Lemma 5 just captures the regret on the whole arm set W. However, line 7 in Algorithm 1 updates the arm
ˆst v by projecting onto set (1 − α)W for 0 < α < 1. The incurred regret by conducting (1 − α)-projection is captured by the following lemma.
Lemma 6. For time horizon T , the incurred regret due to (1 − α)-projection is bounded as follows:
T (cid:88)
L(w) − T L(ˆs∗ v) ≤ 2αT. (7) min w∈(1−α)W t=1
Based on the above assumption and the lemmas, we have the following theorem on the regret bound of our attack:
Theorem 1. Under T rounds attack span, our proposed at-tack algorithm incurs regret Reg(T ), which is upper bounded by O(
N T 3/4) with T queries to the GNN.
√
Remark. Theorem 1 not only demonstrates the sublinear regret our attack achieves, but also presents that our attack is
√ dimension-efficient, i.e., O(
N ). It implies that our attack can be scalable to large graphs. Note that gradient-free ZOO
[6] has query complexity O(N ). From the regret bound, we can see that our attack enjoys a better convergence rate
O(1/T 3/4) than ZOO [19], which has a O(1/T 1/2) con-vergence rate. Note also that Ilyas et al. [15] proposed a bandit-based black-box attack to image classifiers, which can be adapted to solve our problem. However, their approach 1) does not provide theoretical results in terms of regret bound; and 2) is less efficient than ours due to requiring multiple gradient estimation in each iteration.
Computational complexity. Our attack requires 1 query per round and each query has time complexity O(N ). ZOO requires 2N queries per round and each query has time complexity O(N )—Its time complexity per round is O(N 2).
RL-S2V needs to trains an extra Q-network, which is used to perform the attack. During the attack, it has the same time complexity as our attack: 1 query per round and each round has a time complexity O(N ). These analyses show our attack is more efficient than RL-S2V and ZOO. 5.