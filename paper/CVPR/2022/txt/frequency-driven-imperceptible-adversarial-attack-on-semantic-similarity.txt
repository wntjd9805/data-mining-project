Abstract
Current adversarial attack research reveals the vulnera-bility of learning-based classifiers against carefully crafted perturbations. However, most existing attack methods have inherent limitations in cross-dataset generalization as they rely on a classification layer with a closed set of categories.
Furthermore, the perturbations generated by these meth-ods may appear in regions easily perceptible to the hu-man visual system (HVS). To circumvent the former prob-lem, we propose a novel algorithm that attacks semantic similarity on feature representations. In this way, we are able to fool classifiers without limiting attacks to a spe-cific dataset. For imperceptibility, we introduce the low-frequency constraint to limit perturbations within high-frequency components, ensuring perceptual similarity be-tween adversarial examples and originals. Extensive ex-periments on three datasets (CIFAR-10, CIFAR-100, and
ImageNet-1K) and three public online platforms indicate that our attack can yield misleading and transferable ad-versarial examples across architectures and datasets. Ad-ditionally, visualization results and quantitative perfor-mance (in terms of four different metrics) show that the proposed algorithm generates more imperceptible pertur-bations than the state-of-the-art methods. Code is made available at https://github.com/LinQinLiang/
SSAH-adversarial-attack. 1.

Introduction
With the advent of deep learning, neural network mod-els [10, 12, 16, 32] have demonstrated revolutionary perfor-mance in recognition tasks of real-world datasets. Never-theless, the vulnerability of deep neural networks (DNNs) to image corruptions and adversarial examples has been un-veiled [8, 35]. This problem hinders the applications of
*Equal Contribution
†Corresponding Author
Figure 1. Comparison of the adversarial examples and perturba-tions generated by three different attack methods: (a) C&W, (b)
Our SSA (semantic similarity attack), and (c) Our SSAH (seman-tic similarity attack on high-frequency components). For the vi-sualization, we regularize the perturbation by taking its absolute value and multiplying it by 25.
DNNs in security-critical domains and promotes research on understanding the robustness of DNNs, including adver-sarial attack [1, 8] and defense [22, 36, 40, 44].
The most intuitive approaches for white-box attacking are to increase the cost of the classification loss [8] to yield adversarial examples via gradient descent. Besides, they further apply ℓp distance to constrain the visual differences between benign and perturbed images. However, conven-tional approaches may suffer from the two open problems:
• Inherent limitation in cross-dataset generalization.
Due to the classification layer with learned weight vec-tors representing specific class proxies, current attack paradigms based on a white-box or surrogate classifier are limited to this setting, where images of the model training and attack domains are from the same set of categories. In real-world scenarios, however, an im-age from an open set [25] may belong to an unknown category to the classifier.
• Poor imperceptibility to HVS. Sharif et al. [30] have demonstrated that the ℓp distance metric is insufficient for assessing perceptual similarity. In other words, vi-sual imperceptibility may not be explicitly reflected us-ing only the perturbation intensity. For instance, C&W
[1], a well-known attack method, generates easy-to-perceive perturbations on the smooth background, as shown in Fig. 1 (a).
Intuitively, a natural approach to circumvent the classi-fication layer is to perform attacks in the feature space. In this work, we propose a general adversarial attack, namely semantic similarity attack (SSA), which builds on the simi-larity of feature representations. More specifically, we push apart the representations of adversarial and benign exam-ples but pull that of adversarial and target (the most dissim-ilar) examples together. In this way, we can fool classifiers without the knowledge of the specific image category. The underlying assumption is that the high-level representation implies image discrimination and semantics. Hence, per-turbing such representation can guide perturbations towards semantic regions within pixel space. As shown in Fig. 1 (b),
SSA focuses on perturbing semantic regions such as objects in the scene while suppressing redundant perturbations on irrelevant regions.
In addition to ℓp norms [1,2,26,27], other measures such as CIEDE2000 [46], SSIM [9] and LPIPS [18] are applied to approximate perceptual similarity. In this work, we pro-vide a different metric from the frequency domain perspec-tive. Generally, the low-frequency component of an image contains the basic information, whereas the high-frequency components represent trivial details and noise. Inspired by it, we measure the variations of low-frequency components as the perceptual variations in image pixel space. We further build a low-frequency constraint to limit the perturbations within imperceptible high-frequency components. As de-picted in Fig. 1 (c), the perturbations generated by the pro-posed framework, i.e., SSAH, appear mostly on impercep-tible regions such as object edges. Some works show that adversarial examples may be neither in high-frequency nor low-frequency components [23], and low-frequency pertur-bations with much perceptibility are especially effective for attacking defended models [31]. Nevertheless, we consider that developing attacks in high-frequency components is significant, as it helps improve perturbation imperceptibil-ity to HVS and learn robust models that better align with human perception. Recent works [38, 41] also prove that these high-frequency signals are barely perceivable to HVS but can largely determine the prediction results of DNNs.
The main contributions can be summarized as follows:
• We propose a novel adversarial attack, SSA, which is applicable in wide settings by attacking the semantic similarity of images.
• We present a new perturbation constraint, the low-frequency constraint, into the joint optimization of
SSA to limit perturbations within the imperceptible high-frequency components.
• We conduct extensive experiments on three datasets, i.e., CIFAR-10, CIFAR-100, and ImageNet-1K, and the experiment results show that our proposed attack outperforms the state-of-the-art methods by signifi-cantly imperceptible perturbations.
• Experimental results demonstrate that adversarial per-turbations generated by our SSAH are more transfer-able across different architectures and datasets. 2.