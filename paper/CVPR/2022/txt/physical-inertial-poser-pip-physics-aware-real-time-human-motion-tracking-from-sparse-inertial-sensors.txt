Abstract
Motion capture from sparse inertial sensors has shown great potential compared to image-based approaches since occlusions do not lead to a reduced tracking quality and the recording space is not restricted to be within the viewing frustum of the camera. However, capturing the motion and global position only from a sparse set of inertial sensors is inherently ambiguous and challenging. In consequence, re-cent state-of-the-art methods can barely handle very long period motions, and unrealistic artifacts are common due to the unawareness of physical constraints. To this end, we present the first method which combines a neural kinemat-ics estimator and a physics-aware motion optimizer to track body motions with only 6 inertial sensors. The kinemat-ics module first regresses the motion status as a reference, and then the physics module refines the motion to satisfy the physical constraints. Experiments demonstrate a clear improvement over the state of the art in terms of capture accuracy, temporal stability, and physical correctness. 1.

Introduction
Capturing the motion of real humans is a long-standing and challenging problem with many applications in com-puter vision and graphics, movie production, gaming, AR, and VR. However, due to its articulated structure, captur-ing the highly complex and potentially fast movements of the human body is challenging and many works have been proposed in the past [4, 65, 68, 69, 73, 88].
One category of approaches are image-based where the actor motion is recovered by analyzing the image data,
This work was supported by Beijing Natural Science Foundation (JQ19015), the NSFC (No.61727808, 62021002), the National Key R&D
Program of China 2018YFA0704000. This work was supported by
THUIBCS, Tsinghua University and BLBCI, Beijing Municipal Education
Commission. This work was partially supported by the ERC consolidator grant 4DReply (770784). We thank Notiom [31] for the extensive support on inertial sensors, and Liuqing Yang, Liangdi Ma, Siyuan Teng, Wenbin
Lin for the help on live demos. Feng Xu is the corresponding author.
Figure 1. PIP captures physically correct human motion, joint torques, and ground reaction forces solely from a sparse set of six IMUs. Importantly, PIP runs at 60 frames per second with only 16ms latency, which enables real-time applications. which can be either multi-view imagery [4, 6, 58, 82], depth images [71–73, 84], or a single RGB stream [5, 8, 14, 23, 25, 36, 80, 88]. From the setting, it becomes clear that occlu-sions (either object-actor or self occlusions) can lead to a significantly reduced tracking quality. Besides, these meth-ods are sensitive to the lighting and the appearance of the actor as distinct features need to be extracted from images.
Moreover, many methods assume a static camera, resulting in a limited space where the subject can be captured. These drawbacks limit the usability of optical motion capture.
Recently, researchers start to explore alternative sens-ing devices such as inertial measurement units (IMUs).
Production-ready solutions [31, 67] can track the body mo-tion accurately solely from inertial sensors. However, they rely on special suits with densely placed sensors (usually 17
IMUs), which are difficult to wear. Besides, the large num-ber of IMUs can hinder the actor’s movement. Having a sparser set of IMUs on the body is clearly advantageous and more flexible. However, recent sparse methods [18, 62, 69] struggle with physical correctness and cannot disambiguate poses with similar sensor measurements such as sitting and standing; they are non-causal, i.e., need future information,
which introduces large delays; their accuracy is still limited while temporal artifacts such as jitter become visible.
To this end, we propose Physical Inertial Poser (PIP), a new real-time method for motion capture as well as joint torque and ground reaction force estimation using only six
IMUs (see Fig. 1). In contrast to previous works [18, 69] that require future information, our method only requires the information already available at any given time, which means no additional delay is introduced. Our algorithm has two stages: 1) learning-based motion estimation and 2) physics-based motion optimization, which leverage both human kinematics and dynamics in motion capture.
In the estimation stage, we regress human pose, joint ve-locities, and foot-ground contact probabilities from the iner-tia inputs using recurrent neural networks (RNNs). We esti-mate leaf-to-full joint positions as intermediate tasks to im-prove the tracking accuracy as proposed by TransPose [69].
To resolve the pose ambiguity arising from the sparse IMU placement, we further propose a learning-based RNN state initialization strategy, which helps the networks better learn the change of body pose from input inertia measurements.
This results in a significant accuracy improvement espe-cially for ambiguous motions such as sitting still.
In the optimization stage, we recover the physically cor-rect motion, joint torques, and ground reaction forces from the kinematic estimations, leveraging a torque-controlled floating-base simulated character model. Different from previous works that independently control the rotation of each degree of freedom of the character using proportional-derivative (PD) rules [19, 51, 52, 76], we propose a novel dual PD controller to incorporate the global holistic con-trol of the character’s pose. This is achieved by applying
PD rules on both joint positions and rotations. The pro-posed technique significantly improves the translation ac-curacy and physical plausibility of the motion.
In summary, our main contributions are:
• The first physics-aware real-time approach that esti-mates human motion, joint torques, and ground reac-tion forces with only six IMUs, which we call PIP.
• A learning-based RNN state initialization scheme, which helps to better disambiguate human motion re-gression from sparse IMU measurements (Sec. 3.1).
• A dual PD controller, which achieves the combined control of local and global pose to improve the motion tracking accuracy and physical plausibility (Sec. 3.2).
Our experiments demonstrate that PIP significantly out-performs previous sparse IMU-based methods in terms of tracking accuracy, physical plausibility, and disambiguation of challenging poses. 2.