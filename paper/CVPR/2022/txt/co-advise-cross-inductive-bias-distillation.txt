Abstract
The inductive bias of vision transformers is more relaxed that cannot work well with insufﬁcient data. Knowledge dis-tillation is thus introduced to assist the training of transform-ers. Unlike previous works, where merely heavy convolution-based teachers are provided, in this paper, we delve into the inﬂuence of models inductive biases in knowledge distil-lation (e.g., convolution and involution). Our key observa-tion is that the teacher accuracy is not the dominant reason for the student accuracy, but the teacher inductive bias is more important. We demonstrate that lightweight teachers with different architectural inductive biases can be used to co-advise the student transformer with outstanding perfor-mances. The rationale behind is that models designed with different inductive biases tend to focus on diverse patterns, and teachers with different inductive biases attain various knowledge despite being trained on the same dataset. The di-verse knowledge provides a more precise and comprehensive description of the data and compounds and boosts the per-formance of the student during distillation. Furthermore, we propose a token inductive bias alignment to align the induc-tive bias of the token with its target teacher model. With only lightweight teachers provided and using this cross inductive bias distillation method, our vision transformers (termed as
CiT) outperform all previous vision transformers (ViT) of the same architecture on ImageNet. Moreover, our small size model CiT-SAK further achieves 82.7% Top-1 accuracy on ImageNet without modifying the attention module of the
ViT. Code is available at https://github.com/OliverRensu/co-advise. 1.

Introduction
Although convolutional neural network (CNN) has revo-lutionized the ﬁeld of computer vision, it possesses certain limitations. Recent research interests have been intrigued in
*Corresponding authors: Shengfeng He (hesfe@scut.edu.cn), Hang
Zhao (hangzhao@mail.tsinghua.edu.cn).
Figure 1. Comparison with DeiT. Here CiT-SA and CiT-SAK indi-cate models with token inductive bias alignment, without or with distillation. Our cross inductive bias distillation (CiT) outperforms
DeiT where only lightweight teachers are provided. Combining with token inductive bias alignment, the performance of our method can be further improved. replacing convolution layers with novel self-attention-based architectures. For instance, ViT [6] is a pure transformer without convolutional layers. Nevertheless, transformers have fewer inductive biases than CNNs (e.g., translation equivariance and locality) and thus suffer when the given amounts of training data are insufﬁcient [6]. In this con-text, knowledge distillation technique [7, 16] is applied by
DeiT [30] to assist the training of vision transformers. When the CNN teacher is powerful enough, transformers with such distillation [30] (i.e., DeiT) can achieve competitive results as SOTA CNNs on ImageNet. Howerver, DeiT has its own limitations: 1) The trained transformer is over-inﬂuenced by the inductive bias of the teacher CNN and mirrors its classi-ﬁcation error; 2) DeiT requires the teacher CNN to be very large (e.g., RegNetY-16GF), which disturbingly brings about heavy computational overhead (e.g., training a RegNetY-16GF on ImageNet takes four times longer training time under the same training protocols than DeiT-S); 3) Class
alignment by further introducing inductive bias into tokens.
In our experiments, we show that introducing the inductive bias to student model by our inductive bias alignment truly brings improvements on ImageNet. However, we also ﬁnd that comparing with directly introducing the same inductive bias with the teacher model into the model by our inductive bias alignment, knowledge distillation helps the student to perform more similar to the teacher. Therefore, we ﬁnd that although knowledge distillation cannot “transfer” inductive bias to the student, it helps the student to “inherit” more characteristics of the teacher.
Thanks to complementary inductive biases of convolu-tion (spatial-agnostic and channel-speciﬁc) and involution (spatial-speciﬁc and channel-agnostic), our method only re-quires two super lightweight teachers (a CNN and an INN).
In the distillation stage, the knowledge from teachers com-pensates each other and signiﬁcantly prompts the accuracy of the student transformer. Our main observations of this paper are as follows:
• We observe that the intrinsic inductive bias of the teacher model matters much more than its accuracy.
• CNNs and INNs with different inductive biases are inclined to learn complementary patterns, while a vision transformer, a more general architecture with fewer inductive biases, can inherit knowledge from both.
• When several teachers with different inductive biases are provided, a student model with less inductive biases is more compatible to learn various knowledge.
• Compared with introducing the inductive bias into the transformer, knowledge distillation makes student trans-former performs more similar to various inductive bias teachers.
• Our cross inductive bias vision transformers (CiT) out-perform all previous vision transformers of the same architecture and only require super lightweight teachers with 20% and 50% parameters of the teacher in DeiT-Ti and DeiT-S, respectively. 2.