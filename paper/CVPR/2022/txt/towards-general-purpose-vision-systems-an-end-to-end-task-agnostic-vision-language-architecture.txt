Abstract
Computer vision systems today are primarily N-purpose systems, designed and trained for a predefined set of tasks.
Adapting such systems to new tasks is challenging and of-ten requires non-trivial modifications to the network archi-tecture (e.g. adding new output heads) or training process (e.g. adding new losses). To reduce the time and exper-tise required to develop new applications, we would like to create general purpose vision systems that can learn and perform a range of tasks without any modification to the ar-chitecture or learning process. In this paper, we propose
GPV-1, a task-agnostic vision-language architecture that can learn and perform tasks that involve receiving an image and producing text and/or bounding boxes, including clas-sification, localization, visual question answering, caption-ing, and more. We also propose evaluations of generality of architecture, skill-concept1 transfer, and learning efficiency that may inform future work on general purpose vision. Our experiments indicate GPV-1 is effective at multiple tasks, reuses some concept knowledge across tasks, can perform the Referring Expressions task zero-shot, and further im-proves upon the zero-shot performance using a few training samples. 1.

Introduction
Computer vision systems today are N -purpose learn-ers — designed, trained, and limited to N predetermined tasks. Single-purpose models specialize in a single task, and adapting them to a new task or dataset requires an archi-tecture change, minimally replacing the last classification layer. Multi-purpose models, such as Mask-RCNN [13], si-multaneously solve more than one task, but the architecture and learning are tailored to specific tasks which must be de-1For this work, we define concepts, skills and tasks as follows: Con-cepts – nouns (e.g. car, person, dog), Skills – operations that we wish to perform on the given inputs (e.g. classification, object detection, image captioning), Tasks – predefined combinations of a set of skills performed on a set of concepts (e.g. ImageNet classification task involves the skill of image classification across 1000 concepts).
Figure 1. A task-agnostic vision-language architecture. GPV-1 takes an image and a natural language task description and outputs bounding boxes, confidences and text. GPV-1 can be trained end-to-end on any task that requires a box or text output, without any architecture modifications such as adding a new task-head. Results correspond to a model trained to perform VQA, localization, cap-tioning, and classification tasks. Star indicates the output modality supervised during training for each task. fined in advance. In vision-language models [32], dedicated output heads are typically used for each task and dataset.
Analogous to a general purpose computer, a general pur-pose vision (GPV) system is designed to carry out many vi-sion tasks, not all known at the time of design, constrained only by its input modalities, memory/instructions, and out-put modalities. General purpose systems enable new ap-plications to be developed without knowledge of or access to the underlying mechanics. The NLP community has made significant progress in this direction with sequence-to-sequence transformer-based models, such as T5 [41] and
GPT-3 [3], which can be trained to solve many language tasks without changing the architecture. We believe such advances are now possible within computer vision, though with many new challenges.
In this paper, we propose an end-to-end trainable task-agnostic vision-language architecture, GPV-1, as a step to-wards general purpose vision systems. As input, our system
receives an image and a text description of a task. The sys-tem outputs bounding boxes, confidences, and text that are relevant to the task and image. A user can input an im-age and query the system with a variety of requests such as
“What make is the blue car?” (visual question answering),
“Locate all the sedans” (localization), and “Describe the image” (captioning). Each query elicits a different response using output heads that are shared across tasks. Defining the task through natural language allows the user to request
GPV-1 to perform or learn a task without knowledge of its architecture or previous training. For example, our experi-ments show that GPV-1 can perform the referring expres-sions task without any training examples for that task and, when provided training examples, learns more quickly than special purpose models.
Beyond performing well on trained skill-concept combi-nations (contained in training tasks), GPV systems should be able to learn new tasks efficiently with the same archi-tecture as well as generalize to novel skill-concept combi-nations for learned skills by transferring concept knowledge from other skills. These abilities are not usually applicable or measured in specialized systems. Therefore, we propose evaluations that measure three forms of generality:
• Generality of architecture: Learn any task within a broad domain specified only through input/output modal-learn to ities without change to network structure (e.g. classify bird species, without adding new output heads)
• Generality of concepts across skills: Perform tasks in skill-concept combinations not seen during training (e.g. localize “muskrat” after learning to answer questions about “muskrats”)
• Generality of learning: Learn new tasks sample-efficiently with minimal loss to performance on previ-ously learned tasks
To test generality of architecture, we train and eval-uate our system’s ability to perform visual question an-swering (VQA), captioning, object classification, and ob-ject localization on the COCO dataset [29], as well as test zero-shot generalization to a referring expression task. To test generality of concepts across skills, we present a new split of the COCO images and corresponding task anno-tations called COCO-SCE (Skill-Concept Evaluation).
In
COCO-SCE, some concepts (objects) are held-out from each task but exposed via other tasks, and then evaluate perfor-mance on samples containing held-out concepts. To test generality of learning, we fine-tune our system on the re-ferring expressions task and measure its learning curve and extent of forgetting previously learned tasks.
In summary, our main contributions include: (1) An end-to-end trainable, task-agnostic vision-language architec-ture for learning and performing classification, grounding, visual question answering, captioning, and other tasks that involve image, text and bounding box modalities. (2) Eval-uation that tests generality of architecture, skill-concept transfer, and learning ability. 2.