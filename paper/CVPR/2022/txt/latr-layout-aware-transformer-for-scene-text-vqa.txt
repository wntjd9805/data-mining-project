Abstract
We propose a novel multimodal architecture for Scene
Text Visual Question Answering (STVQA), named Layout-Aware Transformer (LaTr). The task of STVQA requires models to reason over different modalities. Thus, we ﬁrst investigate the impact of each modality, and reveal the im-portance of the language module, especially when enriched with layout information. Accounting for this, we propose a single objective pre-training scheme that requires only text and spatial cues. We show that applying this pre-training scheme on scanned documents has certain advantages over using natural images, despite the domain gap. Scanned documents are easy to procure, text-dense and have a vari-ety of layouts, helping the model learn various spatial cues (e.g. left-of, below etc.) by tying together language and layout information. Compared to existing approaches, our method performs vocabulary-free decoding and, as shown, generalizes well beyond the training vocabulary. We further demonstrate that LaTr improves robustness towards OCR errors, a common reason for failure cases in STVQA. In addition, by leveraging a vision transformer, we eliminate the need for an external object detector. LaTr outperforms state-of-the-art STVQA methods on multiple datasets.
In particular, +7.6% on TextVQA, +10.8% on ST-VQA and
+4.0% on OCR-VQA (all absolute accuracy numbers). 1.

Introduction
Scene-Text VQA (STVQA) aims to answer questions by utilizing the scene text in the image. It requires reasoning over rich semantic information conveyed by various modali-ties – vision, language and scene text. Fig. 1 (a) depicts rep-resentative samples in STVQA, showcasing a model’s de-sired abilities, including; (1) a-priori information and world knowledge such as knowing what a website looks like (left image); and (2) the capability to use language, layout, and visual information (middle and right images).
In this work, we introduce Layout-Aware Transformer
*Authors contribute equally.
†Work done during an internship at Amazon.
Figure 1. The Role of Language and Layout in STVQA. (a)
Representative samples from TextVQA. (b) We visualize the in-formation extracted by the OCR system, showing that some ques-tions only require text features, some require both text and layout information and only some need beyond that. Accounting for this, we propose a layout-aware pre-training and architecture. (LaTr), a multimodal encoder-decoder transformer based model for STVQA. We begin by exploring how far lan-guage and layout information can take us in STVQA . In
Fig. 1 (b) we visualize the information extracted by the op-tical character recognition (OCR) system [1, 6, 14, 38], ex-hibiting three question categories: the ﬁrst type can be an-swered with just the text tokens; the second type can be answered with text and layout information (right vs left); the third can only be answered by utilizing text, spatial and visual features all together. We quantitatively show that in the current datasets, most questions fall under the ﬁrst two categories. To methodologically show this, we ﬁrst eval-uate a zero-shot language model on STVQA benchmarks, and then show that LaTr can already correctly answer over 50% of the questions with only text tokens. Next, we show the performance gain achieved by enriching the language modality with layout information via our propose layout-aware pre-training and architecture.
Recently, Yang et al. [74] demonstrated the advantages in pre-training STVQA models on natural images, proposing text-aware pre-training (TAP) scheme, which is designed to foster multi-modal collaboration. Acquiring large quan-tities of natural images with text is challenging and hard to scale, as most natural images do not contain scene text.
Even when they do, the amount of text is often sparse (pre-vious statistics suggest a median of only 6 words per im-age [67, 74]). In addition, and more importantly, TAP did not account for the importance of aligning the layout infor-mation with the semantic representations when designing the pre-training objectives.
To counter these drawbacks, we propose layout-aware pre-training based on a single objective using only text and spatial cues as input. Our pre-training forces the model to learn a joint representation which accounts for the interac-tions between text and layout information, beneﬁting the down-stream task of STVQA. Despite the domain gap, we
ﬁnd that pre-training on documents has certain advantages over natural images. Scanned documents contain more text compared to natural images, therefore it is easier to scale the experiment and expose the model to more data. Words in documents are usually complete sentences, helping the model better learn semantics beyond a simple bag of words.
Moreover, scanned documents provide varied layouts, lead-ing to effective alignment between language and spatial fea-tures. Lastly, performing pre-training without visual fea-tures reduces computational complexity substantially.
Our model utilizes a vision transformer [13] for extract-ing visual features, thus replacing the extensive need for an external object detector [21, 25, 74]. Moreover, in practice, current STVQA models exploit a dataset-speciﬁc vocabu-lary with a pointer mechanism for decoding [17, 21, 24, 25, 71,74–76], creating an over-reliance on the ﬁxed vocabulary and leaving no room for ﬁxing OCR errors. Our model per-forms vocabulary-free decoding, does well even on answers out-of-vocabulary, and even overcomes OCR errors in some cases. LaTr outperforms the state-of-the-art STVQA meth-ods by large margins on multiple public benchmarks. To summarize, the key contributions of our work are: 1. We recognize the key role language and layout play in
STVQA and propose a layout-aware pre-training and ar-chitecture to account for that. 2. We pinpoint a new symbiosis between documents and
STVQA via pre-training. We show empirically that doc-uments are beneﬁcial for tying together language and layout information despite the huge domain gap. 3. We show that existing methods perform poorly on out-of-vocabulary answers. LaTr does not require a vocabu-lary, does well even on answers that are not in the train-ing vocabulary, and can even overcome OCR errors. 4. We provide extensive experimentation and show the ef-fectiveness of our method by advancing the state-of-the-art by +7.6% on TextVQA and +10.8% on ST-VQA and
+4.0% in OCR-VQA dataset. 2.