Abstract
Generating shapes using natural language can enable new ways of imagining and creating the things around us. While significant recent progress has been made in text-to-image generation, text-to-shape generation remains a challenging problem due to the unavailability of paired text and shape data at a large scale. We present a sim-ple yet effective method for zero-shot text-to-shape gener-ation that circumvents such data scarcity. Our proposed method, named CLIP-Forge, is based on a two-stage train-ing process, which only depends on an unlabelled shape dataset and a pre-trained image-text network such as CLIP.
Our method has the benefits of avoiding expensive inference time optimization, as well as the ability to generate multiple shapes for a given text. We not only demonstrate promising zero-shot generalization of the CLIP-Forge model qualita-tively and quantitatively, but also provide extensive compar-ative evaluations to better understand its behavior. 1.

Introduction
Generating 3D shapes from text input has been a chal-lenging and interesting research problem with both signif-In the icant scientific and applied value [19, 22, 23, 34]. artificial intelligence and cognitive science research com-munities, researchers have long sought to bridge the two modalities of natural language and geometric shape [3, 55].
In practice, text-to-shape generation models are a key en-abling component to new smart tools in creative design and manufacture as well as animation and games [6].
Significant progress has been made to connect text and image modalities [10, 18, 26, 27, 52, 54]. Recently, DALL-E [49] and its associated pre-trained visual-textual embed-ding model CLIP [48] has shown promising results on the problem of text-to-image generation [44]. Notably, they have demonstrated strong zero-shot generalization while evaluated on tasks the model has not been specifically trained on. Shape generation is a more fundamental prob-lem than image generation, because images are projections and renderings of the inherently 3D physical world. There-fore, one may wonder if the success in 2D can be transferred to the 3D domain. This turns out to be a non-trivial prob-lem. Unlike the text-to-image case, where paired data is abundant, it is impractical to acquire huge paired datasets of texts and shapes.
Leveraging the progress of text-to-image generation, we present CLIP-Forge. As shown in Figure 2, we overcome the limitation of shape-text pair data scarcity via a simple and effective approach. We exploit the fact that 3D shapes can be easily and automatically rendered into images us-ing standard graphics pipelines. We then utilize pre-trained image-text joint embedding models such as [25, 48], which bring text and image embeddings in a similar latent space so that they can be used interchangeably. Hence, we can train
2.