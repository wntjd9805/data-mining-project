Abstract
Finding relevant moments and highlights in videos ac-cording to natural language queries is a natural and highly valuable common need in the current video content explo-sion era. Nevertheless, jointly conducting moment retrieval and highlight detection is an emerging research topic, even though its component problems and some related tasks have already been studied for a while. In this paper, we present the first unified framework, named Unified Multi-modal
Transformers (UMT), capable of realizing such joint opti-mization while can also be easily degenerated for solving individual problems. As far as we are aware, this is the first scheme to integrate multi-modal (visual-audio) learn-ing for either joint optimization or the individual moment retrieval task, and tackles moment retrieval as a keypoint
âˆ—Corresponding author. detection problem using a novel query generator and query decoder. Extensive comparisons with existing methods and ablation studies on QVHighlights, Charades-STA, YouTube
Highlights, and TVSum datasets demonstrate the effective-ness, superiority, and flexibility of the proposed method un-der various settings. Source code and pre-trained models are available at https://github.com/TencentARC/UMT. 1.

Introduction
Video has already become the major media in content production, distribution, and consumption in our daily lives.
It has the unique advantage of being able to include visual, audio, and linguistic information in the same media, in line with our natural experiences. Such an advantage on infor-mation richness, however, is also a challenging factor limit-ing its production and consumption, as it brings about very
high costs on satisfying two critical needs. The first one is to find relevant moments in existing videos for producing new content or just getting creation hints from such refer-ences. The second one is to glance at large amounts of video content quickly by scanning video highlights rather than watching the entire original videos or video moments at a normal speed, which is needed by both video producers and consumers in such a content explosion era.
These two critical needs lead to two important research topics: video moment retrieval [1,8] and video highlight de-tection [32, 37, 44], respectively. Although one may realize that these two tasks are closely related (especially when a text query is given), they have not yet been explicitly jointly studied until a very recent work [18] which builds a novel dataset called QVHighlights for this purpose and presents the first model Moment-DETR optimized for jointly solving both problems. Nevertheless, this seminal work has several limitations. It assumes a text query always exists and it has only applied to the visual modality of each video. More-over, it is still a very basic model called a strong baseline, although it adopts a transformer framework, the latest and fast-rising neural network architecture type.
This paper goes deeper into designing joint video mo-ment retrieval and highlight detection approaches by mainly exploring two aspects: multi-modal learning and flexibility, as shown in Figure 1. Apart from text and video (i.e. visual information), audio is also treated as an important input.
Moreover, a unified yet flexible framework called Unified
Multi-modal Transformers (UMT) is proposed to handle different modality reliability situations and combinations.
For example, when the text input is unavailable, the task de-generates to highlight detection only. When there is some significant distraction in the text, its reliability will be com-promised. Moreover, the audio may also be noisy, which may limit effective exploration. UMT covers all these nat-ural variations which conventionally need to be resolved by different specifically designed models.
To demonstrate the effectiveness and superiority of the proposed framework, we conduct experiments not only on the QVHighlights dataset [18], the only one built for joint video moment retrieval and highlight detection, but also on popular public datasets for moment retrieval (Charades-STA [8]) and highlight detection (TVSum [31], YouTube
Highlights [32]), with or without text guidance. For each case, we compare the proposed scheme with several state-of-the-art approaches. Detailed ablation studies are also carried out to evaluate the essential components of the pro-posed scheme and to reveal meaningful insights. 2.