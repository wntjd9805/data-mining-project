Abstract
Dataset condensation aims at reducing the network training effort through condensing a cumbersome train-ing set into a compact synthetic one. State-of-the-art ap-proaches largely rely on learning the synthetic data by matching the gradients between the real and synthetic data batches. Despite the intuitive motivation and promising re-sults, such gradient-based methods, by nature, easily over-ft to a biased set of samples that produce dominant gra-dients, and thus lack a global supervision of data distribu-tion. In this paper, we propose a novel scheme to Condense dataset by Aligning FEatures (CAFE), which explicitly at-tempts to preserve the real-feature distribution as well as the discriminant power of the resulting synthetic set, lend-ing itself to strong generalization capability to various ar-chitectures. At the heart of our approach is an effective strategy to align features from the real and synthetic data across various scales, while accounting for the classifca-tion of real samples. Our scheme is further backed up by a novel dynamic bi-level optimization, which adaptively ad-justs parameter updates to prevent over-/under-ftting. We validate the proposed CAFE across various datasets, and demonstrate that it generally outperforms the state of the art: on the SVHN dataset, for example, the performance gain is up to 11%. Extensive experiments and analysis ver-ify the effectiveness and necessity of proposed designs. (a) The gradient distribution changes from a uniform to long-tailed distribution during the training. Meanwhile, the overlap of large-gradient samples are small among different architectures. (b) The visualization of synthetic images and their distributions generated by gradient matching and CAFE. ConvNet is used.
Figure 1: (a) At the later training stage, most examples do not contribute meaningful gradients, making the syn-thetic set learned by gradient matching extremely bias to-wards those large-gradient samples, which downgrades its generalization to unseen architectures. (b) Compared with gradient-based method [53], the synthetic set learned by our approach effectively captures the whole distribution thus generalizes well to other network architectures. 1.

Introduction
Deep neural networks (DNNs) have demonstrated un-precedented results in many if not all applications in com-puter vision [9, 21, 39, 27, 10, 23, 29, 8, 46, 49, 28, 38, 37, 36]. These gratifying results, nevertheless, come with costs: the training of DNNs heavily rely on the sheer amount of data, sometimes up to tens of millions of samples, which
*Equal contribution. (kai.wang@comp.nus.edu.sg, bo.zhao@ed.ac.uk)
â€ Corresponding author (youy@comp.nus.edu.sg). consequently requires enormous computational resources.
Numerous research endeavours have, therefore, focused on alleviating the cumbersome training process through constructing small training sets [1, 14, 7, 13, 42, 31, 33, 44, 45, 50]. One classic approach is known as coreset or sub-set selection [1, 31, 12], which aims to obtain a subset of salient data points to represent the original dataset of inter-est. Nevertheless, coreset selection is typically a NP-hard problem [19], making it computationally intractable over large-scale datasets. Most existing approaches have thus re-sorted to greedy algorithms with heuristics [7, 31, 2, 47, 35] to speed up the process by trading-off the optimality.
Recently, dataset condensation [40, 53] has emerged as a competent alternative with promising results. The goal of dataset condensation is, as its name indicates, to con-dense a large training set into a small synthetic one, upon which DNNs are trained and expected to preserve the per-formances. Along this line, the pioneering approach of [40] proposed a meta-learning-based strategy; however, the nest-loop optimization precludes its scaling up to large-scale in-the-wild datasets. The work of [53] alleviates this issue by enforcing the batch gradients of the synthetic samples to ap-proach those of the original ones, which bypasses the recur-sive computations and achieves impressive results. The op-timization of the synthetic examples is explicitly supervised by minimizing the distance between the gradients produced by the synthetic dataset and the real dataset.
However, gradient matching method has two potential problems. First, due to the memorization effect of deep neural networks [48], only a small number of hard exam-ples or noises produce dominant gradients over the network parameters. Thus, gradient matching may overlook those representative but easy samples, while overft to those hard samples or noises. Second, these hard examples that pro-duce large gradients may vary across different architectures; relying solely on gradients, therefore, will yield poor gen-eralization performance to unseen architectures. The dis-tributions of gradients and hard examples are illustrated in
Fig. 1a. The synthetic data learned by gradient matching may be highly biased towards a small number of unrepre-sentative data points, which is illustrated in Fig. 1b.
To go beyond the learning bias and better capture the whole dataset distribution, in this paper, we propose a novel strategy to Condense dataset by Aligning FEatures, termed as CAFE. Unlike the approach of [53], we account for the distribution consistency between synthetic and real datasets by applying distribution-level supervision. Our approach, through matching the features that involve all intermedi-ary layers, expands the attention across all samples and hence provides a much more comprehensive characteriza-tion of the distribution while avoiding over-ftting on hard or noisy samples. Such distribution-level supervision will, in turn, endow CAFE with stronger generalization power than gradient-based methods, since the hard examples may easily vary across different architectures.
Specifcally, we impose two complementary losses into the objective of CAFE. The frst one concerns capturing the data distribution, in which the layer-wise alignment be-tween the features of the real and synthetic samples is en-forced and further the distribution is preserved. The sec-ond loss, on the other hand, concerns discrimination. Intu-itively, the learned synthetic samples from one class should well represent the corresponding clusters of the real sam-ples. Hence, we may treat each real sample as a testing sample, and classify it based on its affnity to the synthetic clusters. Our second loss is then defned upon the classif-cation result of the real samples, which, effectively, injects the discriminant capabilities into the synthetic samples.
The proposed CAFE is further backed up by a novel bi-level optimization scheme, which allows our network and synthetic data to be updated through a customized number of SGD steps. Such a dynamic optimization strategy, in practice, largely alleviates the under- and over-ftting issues of prior methods. We conduct experiments on several pop-ular benchmarks and demonstrate that, the results yielded by CAFE are signifcantly superior to the state of the art: on the SVHN dataset, for example, our method outper-forms the runner-up by 11% when learning 1 image/class synthetic set. We also especially prove that synthetic set learned by our method has better generalization ability than that learned by [53].
In summary, our contribution is a novel and effective approach for condensing datasets, achieved through align-ing layer-wise features between the real and synthetic data, and meanwhile explicitly encoding the discriminant power into the synthetic clusters. In addition, a new bi-level op-timization scheme is introduced, so as to adaptively alter the number of SGD steps. These strategies jointly enable the proposed CAFE to well characterize the distribution of the original samples, yielding state-of-the-art performances with strong generalization and robustness across various learning settings. 2.