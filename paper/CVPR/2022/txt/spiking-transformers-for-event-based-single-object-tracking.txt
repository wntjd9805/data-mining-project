Abstract
Event-based cameras bring a unique capability to track-ing, being able to function in challenging real-world condi-tions as a direct result of their high temporal resolution and high dynamic range. These imagers capture events asyn-chronously that encode rich temporal and spatial informa-tion. However, effectively extracting this information from events remains an open challenge.
In this work, we pro-pose a spiking transformer network, STNet, for single ob-ject tracking. STNet dynamically extracts and fuses infor-In par-mation from both temporal and spatial domains. ticular, the proposed architecture features a transformer module to provide global spatial information and a spik-ing neural network (SNN) module for extracting temporal cues. The spiking threshold of the SNN module is dynami-cally adjusted based on the statistical cues of the spatial in-formation, which we ﬁnd essential in providing robust SNN features. We fuse both feature branches dynamically with a novel cross-domain attention fusion algorithm. Extensive experiments on three event-based datasets, FE240hz, EED and VisEvent validate that the proposed STNet outperforms existing state-of-the-art methods in both tracking accuracy and speed with a signiﬁcant margin. The code and pre-trained models are at https://github.com/Jee-King/CVPR2022_STNet. 1.

Introduction
Event-based cameras are bio-inspired sensors that offer attractive properties compared to conventional frame-based cameras: high temporal resolution (in the order of μs), high dynamic range (140 dB vs. 60 dB), low power consump-tion, and high pixel bandwidth (on the order of kHz) re-sulting in drastically reduced motion blur [18]. With these unique sensing capabilities, event-based cameras can ro-bustly function in degraded conditions, such as low-light, fast motion, and high dynamic range scenes. Recently, event-based cameras have been proposed for object track-ing tasks [2,7–9,24,36,38], especially in adverse conditions (cid:2) Xin Yang (xinyang@dlut.edu.cn) is the corresponding authors. Xin
Yang and Bo Dong lead this project.
KYS
PrDiMP
STARK-S
TransT
STNet
GT
Figure 1. A comparison of our method (STNet) with state-of-the-art (SOTA) trackers. Unlike existing SOTA approaches, our method dynamically fuses temporal and spatial cues, resulting in robust tracking performance. that conventional imagers struggle with.
Event-based cameras measure per-pixel brightness changes and output events asynchronously. As a result, existing CNN-based approaches cannot directly work with event-based sensors, as they require synchronous input.
Several works proposed to convert asynchronous events to conventional frames for downstream processing, mainly based on handcrafted features [27, 34, 41]. Unfortunately, in contrast to images captured with traditional cameras, the accumulated event frames are much sparser and lack texture information. Thus, directly applying CNN-based methods designed for conventional images does not offer a solution, as evidenced by extensive experiments in this work.
Recently, a line of works focused on developing learning-based object tracking approaches tailored to event frames. Zhang et al. [50] proposed to combine conventional and event frames with a cross-domain attention fusion algo-rithm to track objects under different degraded conditions.
However, their approach requires both traditional and accu-mulated event frames as inputs. More importantly, the ap-proach ignores the rich temporal information encoded in the event domain. In contrast, another body of work focused on the event domain only and introduced various event prepro-cessing approaches to encoding both temporal and spatial information into the processed event frames [8, 9, 30, 36].
However, these existing approaches do not consider the downstream network, making it challenging to choose suit-able preprocessing methods for a speciﬁc computer vision task. Perhaps even more importantly, existing methods may suppress informative events from a downstream network
perspective.
In this work, we propose a spiking transformer network, dubbed STNet, for single object tracking, which only re-quires events as input. The proposed STNet does not require handcrafted event preprocessing but instead is designed to directly extract spatial and temporal information from the event positions over a small interval of time. The temporal information is extracted based on global spatial cues, and it plays an essential role in generalizing to different exter-nal conditions. The extracted temporal and spatial features are fused by a novel cross-domain attention method, which dynamically suppresses events based on the current scene.
In particular, the proposed architecture features a LIF-based spiking neural network (SNN) [45] for extracting temporal features and a reduced Swin-transformer [33] for extracting spatial information. We ﬁnd it critical to oper-ating the SNN based on spatial cues for this to function as intended. Speciﬁcally, the SNN neurons maintain mem-brane potential, which is increased by accumulating incom-ing spikes and decreased based on a decay function in the time domain [1, 21, 23]. A spike is generated when the ac-cumulated potential is higher than a spiking threshold, and the potential is reset based on a refractory function [26, 42].
The potential accumulation, decay, and resting process can be regarded as a temporal memory, motivating us to treat the SNN as a temporal feature extractor. We ensure effec-tive dynamic temporal features extraction by proposing to assign spiking thresholds to the LIF neurons based on the statistical cues of the spatial information.
Extensive experiments on the FE240hz [50], EED [36], and VisEvent [16] datasets validate the effectiveness of the proposed STNet (see Figure 1), which outperforms exist-ing state-of-the-art methods by signiﬁcant margins in rep-resentative success rate (RSR), representative precision rate (RPR), and processing speed (see Figure 7). Ablation ex-periments evidence the importance of each key component of STNet. More importantly, we show the spatial-aware dy-namic spiking threshold is essential for robust tracking. Our work is the ﬁrst work to verify the importance of the SNN dynamic threshold in object tracking tasks.
In summary, we make the following contributions:
• We propose a novel spiking transformer architecture for event-based single object tracking, allowing us to ex-tract and fuse temporal and spatial information based on the dynamically deﬁned informativeness of both temporal and spatial domains.
• We dynamically adjust the spiking threshold of a LIF-based SNN according to statistical cues of global spatial scene information.
• Extensively experimental results validate that the pro-posed approach outperforms state-of-the-art methods. Our ablation study evidences the effectiveness of each key com-ponent of the proposed STNet. 2.