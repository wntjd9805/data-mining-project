Abstract
The inversion of real images into StyleGAN’s latent space is a well-studied problem. Nevertheless, applying ex-isting approaches to real-world scenarios remains an open challenge, due to an inherent trade-off between reconstruc-tion and editability: latent space regions which can accu-rately represent real images typically suffer from degraded semantic control. Recent work proposes to mitigate this trade-off by ﬁne-tuning the generator to add the target im-age to well-behaved, editable regions of the latent space.
While promising, this ﬁne-tuning scheme is impractical for prevalent use as it requires a lengthy training phase for each new image. In this work, we introduce this approach into the realm of encoder-based inversion. We propose HyperStyle, a hypernetwork that learns to modulate StyleGAN’s weights to faithfully express a given image in editable regions of the latent space. A naive modulation approach would re-quire training a hypernetwork with over three billion pa-rameters. Through careful network design, we reduce this to be in line with existing encoders. HyperStyle yields reconstructions comparable to those of optimization tech-niques with the near real-time inference capabilities of en-coders. Lastly, we demonstrate HyperStyle’s effectiveness on several applications beyond the inversion task, includ-ing the editing of out-of-domain images which were never seen during training. Code is available on our project page: https://yuval- alaluf.github.io/hyperstyle/. 1.

Introduction
Generative Adversarial Networks (GANs) [20], and in particular StyleGAN [32–35] have become the gold stan-dard for image synthesis. Thanks to their semantically rich latent representations, many works have utilized these mod-els to facilitate diverse and expressive editing through latent space manipulations [4, 6, 9, 12, 24, 38, 44, 48, 56]. Yet, a signiﬁcant challenge in adopting these approaches for real-world applications is the ability to edit real images. For editing a real photo, one must ﬁrst ﬁnd its corresponding latent representation via a process commonly referred to as
GAN inversion [74]. While the inversion process is a well-studied problem, it remains an open challenge.
Recent works [2, 61, 73, 75] have demonstrated the ex-*Denotes equal contribution
Figure 1. Given a desired input image, our hypernetworks learn to modulate a pre-trained StyleGAN network to achieve accurate image reconstructions in editable regions of the latent space. Do-ing so enables one to effectively apply techniques such as Style-CLIP [48] and InterFaceGAN [56] for editing real images. istence of a distortion-editability trade-off: one may invert an image into well-behaved [75] regions of StyleGAN’s la-tent space and attain good editability. However, these re-gions are typically less expressive, resulting in reconstruc-tions that are less faithful to the original image. Recently,
Roich et al. [54] showed that one may side-step this trade-off by considering a different approach to inversion. Rather than searching for a latent code that most accurately recon-structs the input image, they ﬁne-tune the generator in order to insert a target identity into well-behaved regions of the latent space. In doing so, they demonstrate state-of-the-art reconstructions while retaining a high level of editability.
Yet, this approach relies on a costly per-image optimization of the generator, requiring up to a minute per image.
A similar time-accuracy trade-off can be observed in classical inversion approaches. On one end of the spec-trum, latent vector optimization approaches [1, 2, 14, 35, 40]
achieve impressive reconstructions, but are impractical at scale, requiring several minutes per image. On the other end, encoder-based approaches leverage rich datasets to learn a mapping from images to their latent representations.
These approaches operate in a fraction of a second but are typically less faithful in their reconstructions.
In this work, we aim to bring the generator-tuning tech-nique of Roich et al. [54] to the realm of interactive applica-tions by adapting it to an encoder-based approach. We do so by introducing a hypernetwork [23] that learns to reﬁne the generator weights with respect to a given input image. The hypernetwork is composed of a lightweight feature extrac-tor (e.g., ResNet [25]) and a set of reﬁnement blocks, one for each of StyleGAN’s convolutional layers. Each reﬁne-ment block is tasked with predicting offsets for the weights of the convolutional ﬁlters of its corresponding layer. A ma-jor challenge in designing such a network is the number of parameters comprising each convolutional block that must be reﬁned. Na¨ıvely predicting an offset for each param-eter would require a hypernetwork with over three billion parameters. We explore several avenues for reducing this complexity: sharing offsets between parameters, sharing network weights between different hypernetwork layers, and an approach inspired by depthwise-convolutions [26].
Lastly, we observe that reconstructions can be further im-proved through an iterative reﬁnement scheme [5] which gradually predicts the desired offsets over a small number of forward passes through the hypernetwork. By doing so, our approach, HyperStyle, essentially learns to “optimize” the generator in an efﬁcient manner.
The relation between HyperStyle and existing generator-tuning approaches can be viewed as similar to the relation between encoders and optimization inversion schemes. Just as encoders ﬁnd a desired latent code via a learned network, our hypernetwork efﬁciently ﬁnds a desired generator with no image-speciﬁc optimization.
We demonstrate that HyperStyle achieves a signiﬁcant improvement over current encoders. Our reconstructions even rival those of optimization schemes, while being sev-eral orders of magnitude faster. We additionally show that
HyperStyle preserves the appealing structure and semantics of the original latent space, allowing one to leverage off-the-shelf editing techniques on the resulting inversions, see
Fig. 1. Finally, we show that HyperStyle generalizes well to out-of-domain images, such as paintings and animations, even when unobserved during the training of the hypernet-work itself. This hints that the hypernetwork does not only learn to correct speciﬁc ﬂawed attributes, but rather learns to reﬁne the generator in a more general sense. 2.