Abstract
Existing motion capture datasets are largely short-range and cannot yet fit the need of long-range applications. We propose LiDARHuman26M, a new human motion capture dataset captured by LiDAR at a much longer range to over-come this limitation. Our dataset also includes the ground truth human motions acquired by the IMU system and the synchronous RGB images. We further present a strong base-line method, LiDARCap, for LiDAR point cloud human motion capture. Specifically, we first utilize PointNet++ to encode features of points and then employ the inverse kinematics solver and SMPL optimizer to regress the pose through aggregating the temporally encoded features hier-archically. Quantitative and qualitative experiments show that our method outperforms the techniques based only on
RGB images. Ablation experiments demonstrate that our dataset is challenging and worthy of further research. Fi-nally, the experiments on the KITTI Dataset and the Waymo
Open Dataset show that our method can be generalized to
*Equal contribution.
†Corresponding author. different LiDAR sensor settings. 1.

Introduction
The past ten years have witnessed a rapid development of marker-less human motion capture [9, 18, 48, 60], with various applications like VR/AR and interactive entertain-ment. However, conveniently capturing long-range 3D hu-man motions in a large space remains challenging, which is critical for sports and human behavior analysis.
So far, vision-based mocap solutions take the majority in this topic. The high-end solutions require dense opti-cal markers [55, 72] or dense camera rigs [8, 25, 26, 48] for faithfully motion capture, which are infeasible for consumer-level usage. In contrast, monocular capture meth-ods are more practical and attractive. The recent learning-based techniques have enable robust human motion cap-ture from a single RGB stream, using pre-scanned human templates [16, 17, 19, 62, 64] or parametric human mod-els [5, 27, 31, 32, 36, 37, 39]. However, in the long-range capture scenarios where the performers are far away from the cameras, the captured images suffer from degraded and blurred artifacts, leading to fragile motion capture. Vari-ous methods [65, 66] explore to capture 3D human motions under such degraded and low-resolution images. But such approaches are still fragile to capture the global positions under the long-range setting, especially when handling the textureless clothes or environment lighting changes. In con-trast, motion capture using body-worn sensor like Inertial
Measurement Units (IMUs) [22, 43, 69] is widely adopted due to its environment-independent property. However, the requirement of body-worn sensors makes them unsuitable to capture motions of people wearing everyday apparel.
Moreover, the IMU-based methods will suffer from an ac-cumulated global drifting artifact, especially for the long-range setting. Those motion capture methods [11,15,49,61] using consumer-level RGBD sensors are also infeasible for the long-range capture in a large scene, due to the relatively short effective range (less than 5 m) of RGBD cameras.
In this paper, we propose a rescue to the above prob-lems by using a consumer-level LiDAR. A LiDAR sensor provides accurate depth information of a large-scale scene with a large effective range (up to 30 m). These properties potentially allow capturing human motions under the long-range setting in general lighting conditions, without suffer-ing from the degraded artifacts of visual sensors. Neverthe-less, capturing long-range 3D human motions using a single
LiDAR is challenging. First, under the long-range setting, the valid observed point clouds corresponding to the target performer is sparse and noisy, making it difficult for robust motion capture. Second, despite the popularity of LiDAR for 3D modeling, most existing work [20, 33, 40, 46, 52, 74] focus on scene understanding and 3D perception. The lack of a large-scale LiDAR-based dataset with accurate 3D hu-man motion annotations leads to the feasibility of a data-driven motion capture pipeline using LiDAR.
To tackle these challenges, we propose LiDARCap – the first marker-less, long-range and data-driven motion cap-ture method using a single LiDAR sensor as illustrated in
Fig. 1. More specifically, we first introduce a large bench-mark dataset LiDARHuman26M for LiDAR-based human motion capture. Our dataset consists of various modalities, including synchronous LiDAR point clouds, RGB images and ground-truth 3D human motions obtained from profes-sional IMU-based mocap devices [41]. It covers 20 kinds of daily motions and 13 performers with 184.0k capture frames, resulting in roughly 26 million valid 3D points of the observed performers with a large capture distance rang-ing from 12 m to 28 m. Note that our LiDARHuman26M dataset is the first of its kind to open up the research direc-tion for data-driven LiDAR-based human motion capture in the long-range setting. The multi-modality of our dataset also brings huge potential for future direction like multi-modal human behavior analysis. Secondly, based on our novel LiDARHuman26M dataset, we provide LiDARCap, a strong baseline motion capture approach on LiDAR point clouds. Finally, we provide a thorough evaluation of various stages in our LiDARCap as well as state-of-the-art image-based methods baselines using our dataset. These eval-uations highlight the benefit of the LiDAR-based method against the image-based method under the long-range set-ting. We also provide preliminary results to indicate that
LiDAR-based long-range motion capture remains to be a challenging problem for future investigations of this new research direction. To summarize, our main contributions include:
• We propose the first monocular LiDAR-based ap-proach for marker-less, long-range 3D human motion capture in a data-driven manner.
• We propose a three-stage pipeline consisting of a tem-poral encoder, an inverse kinematics solver, and an
SMPL optimizer to improve pose estimation perfor-mance.
• We provide the first large-scale benchmark dataset for
LiDAR-based motion capture, with rich modalities and ground-truth annotations. The dataset will be made publicly available. 2.