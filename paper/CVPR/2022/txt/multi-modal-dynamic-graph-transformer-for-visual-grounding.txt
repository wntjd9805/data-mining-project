Abstract
Visual grounding (VG) aims to align the correct regions of an image with a natural language query about that im-age. We found that existing VG methods are trapped by the single-stage grounding process that performs a sole evaluate-and-rank for meticulously prepared regions. Their performance depends on the density and quality of the can-didate regions, and is capped by the inability to optimize the located regions continuously. To address these issues, we propose to remodel VG into a progressively optimized visual semantic alignment process. Our proposed multi-modal dynamic graph transformer (M-DGT) achieves this by building upon the dynamic graph structure with regions as nodes and their semantic relations as edges. Starting from a few randomly initialized regions, M-DGT is able to make sustainable adjustments (i.e., 2D spatial transforma-tion and deletion) to the nodes and edges of the graph based on multi-modal information and the graph feature, thereby efﬁciently shrinking the graph to approach the ground truth regions. Experiments show that with an average of 48 boxes as initialization, the performance of M-DGT on the
Flickr30k Entities and RefCOCO datasets outperforms ex-isting state-of-the-art methods by a substantial margin, in terms of both accuracy and Intersect over Union (IOU) scores. Furthermore, introducing M-DGT to optimize the predicted regions of existing methods can further signiﬁ-cantly improve their performance. The source codes are available at https://github.com/iQua/M-DGT. 1.

Introduction
Visual grounding (VG) is a crucial task in the interdis-ciplinary subject of computer vision and natural language processing. With a focus on aligning semantically consis-tent phrase-region pairs from the given image and sentence, the general visual grounding problem can be extended to phrase localization [5, 9, 36] and referring expression com-prehension [17, 28]. Then, tasks including translation [24], cross-modal retrieval [14], image caption [5, 39], and visual query answering [1, 38] can beneﬁt from aligned phrase-region pairs.
Although there have been signiﬁcant breakthroughs in recent years, we noticed that the main bodies of proposed state-of-the-art VG methods [3, 6, 7, 16, 18, 20, 27, 28, 42, 46] follow the one-step evaluate-and-rank matching ar-chitecture. Methods with this architecture evaluate regions in the image to select the correct ones, a single-execute process without continuous optimization. Existing works
[16, 19, 20, 27, 42] relying on the region proposal build models based on candidate regions and make once predic-tions from these regions. In some works [6, 18, 43] with neural attention mechanism, the attention scores are as-signed to rough regions that are reﬁned once to generate the grounding boxes. The inherent issues in this architecture are that solving a complex nonlinear matching problem with a one-step manner leads to poor region-phrase matching re-sults caused by local optimum, and as a result the matched regions for phrases cannot be further adjusted. For exam-ple, if there is a signiﬁcant deviation between the predicted regions and ground truth regions, existing works cannot ad-just the regions to approach the target. This is proven by our experiments, in that the accuracy of these works decreases substantially with the increase of the Intersect over Union (IOU) threshold.
Some more recent works [9, 33, 41] tried to alleviate problems in such matching architecture by introducing the idea of progressive learning. Dogan et al. [9] achieved this by performing phrase grounding sequentially while Sun et al. [33] proposed the idea of iterative shrinking the detec-tion area through reinforcement learning to locate the target.
However, they are still trapped in the one-step matching ar-chitecture because the failure of any matching step in the learning process will ultimately produce poor results.
In this work, we break the limits in such design by proposing a search-based visual grounding mechanism.
More speciﬁcally, we remodel VG into a progressively op-timized visual semantic alignment process. By doing this, the simple initialization regions can be continuously opti-mized to approach the target regions. To achieve this goal,
the main challenge is to pass the information in each re-gion from local to global with the minimum cost required.
Motivated by works [16, 20, 40], the highly structured in-formation in the spatial regions and multi-modal input can be modeled by graph theory.
With these insights, we propose a multi-modal dynamic graph transformer (M-DGT), a framework that regards re-gions as nodes and semantic relations as edges, making the process of progressively approaching the ground truth re-gions equivalent to the transformation of a graph. M-DGT
ﬁrst constructs a graph from a few simple initialization re-gions and then continuously transforms the nodes and edges according to the multi-modal information and the graph feature, thereby shrinking the graph to the target layout.
The image regions corresponding to the nodes of the tar-get graph are most aligned with the semantics in the query.
M-DGT with a graph-based progressive search method can continuously correct the deviation of the previous transfor-mation in the subsequent learning to alleviate the impact of a failed transformation on the ﬁnal result, which improves robustness. In addition, M-DGT naturally exploits the spa-tial relations between regions and the cross-modal semantic relations by modeling them in a multi-modal graph struc-ture.
The original contributions of this paper are as follows.
First, we rethink the VG as a progressively optimized vi-sual semantic alignment process, making the VG can be di-vided into sub-problems that can be easily solved progres-sively. Second, we propose a novel multi-modal dynamic graph transformer (M-DGT) to model the process of search-ing for a matching region-text as a graph structure trans-formation. Third, M-DGT is fast, accurate, and with high generality. Starting with just a few simple initialization re-gions, our framework can gradually obtain tighter matching regions for phrases without missing targets, enabling it to work on arbitrary datasets. Finally, in two tasks, including phrase localization on the Flickr30k Entities and referring expression comprehension on three RefCOCO datasets, M-DGT not only achieves state-of-the-art accuracies but also produces bounding boxes with high Intersect over Union (IOU) scores. 2.