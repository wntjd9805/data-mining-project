Abstract
Sign languages are visual languages, with vocabular-ies as rich as their spoken language counterparts. How-ever, current deep-learning based Sign Language Produc-tion (SLP) models produce under-articulated skeleton pose sequences from constrained vocabularies and this limits applicability. To be understandable and accepted by the deaf, an automatic SLP system must be able to generate co-articulated photo-realistic signing sequences for large domains of discourse.
In this work, we tackle large-scale SLP by learning to co-articulate between dictionary signs, a method capable of producing smooth signing while scaling to unconstrained domains of discourse. To learn sign co-articulation, we propose a novel Frame Selection Network (FS-NET) that improves the temporal alignment of interpolated dictionary signs to continuous signing sequences. Additionally, we propose SIGNGAN, a pose-conditioned human synthesis model that produces photo-realistic sign language videos direct from skeleton pose. We propose a novel keypoint-based loss function which improves the quality of synthe-sized hand images.
We evaluate our SLP model on the large-scale meineDGS (mDGS) corpus, conducting extensive user evaluation showing our FS-NET approach improves co-articulation of interpolated dictionary signs. Additionally, we show that SIGNGAN significantly outperforms all base-line methods for quantitative metrics, human perceptual studies and native deaf signer comprehension. 1.

Introduction
Sign languages are rich visual languages with large lex-ical vocabularies [52] and intricate co-articulated move-ments of both manual (hands and body) and non-manual (facial) features. Sign Language Production (SLP), the au-tomatic translation from spoken language sentences to sign language sequences, must be able to produce photo-realistic continuous signing for large domains of discourse to be use-ful to the deaf communities.
Prior deep-learning approaches to SLP have either pro-duced concatenated isolated sequences that disregard the natural co-articulation between signs [53, 68] or contin-uous sequences end-to-end [22, 44, 46, 68] which suffer from under-articulation [43]. Furthermore, these methods have struggled to generalise beyond the limited domain of weather [14].
In this paper, we propose an SLP method to produce photo-realistic continuous sign language videos direct from
Firstly, we unconstrained spoken language sequences. translate from spoken language to gloss1 sequences. We next learn the temporal co-articulation between gloss-based dictionary signs, modelling the temporal prosody of sign language [3].
To model sign co-articulation, we propose a novel Frame
Selection Network (FS-NET) that learns the optimal subset of frames that best represents a continuous signing sequence (Fig. 2 middle). We build a transformer encoder with cross-attention [58] to predict a temporal alignment path super-vised by Dynamic Time Warping (DTW).
The resulting skeleton pose sequences are subsequently used to condition a video-to-video synthesis model capable of generating photo-realistic sign language videos, named
SIGNGAN (Fig. 2 right). Due to the natural presence of motion blur in sign language datasets from fast moving hands [15], a classical application of a hand discriminator leads to an increase in blurred hand generation. To avoid this, we propose a novel keypoint-based loss that signifi-cantly improves the quality of hand image synthesis in our photo-realistic signer generation module. To enable train-ing on diverse sign language datasets, we propose a method for controllable video generation that models a multi-modal distribution of sign language videos in different styles.
Our deep-learning based SLP model is able to generalise to large domains of discourse, as it is trivial to increase vo-cabulary with a few examples of this new sign in a continu-ous signing context. We conduct extensive deaf user evalu-ation on a translation protocol of mDGS [20], showing that
FS-NET improves the natural signing motion of interpo-lated dictionary sequences and is overwhelmingly preferred to baseline SLP methods [47]. Additionally, we achieve state-of-the-art back translation performance on RWTH-PHOENIX-Weather-2014T (PHOENIX14T) with a 43% improvement over baselines, highlighting the understand-able nature of our approach.
Furthermore, we evaluate SIGNGAN using the high quality Content4All (C4A) dataset [5], outperforming state-of-the-art synthesis methods [6, 54, 62, 63] for quantitative evaluation and human perception studies. Finally, we con-duct a further deaf user evaluation to show that SIGNGAN is more understandable than the skeletal sequences previ-ously used to represent sign [44]. 1Glosses are a written representation of sign that follow sign language ordering and grammar, defined as minimal lexical items [52].
The contributions of this paper can be summarised as:
• The first SLP model to produce large-scale sign lan-guage sequences from an unconstrained domain of dis-course to a level understandable by a native deaf signer
• A novel Frame Selection Network, FS-NET, that learns to co-articulate between dictionary signs via a monotonic alignment to continuous sequences
• A method to generate photo-realistic continuous sign language videos, SIGNGAN, with a novel hand key-point loss that improves the hand synthesis quality
• Extensive user evaluation of our proposed approach, showing preference of our proposed method, alongside state-of-the-art back translation results 2.