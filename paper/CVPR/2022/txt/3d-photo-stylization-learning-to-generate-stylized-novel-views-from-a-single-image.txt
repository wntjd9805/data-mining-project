Abstract 1.

Introduction
Visual content creation has spurred a soaring interest given its applications in mobile photography and AR / VR.
Style transfer and single-image 3D photography as two rep-resentative tasks have so far evolved independently. In this paper, we make a connection between the two, and address the challenging task of 3D photo stylization — generating stylized novel views from a single image given an arbitrary style. Our key intuition is that style transfer and view syn-thesis have to be jointly modeled. To this end, we propose a deep model that learns geometry-aware content features for stylization from a point cloud representation of the scene, resulting in high-quality stylized images that are consistent across views. Further, we introduce a novel training proto-col to enable the learning using only 2D images. We demon-strate the superiority of our method via extensive qualitative and quantitative studies, and showcase key applications of our method in light of the growing demand for 3D content creation from 2D image assets.1
*Work partially done when Fangzhou was an intern at Snap Research
†co-corresponding authors 1Project page: http://pages.cs.wisc.edu/˜fmu/style3d
Given an input content image and a reference style im-age, neural style transfer [4, 13, 14, 16, 22, 24, 32, 35, 42, 50] creates a novel image that “paints” the content with the style. Despite a high quality stylized image, the result is limited to the same viewpoint of the content image. What if we can render stylized images from different views? See
Fig. 1 for two examples. When displayed with parallax, this capacity will provide drastically more immersive visual experience for 2D images, and support the application of interactive browsing of 3D photos on mobile and AR/VR devices.
In this paper, we address this new task of gen-erating stylized images of novel views from a single input image and an arbitrary reference style image, as illustrated in Fig. 1. We refer to this task as 3D photo stylization — a marriage between style transfer and novel view synthesis. 3D photo stylization has several major technical barriers.
As observed in [21], directly combining existing methods of style transfer and novel view synthesis yields blurry or in-consistent stylized images, even with dense 3D geometry obtained from structure from motion and multi-view stereo.
This challenge is further manifested with a single content image as the input, where a method must resort to monocu-lar depth estimation with incomplete and noisy 3D geome-try, leading to holes and artifacts when synthesizing stylized images of novel views. In addition, training deep models for this task requires a large-scale dataset of diverse scenes with dense geometry annotation that is currently lacking.
To bridge this gap, we draw inspiration from one-shot 3D photography [28, 41, 51], and adopt a point cloud based scene representation [21, 41, 58]. Our key innovation is a deep model that learns 3D geometry-aware features on the point cloud without using 2D image features from the content image for rendering novel views with a consistent style. Our method accounts for the input noise from depth maps, and jointly models style transfer and view synthe-sis. Moreover, we propose a novel training scheme that enables learning our model using standard image datasets (e.g., MS-COCO [33]), without the need of multi-view im-ages or ground-truth depth maps.
Our contributions are summarized into three folds. (1)
We present the first method to address the new task of 3D photo stylization — synthesizing stylized novel views from a single content image with arbitrary styles. (2) Unlike pre-vious methods, our method learns geometry-aware features on a point cloud without using 2D content image features and from only 2D image datasets. (3) Our method demon-strates superior qualitative and quantitative results, and en-ables several interesting applications. 2.