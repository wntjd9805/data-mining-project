Abstract 1.

Introduction
This article addresses the problem of distilling knowl-edge from a large teacher model to a slim student network for LiDAR semantic segmentation. Directly employing pre-vious distillation approaches yields inferior results due to the intrinsic challenges of point cloud, i.e., sparsity, ran-domness and varying density. To tackle the aforementioned problems, we propose the Point-to-Voxel Knowledge Distil-lation (PVD), which transfers the hidden knowledge from both point level and voxel level. Specifically, we first lever-age both the pointwise and voxelwise output distillation to complement the sparse supervision signals. Then, to better exploit the structural information, we divide the whole point cloud into several supervoxels and design a difficulty-aware sampling strategy to more frequently sample supervoxels containing less-frequent classes and faraway objects. On these supervoxels, we propose inter-point and inter-voxel affinity distillation, where the similarity information be-tween points and voxels can help the student model better capture the structural information of the surrounding en-vironment. We conduct extensive experiments on two pop-ular LiDAR segmentation benchmarks, i.e., nuScenes [3] and SemanticKITTI [1]. On both benchmarks, our PVD-consistently outperforms previous distillation approaches by a large margin on three representative backbones, i.e.,
Cylinder3D [36, 37], SPVNAS [25] and MinkowskiNet [5].
Notably, on the challenging nuScenes and SemanticKITTI datasets, our method can achieve roughly 75% MACs re-duction and 2× speedup on the competitive Cylinder3D model and rank 1st on the SemanticKITTI leaderboard among all published algorithms1. Our code is available at https://github.com/cardwing/Codes- for-PVKD. 1https://competitions.codalab.org/competitions/20331#results (single-scan competition) till 2021-11-18 04:00 Pacific Time, and our method is termed Point-Voxel-KD. Our method (PV-KD) ranks 3rd on the multi-scan challenge till 2021-12-1 00:00 Pacific Time.
LiDAR semantic segmentation plays a vital role in the perception of autonomous driving as it provides per-point semantic information of the surrounding environment. With the advent of deep learning, plenty of LiDAR segmentation models have been proposed [14, 25, 26, 37] and have domi-nated the leaderboard of many benchmarks [1,3]. However, the impressive performance comes at the expense of heavy computation and storage, which impedes them from being deployed in resource-constrained devices.
To enable the deployment of these powerful LiDAR segmentation models on autonomous vehicles, knowledge distillation [10] is a prevailing technique to transfer the dark knowledge from the overparameterized teacher model to the slim student network to achieve model compres-sion. However, directly applying previous distillation algo-rithms [9, 10, 17, 24, 28] to LiDAR semantic segmentation brings marginal gains due to the intrinsic difficulty of point cloud, i.e., sparsity, randomness and varying density.
To address the aforementioned challenges, we propose the Point-to-Voxel Knowledge Distillation (PVD). As the name implies, we propose to distil the knowledge from both point-level and voxel-level. Specifically, to combat against the sparse supervision signals, we first propose to distil the pointwise and voxelwise probabilistic outputs from the teacher, respectively. The pointwise output contains fine-grained perceptual information while the voxelwise predic-tion embraces coarse but richer clues about the surrounding environment.
To effectively distil the valuable structural knowledge from the unordered point sequences, we propose to ex-ploit the point-level and voxel-level affinity knowledge. The affinity knowledge is obtained via measuring the pairwise semantic similarity of the point features and voxel features.
However, straightforwardly mimicking the affinity knowl-edge of the whole point cloud is intractable since there are tens of thousands of points and the affinity matrix of these point features have over ten billion elements. Consequently,
those minority classes and faraway objects.
We conduct extensive experiments on the nuScenes [3] and SemanticKITTI [1] datasets and the results demon-strate that our algorithm consistently outperforms previ-ous distillation approaches by a large margin on three i.e., Cylinder3D [36, 37], SPV-contemporary models,
NAS [25] and MinkowskiNet [5]. Notably, on the nuScenes and SemanticKITTI benchmarks, PVD achieves approx-imately 75% MACs reduction and 2× speedup on the top-performing Cylinder3D model with very minor perfor-mance degradation. 2.