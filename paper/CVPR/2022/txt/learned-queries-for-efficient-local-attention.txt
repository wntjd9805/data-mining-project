Abstract
Vision Transformers (ViT) serve as powerful vision mod-els. Unlike convolutional neural networks, which domi-nated vision research in previous years, vision transform-ers enjoy the ability to capture long-range dependencies in the data. Nonetheless, an integral part of any trans-former architecture, the self-attention mechanism, suffers from high latency and inefficient memory utilization, mak-ing it less suitable for high-resolution input images. To alle-viate these shortcomings, hierarchical vision models locally employ self-attention on non-interleaving windows. This relaxation reduces the complexity to be linear in the input size; however, it limits the cross-window interaction, hurt-In this paper, we propose a ing the model performance. new shift-invariant local attention layer, called query and attend (QnA), that aggregates the input locally in an over-lapping manner, much like convolutions. The key idea be-hind QnA is to introduce learned queries, which allow fast and efficient implementation. We verify the effectiveness of our layer by incorporating it into a hierarchical vision transformer model. We show improvements in speed and memory complexity while achieving comparable accuracy with state-of-the-art models. Finally, our layer scales espe-cially well with window size, requiring up to x10 less mem-ory while being up to x5 faster than existing methods. The code is publicly available at https://github.com/ moabarar/qna. 1.

Introduction
Two key players take the stage when considering data ag-gregation mechanisms for image processing. Convolutions were the immediate option of choice. They provide local-ity, which is an established prior for image processing, and efficiency while doing so. Nevertheless, convolutions cap-ture local patterns, and extending them to global context is difficult if not impractical. Attention-based models [56], on the other hand, offer an adaptive aggregation mechanism, where the aggregation scheme itself is input-dependent, or spatially dynamic. These models [4, 12] are the de-facto
Figure 1. Performance-Efficiency comparisons on 2242 in-put size. QnA-ViT (our method) demonstrates better accuracy-efficiency trade-off compared to state-of-the-art baselines. As sug-gested by Dehghani et al. [11], we report the ImageNet-1k [46]
Top-1 accuracy (y-axis) trade-off with respect to parameter count (left), floating point operations (middle) and inference throughput (right). The throughput is measured using the timm [59] library, as tested on NVIDIA V100 with 16GB memory. Other metrics, are from the original publications [8, 14, 35, 60, 66, 70] choice in the natural-language processing field and have re-cently blossomed for vision tasks as well.
Earlier variants of the Vision Transformers (ViT) [13] provide global context by processing non-interleaving im-age patches as word tokens. For these models to be ef-fective, they usually require a vast amount of data [13, 49], heavy regularization [48, 52] or modified optimization ob-jectives [7, 16]. Even more so, it was observed that large scale-training drives the models to attend locally [44], espe-cially for early layers, encouraging the notion that locality is a strong prior.
Local attention mechanisms are the current method of choice for better vision backbones. These backbones fol-low a pyramid structure similar to convolutional neural net-works (CNNs) [8, 15, 58, 70], and process high-resolution inputs by restricting the self-attention to smaller windows, preferably with some overlap [55] or other forms of inter-communication [8, 35, 66]. The latter approaches naturally induce locality while benefiting from spatially dynamic ag-gregation. On the other hand, these architectures come at the cost of computational overhead and, more importantly, are not shift-equivariant.
In this paper, we revisit the design of local attention and introduce a new aggregation layer called Query and Attend (QnA). The key idea is to leverage the locality and shift-invariance of convolutions and the expressive power of at-tention mechanisms.
In local self-attention, attention scores are computed be-tween all window elements. This is a costly operation of quadratic complexity in the window size. We propose us-ing learned queries to compute the aggregation weights, al-lowing linear memory complexity, regardless of the cho-sen window size. Our layer is also flexible, showing that it can serve as an effective up- or down-sampling operation.
We Further observe that combining different queries al-lows capturing richer feature subspaces with minimal com-putational overhead. We conclude that QnA layers inter-leaved with vanilla transformer blocks form a family of hi-erarchical ViTs that achieve comparable or better accuracy compared to SOTA models while benefiting from up-to x2 higher throughput and fewer parameters and floating-point operations (see Figure 1).
Through rigorous experiments, we demonstrate that our novel aggregation layer holds the following benefits:
• QnA imposes locality, granting efficiency without compromising accuracy.
• QnA can serve as a general-purpose layer. For ex-ample, strided QnA allows effective down-sampling, and multiple-queries can be used for effective up-sampling, demonstrating improvements over alterna-tive baselines.
• QnA naturally incorporates locality into existing transformer-based frameworks.
For example, we demonstrate how replacing self-attention layers with
QnA ones in an attention-based object-detection framework [5] is beneficial for precision, and in par-ticular for small-scale objects. 2.