Abstract
Graph convolutional network (GCN) has achieved great success in single hand reconstruction task, while interact-ing two-hand reconstruction by GCN remains unexplored.
In this paper, we present Interacting Attention Graph Hand the first graph convolution based network (IntagHand), that reconstructs two interacting hands from a single RGB image. To solve occlusion and interaction challenges of two-hand reconstruction, we introduce two novel attention based modules in each upsampling step of the original
GCN. The first module is the pyramid image feature at-tention (PIFA) module, which utilizes multiresolution fea-tures to implicitly obtain vertex-to-image alignment. The second module is the cross hand attention (CHA) module that encodes the coherence of interacting hands by build-ing dense cross-attention between two hand vertices. As a result, our model outperforms all existing two-hand re-construction methods by a large margin on InterHand2.6M benchmark. Moreover, ablation studies verify the effec-tiveness of both PIFA and CHA modules for improving the reconstruction accuracy. Results on in-the-wild im-ages and live video streams further demonstrate the gen-eralization ability of our network. Our code is available at https://github.com/Dw1010/IntagHand. 1.

Introduction
Interacting two-hand reconstruction is one of the fun-damental tasks towards manifold industrial applications such as virtual reality (VR), human-computer-interaction (HCI), robotics, holoportation, digital medicine, etc. Re-cently, monocular single hand pose and shape recovery has witnessed great success owing to deep neural networks [3, 12,21,49,52] and large scale datasets [13,16,25,26,53,54].
However, two-hand reconstruction is more challenging and remains unsolved for two reasons. First, severe mutual oc-clusions and appearance similarity confuse the feature ex-tractors, making it difficult for networks to align hand poses with image features. Second, the interaction context be-tween two hands is difficult to be effectively formulated during network design and training.
Figure 1. Illustration of our IntagHand for two-hand reconstruc-tion. Top: results on InterHand2.6M [25] dataset. Bottom: real-time two-hand motion capture results on live video streams. Our method produces high quality two-hand mesh reconstruction of flexible hand poses under severe occlusions.
Monocular depth-based two-hand tracking [22, 27, 28, 38–40] has been studied for years and promising results have been demonstrated. However, the energy demand and algorithm complexity restrict the ubiquitous application of depth-based methods. Recently, Wang et al. [40] con-tributes a monocular RGB based two-hand reconstruction by tracking dense matching map. However, the tracking procedure itself is inherently sensitive to fast motion, and does not take full advantage of prior knowledge between interacting hands. Since the proposal of the large scale two-hand dataset InterHand2.6M [25], learning based sin-gle image two-hand reconstruction methods have emerged.
[11, 18, 25, 46] either employ 2.5D
Existing methods heatmaps to estimate hand joint positions [11, 18, 25], or use them as attention maps to extract sparse image fea-tures [46]. However, such sparse local image features en-coded in the heatmaps could not effectively model hand surface occlusions, and could not extract dense interaction context. In contrast, vertex-based graph convolutional net-work (GCN) has achieved great success in single hand re-construction [12,23,24,37], yet it has not been demonstrated in two-hand conditions, and the previously mentioned chal-lenges remain to be addressed.
In this paper, we propose Interacting Attention Graph
Hand (IntagHand), a novel GCN based single image two-hand reconstruction method. As a basic pipeline, we ini-tially utilize GCN to regress mesh vertices of each hand in a coarse-to-fine manner, similar to traditional GCN [12].
However, for the two-hand task, naively using a two-stream
GCN to generate two hand vertices fails to utilize the inter-action context between two hands, making the network con-fused regarding two-hand mutually occluded parts. More-over, without any image feature feedback, the network has difficulty aligning vertices to image features as suggested by [24,47]. To address these issues, we equip the GCN with two novel attention modules. The first module is a pyra-mid image feature attention (PIFA) module, which uses a transformer encoder to update the latent vertex features with patched image features. Unlike projection based vertex-image alignment [47], PIFA benefits from the global sens-ing ability of the attention mechanism to help each vertex seek alignment over all image patches. Furthermore, as
GCN upsamples mesh vertices in a coarse-to-fine manner, we design an encoder-decoder based image feature extrac-tion module to extract pyramid features, forcing the high resolution mesh to leverage fine-grained features. The sec-ond module is a cross hand attention (CHA) module that encodes interaction context into hand vertex features. The
CHA module allows vertices of each hand to pay dense at-tention to the other hand’s vertex features in order to dis-ambiguate interhand occlusions. Benefiting from the GCN structure and the novel attention based modules, the Intag-Hand outperforms existing methods on InterHand2.6M [25] by a large margin (8.8mm v.s. 13.5mm). Moreover, our method is efficient for real-time applications, producing well-aligned two-hand results on in-the-wild images and live video streams, as shown in Fig. 1 and our project page.
Overall, our contributions are summarized as:
• We propose the first two-hand reconstruction method using GCN based mesh regression, named IntagHand, demonstrating the effectiveness of GCN for the two-hand reconstruction task.
• We propose a pyramid image feature attention (PIFA) module to distill local occlusion information with global image patch attention, producing better align-ment between the hand vertices and the image features.
• We propose a cross hand attention (CHA) module to implicitly model the two-hand interaction context, im-proving the reconstruction accuracy for closely inter-acting poses.
• Our method achieves the new state-of-the-art results and outperforms existing solutions by a large margin on the InterHand2.6M benchmark. Furthermore, We demonstrate the generalization ability of our method on in-the-wild images. 2.