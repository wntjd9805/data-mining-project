Abstract
Recognizing discriminative details such as eyes and beaks is important for distinguishing fine-grained classes since they have similar overall appearances. In this regard, we introduce Task Discrepancy Maximization (TDM), a simple module for fine-grained few-shot classification. Our objec-tive is to localize the class-wise discriminative regions by highlighting channels encoding distinct information of the class. Specifically, TDM learns task-specific channel weights based on two novel components: Support Attention Module (SAM) and Query Attention Module (QAM). SAM produces a support weight to represent channel-wise discriminative power for each class. Still, since the SAM is basically only based on the labeled support sets, it can be vulnerable to bias toward such support set. Therefore, we propose QAM which complements SAM by yielding a query weight that grants more weight to object-relevant channels for a given query image. By combining these two weights, a class-wise task-specific channel weight is defined. The weights are then applied to produce task-adaptive feature maps more focusing on the discriminative details. Our experiments validate the effectiveness of TDM and its complementary benefits with prior methods in fine-grained few-shot classification. 1.

Introduction
With the advancement of deep learning, it has achieved remarkable performance beyond humans in various down-stream tasks [5, 10]. However, there is a strong assumption that numerous labeled images should exist to achieve such performance. If the number of labeled images is insufficient, it shows drastic degradation in the performance [3, 8, 38].
To resolve such degradation from a shortage of labeled im-ages and reduce the cost of labeling, the computer vision community recently paid more attention to few-shot classifi-cation [8, 34, 38]. Briefly, the goal of few-shot classification is to train a model with high adaptability to novel classes.
To achieve this goal, the episodic learning strategy is mainly
*Corresponding author
Figure 1. Effect of the channel weight in the CUB dataset. (a) Ex-isting methods treat channels of feature maps equally. In such case, high variance channels within a class highly likely to disturb the classification task, where channel variance represents the channel-wise variances of feature maps of the same class – intuitively, the instances of the same class having similar features at a channel lead a low channel variance for the corresponding channel. This is mainly because it is hard to make a consensus among features for classification criteria. (b) However, in fine-grained datasets, simply removing high variance channels for each class shows marginal improvements. It is because classes share similar features, e.g., feather, and wings in CUB dataset, and thus channels with a low variance may not be discriminative. Therefore, in fine-grained datasets, we should grant different weights to channels depending on whether each channel reflects distinct characteristics. (c) TDM produces per-class channel weight by discovering discriminative channels for each class in the episode. Note that, the numbers in boxes are classification accuracies. used, where each episode consists of sampled categories from the dataset. Furthermore, each class has a support set for training and a query set for evaluation.
The stream of metric-based learning is a promising di-rection for the few-shot classification. These methods
[16, 34, 35, 38] learn a deep representation with a predefined metric or online-trained metric. Specifically, the inference for a query is performed based on the distances among sup-port and query sets under such metric.
However, the features of a novel class extracted by a model trained on the base classes hardly form a tight cluster, since the feature extractor is highly sensitive and activates the semantically discriminative variations in the distribution of base classes [32, 46]. To alleviate this, recent methods
utilize primitive knowledge [20,46] or propose task-dynamic feature alignment strategies [7, 12, 14, 33, 42, 44, 45]. Among two strategies, task-dynamic feature alignment methods are being spotlighted. The task-dynamic feature alignment meth-ods can be further divided into two main streams: spatial alignment and channel alignment. The spatial alignment methods [7, 12, 14, 42, 42, 44] aim to resolve the spatial mis-match between key features on the feature maps of different instances. On the other hand, the channel alignment meth-ods [14, 33, 44, 45] modify feature maps to better represent the semantic features for novel classes.
Although these alignment methods are shown to be effec-tive on the general few-shot classification task, they achieved insignificant gains for fine-grained datasets. This is mainly because they only focus on exploiting features that describe novel objects, which may not be discriminative in such tasks.
Indeed, localizing discriminative details is important in fine-grained classification, since categories share similar overall appearances [6, 9, 27, 50]. Therefore, distinct clues for each category also should be discovered for fine-grained few-shot classification. In Fig. 1 (c), we verify that localizing dis-criminatory details of the object through channel weights is effective for fine-grained few-shot task.
In this context, we introduce novel Task Discrepancy
Maximization (TDM), a module that localizes discrimina-tive regions by weighting channels per class. TDM high-lights the channels that represent discriminative regions and restrains the contributions of other channels based on class-wise channel weight. Specifically, TDM is composed of two components: Support Attention Module (SAM) and
Query Attention Module (QAM). Given a support set, SAM outputs a support weight per class that presents high activa-tions on discriminative channels. On the other hand, QAM is fed with the query set to produce a query weight per in-stance. The query weight is to highlight the object-relevant channels. To infer these weights, the relation between each feature map and the average channel pooled features are considered. Note that, the channel pooled average feature map has the spatial information of the object [22, 43] as described in Fig. 2. Therefore, channels are highly likely to represent objects when they are similar to spatially averaged feature map. By combining two weights computed from our sub-modules, a task-specific weight is finally defined.
Consequently, the task-specific weight is utilized to produce task-adaptive feature maps.
Our main contributions are summarized as follows:
• We propose a novel feature alignment method, TDM, to define the class-wise channel importance, for fine-grained few-shot classification.
• Our proposed TDM is highly applicable to prior metric-based few-shot classification models.
• When combined with the recent few-shot classification
Figure 2. Visualization of pooling results. Each column shows the locations where each pooling method focuses on the image. The second and third columns visualize the results of average pooling and the max pooling, respectively. GAP tends to concentrate on the object parts in the images, while GMP is often out of focus. models, the TDM achieves the state-of-the-art perfor-mance in fine-grained few-shot classification task. 2.