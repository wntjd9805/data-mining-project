Abstract
We present a new neural representation, called Neural
Ray (NeuRay), for the novel view synthesis task. Recent works construct radiance fields from image features of in-put views to render novel view images, which enables the generalization to new scenes. However, due to occlusions, a 3D point may be invisible to some input views. On such a 3D point, these generalization methods will include incon-sistent image features from invisible views, which interfere with the radiance field construction. To solve this problem, we predict the visibility of 3D points to input views within our NeuRay representation. This visibility enables the ra-diance field construction to focus on visible image features, which significantly improves its rendering quality. Mean-while, a novel consistency loss is proposed to refine the vis-ibility in NeuRay when finetuning on a specific scene. Ex-periments demonstrate that our approach achieves state-of-the-art performance on the novel view synthesis task when generalizing to unseen scenes and outperforms per-scene optimization methods after finetuning. Project page: https://liuyuan-pal.github.io/NeuRay/ 1.

Introduction
Novel View Synthesis (NVS) is an important problem in computer graphics and computer vision. Given a set of in-put images with known camera poses, the goal of NVS is to synthesize images of the scene from arbitrary virtual camera poses. Recently, neural rendering methods have achieved impressive improvements on the NVS problem compared to earlier image-based rendering methods [14, 19, 30]. Neu-ral Radiance Field (NeRF) [31] shows that photo-realistic images of novel views can be synthesized by volume ren-dering on a 5D radiance field encoded in a neural network which maps a position and a direction to a density and a color. However, these methods cannot generalize to unseen scenes as they learn scene-specific networks, which usually take hours or days for a single scene.
Recent works [5, 55, 56, 63] propose NeRF-like neural
Figure 1. (a) Without occlusions, local image features are consis-tent on the surface point. (b) Local image features are inconsistent on a surface point due to occlusions. (c) Local image features are inconsistent on a non-surface point. Generalization methods will correctly assign a large density to the surface point in (a) due to the feature consistency. However, when features are not very consistent in (b) and (c), it is relatively hard for these methods to correctly determine the density. rendering frameworks that can generalize to unseen scenes.
Given a set of input views, they construct a radiance field on-the-fly by extracting local image features on these views and matching multi-view features to predict colors and den-sity on 3D points. This is similar to traditional stereo match-ing methods [47, 60] that check the multi-view feature con-sistency to find a surface point, as shown in Fig. 1 (a). How-ever, when features are not consistent on a point, it is rela-tively hard for these methods to correctly determine whether such inconsistency is caused by occlusions as shown in
Fig. 1 (b) or this point is a non-surface point as shown in
Fig. 1 (c), leading to rendering artifacts.
To address this problem, we introduce a new neural rep-resentation called Neural Rays (NeuRay) in this paper. Neu-Ray consists of pixel-aligned feature vectors on every input view. On a camera ray emitting from a pixel on the input view, the associated NeuRay feature vector on this pixel is able to predict visibility to determine whether a 3D point at a specific depth is visible or not. With such visibility, we can easily distinguish the occlusion-caused feature in-consistency from the non-surface-caused feature inconsis-tency in Fig. 1, which leads to more accurate radiance field construction and thus better rendering quality on difficult
scenes with severe self-occlusions.
A key challenge is how to estimate the visibility in an unseen scene. This is a chicken-and-egg problem because the estimation of visibility requires knowing the surface lo-cations while the estimated visibility is intended for bet-ter surface estimation in the radiance field construction.
To break this cycle, we propose to apply well-engineered multi-view stereo (MVS) algorithms, like cost volume con-struction [60] or patch-matching [47], to reconstruct the scene geometry and then extract the pixel-aligned feature vectors of NeuRay from the reconstructed geometry. In the end, NeuRay will be used in the computation of the visibil-ity to improve the radiance field construction.
Another problem is how to parameterize such visibility in NeuRay. A direct way is to predict the densities along the camera ray from the view to the 3D point and then accumu-late these densities to compute the transmittance as the vis-ibility like NeRF [31]. However, computing visibility with this strategy is computationally impractical because given
N input views and a 3D point, we should accumulate the density along all N camera rays from every input view to this 3D point, which means we need to sample K points on every camera ray and evaluate the density on all N × K sample points. To reduce the computation complexity, we directly parameterize the visibility with a Cumulative Dis-tribution Function (CDF) in NeuRay, which avoids density accumulation along rays and only requires N network for-ward passes to compute the visibility of all N input views.
NeuRay not only help the radiance field construction on unseen scenes but also be able to refine itself by finetun-ing on a specific scene with a novel consistency loss. Since both the NeuRay representation and the constructed radi-ance field depict the scene geometry, we propose a loss to enforce the consistency between the surface locations from the NeuRay representation and those from the constructed radiance field. This loss enables NeuRay to memorize the scene geometry predicted by the radiance field. At the same time, the memorized scene geometry in NeuRay will in turn improve the radiance field construction by providing better occlusion inference.
We conducted extensive experiments on the NeRF syn-thetic dataset [31], the DTU dataset [18] and the LLFF dataset [30] to demonstrate the effectiveness of NeuRay.
The results show that 1) without scene-specific optimiza-tion, our method already produces satisfactory rendering results that outperforms other generalization methods by a large margin; 2) finetuning NeuRay produces much su-perior results than finetuning other generalization models and achieve even better rendering quality than NeRF [31].
Moreover, we can speed up rendering with the help of
NeuRay by caching features on input views and predicting coarse surface locations, which costs ∼3 seconds to render an image of size 800 × 600. 2.