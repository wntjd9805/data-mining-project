Abstract
Optimization within a layer of a deep-net has emerged as a new direction for deep-net layer design. However, there are two main challenges when applying these layers to com-puter vision tasks: (a) which optimization problem within a layer is useful?; (b) how to ensure that computation within a layer remains efﬁcient? To study question (a), in this work, we propose total variation (TV) minimization as a layer for computer vision. Motivated by the success of total variation in image processing, we hypothesize that TV as a layer pro-vides useful inductive bias for deep-nets too. We study this hypothesis on ﬁve computer vision tasks: image classiﬁca-tion, weakly supervised object localization, edge-preserving smoothing, edge detection, and image denoising, improving over existing baselines. To achieve these results we had to address question (b): we developed a GPU-based projected-Newton method which is 37× faster than existing solutions. 1.

Introduction
Optimization within a deep-net layer has emerged as a promising direction to designing building blocks of deep-nets [2, 4, 29]. For this, optimization problems are viewed as a differentiable function, mapping its input to its exact solution. The derivative of this mapping can be computed via implicit differentiation. Combined, this provides all the ingredients for a deep-net “layer.”
Designing effective layers for deep-nets is crucial for the success of deep learning. For example, convolution [28, 43], recurrence [35, 63], normalization [37, 77], attention [75] layers and other specialized layers [41, 46, 64, 80] are the fundamental building-blocks of modern computer vision models. Recently, optimization layers, e.g., OptNet [4], have also found applications in reinforcement learning [3], logical reasoning [76], hyperparamter tuning [10, 58], scene-ﬂow estimation [72], and graph-matching [61], providing useful inductive biases for these tasks. Despite these successes, optimization as a layer has not been as widely adopted in computer vision because of two unanswered questions: (a) which optimization problem is useful?; (b) how to efﬁciently solve for the exact solution of the optimization problem if the input is reasonably high-dimensional?
In this work, we propose and study Total Variation (TV) [62] minimization as a layer within a deep-net for computer vision, speciﬁcally, the TV proximity operator.
We are motivated by the fact that TV has had numerous successes in computer vision, incorporating the prior knowl-edge that images are piece-wise constant. Notably, TV has been used as a regularizer in applications such as image denoising [16], super-resolution [48], stylization [40], and blind deconvolution [17] to name a few. Because of these successes, we hypothesize that TV as a layer would be an effective building-block in deep-nets, enforcing piece-wise properties in an end-to-end manner.
However, existing solutions [2, 9] which can support TV as a deep-net layer are limited. For example, CVXPYLay-ers [2] supports back-propagation through disciplined convex programs. However, CVXPYLayers uses a generic solver and lacks GPU support. While specialized solvers [9, 38] for
TV minimization exists, they also lack GPU and batching support. Hence, to meaningfully study TV as a layer at the scale of a computer vision task, we need a fast GPU imple-mentation. To achieve this goal, we developed a fast GPU
TV solver with custom CUDA kernels. For the ﬁrst time, this enables use of TV as a layer across computer vision tasks. Our implementation is 1770× faster than a generic solver and 37× faster than a specialized TV solver.
With this fast implementation, we study the hypothesis of TV as a layer on ﬁve tasks, spanning from high-level to low-level computer vision: classiﬁcation, object localization, edge detection, edge-aware smoothing, and image denois-ing. We incorporate TV layers into existing deep-nets, e.g.,
ResNet and VGGNet, and found them to improve results.
Our Contributions:
• We propose total variation as a layer for use as a building block in deep-nets for computer vision tasks.
• We develop a fast GPU-based TV solver. It signiﬁcantly reduces training and inference time, allowing a TV layer to be incorporated into classic deep-nets. The implemen-tation is publicly available.1 1github.com/raymondyeh07/tv_layers_for_cv
• We demonstrate efﬁcacy and practicality of TV layers by evaluating on ﬁve different computer vision tasks. 2.