Abstract
This paper studies the BERT pretraining of video trans-It is a straightforward but worth-studying ex-formers. tension given the recent success from BERT pretraining of image transformers. We introduce BEVT which decou-ples video representation learning into spatial represen-In par-tation learning and temporal dynamics learning. ticular, BEVT ﬁrst performs masked image modeling on image data, and then conducts masked image modeling jointly with masked video modeling on video data. This design is motivated by two observations: 1) transformers learned on image datasets provide decent spatial priors that can ease the learning of video transformers, which are often times computationally-intensive if trained from scratch; 2) discriminative clues, i.e., spatial and tempo-ral information, needed to make correct predictions vary among different videos due to large intra-class and inter-class variations. We conduct extensive experiments on three challenging video benchmarks where BEVT achieves very promising results. On Kinetics 400, for which recogni-tion mostly relies on discriminative spatial representations,
BEVT achieves comparable results to strong supervised baselines. On Something-Something-V2 and Diving 48, which contain videos relying on temporal dynamics, BEVT outperforms by clear margins all alternative baselines and achieves state-of-the-art performance with a 71.4% and 87.2% Top-1 accuracy respectively. Code is available at https://github.com/xyzforever/BEVT. 1.

Introduction
Transformers [54, 59] have become the dominant net-work structures in the natural language processing (NLP)
ﬁeld and have demonstrated tremendous success in differ-ent NLP tasks. Recently, the pioneering work ViT [19] tokenizes one image into a series of patch-based tokens
∗Corresponding authors.
Figure 1. A conceptual overview of BEVT. With a decoupled de-sign, BEVT ﬁrst conducts masked image modeling on image data, and then conducts jointly masked image modeling and masked video modeling on image&video data by weight sharing. and applies the transformer architecture for image recogni-tion. Many approaches [12, 17, 36, 60] further demonstrate the power of transformers as generic vision backbones and achieve impressive performance on various vision tasks.
Beyond image tasks, there are also a few studies showing the promise of transformers for video understanding [2,37].
The key to the success of Transformers in NLP is BERT pretraining [4, 14, 35], one of the most successful pretrain-ing tasks, which predicts masked tokens in corrupted texts.
This motivates a few recent studies to explore the BERT-style pretraining for image representation learning by re-covering raw pixels [29] or latent codes [3, 18] of masked image patches. However, how to leverage such a strategy for video understanding has never been explored.
In this paper, we study BERT pretraining of video trans-formers. Unlike static images, videos depict how objects move and interact over time. Such dynamic nature brings additional difﬁculty for representation learning. It is often found that learning representations from scratch on videos is computationally expensive and requires extremely large-scale datasets with millions of samples [24], if not hun-dreds of millions of samples [1]. Instead of training from scratch, a few methods demonstrate that self-supervised models pretrained on image datasets beneﬁt video recog-nition under both supervised [2, 37] and unsupervised set-tings [5]. These approaches simply leverage pretrained models as better initializations to learn spatial-temporal fea-tures in videos. While widely used and sometimes effective, the spatial context relationships learned from the image pre-rtaining phase are likely to be drastically modiﬁed during video feature learning.
We argue that spatial priors encoded in pretrained self-supervised models should be explicitly preserved when per-forming video representation learning. The intuition behind is that there are large inter-class variations among different videos and their dependencies on what discriminative in-formation to use (i.e., spatial and temporal clues) to make correct predictions differ. For instance, for actions like “ap-plying lipstick”, spatial knowledge is generally sufﬁcient, as evidenced by the fact simply using 2D features offers de-cent results on datasets like Kinetics [9]. On the other hand, temporal dynamics are crucial for differentiating actions be-tween two ﬁne-grained diving sequences [33]. This high-lights the importance of considering the differences among video samples during feature learning.
In light of this, we introduce BEVT, which decou-ples video representation learning into spatial representa-tion learning and temporal dynamics learning. More specif-ically, BEVT builds upon the Video Swin Transformer [37] due to their computationally efﬁcient architectures ∗, and is trained with a BERT-style objective to fully unleash the power of transformers for representation learning. BEVT contains an image stream for spatial modeling and a video stream for temporal modeling, interacting with each other for video modeling. In particular, the image stream, oper-ating on RGB images, learns spatial priors ﬁrst in an un-supervised fashion by predicting masked image patches in the form of latent codes derived from a pretrained VQ-VAE [3]. It is then used to initialize the attention weight ma-trices of the video stream, whose inputs are sampled video clips, so as to save computation for video transformers. The video stream, on the other hand, learns temporal dynamics through predicting masked 3D tubes represented by latent codes. The two streams, taking image and video pairs as in-puts, are then jointly trained on video data through a weight sharing strategy. Such a design not only maintains spatial knowledge learned from image datasets to ensure decent results for static video samples but also learns temporal in-formation to guarantee correct predictions for samples that contain dynamic movements. Finally, BEVT is ﬁnetuned on targeted datasets for downstream evaluation.
We conduct extensive experiments on three challenging
∗Note that we only use the architecture and do not load the pretrained weights. video datasets, i.e., Kinetics-400 (K400) [9], Something-Something-v2 (SSV2)
[26], and Diving-48 (DIVING-48) [33]. On K400, BEVT offers 81.1% Top-1 accu-racy, which is better than the strong supervised baseline 80.6% [37]. On SSV2 and DIVING48, BEVT achieves 71.4% and 87.2% Top-1 accuracy outperforming state-of-the-art methods [2, 5, 23, 37] by clear margins. To fur-ther analyze the performance difference among these three datasets, we further provide the temporal dependency anal-ysis and demonstrate that videos in K400 mainly rely on spatial clues for correct predictions while videos from
SSV2 and DIVING48 require more temporal information.
Our main contributions are summarized as follows: (1) We explore the BERT-style training objective to fully unleash the power of transformers to learn discriminative video representations; (2) We introduce a novel two-stream network that decouples spatial representation learning and temporal dynamics learning; (3) We demonstrate different video samples have different preferences towards spatial and temporal clues; (4) We conduct extensive experiments on three challenging video benchmarks and achieve compa-rable or better results with state-of-the-art methods. 2.