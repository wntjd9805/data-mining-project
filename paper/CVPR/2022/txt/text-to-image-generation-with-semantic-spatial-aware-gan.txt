Abstract
Input Text
GT
Generated Image
Text-to-image synthesis (T2I) aims to generate photo-realistic images which are semantically consistent with the text descriptions. Existing methods are usually built upon conditional generative adversarial networks (GANs) and initialize an image from noise with sentence embedding, and then refine the features with fine-grained word embed-ding iteratively. A close inspection of their generated im-ages reveals a major limitation: even though the gener-ated image holistically matches the description, individual image regions or parts of somethings are often not rec-ognizable or consistent with words in the sentence, e.g.
“a white crown”. To address this problem, we propose a novel framework Semantic-Spatial Aware GAN for synthe-sizing images from input text. Concretely, we introduce a simple and effective Semantic-Spatial Aware block, which (1) learns semantic-adaptive transformation conditioned on text to effectively fuse text features and image features, and (2) learns a semantic mask in a weakly-supervised way that depends on the current text-image fusion process in order to guide the transformation spatially. Experiments on the challenging COCO and CUB bird datasets demon-strate the advantage of our method over the recent state-of-the-art approaches, regarding both visual fidelity and alignment with input text description. Code available at https://github.com/wtliao/text2image. 1.

Introduction
The great advances made in Generative Adversarial Net-works (GANs) [7, 20, 22, 38, 11, 35, 2, 13] boost a remark-able evolution in synthesizing photo-realistic images with diverse conditions, such as layout [19, 8], text [34, 30] and scene graph [12, 1, 5]. Particularly, generating images con-ditioned on text descriptions (see Fig. 1) has been catch-ing increasing attention in computer vision and natural lan-guage processing communities because: (1) it bridges the gap between these two domains, and (2) linguistic descrip-*Equal contribution
†Kai Hu contributed to this work when he was doing his master thesis supervised by Wentong Liao at TNT.
This is a gray bird with black wings and white wingbars light yellow sides and yellow eyebrows.
A horse in a grassy field set against a foggy mountain range.
Figure 1: Examples of images generated by our method (3rd column) conditioned on the given text descriptions. tion (text) is the most natural and convenient medium for human being to describe a visual scene. Nonetheless, T2I remains a challenging task because of the cross-modal prob-lem (text to image transformation) and the ability to keep the generated image holistically as well as locally semanti-cally consistent with the given text.
The most recent T2I methods are usually multi-stage refinement frameworks which generates an initial image from noise with sentence embedding and refines the details with fine-grained word embedding in each following stage
[34, 35, 10, 30, 16, 31]. In each stage, there is a pair of gen-erator and discriminator to synthesize higher-resolution im-age and decide whether the generated image is real enough, respectively. This method has proved effective in synthesiz-ing high-resolution images. However, multiple generator-discriminator pairs lead to higher computation and more unstable training processes. Moreover, the quality of the image generated by the earlier generator decides the final output. If the early generated image is poor, the later gener-ators can not improve its quality. To address this problem, the one-stage generator is introduced in [28] which has one generator-discriminator pair. In this work, we also follow this one-stage structure.
On the other hand, the generated image should be holis-1
tically consistent with the description and locally consistent with words in the sentence. For this purpose, the multi-stage refinement framework is used to fuse text and image information in each stage of the generation process to en-courage the generated image to be semantically consistent with the corresponding text. AttGAN [30] plays a role in this task. It uses sentence embedding to initialize an image from noise, and judge whether the generated image matches the corresponding text in each stage. This helps the gener-ated images holistically consistent with the description. In parallel, the attention mechanisms are used to select the im-portant words in the text to complement the details in the sub-regions of images in each refinement stage. In this way, the generated image is encouraged to match the words in text semantically. Most of the recent T2I methods follow this framework [16, 21, 4, 24, 33]. Despite the remarkable performance that has been made with these methods, there still exists an important but unsolved limitation: local se-mantic are not well explored during the synthesis process due to the limited and abstractive textual information. Usu-ally, a text description only describes part of a scene or an object (e.g. “a white crown”), and lacks explicit spatial in-formation. To address this problem, previous methods nor-mally utilize cross-modal attention mechanisms to attend word-level features to the image sub-regions [30, 16, 33].
However, the computation cost increases rapidly with larger image size. Moreover, the natural language description is in high-level semantics, while a sub-region of the image is relatively low-level [3, 32]. Last but also important, im-age sub-regions are still to coarse for complementing the details of somethings. Therefore, the high-level textual se-mantics cannot be explored well to control the image gen-eration process, especially for complex image with multiple objects, such as in the COCO [18] dataset. Some methods
[10, 17, 15] propose object-driven T2I approaches, which first predict object bounding box from text description, and then infer the corresponding segmentation masks. Finally, images are generated from the segmentation masks using
PixelGAN [11]. However, such approaches convert T2I task to segmentation to image generation in practice, and the lo-cal features of objects are lost completely.
To address the aforementioned issues, we propose a novel T2I framework dubbed as Semantic-Spatial Aware
Generative Adversarial Network (SSA-GAN) (see Fig. 2).
First, it has only one generator-discriminator pair and is trained in end-to-end fashion so that it can be trained more efficiently and stably compared to the multi-stage refine-ment framework. Second, only sentence embedding is used to control the image generation process. Compared to the previous methods that also use world-level features, our method requires lower computation. Last but important, our method complements the local details in pixel level rather than in sub-region level. Thus, the generated images are better consistent with the words in text semantically and lo-cally. To realize the pixelwise control of image synthesis, we propose a novel Semantic-Spatial Aware (SSA) block (Fig. 3). On one hand, SSA block learns semantic-aware channel-wise affine parameters conditioned on the learned text feature vector (sentence embedding). On the other hand, a semantic mask is predicted depending on the current text-image fusion process (i.e. output of last SSA block).
The semantic mask indicates where the generated images still need to be enhanced with the textual information in pixel level. This is how the name Semantic-Spatial Aware from. It is worth noting that the mask predictor is trained with weak supervision so that no additional mask annota-tion is required. Comprehensive experiments are conducted on the challenging benchmarks COCO [18] and CUB bird dataset [29] to validate the performance of SSA-GAN for
T2I. The quantitative as well as qualitative experimental results show our superior performance over the previous methods. In summary, the main contributions of this paper are as follows:
• We propose a novel one-stage framework SSA-GAN for image synthesis from text. Compared to the pop-ular multi-stage framework, one-stage framework re-quires less computation and can be trained more effi-ciently and stably.
• Our method only uses sentence embedding during the synthesis process. Compared to the methods which use world-level futures, our method is simple and has lower computation cost.
• A novel SSA block is introduced to fuse the text and image features effectively and deeply by predicting se-mantic mask to guide the learned text-adaptive affine transformation in pixel level.
• The semantic mask predictor is trained in a weakly-supervised way, such that no additional annotation is required and this block is potential to be applied on other T2I datasets. 2.