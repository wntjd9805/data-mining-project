Abstract
Modern smartphones can continuously stream multi-megapixel RGB images at 60 Hz, synchronized with high-quality 3D pose information and low-resolution LiDAR-driven depth estimates. During a snapshot photograph, the natural unsteadiness of the photographer’s hands offers millimeter-scale variation in camera pose, which we can capture along with RGB and depth in a circular buffer. In this work we explore how, from a bundle of these measure-ments acquired during viewfinding, we can combine dense micro-baseline parallax cues with kilopixel LiDAR depth to distill a high-fidelity depth map. We take a test-time op-timization approach and train a coordinate MLP to out-put photometrically and geometrically consistent depth es-timates at the continuous coordinates along the path traced by the photographer’s natural hand shake. With no addi-tional hardware, artificial hand motion, or user interaction beyond the press of a button, our proposed method brings high-resolution depth estimates to point-and-shoot “table-top” photography – textured objects at close range. 1.

Introduction
The cell-phone of the 90s was a phone, the modern cell-phone is a handheld computational imaging platform [9] that is capable of acquiring high-quality images, pose, and depth. Recent years have witnessed explosive advances in passive depth imaging, from single-image methods that leverage large data priors to predict structure directly from image features [40, 41] to efficient multi-view approaches grounded in principles of 3D geometry and epipolar projec-tion [50, 47]. At the same time, progress has been made in the miniaturization and cost-reduction [3] of active depth systems such as LiDAR and correlation time-of-flight sen-sors [29]. This has culminated in their leap from industrial and automotive applications [45, 11] to the space of mobile phones. Nestled in the intersection of high-resolution imag-ing and miniaturized LiDAR we find modern smartphones,
† Developed data acquisition pipeline during Adobe internship.
Figure 1. We reconstruct centimeter-scale depth features for this tabletop object from nothing more than a handheld snapshot. such as the iPhone 12 Pro, which offer access to high frame-rate, low-resolution depth and high-quality pose estimates.
As applications of mixed reality grow, particularly in in-dustry [30] and healthcare [17] settings, so does the de-mand for convenient systems to extract 3D information from the world around us. Smartphones fit this niche well, as they boast a wide array of sensors – e.g. cameras, mag-netometer, accelerometer, and the aforementioned LiDAR system – while remaining portable and affordable, and con-sequently ubiquitous.
Image, pose, and depth data from mobile phones can drive novel problems in view synthe-sis [35, 39], portrait relighting [38, 49], and video interpo-lation [2] that either implicitly or explicitly rely on depth cues, as well as more typical 3D understanding tasks con-cerning salient object detection [59, 13], segmentation [46], localization [61], and mapping [44, 37].
Although 3D scene information is essential for a wide array of 3D vision applications, today’s mobile phones do not offer accurate high-resolution depth from a single snapshot. While RGB image data is available at more than 100 megapixels (e.g. Samsung ISOCELL HP1), the most successful depth sensors capture at least three orders of magnitude fewer measurements, with pulsed time-of-flight sensors [36] and modulated correlation time-of-flight imagers [28, 20, 27] offering kilopixel resolutions. Pas-sive approaches can offer higher spatial resolution by ex-ploiting RGB data; however, existing methods relying on stereo [7, 4, 25] depth estimation require large baselines, monocular depth methods [6, 40] suffer from scale ambigu-ity, and structure-from-motion methods [43] require diverse 1
poses that are not present in a single snapshot. Accurate high-resolution snapshot depth remains an open challenge.
For imaging tasks, align and merge computational pho-tography approaches have long exploited subtle motion cues during a single snapshot capture. These take ad-vantage of the photographer’s natural hand tremor dur-ing viewfinding to capture a sequence of slightly mis-aligned images, which are fused into one super-resolved im-age [55, 51]. These misaligned frames can also be seen as mm-baseline stereo pairs, and works such as [57, 24] find that they contain enough parallax information to produce coarse depth estimates. Unfortunately, this micro-baseline depth is not enough to fuel mixed reality applications alone, as it lacks the ability to segment clear object borders or de-tect cm-scale depth features. In tandem with high-quality poses from phone-based SLAM [12] and low-resolution Li-DAR depth maps, however, we can use the high-resolution micro-baseline depth cues to guide the reconstruction of a refined high-resolution depth map. We develop a pipeline for recording LiDAR depth, image, and pose bundles at 60 Hz, with which we can conveniently record 120 frame bundles of measurements during a single snapshot event.
With this hand shake data in hand, we take a test-time op-timization approach to distill a high-fidelity depth estimate from hand tremor measurements. Specifically, we learn an implicit neural representation of the scene from a bun-dle of measurements. Depth represented by a coordinate multilayer perceptron (MLP) allows us to query for depth at floating point coordinates, which matches our measure-ment model, as we effectively traverse a continuous path of camera coordinates during the movement of the pho-tographer’s hand. We can, during training, likewise con-veniently incorporate parallax and LiDAR information as photometric and geometric loss terms, respectively. In this way we search for an accurate depth solution that is con-sistent with low-resolution LiDAR data, aggregates depth measurements across frames, and matches visual features between camera poses similar to a multi-view stereo ap-proach. Specifically, we make the following contributions
• A smartphone app with a point-and-shoot user inter-face for easily capturing synchronized RGB, LiDAR depth, and pose bundles in the field.
• An implicit depth estimation approach that aggregates this data bundle into a single high-fidelity depth map.
• Quantitative and qualitative evaluations showing that our depth estimation method outperforms existing sin-gle and multi-frame techniques. training code, experimental data,
The smartphone app, and trained models are available at github.com/princeton-computational-imaging/HNDR . 2.