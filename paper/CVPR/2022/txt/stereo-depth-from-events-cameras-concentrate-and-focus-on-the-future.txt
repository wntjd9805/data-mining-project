Abstract
Neuromorphic cameras or event cameras mimic human vision by reporting changes in the intensity in a scene, in-stead of reporting the whole scene at once in a form of an image frame as performed by conventional cameras. Events are streamed data that are often dense when either the scene changes or the camera moves rapidly. The rapid movement causes the events to be overridden or missed when creating a tensor for the machine to learn on. To alleviate the event missing or overriding issue, we propose to learn to concen-trate on the dense events to produce a compact event rep-resentation with high details for depth estimation. Speciﬁ-cally, we learn a model with events from both past and fu-ture but infer only with past data with the predicted future.
We initially estimate depth in an event-only setting but also propose to further incorporate images and events by a hier-archical event and intensity combination network for better depth estimation. By experiments in challenging real-world scenarios, we validate that our method outperforms prior arts even with low computational cost. Code is available at: https://github.com/yonseivnl/se-cff. 1.

Introduction
A common practice to tackle design challenges is learn-ing from nature. Mimicking natural strategies by copying their form, shape, process, or even ecosystem for speciﬁc applications is called biomimicry [2]. Stereo depth estima-tion mimics the human visual ability to understand depth from a pair of cameras. The computer vision community has shown signiﬁcant interest in stereo vision, while it has remained a challenging task. The ill-posed nature of stereo depth estimation, shortcomings from RGB sensors (e.g., low dynamic range, motion blur and etc.), and algorith-mic limitations make stereo vision very challenging. Ex-amples of imperfect sensing the scene include low dynamic range, blurry, or noisy images. Special cases that the algo-rithms cannot handle include repeating patterns, reﬂective
*: equal contribution. †: corresponding author. This work is done while
YN and JC are an intern, AI tech advisor at NAVER AI Lab., respectively.
Concentrate Net
Dense Depth multi-density  stacks from  stereo events attention-based  concentration  network concentrated  left and right  event pairs knowledge  distillation from  future events intensity image for reference missing details with few events overriding details with many events concentrated events with high details
Figure 1. Overview of our stereo depth estimation from events.
We predict dense depth with sharp edges by ‘concentrating’ the event representation tensors to preserve details. We further transfer the knowledge of future events by training with past and future events while keeping our system causal at inference. (i.e., shiny) objects, and low-texture areas [23, 27].
Event cameras which are also referred to as neuromor-phic cameras follow the same concept of biomimicry as they mimic human vision. Similar to the human eye, an event camera captures only pixel-wise intensity differences and reports them as a stream instead of the whole scene at once as a frame. Although event cameras bring new and unique speciﬁcations, they require a paradigm shift in the algorithms that use these devices. This paper falls within both mentioned trends coined as neuromorphic stereo vi-sion and mimics the human eye to estimate depth from a stereo pair of event cameras. This line of research has gath-ered interest within the event camera community and has advanced in many aspects [23] including novel algorithms and attempts to generalize to real-world situations.
The stream of events is sparse in nature and does not fol-low any predeﬁned pattern in terms of the density of events either in time or space, and relies purely on the scene and camera movements. We use this unpredictable sparse-dense stream and divide it into a sequence of stacked events that
each stack holds the most recent event location and their time information. The sequence is made with multiple num-bers of events per stack, and we call it the multi-density se-ries of stacks. As depicted in Fig 1, we feed this collection of event stacks with event slices that end at the GT times-tamp to an event concentration network. As the name sug-gests, it concentrates all events into a clear edge-like tensor without any blur-like artifacts or omitting any details.
Event concentration helps create further details and sharp edges. It only depends on previous information, the events, thus it is a causal system. However, some details may also be omitted from the scene as we only use previ-ously ﬁred events. As a remedy, we further take into ac-count the future events, but not directly as inputs. Speciﬁ-cally, we teach our network to distill from future events by feeding the past and future events at training time which may help the network to understand the scene contents and produce better predictions. Intuitively, we implicitly teach the network to contemplate or distillate from the knowledge of previously observed future events. Our experiments sup-port this intuition by showing a signiﬁcant increase in per-formance when training using the past and future events in comparison to training only with the past events.
Furthermore, unlike image frames that the conventional camera takes, events are sparse, which may lead to larger depth estimation errors in comparison to the depth from im-age frames. We supplement the sparse information by the combination of concentrating events and using the trans-ferred knowledge from the future events. We evaluate our method on the challenging outdoor stereo events from the public benchmark dataset of DSEC [13], and use their met-rics to compare with the state-of-the-art event stereo depth estimation methods. We present qualitative and quantitative comparisons to show how we outperform previous arts. 2. Preliminary: Event Cameras
Unlike traditional cameras, an event camera reports the scene as a stream of sparse and disconnected events, i.e., per-pixel intensity changes larger than a predeﬁned thresh-old. Each event is ﬁred when it happens with very low la-tency, in the order of microseconds. The asynchronous na-ture of events brings the unique capability of being less ad-versely impacted by motion blur under rapid scene changes and camera movements but not completely immune to it
[17]. Event cameras have higher dynamic ranges that re-veals scene details that ordinary cameras may miss. We dis-cuss more details in supplementary material for space sake. 3.