Abstract 1.

Introduction
Our work focuses on addressing sample deficiency from low-density regions of data manifold in common image datasets. We leverage diffusion process based generative models to synthesize novel images from low-density re-gions. We observe that uniform sampling from diffusion models predominantly samples from high-density regions of the data manifold. Therefore, we modify the sampling pro-cess to guide it towards low-density regions while simulta-neously maintaining the fidelity of synthetic data. We rigor-ously demonstrate that our process successfully generates novel high fidelity samples from low-density regions. We further examine generated samples and show that the model does not memorize low-density data and indeed learns to generate novel samples from low-density regions. 1ImageNet [8, 29] has no explicit category for humans, though one might be present in some images. Thus generative models might generate synthetic images that include a human. We further conduct a rigorous anal-ysis to validate whether the network has memorized any such information from training samples.
Most common image datasets have a long-tailed distribu-tion of sample density2, where the majority of samples lie in high-density neighborhoods of the data manifold. Samples from low-density regions often comprise novel attributes (Figure 1a) and have higher entropy than high-density sam-ples [1]. However, due to their lower likelihood, curating even a small amount of such samples requires a dedicated effort [16].
Our goal is to leverage generative models to generate synthetic images from low-density neighborhoods. A nat-ural requirement for this task is that the model should gen-eralize to low-density regions. While generative adversarial networks (GANs) excel at generating high-fidelity samples, they have poor coverage, thus struggle to generate high-fidelity samples from low-density regions [4] (Figure 1b).
In contrast, autoregressive models have a high coverage but 2We refer to the long-tail w.r.t. sample density for each class.
It is different from the long-tailed distribution over classes [24], i.e., when some classes are heavily underrepresented than others.
fail to generate high fidelity images [7]. We use diffusion-based models due to their ability to achieve high fidelity and high coverage of the distribution, simultaneously [17, 26].
In training diffusion models, the goal is to approximate data distribution, which is often long-tailed. Diffusion mod-els excel at this task, as we observe that the density distri-bution of uniformly sampled instances from the diffusion model is very similar to real data.
Thus uniform sampling from these models leads to an imitation of real data density distribution, i.e., a long-tailed density distribution, where it generates samples from high-density regions with a much higher probability than from low-density regions (figure 1c). To alleviate this issue, we first modify the sampling process to include an additional guidance signal to steer it towards low-density neighbor-hoods. However, at higher magnitudes of this signal, the generative process is steered off the manifold, thus generat-ing low fidelity samples. We circumvent this challenge by including a second guidance signal which incentives diffu-sion models to generate samples that are close to the real data manifold.
Since a very limited number of training samples are available from low-density regions, it is natural to ask whether diffusion models are generalizing in the low-density regions or simply memorizing the training data. Af-ter all, recent works have uncovered such memorization in language-based generative models [5, 6]. We conduct an extensive analysis to justify that diffusion models do not show signs of memorizing training samples from low-density neighborhoods and indeed learn to interpolate in these regions. We make the following key contributions.
• We propose an improved sampling process for dif-fusion models that can generate samples from low-density neighborhoods of the training data manifold.
• We validate the success of our approach using three different metrics for neighborhood density and provide extensive comparisons with the baseline sampling pro-cess in diffusion models.
• We show that our sampling process successfully gen-erates novel samples, which aren’t simply memorized training samples, from low-density regions. This ob-servation from our sampling process also uncover that despite a limited number of training images available from low-density regions, diffusion models success-fully generalize in low-density regions. vances [10], diffusion models achieve state-of-the-art per-formance, outperforming other classes of generative mod-els, such as Generative adversarial networks (GANs), VQ-VAE [28], and Autoregressive models [7] on various met-rics in image fidelity and diversity [10, 26]. Some of the key factors behind their success are the innovation on the architecture of the diffusion models [10, 17], simplified for-mulation for the training objective [17], and use of cascaded diffusion processes [10, 18, 26].
Sampling from diffusion models is quite slow since it re-quires an iterative denoising operation. Reducing this over-head by developing fast sampling techniques is a topic of tremendous research interest [19, 21, 37, 42]. Orthogonal to this direction, our interest is in sampling data from low-density neighborhoods. We further show that our sampling approach can be easily integrated with fast sampling tech-niques.
To measure neighborhood density around a sample, we use the Gaussian model of training data in the embedding space of a pre-trained classifier. Modeling images in em-bedding space is a common approach, particularly due to their alignment with human perception [45], in numerous vision applications, such as outlier detection [32] and in-stance selection [9].
Across generative models, given a distribution learned by the model, there have been previous attempts in sampling from a targeted data distribution. Discriminator rejection sampling (DRS) and its successors [2, 11] consider rejec-tion sampling using the discriminator in a generative adver-sarial network (GAN). Similarly, Razavi et al. [28] exploits a pre-trained classifier to reject samples that are classified with low confidence. Most often the goal is to filter out low fidelity samples, thus improving the quality of synthetic data. In contrast, our goal is to generate high fidelity sam-ples from low-density regions of the data manifold. These samples are rarely generated by the model under uniform sampling, thus sampling them using a naive classifier-based rejection sampling approach leads to high-cost overheads.
We instead opt to modify the generative process of diffu-sion models to guide it towards low-density neighborhoods of the data manifold.
The most closely related work to ours is from Li et al. [23], which smoothes class embeddings of a BigGAN model to generate diverse images.
In contrast, we focus on diffusion-based generative models. We also demonstrate the limitation of their approach with diffusion models in
Appendix A.6. 2.