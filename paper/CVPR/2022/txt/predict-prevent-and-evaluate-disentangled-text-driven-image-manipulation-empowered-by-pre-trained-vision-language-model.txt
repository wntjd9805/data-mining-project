Abstract available at https://github.com/zipengxuc/PPE.
To achieve disentangled image manipulation, previous works depend heavily on manual annotation. Meanwhile, the available manipulations are limited to a pre-deﬁned set the models were trained for. We propose a novel framework, i.e., Predict, Prevent, and Evaluate (PPE), for disentangled text-driven image manipulation that requires little manual annotation while being applicable to a wide variety of ma-nipulations. Our method approaches the targets by deeply exploiting the power of the large-scale pre-trained vision-language model CLIP [32]. Concretely, we ﬁrstly Predict the possibly entangled attributes for a given text command.
Then, based on the predicted attributes, we introduce an en-tanglement loss to Prevent entanglements during training.
Finally, we propose a new evaluation metric to Evaluate the disentangled image manipulation. We verify the effec-tiveness of our method on the challenging face editing task.
Extensive experiments show that the proposed PPE frame-work achieves much better quantitative and qualitative re-sults than the up-to-date StyleCLIP [31] baseline. Code is
*The work was done during Zipeng Xu’s internship at VIS, Baidu. 1.

Introduction
Disentangled image manipulation [1, 8, 10, 12, 21, 23, 37, 38, 43, 44] aiming at changing the desired attributes of the image while keeping the others unchanged, has long been studied for its research signiﬁcance and application value.
Reaching this target is not easy, especially when attributes naturally entangle in the real world. Therefore, concrete attribute annotations are of vital importance, making disen-tangled image manipulation a labor-consuming task.
Several works [8, 10, 21, 23] use an encoder-decoder architecture and need manual annotations on multiple at-tributes of images. The models encode the original image and the manipulating attribute, then decode the manipu-lated image. Speciﬁcally, they use an attribute-speciﬁc loss to encourage the manipulation of a speciﬁc attribute while discouraging the others. The loss comes from pre-trained classiﬁers for all annotated attributes. Many recent works focus on latent space image manipulation since large-scale pre-trained GANs, e.g., StyleGANs [15, 16], can generate
high-quality images from well-disentangled latent spaces.
Despite the convenience of directly using the pre-trained
GANs to generate images, all these methods need human annotations [1, 12, 37, 38, 43, 44]. Moreover, the available manipulating attributes are limited to the annotated set.
Recently, the rise of the large-scale pre-trained vision-language model CLIP [32] has brought a new insight. Since
CLIP provides effective signals about the semantic similar-ity of image and text, various manipulations [11,31,36] can be performed with a text command and a CLIP-based loss, instead of exhaustive human annotations. Nevertheless, achieving disentangled image manipulation is still tricky.
For instance, StyleCLIP [31] introduces three methods: la-tent optimization and latent mapper take no consideration of achieving disentangled results; global direction, which is based on the more disentangled latent space [45], needs human trials-and-errors to ﬁnd appropriate parameters in each case to reach the expected effects. To only manipu-late a desired attribute, TediGAN [46, 47] merely revise the latent vectors of layers corresponding to that attribute. Yet, they have to ﬁgure out in advance the relations between at-tributes and layers in StyleGAN.
S
In this paper, we explore achieving disentangled image manipulation with as less human labor as possible. We pro-pose a novel framework, i.e., Predict, Prevent, and Evaluate (PPE), to approach the target by leveraging the power of
CLIP in depth. Firstly, we propose to Predict the possibly entangled attributes for given text commands. We assume that the entanglements result from the distributions of at-tributes in the real world. Therefore, we draw support from
CLIP to ﬁnd the attributes that appear most frequently in the command-related images, then regard the attributes of high co-occurrence frequency as the possibly entangled at-tributes. Secondly, we introduce a novel entanglement loss to Prevent entanglements during training. The loss pun-ishes the changes of the possibly entangled attributes before and after the manipulation, so as to enforce the model to ﬁnd a less disentangled manipulating direction. Lastly, based on the predicted entangled attributes, we introduce a new eval-uation metric to simultaneously Evaluate the manipulation effect and the entanglement condition. The manipulation ef-fect is measured based on the change of command-attribute while the entanglement condition is based on the change of the entangled attributes, before and after manipulation. All the changes are estimated according to the CLIP distance between the texts of attributes and the images.
To evaluate, we implement our method based on the sim-ple and versatile latent mapper from StyleCLIP and conduct experiments on the challenging face editing task, using the large-scale human face dataset CelebA-HQ [14, 25]. Quali-tative and quantitative results indicate that we achieve supe-rior disentangled performance compared to the StyleCLIP baseline. Meanwhile, we show that our results present a better linear consistency.
To conclude, our main contributions are as follows:
• We propose to predict entangled attributes for disentan-gled image manipulation.
• We propose a novel entanglement loss to prevent entan-gled manipulations during training.
• We propose a new evaluation metric that jointly measures the manipulation effect and the entanglement condition for disentangled image manipulation.
• By applying our method to the versatile StyleCLIP base-line, we manage to achieve disentangled image manipu-lation with very little manual labor. We conduct extensive experiments on the CelebA-HQ dataset and ﬁnd that our qualitative and quantitative results are rather impressive. 2.