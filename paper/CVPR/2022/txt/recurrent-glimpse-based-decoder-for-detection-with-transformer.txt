Abstract
Although detection with Transformer (DETR) is increas-ingly popular, its global attention modeling requires an extremely long training period to optimize and achieve promising detection performance. Alternative to existing studies that mainly develop advanced feature or embed-ding designs to tackle the training issue, we point out that the Region-of-Interest (RoI) based detection refinement can easily help mitigate the difficulty of training for DETR methods. Based on this, we introduce a novel REcurrent
Glimpse-based decOder (REGO) in this paper. In partic-ular, the REGO employs a multi-stage recurrent process-ing structure to help the attention of DETR gradually fo-cus on foreground objects more accurately.
In each pro-cessing stage, visual features are extracted as glimpse fea-tures from RoIs with enlarged bounding box areas of detec-tion results from the previous stage. Then, a glimpse-based decoder is introduced to provide refined detection results based on both the glimpse features and the attention mod-eling outputs of the previous stage. In practice, REGO can be easily embedded in representative DETR variants while maintaining their fully end-to-end training and inference pipelines.
In particular, REGO helps Deformable DETR achieve 44.8 AP on the MSCOCO dataset with only 36 training epochs, compared with the first DETR and the De-formable DETR that require 500 and 50 epochs to achieve comparable performance, respectively. Experiments also show that REGO consistently boosts the performance of different DETR detectors by up to 7% relative gain at the same setting of 50 training epochs. Code is available via https://github.com/zhechen/Deformable-DETR-REGO. 1.

Introduction
Object detection aims to locate and recognize foreground objects from images.
In recent years, deep learning has made rapid development in object detection. With deep convolutional neural networks [18, 19, 27, 33, 46], vari-ous powerful detectors have been developed [4, 22, 32, 37].
Figure 1. Concept of the proposed recurrent glimpse-based de-coder (REGO) for augmenting the training of attention modeling in Detection with Transformer (DETR). Using original DETR re-sults, the REGO performs a multi-stage Region-of-Interest (RoI) based attention modeling refinement procedure by gradually fo-cusing on more accurate areas. In each stage, glimpse features are extracted and a glimpse-based decoder is employed to provide re-fined detection outputs based on both the glimpse features and the attention modeling output of the previous stage. The REGO main-tains the fully end-to-end pipeline of different DETR methods and can improve their training performance promisingly.
In general, modern detectors produce redundant results and require Non-Maximum Suppression (NMS) to reduce the redundancy in detection. Different from this popular paradigm, Detection with Transformer (DETR) [3] applied
Transformer [39] for detection and is the first fully end-to-end detector that avoids the need for NMS. In particular, a
Transformer is a powerful attention-based encoder-decoder pipeline for translating an input sequence to the target se-quence. By formulating the detection task as a direct set prediction problem, the authors of DETR managed to trans-late visual features into a set of detection results based on the global attention modeling of a Transformer. Despite benefits, the DETR suffers from a difficult training problem.
Using MS COCO dataset [24], the original DETR requires 500 training epochs to obtain promising performance, while the other popular detectors like FPN [22] only require less than 36 epochs to get similar results. Even using a machine with 8 powerful V100 GPUs, a DETR detector costs more than 10 days to finish the training [3].
By addressing the training problem, researchers found
that the lack of effective locality modeling could affect the training of attention modeling in DETR methods. For ex-ample, Zhu et al. [48] analyzed that the Transformer would distribute almost uniform attentional weights to all features initially. It is then necessary to apply long training epochs to make the Transformer learn to focus on sparse and mean-ingful local areas. To tackle this issue, researchers de-veloped advanced multi-scale feature encoding [9, 12] and object embedding designs [28, 41] to improve the locality modeling in Transformer before final detection, so that the attention of Transformer can be trained more efficiently and the detection results can be improved properly.
Different from existing methods, we propose that the training of the attention modeling in DETR can be easily improved based on Region-of-Interest (RoI). More specif-ically, considering local areas around bounding boxes de-tected by DETR as RoIs that may contain objects, we can directly restrict the attention of DETR by only focusing on these RoIs. Therefore, modeling the features within
RoIs can help introduce more locality inductive biases in
DETR and thus improve its training efficiency effectively.
In fact, researchers have demonstrated that gradual refine-ments according to RoIs can boost training and detection performance for two-stage [15, 32] and multi-stage detec-tors [2, 32]. Nevertheless, these multi-stage detection meth-ods mainly follow RCNN detection methodology [16] for training and inference which still requires NMS. To our best knowledge, the RoI-based refinement for attention model-ing in DETR has rarely been studied.
To develop a proper RoI-based DETR refinement method, we take inspiration from the glimpse mechanism as studied in the work [29] which extracted features from a few selected local areas of different scales as glimpses and applied a recurrent network to encode the glimpse in-formation. Similar to DETR, this glimpse method also for-mulates the visual understanding as a sequence translation task and has proven to be effective for image recognition.
We follow this mechanism and propose a novel recurrent glimpse-based decoder (REGO) module to help existing
DETR methods relieve training difficulty and improve de-tection performance.
The proposed REGO module refines DETR with multi-stage processing. Taking detection and attention modeling outputs of the original DETR as initial states, each stage of
REGO first extracts glimpse features from local areas sur-rounding the detected bounding boxes. Then, a Transformer decoder is employed to translate the glimpse features based on previous attention modeling outputs into augmented at-tention modeling outputs and refined detection results. For early stages, we extract glimpse features from the local ar-eas at larger scales w.r.t. the detected bounding box areas, enabling the incorporation of rich contexts to boost the de-tection that can possibly be unreliable in early stages. After multiple stages of processing, the REGO performs a coarse-to-fine RoI-based refinement which is shown to be effective for improving the training of different DETR methods.
To sum up, the contributions of this paper are three-fold:
• We proposed a novel RoI-based refinement module that can effectively tackle the difficult training prob-lem for the attention modeling in DETR and improve detection performance.
• The REGO is easy-to-implement and is a comple-mentary module that can be embedded in different
DETR variants. It keeps the fully end-to-end detection pipeline of DETR while accelerating convergence and improving detection performance for different DETR methods effectively.
• Extensive experiments show that the REGO helps de-liver promising performance using only 36 training epochs with a DETR pipeline, which is 13× shorter than the first DETR method. Moreover, REGO also consistently boosts the performance of different DETR methods by up to 7% relative gain using the same 50 training epochs. 2.