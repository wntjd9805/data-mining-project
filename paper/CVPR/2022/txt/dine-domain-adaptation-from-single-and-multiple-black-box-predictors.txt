Abstract
To ease the burden of labeling, unsupervised domain adaptation (UDA) aims to transfer knowledge in previous and related labeled datasets (sources) to a new unlabeled dataset (target). Despite impressive progress, prior meth-ods always need to access the raw source data and develop data-dependent alignment approaches to recognize the tar-get samples in a transductive learning manner, which may raise privacy concerns from source individuals. Several re-cent studies resort to an alternative solution by exploiting the well-trained white-box model from the source domain, yet, it may still leak the raw data via generative adversar-ial learning. This paper studies a practical and interesting setting for UDA, where only black-box source models (i.e., only network predictions are available) are provided dur-ing adaptation in the target domain. To solve this problem, we propose a new two-step knowledge adaptation frame-work called DIstill and fine-tuNE (DINE). Taking into con-sideration the target data structure, DINE first distills the knowledge from the source predictor to a customized target model, then fine-tunes the distilled model to further fit the target domain. Besides, neural networks are not required to be identical across domains in DINE, even allowing ef-fective adaptation on a low-resource device. Empirical re-sults on three UDA scenarios (i.e., single-source, multi-source, and partial-set) confirm that DINE achieves highly competitive performance compared to state-of-the-art data-dependent approaches. Code is available at https:// github.com/tim-learn/DINE/. 1.

Introduction
Deep neural networks achieve remarkable progress with massive labeled data, but it is expensive and not efficient to collect enough labeled data for each new task. To reduce the burden of labeling, considerable attention has been devoted to the transfer learning field [55,89], especially for unsuper-vised domain adaptation (UDA) [3, 10], where one or many related but different labeled datasets are collected as source domain(s) to help recognize unlabeled instances in the new
Figure 1. A challenging but interesting domain adaptation problem setting. One or many source agents only provide their black-box predictors (e.g., via the cloud API service) to the target user with certain unlabeled data, where neither the raw source data nor the details about the source models is accessible during adaptation. dataset (called target domain). Recently, UDA methods have been widely applied in a variety of computer vision problems, e.g., image classification [18, 50, 73], semantic segmentation [25, 72, 87], and object detection [8, 33, 61].
Existing UDA methods always need to access the raw source data and resort to domain adversarial training [18, 73] or maximum mean discrepancy minimization [49,74] to align source features with target features. However, in many situations like personal medical records, the raw source data may be missing or must not be shared due to the privacy pol-icy. To tackle this issue, several recent studies [36, 39, 44] attempt to utilize the trained model instead of the raw data from the source domain as supervision and obtain surpris-ingly good adaptation results. Even so, these methods al-ways require source models to be elegantly trained and pro-vided to the target domain with all the details, which raises two important concerns. First, through generation tech-niques like generative adversarial learning [21], it is still possible to recover the raw source data, leaking the indi-vidual information. Second, the target domain employs the same neural network as the source domain, which is not desirable for low-resource target users. Thus, this paper focuses on a realistic and challenging scenario for UDA, where the source model is trained without bells and whistles
and provided to the target domain as a black-box predictor.
For a better illustration, as shown in Fig. 1, the target user exploits the API service offered by one or many source vendors to get the predictions for each instance and utilizes them for adaptation in the unlabeled target domain. To ad-dress such a challenging UDA problem, we propose a novel adaptation framework called DIstill and fine-tuNE (DINE).
In a nutshell, DINE first distills the knowledge from pre-dictions by source models and then fine-tunes the distilled model with the target data itself, forming a simple two-step approach. Note that, vanilla knowledge distillation [23] re-quires the existence of labeled data and learns the target model (student) by imitating the full outputs of the source model (teacher). Yet, besides the absence of labeled target data, acquiring the full teacher outputs is also impracticable in many situations, e.g., some predictors merely offer sev-eral highest soft-max probability and their associated labels.
To alleviate this issue, we devise an adaptive label smooth-ing technique on source predictions by keeping the largest soft-max value and forcing the rest with the same values.
On top of the point-wise supervision above, we intro-duce two kinds of structural regularizations into distillation for the first time: interpolation consistency training [77]— which encourages the prediction of interpolated samples to be consistent with the interpolated predictions; and mutual information maximization [26, 44]—which helps increase the diversity among the target predictions. Thereafter, we aim to fit the target structure by adjusting parameters in the learned distilled model using the target data alone. For the sake of simplicity, we re-use mutual information maximiza-tion to fine-tune the distilled target model. As for multi-source UDA, we readily extend DINE by aggregating the outputs from multiple source predictors instead. We high-light the main contribution as follows:
• We study a realistic and challenging UDA problem and propose a new adaptation framework (DINE) with only black-box predictors provided from source domains.
• We propose an adaptive label smoothing strategy and a structural distillation method by first introducing struc-tural regularizations into unsupervised distillation.
• Empirical results on various benchmarks validate the superiority of the DINE framework over baselines.
Provided with large source predictors like ViT [13],
DINE even yields state-of-the-art performance for single-source, multi-source, and partial-set UDA.
Compared to existing methods, DINE has several ap-pealing aspects: 1) safe.
It does not access the raw source data nor the source model, avoiding information leakage from source agents; 2) efficient.
It does not as-sume the same architecture across domains, so it can learn a lightweight target model from large source models. More-over, it does not involve adversarial training [39] nor data synthesis [36], making the algorithm converge much faster. 2.