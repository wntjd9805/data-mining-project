Abstract
Egocentric augmented reality devices such as wearable glasses passively capture visual data as a human wearer tours a home environment. We envision a scenario wherein the human communicates with an AI agent powering such a device by asking questions (e.g., “where did you last see my keys?”). In order to succeed at this task, the egocen-tric AI assistant must (1) construct semantically rich and efﬁcient scene memories that encode spatio-temporal infor-mation about objects seen during the tour and (2) possess the ability to understand the question and ground its answer into the semantic memory representation. Towards that end, we introduce (1) a new task — Episodic Memory Question
Answering (EMQA) wherein an egocentric AI assistant is provided with a video sequence (the tour) and a question as an input and is asked to localize its answer to the ques-tion within the tour, (2) a dataset of grounded questions de-signed to probe the agent’s spatio-temporal understanding of the tour, and (3) a model for the task that encodes the scene as an allocentric, top-down semantic feature map and grounds the question into the map to localize the answer. We show that our choice of episodic scene memory outperforms naive, off-the-shelf solutions for the task as well as a host of very competitive baselines and is robust to noise in depth, pose as well as camera jitter. 1.

Introduction
Imagine wearing a pair of AI-powered, augmented re-ality (AR) glasses and walking around your house. Such smart-glasses will possess the ability to “see” and passively capture egocentric visual data from the same perspective as its wearer, organize the surrounding visual information into its memory, and use this encoded information to communi-cate with humans by answering questions such as, “where did you last see my keys?”. In other words, such devices
Figure 1. (a) An egocentric AI assistant, assumed to be running on a pair of augmented reality glasses, is taken on a guided ex-ploratory tour by virtue of its human wearer moving about inside the environment. (b) The agent passively records an egocentric stream of RGB-D maps, (c) builds an internal, episodic memory representation of the scene and (d) exploits this spaito-temporal memory representation to answer (multiple) post-hoc questions about the tour. can act as our own personal egocentric AI assistants.
There has been a rich history of prior work in train-ing navigational agents to answer questions grounded in in-door environments – a task referred to as Embodied Ques-tion Answering (EQA) in literature [9, 15, 29, 32]. How-ever, egocentric AI assistants differ from EQA agents in several important ways. First, such systems passively ob-serve a sequence of egocentric visual frames as a result of the human wearer’s navigation, as opposed to taking ac-tions in an environment. Second, AI systems for egocentric assistants would be required to build scene-speciﬁc mem-ory representations that persist across different questions.
This is in direct contrast with EQA where contemporary approaches have treated every question as a clean-slate nav-igation episode. EQA agents start navigating with no prior information about the scene (even if the current question is about a scene that they have witnessed before). And third,
EQA agents respond to questions by uttering language to-ken(s). Responding to a question such as, “where did you last see my keys?” with the answer — “hallway” isn’t a very helpful response if there are multiple hallways in the house. In contrast, our setup presents a scenario wherein an egocentric assistant can potentially localize answers by grounding them within the environment tour.
Therefore, as a step towards realizing the goal of such egocentric AI assistants, we present a novel task wherein the AI assistant is taken on a guided tour of an indoor envi-ronment and then asked to localize its answers to post-hoc questions grounded in the environment tour (Fig. 1). This pre-exploratory tour presents an opportunity to build an in-ternal, episodic memory of the scene. Once constructed, the
AI assistant can utilize this scene memory to answer multi-ple, follow-up questions about the tour. We call this task –
Episodic Memory Question Answering (EMQA).
More concretely, in the proposed EMQA task, the sys-tem receives a pre-recorded sequence of RGB-D images with the corresponding oracle pose information (the guided agent tour) as an input. It uses the input tour to construct a memory representation of the indoor scene. Then, it ex-ploits the scene memory to ground answers to multiple text questions. The localization of answers can happen either within the tour’s egocentric frame sequence or onto a top-down metric map (such as the house ﬂoorplan). The two output modalities are equivalent, given the agent pose.
The paper makes several important contributions. First, we introduce the task of Episodic Memory Question An-swering. We generate a dataset of questions grounded in pre-recorded agent tours that are designed to probe the sys-tem’s spatial understanding of the scene (”where did you see the cushion?”) as well as temporal reasoning abilities (“where did you ﬁrst/last see the cushion?”). Second, we propose a model for the EMQA task that builds allocen-tric top-down semantic scene representations (the episodic scene memory) during the tour and leverages the same for answering follow-up questions.
In order to build the episodic scene memory, our model combines the seman-tic features extracted from egocentric observations from the tour in a geometrically consistent manner into a sin-gle top-down feature map of the scene as witnessed dur-ing the tour [5]. Third, we extend existing scene memories that model spatial relationships among objects [5] (“what” objects were observed and “where”) by augmenting them with temporal information (“when” were these objects ob-served), thereby making the memory amenable for reason-ing about temporal localization questions.
Fourth, we compare our choice of scene representa-tion against a host of baselines and show that our proposed model outperforms language-only baselines by ∼ 150%, naive, “off-the-shelf” solutions to the task that rely on mak-ing frame-by-frame localization predictions by 37% as well as memory representations from prior work that aggregate (via averaging [13], GRU [4], context-conditioned atten-tion [14]) buffers of observation features across the tour.
Finally, in addition to photorealistic indoor environ-ments [6], we also test the robustness of our approach under settings that have a high ﬁdelity to the real world. We show qualitative results of a zero-shot transfer of our approach to a real-world, RGB-D dataset [26] that presents signiﬁ-cantly challenging conditions of imperfect depth, pose and camera jitter - typical deployment conditions for egocentric
AR assistants. In addition to that, we break away from the unrealistic assumption of the availability of oracle pose in indoor settings [11, 33] and perform a systematic study of the impact of noise (of varying types and intensities) in the agent’s pose. We show that our model is more resilient than baselines to such noisy perturbations. 2.