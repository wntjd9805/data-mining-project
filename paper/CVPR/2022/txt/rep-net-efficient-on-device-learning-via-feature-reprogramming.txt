Abstract
Transfer learning, where the goal is to transfer the well-trained deep learning models from a primary source task to a new task, is a crucial learning scheme for on-device machine learning, due to the fact that IoT/edge devices col-lect and then process massive data in our daily life. How-ever, due to the tiny memory constraint in IoT/edge devices, such on-device learning requires ultra-small training mem-ory footprint, bringing new challenges for memory-efficient learning. Many existing works solve this problem by re-ducing the number of trainable parameters. However, this doesn’t directly translate to memory saving since the ma-jor bottleneck is the activations, not parameters. To de-velop memory-efficient on-device transfer learning, in this work, we are the first to approach the concept of transfer learning from a new perspective of intermediate feature re-programming of a pre-trained model (i.e., backbone). To perform this lightweight and memory-efficient reprogram-ming, we propose to train a tiny Reprogramming Network (Rep-Net) directly from the new task input data, while freez-ing the backbone model. The proposed Rep-Net model in-terchanges the features with the backbone model using an activation connector at regular intervals to mutually bene-fit both the backbone model and Rep-Net model features.
Through extensive experiments, we validate each design specs of the proposed Rep-Net model in achieving highly memory-efficient on-device reprogramming. Our experi-ments establish the superior performance (i.e., low training memory and high accuracy) of Rep-Net compared to SOTA on-device transfer learning schemes across multiple bench-marks. Code is available at https://github.com/ASU-ESIC-FAN-Lab/RepNet. 1.

Introduction
Nowadays, the utilization of IoT devices is significantly increased (e.g., 250 billion microcontrollers in the world today1), which collect and then process massive new data 1https://venturebeat.com/2020/01/11/why-tinyml-is-a-giant-opportunity/
Figure 1. The workflow of adversarial reprogramming (a) and the
Rep-Net (b). Adversarial reprogramming reprograms the input by introducing an additive learnable parameter. Differently, Rep-Net takes the input data directly and learns to reprogram the interme-diate activation features. crossing various domains/tasks in our daily life. This fact arouses researchers great interest in on-device AI that ex-pect not only inference but also the training or transfer-ring of pre-trained models to new data on the device. Such learning scheme is strongly associated with the concept of transfer learning [5], where the goal is to transfer the well-trained deep learning models from a primary source task to a new task, leading to a recently rising research direction about on-device transfer learning. Compared with conven-tional scheme that learns the deep learning models on could and then performs inference on device, on-device trans-fer learning eliminates the communication issue between cloud and edge devices, as well as data-privacy concern.
Although the benefits of learning on-device are clear, the memory-hungry training process is a extremely challenge for memory-constrained IoT/edge devices. Recently, the studies on memory-efficient learning [3, 6, 37] reveal that the bottleneck of training memory consumption is the stor-age of intermediate activations, not parameters. Thus, re-ducing the training memory footprint, especially the activa-tion storage of the pre-trained model, is the crucial issue to enable on-device transfer learning. However, the existing transfer learning methods either suffer from huge training memory consumption, or having limited transfer capacity.
Usually, it is a common practice to fine-tune the models that pre-trained on large scale dataset, like ImageNet [8], to perform transfer learning [5, 7, 12, 15, 20, 29, 38]. Com-pared with training from scratch, fine-tuning a pre-trained convolutional neural network on a target dataset can signif-icantly improve performance. Generally, there are mainly two ways of fine-tuning: 1) Treating the pre-trained model as a fixed feature extractor, then only fine-tuning the last classification layer [5,38]. This method is memory efficient for training, as it eliminates the intermediate activation stor-age of the pre-trained model. However, the low transfer capacity of this has method has been widely demonstrated on previous works [7, 20, 29]. 2) Fine-tuning the full or partial pre-trained model. This method can achieve better accuracy [7, 15, 20, 20, 29]. However, it updates the pre-trained model, causing a vast activation memory footprint.
Thus it is not friendly for on-device learning. In addition, mask-based methods [28, 36] perform transfer learning by learning a binary mask w.r.t all the wights of the pre-trained model. But it only reduces the trainable parameter size, not the activation memory, still resulting in high memory foot-print.
Alternatively, adversarial input reprogramming [10] performs transfer learning by reprogramming input. As shown in Fig. 1 (a), input reprogramming is basically an additive operation to change the existing input features us-ing a trainable element-wise bias for each input pixels (re-fer to Eq. (3)). Unlike fine-tuning methods that need to update the model parameters, adversarial reprogramming keeps the model architecture unchanged. Instead, it intro-duces an additive learnable adversarial parameter for input data and designs an output label mapping on the target-domain data samples to perform transferring as shown in
Fig. 1(a). Such a method is memory efficient during train-ing as it only learns additive parameters to reprogram the input that does not require any updates to the pre-trained model, as well eliminating the memory-dominating activa-tion storage. However, it comes with several shortcomings: 1) the transfer capacity of only adding a learnable param-eter to input is limited; 2) such design only works when the input size of target data is much smaller than the source data (e.g., ImageNet (224×224) to MNIST (28×28)), es-sentially constraining the generality of the method on more complex dataset; 3) the additional parameter size and train-ing memory cost are still high. For example, considering the ImageNet pre-trained model, it needs 224 × 224 × 3 ∼ 150K additional trainable parameters overhead. Despite these shortcomings, the reprogramming concept is simple and has the potential to solve the existing on-device learn-ing challenges.
To tackle the challenges and shortcomings of prior works, we propose Reprogramming Network (Rep-Net), which, for the first time, treats the on-device transfer learn-ing problem from a new perspective of intermediate fea-ture reprogramming. As shown in Fig. 1(b), Rep-Net serves as a lightweight side-network that is executed with the pre-trained backbone model in parallel. The principle working mechanism of this design is to reprogram the fixed back-bone model from the input data via proposed activation con-nector that enables feature exchange between the backbone and Rep-Net at regular intervals. Such feature exchange uses an additive operation that not only helps both the back-bone model and Rep-Net model to update and improve their features, but also inherits the property of memory-efficiency from adversarial input reprogramming. To perform on-device transfer learning, we only train Rep-Net model and a task-specific last classification layer for the new task, while freezing the backbone model.
In summary, our technical contributions include:
• In this work, for the first time, we approach the on-device transfer learning from a new perspective of intermediate feature reprogramming problem for a given pre-trained backbone model. To develop this lightweight reprogramming scheme, we propose a novel memory-efficient Reprogramming Network (Rep-Net) that is trained directly from the new task input data to reprogram the intermediate features of a backbone model. It is a small convolution network with little memory overhead.
In fact, our proposed
Rep-Net improves memory efficiency in comparison to adversarial input reprogramming, but providing much improved transfer capacity.
• We design dedicated activation connectors to enable feature exchange between the backbone and the Rep-Net models at regular intervals. Both models bene-fit mutually from the feature exchange operation. We perform an extensive analysis of the design specs of the connector, investigate and answer the following design i.e., How to exchange the features space challenges: considering the limited memory budget during train-ing? Why exchanging both (i.e., backbone and Rep-Net model) features? How many activation connec-tors are necessary? How many layers are necessary in
Rep-Net?.
• We conduct extensive experiments to corroborate the superiority of Rep-Net over current state-of-the-art (SOTA) transfer learning approaches across multiple benchmarks.
2.