Abstract
Semi-supervised learning (SSL) is a method to make bet-ter models using a large number of easily accessible un-labeled data along with a small number of labeled data obtained at a high cost. Most of existing SSL studies fo-cus on the cases where sufficient amount of labeled sam-ples are available, tens to hundreds labeled samples for each class, which still requires a lot of labeling cost.
In this paper, we focus on SSL environment with extremely scarce labeled samples, only 1 or 2 labeled samples per class, where most of existing methods fail to learn. We pro-pose a propagation regularizer which can achieve efficient and effective learning with extremely scarce labeled sam-ples by suppressing confirmation bias. In addition, for the realistic model selection in the absence of the validation dataset, we also propose a model selection method based on our propagation regularizer. The proposed methods show 70.9%, 30.3%, and 78.9% accuracy on CIFAR-10, CIFAR-100, SVHN dataset with just one labeled sample per class, which are improved by 8.9% to 120.2% compared to the ex-isting approaches. And our proposed methods also show good performance on a higher resolution dataset, STL-10. 1.

Introduction
Semi-supervised learning (SSL) is a machine learning technique that trains a model using a small number of la-beled data and a large number of unlabeled data. As it can show comparable performance to supervised learning, it is attracting more attention from researchers. Semi-supervised learning techniques have shown remarkable performances in various fields such as image segmenta-tion [5, 8, 31], object detection [1, 11, 19], text classifica-tion [4, 10, 20], and graph embedding [27, 29] as well as image classification [17, 27].
*Corresponding author.
Most SSL methods [2, 3, 12, 17, 27, 32] are based on con-sistency regularization [14,26] and pseudo labeling [15,21].
Consistency regularization is a method developed under the assumption that the prediction will not change significantly even if a slight perturbation is applied to the sample. Pseudo labeling is a special case of self-training [25,33], which uses the predicted output of unlabeled samples as pseudo-labels to train a model.
In SSL, maximum utilization of unlabeled samples is im-portant, but it is also important learning with a small number of labeled samples because labeled samples are usually at a high cost. However, only a few researches focused on learn-ing with scarce labeled samples. We need to study on how
SSL works and how to improve its performance in label-scarce situations.
MixMatch [3], Unsupervised Data Augmentation for
Consistency Training (UDA) [32], and ReMixMatch [2] showed good performances with relatively many labeled samples such as 25, 50, 100, 200, and 400 labeled examples per class for CIFAR-10 [13]/SVHN [22] dataset. Recent approaches such as FixMatch [27], SelfMatch [12], Flex-Match [35], and CoMatch [17] considered label-scarce sit-uations. FixMatch, SelfMatch, and FlexMatch used at least 4 labeled examples per class and CoMatch used at least 2 samples per class. However, they were unstable and showed a poor performance with a small number of labeled samples.
One of the serious problems of scarce-label situations is confirmation bias [16, 18, 30] that can occur in the label propagation [9]. Confirmation bias refers to a phenomenon in which the model learns incorrect predictions for unla-beled data, so that the confidence of the incorrect prediction is increased and the model has resistance to new (correct) information that can be corrected. If there are enough la-beled data, the propagation of wrong information can be canceled out by the correct information around the incorrect prediction. SSL can avoid confirmation bias of the model.
On the other hand, if labeled data is few in number, incorrect predictions can be propagated widely, and the probability of not receiving appropriate correct information can increase.
Confirmation bias makes a significant adverse effect on the training of the model through SSL. This problem can be more serious in the approaches based on hard pseudo labels such as UDA, FixMatch, SelfMatch, FlexMatch.
Another serious problem in extremely label-scarce sit-uations is the model selection. As in supervised learn-ing, the stopping condition is very important in semi-supervised learning environments. In supervised learning, validation datasets are usually used to check the stopping condition, but in semi-supervised learning, especially in scarce-label environments, there are not enough labeled samples for validation. However, previous SSL approaches
[2, 3, 12, 17, 27, 32] disregard about the stopping condi-tion or the model selection. For performance evaluation, they simply took the median of the last 20 model perfor-mances [3, 28]. In scarce-label situations, the learning of
SSL can be very unstable because of confirmation bias. A model with a low training loss does not guarantee a good test accuracy.
We propose propagation regularizer to improve the per-formance of SSL in extreme scarce-label environments where just 1 or 2 labeled samples are available per class.
The propagation regularizer suppresses confirmation bias that can propagate incorrect predictions due to the ex-tremely small number of labeled data, allowing SSL learn-ing to proceed stably. We also propose a model selection method based on the loss of the propagation regularizer to select a well-trained model in extremely label-scarce sce-nario. These methods require very low additional compu-tational cost and they are easy to adopt to the existing SSL approaches.
We show that confirmation bias can easily occur and make an adverse effect on model training in the extremely label-scarce scenario through toy examples and CIFAR-10 dataset. We propose propagation regularizer and the model selection method. We present the state-of-the-arts perfor-mance in an extremely label-scarce scenario with 1 or 2 la-beled examples per class. 2. Confirmation Bias in Extremely Label-scarce Setting
Most SSL approaches have troubled with confirmation
In extreme scarce-label situation, confirmation bias bias. problem is worsen.
In this section, we evaluate how much confirmation bias makes an adverse effect on semi-supervised learning process in extreme label-scarce situa-tions.
We conduct experiments with FixMatch [27], a represen-tative pseudo-labeling method in SSL, and three datasets: moon dataset, star dataset and CIFAR-10 [13]. The experi-ments confirm that confirmation bias easily occurs in label-scarce settings and can have a significant impact on perfor-mance. (a) Moon dataset with Rand2 (d) Star dataset with Rand2 (b) Moon dataset with Exp2 (e) Star dataset with Exp2 (c) Moon dataset with Rand20 (f) Star dataset with Rand20
Figure 1. Class boundaries by FixMatch. Labeled samples are in colors and unlabeled samples are in grey. Each crescent is a class in moon dataset and each wing is a class in star dataset. 2.1. Analysis with Toy Examples
In order to check confirmation bias occurring in SSL with extremely scarce labeled samples, we train a 3-layer neural network model with FixMatch on 2-dimensional moon and star datasets. The moon dataset consists of two classes, 1k unlabeled samples. The star dataset consists of 5 classes generated with a Gaussian distribution. Each class has 200 unlabeled samples. In each dataset, three sets of labeled examples, Rand2, Exp2 and Rand20, are given to verity that the initial labeled samples have a significant ef-fect on confirmation bias during training. Rand2 contains two labeled samples per class, randomly selected from the unlabeled samples; Exp2 consists of two labeled samples per class selected by experts so that the labeled samples represent the distribution of the unlabeled dataset well; and
Rand20 contains 20 labeled samples per class randomly se-lected from the unlabeled samples. For FixMatch, Gaus-sian noise with different strength are used for the weak and strong augmentations.
Figures 1a to 1c show the moon datasets and the learn-ing results of FixMatch, and Figs. 1d to 1f show the result for the star datasets. It can be seen that the class boundaries do not match the data distribution when 2 randomly cho-sen labeled samples per class are given as shown in Figs. 1a and 1d. As shown in Figs. 1c and 1f, when more labeled samples are given, the class boundaries are properly gener-ated. In Figs. 1b and 1e, we can notify that confirmation bias has less effect on the models if labeled samples are carefully chosen.
As shown in Figs. 1a and 1d, if labeled samples do not
Method
Fold
FixMatch
Fold 1
Fold 2
Fold 3
Fold 4
Fold 5
Class 0 0.11 0.11 0.22 0.00 0.10 1 0.09 0.10 0.01 0.09 0.09 2 0.02 0.02 0.08 0.01 0.01 3 0.00 0.08 0.00 0.00 0.00 4 0.45 0.26 0.36 0.19 0.01 5 0.00 0.00 0.00 0.01 0.17 6 0.09 0.10 0.07 0.36 0.13 7 0.01 0.09 0.08 0.10 0.30 8 0.11 0.10 0.00 0.11 0.10 9 0.11 0.13 0.17 0.11 0.09
Entropy Accuracy 0.72 0.90 0.72 0.77 0.84 62.29 67.18 53.05 51.31 66.23
Table 1. Class ratio and entropy of pseudo labels for CIFAR-10 dataset with 10 labeled samples. well represent the distribution of each class, label propaga-tion occurs in a skewed way during SSL learning process and confirmation bias can be intensified. The label propa-gation process is prone to bias because there are only two labeled samples per class and the distributions of unlabeled and labeled samples do not match each other.
Even in the case where the number of labeled samples is very small, the confirmation bias can be suppressed if the labeled samples can represent the class distribution, as shown in Figs. 1b and 1e. However, it is not usually ex-pected that a few randomly selected samples properly rep-resent the data distribution. As seen in Figs. 1c and 1f, if there are many randomly chosen labeled samples, they can represent the class distribution, and the class boundaries are learned properly. 2.2. Analysis with CIFAR-10 Dataset
In order to verify that real-world datasets are prone to the confirmation bias problem, we conduct the experiment with the CIFAR-10 dataset. In this experiment, we use 1 labeled sample per class and train FixMatch with Wide Residual
Network 28-2 model [34]. The experiment was performed in 5 folds and, in each fold, labeled examples are randomly selected from the training data.
The performance is the median accuracy of the last 20 models, and it is averaged over 5 folds. The average ac-curacy is 60.01%. The highest accuracy among 5 folds is 67.18%, and the lowest is 51.31%, showing a large variance in performance.
In order to prove that each model of 5 folds does not generate a good model due to confirmation bias, we observe
If each model the class ratio of pseudo-labels in Tab. 1. is well trained without confirmation bias on the CIFAR-10 dataset, the ratio of each class in pseudo-labels will appear as 0.1, and the entropy of the ratios will be 1.0. The entropy is defined as follows:
Entropy = − c (cid:88) i ri logc ri (1) where ri is the ratio of class i and c is the number of classes.
We notice that the class ratios in each fold are not bal-anced in Tab. 1. In detail, in the first fold, unlabeled sam-Figure 2. Training loss, entropy of pseudo-labels and test accuracy of FixMatch on CIFAR-10 with 10 labeled samples (Fold 3). ples are never pseudo-labeled as Class 3 or 5 and only small number of unlabeled samples are pseudo-labeled as Class 7, which results in a low entropy of 0.72. Such tendency is observed for all 5-fold. And even in Fold 2 with the highest entropy of 0.90, the ratio of Class 3 is 0. The av-erage of 5-fold entropy for pseudo-labeling of FixMatch on
CIFAR-10 with 10 labeled samples is 0.79. What is inter-esting is that the entropy and the accuracy of the model has a strong correlation of 0.69. Higher entropy means smaller confirmation bias; thus, we surmise that confirmation bias has a significant effect on a model’s performance. We also run the same experiment with 25 labeled samples per class.
The average entropy is 0.99, which means that there is little confirmation bias.
Through the experiment, we confirm that confirmation bias is also easy to occur in real-world datasets, that the smaller the number of labeled samples, the stronger effect confirmation bias makes, and that the strength of confir-mation bias, measured as entropy of pseudo-class ratios, is strongly associated with the model performance.
We also observe the test accuracy, the training loss, and the entropy of pseudo-labels by epoch. Figure 2 shows the accuracy, the training loss, and the entropy of Fold 3 by epoch. We see that the training is very unstable. At the beginning of training, the entropy of pseudo-labels in-creases, and the test accuracy also increases. This shows that consistency regularization performs beneficially along with pseudo-labeling and the SSL model is being trained well. However, the test accuracy drops sharply around 600
Pearson’s Correlation Coefficient
Fold 1
Fold 2
Fold 3
Fold 4
Fold 5 Average
Training Loss-Accuracy
Entropy-Accuracy 0.175 0.835 0.364 0.807 0.747 0.950 0.257 0.841 0.406 0.846 0.390 0.856
Table 2. Pearson’s Correlation Coefficient of training loss-test accuracy and entropy-test accuracy during FixMatch training on the CIFAR-10 with 10 labeled samples. epochs and the same time the entropy also sharply drops.
Afterward, the performance of the model recovers to some extent, but it hardly regains the previous best performance.
From the observation, we notice two things. The per-formance of the model does not gradually improve as the training progresses. The learning of the model is unstable, showing the best performance in the middle of training, and then rapidly dropping at some point. Therefore, choosing the last updated model or the model with the lowest train-ing loss does not guarantee the best model.
The second is more important. We perceive that the cor-relation of test accuracy and pseudo-label class entropy is much higher than that of test accuracy and training loss.
Table 2 shows the correlation coefficients between test ac-curacy and entropy, and between test accuracy and training loss. This observation gives us a hint on how to suppress confirmation bias in training and how to better select a good model. 3. Proposed Method
In pseudo-labeling process, the model learns the model output, i.e., it repeatedly learns its own erroneous predic-tion, resulting in confirmation bias [18, 30]. This phe-nomenon can be amplified especially in an extremely scarce labeled scenario with one or two labeled examples in each class.
Based on our experimental observations, we propose a propagation regularizer method to suppress confirmation bias in an extremely scarce label environment, and a model selection method that selects the optimal model without val-idation data among models generated during the learning process. 3.1. Propagation Regularizer
In pseudo-labeling process, incorrect predictions of the model can be used for the next model training, which causes confirmation bias. We may infer that we need to keep the balance between pseudo-labels based on our observation that the correlation between test accuracy and the entropy of pseudo-classes is high as shown in Tab. 2. Learning im-balanced pseudo-labeled sample will augment confirmation bias.
For example, let us consider SSL learning with two classes, A and B. If a model in the middle of SSL training produces more pseudo-labels of class A than B, the imbal-anced pseudo-labeled samples are used for the next model training. Then, the next model is easy to be biased to class
A, and the confirmation bias will be inflated.
To solve this problem, a regularization term is designed so that the pseudo-labeling for the unlabeled samples should be balanced for each class as follows:
Lpr = 1 − (−PU · logc(PU)) (2) where c is the number of classes. PU is the masked aver-aged probability distribution of unlabeled examples, unla-beled samples U in a batch, defined as follows:
PU = 1
|U| (cid:88) 1 (max (p (u)) ≥ τ ) p (u) (3) u∈U where τ is the confidence threshold for pseudo-labeling and p(u) is the softmax output of an unlabeled example u.
In Eq. (3), the average of predictions is obtained for sam-ples having values greater than or equal to a threshold τ in a batch of unlabeled examples. To convert this to a minimiza-tion form, the entropy of PU is subtracted from 1. If the pseudo-labels of unlabeled examples are evenly distributed, the value of Lpr will converge to 0. By simply adding this regularization term to the SSL loss, class-balanced pseudo-labeling can be achieved. Through this, we can alleviate the confirmation bias in extremely scarce example scenario. 3.2. Model Selection based on Propagation Regu-larizer and Utilization
Model selection is crucial in semi-supervised learning.
As observed in Sec. 2, the performance of the model is not stable during training, because it is affected much by confir-mation bias. If we have a validation dataset, we may choose the best model as we do in supervised learning. However, there are not sufficient labeled samples to be used for vali-dation in our case.
Some SSL approaches [14, 21, 24, 30, 32] simply select the last model. Many recent SSL studies [2,3,12,17,27,32] did not propose a model selection method. They took the median of the performance of the last 20 models for model evaluation. This may be acceptable as a model performance comparison method [3,28] in plain environments. However, as shown in Fig. 2, model performance is very unstable in an extreme label-scarce environment. This makes model
Method
FixMatch + Sel + Reg
Fold
Fold 1
Fold 2
Fold 3
Fold 4
Fold 5
Class 0 0.10 0.09 0.07 0.17 0.12 1 0.12 0.12 0.17 0.11 0.11 2 0.07 0.06 0.04 0.02 0.01 3 0.08 0.06 0.09 0.05 0.10 4 0.18 0.09 0.13 0.10 0.03 5 0.11 0.09 0.08 0.15 0.10 6 0.12 0.18 0.12 0.13 0.12 7 0.03 0.10 0.03 0.07 0.21 8 0.11 0.10 0.09 0.12 0.12 9 0.09 0.11 0.17 0.08 0.08
Entropy Accuracy 0.97 0.98 0.95 0.95 0.94 68.60 59.47 72.89 78.81 74.58
Table 3. Class ratio and entropy of pseudo labels for CIFAR-10 dataset with 10 labeled samples. The proposed model selection and propagation regularizer are applied. selection very difficult and hinders SSL approaches from being used in real-world applications.
To select an appropriate model, we propose a measure based on confirmation bias and utilization of unlabeled sam-ples. A good SSL model would utilize unlabeled samples as much as possible and be less affected by confirmation bias.
To choose such models, we propose the utilization measure of unlabeled samples and the influence measure of confir-mation bias.
For the utilization measure of unlabeled samples, we propose the following equation:
TU = 1
|U| (cid:88) u∈U 1(max(p(u)) ≥ τ ) (4)
Equation (4) shows the ratio of pseudo-labeled examples satisfying the confidence threshold for pseudo-labeling, τ .
If the model uses all unlabeled examples in a batch for train-ing, the value of TU is 1; and if none of the unlabeled ex-amples is used at all, it is 0. To measure the influence of confirmation bias, we use Eq. (2), which is the proposed propagation regularizer. By combining Eqs. (2) and (4), we develop a metric for the model selection. It is defined as follows:
Sel = (1 − Lpr) + TU (5)
A good SSL model utilizes most unlabeled samples and is less affected by confirmation bias, so the value will be max-In the training process, we evaluate Sel at each imized. epoch, and choose the model with the maximum value of
Sel as the final model.
The proposed model selection method does not use an additional validation dataset. We can select an appropriate
SSL model without a validation dataset in scarce-label situ-ations. 4. Experiment
To verify the proposed propagation regularizer and model selection methods, we combine the proposed meth-ods to each of UDA [32] and FixMatch [27], and we per-form SSL image classification benchmarks. We compare (a) Moon dataset with Rand2 (b) Star dataset with Rand2
Figure 3. Datasets and class boundaries by FixMatch with the proposed method.
In the dataset, labeled samples are in colors unlabeled samples are in grey. In moon datasets, each crescent is a class and in star datasets, each wing is a class. the performance with the current SOTA approaches, Co-Match [17] and FlexMatch [35], with SVHN [22], CIFAR-10 and CIFAR-100 [13]. Also, we conduct experiments on a higher resolution dataset, STL-10 [6], for FixMatch with our proposed methods. We perform the SSL methods on the datasets with a various number of labeled examples includ-ing extremely label-scare scenarios. All experiments were performed according to SSL evaluation protocols [2, 3, 23].
The experiment results show the superiority of the proposed method. Our methods show the best performance in the ex-tremely label-scarce scenario. 4.1. Propagation Regularizer with Toy Examples and CIFAR-10 Dataset
To confirm that our proposed propagation regularizer works effectively, we apply the proposed method to the ex-periment in Sec. 2.
The experimental results for the moon and star dataset are shown in Fig. 3. In Figs. 1a and 1d, the learned class distribution are not well represented the data class distribu-tion because confirmation bias can be intensified. When the proposed propagation regularizer is applied to FixMatch, it can be seen that the class distribution is properly learned as
Figs. 3a and 3b.
Table 3 shows the class ratio of pseudo-labels of unla-beled examples, entropy, and accuracy when FixMatch with
Method
UDA
FixMatch
CoMatch
FlexMatch
UDA
+ Sel
UDA
+ Sel + Reg
FixMatch
+ Sel
FixMatch
+ Sel + Reg
CIFAR-10
CIFAR-100
SVHN 10 labels 20 labels 40 labels 100 labels 200 labels 400 labels 10 labels 20 labels 40 labels 51.82
±8.51 60.01
±7.41 65.10
±7.81 59.06
±19.80 59.71
±16.01 69.87
±9.96 65.73
±10.32 70.87
±7.35 83.53
±8.05 75.50
±10.93 88.26
±8.29 94.62
±0.15 83.60
±8.09 84.33
±7.23 79.24
±10.00 88.20
±4.29 90.06
±4.37 85.57
±5.21 92.16
±4.97 94.86
±0.05 90.12
±4.35 91.54
±2.53 89.87
±4.96 91.52
±2.81 24.96
±2.22 24.11
±1.50 24.19
±0.98 4.47
±0.81 24.95
±2.23 30.30
±0.99 24.17
±1.55 27.97
±1.12 37.76
±0.74 35.83
±1.63 32.51
±1.15 30.59
±1.69 37.76
±0.84 42.34
±1.54 35.78
±1.58 38.96
±1.42 48.98
±1.73 46.04
±1.41 41.72
±2.04 46.11
±2.83 49.11
±1.77 50.61
±1.48 46.05
±1.28 48.01
±1.72 24.02
±19.68 35.84
±10.12 25.36
±4.64 11.02
±1.89 78.91
±12.30 77.98
±32.06 51.40
±26.66 69.61
±24.33 67.84
±11.05 55.79
±30.83 45.62
±7.12 34.93
±36.00 97.78
±0.27 96.01
±3.71 91.90
±5.77 96.26
±2.86 96.51
±2.54 86.73
±21.70 76.07
±9.31 77.04
±23.16 96.48
±3.24 97.46
±0.45 96.41
±3.06 97.61
±0.33
Table 4. Comparison of accuracy for CIFAR-10, CIFAR-100 and SVHN on 5 different folds with 1, 2 and 4 labeled samples per class.
Method
FixMatch
FixMatch
+ Sel
FixMatch
+ Sel + Reg
STL-10 10 labels 20 labels 40 labels 30.82
±6.73 30.07
±5.82 37.91
±6.66 43.24
±6.32 45.45
±3.24 61.00
±15.87 60.92
±5.60 63.93
±9.65 74.45
±13.50
Table 5. Comparison of accuracy for STL-10 on 5 different folds with 1, 2 and 4 labeled samples per class. the proposed method is applied to the CIFAR-10 dataset. In
Tab. 3, the class ratios in each fold are more balanced than in Tab. 1, and the accuracy is also improved. The average of entropy increases from 0.79 to 0.96, and the average perfor-mance increases from 60.01% to 70.87%. It shows that the proposed method is working effectively in extremely label-scarce scenario. 4.2. Dataset and Implementation Details
We conduct experiments on CIFAR-10, CIFAR-100,
SVHN and STL-10. CIFAR-10/100 and SVHN datasets consist of 3 channels of 32×32 size, and STL-10 consists of 3 channels of 96 × 96. CIFAR-10,SVHN and STL-10 con-sist of 10 classes, and CIFAR-100 consists of 100 classes.
CIFAR-10 consists of 50,000 training images and 10,000 test images. CIFAR-100 consists of 60,000 training images and 10,000 test images. SVHN consists of 73,257 training images, 26,302 test images, and 531,131 additional images.
STL-10 consists of 5,000 training images and 100,000 un-labeled images, and 8,000 test images. In CIFAR-10 and
CIFAR-100, images to be used as labeled data are randomly selected class-evenly from the training images and the re-maining training images are treated as unlabeled data. In
SVHN and STL-10, images to be used labeled data are chosen as the same way from the training and additional images, and the remainders are treated as unlabeled data.
Unlike CIFAR-10/100 and STL-10, SVHN is not class bal-anced dataset. The number of samples for each class varies from 6.47% to 17.28% of the total data.
We set λU = 1, η = 0.03, β = 0.9, τ = 0.95, µ = 7, and
B = 64 for FixMatch, and λcls = 1, η = 0.03, τ = 0.95,
µ = 7, B = 64, α = 0.9, τ = 0.2, K = 2560, T = 0.8, and
λctr = 1 for CoMatch. Those hyperparameters are set based on the original works [17, 27]. For UDA, we adopt the same values used by Sohn et al. [27]: λU = 1, η = 0.03, temperature τ = 1, confidence threshold β = 0.9, µ = 7, and
B = 64. We use RandAugment [7] as strong augmenta-tion for CoMatch and CTAugment [2] for FixMatch and
UDA. The weight factor of the propagation regularizer, Lpr, is set 1.0 for CIFAR-100, and 0.4 for CIFAR-10, SVHN, and STL-10. We use a Wide ResNet-28-2 [34] for CIFAR-10/100 and SVHN, and Wide ResNet-37-2 for STL-10. 4.3. Results for Extremely Label-scarce Scenario
Results for extremely label-scarce scenario are shown in Tab. 4. The baselines are UDA, FixMatch and Co-Match. MixMatch and ReMixMatch were excluded from the baselines because of low performance with extremely label-scarce scenario. Their performance of CIFAR-10
Method
FixMatch
FixMatch
+ Sel
FixMatch
+ Sel + Reg
CIFAR-10
CIFAR-100
SVHN 40 labels 100 labels 250 labels 400 labels 1000 labels 2500 labels 40 labels 100 labels 250 labels 85.57
±5.21 89.87
±4.96 91.52
±2.81 92.63
±3.21 93.72
±1.14 94.02
±0.98 95.08
±0.08 94.13
±0.69 94.43
±0.62 46.04
±1.41 46.05
±1.28 48.01
±1.72 57.90
±1.21 57.96
±1.04 57.63
±0.96 64.78
±0.48 64.71
±0.35 65.21
±0.49 86.73
±21.70 96.41
±3.06 97.61
±0.33 97.78
±0.20 97.81
±0.15 97.73
±0.44 97.98
±0.21 97.95
±0.21 97.69
±0.56
Table 6. Comparison of accuracy for CIFAR-10, CIFAR-100 and SVHN on 5 different folds with 4, 10 and 25 labeled samples per class. with 10 labeled samples was 17.48% and 31.00%, re-spectively, which are very lower than the other baselines.
Our approaches are applied to UDA and FixMatch. For example, FixMatch+Sel is the performance of the model trained by FixMatch with our model selection, and Fix-Match+Sel+Reg is for the model trained by FixMatch with our propagation regularizer and model selection. We evalu-ate the cases where the number of labeled samples per class is 1, 2, and 4. The baseline approaches did not propose how to select trained models, we evaluate them as the authors did. We choose the median of the last 20 models.
The methods combined with the proposed propagation regularizer and model selection show the best performance in every case except CIFAR-10 with 20 and 40 labeled sam-ples.
In CIFAR-10 with 10 labeled samples, the accuracy of
FixMatch+Sel+Reg is 70.87%, which shows an 8.9% im-provement compared to CoMatch. With 20 and 40 labels,
CoMatch performs slightly better than models with our ap-proaches. However, the variance of CoMatch is almost twice of ours the best model.
In 20 labels, the variance of CoMatch is 8.29 but that of FixMatch+Sel+Reg is 4.29, and they are 4.97 and 2.81 in 40 labels, respectively. Flex-Match shows best performance with 20 and 40 labels, but the performance drops sharply with 10 labels, showing the second-worst performance.
The experiments with CIFAR-100 also interesting re-sults. UDA+Sel+Reg is the best with 100, 200, and 400 la-bels. The improvements over the best baselines are 21.4%, 12.1% and 3.3%, respectively. These results show the ef-fectiveness of our approaches in large datasets.
The performance improvements with SVHN are 120.2%, 44.1% and 1.1% for 10, 20 and 40 labels, respectively. In the most scarce case where there is 1 labeled sample per class, our approaches improve the most. The proposed reg-ularization and model selection methods effectively worked on class imbalanced datasets, such as SVHN, as well as on class balanced datasets, such as CIFAR-10 and CIFAR-100.
Our method most improves the performance on SVHN than other class-balanced datasets. Our method main-tains the class balance of pseudo-labels even in imbalanced datasets. Performance can be improved by preventing the confirmation bias in the early stage of learning that may oc-cur due to the class imbalance. It helps suppressing confir-mation bias as well as dealing with class imbalance.
Especially, our approaches improve much with 1 labeled sample per class. In CIFAR-10 with 10 labeled samples, the improvements of UDA+Sel and UDA+Sel+Reg over UDA are 15.2% and 34.8%, respectively, and the improvements of FixMatch are 9.5% and 18.1%, respectively. In SVHN with 10 labels, the improvements of UDA are 228.5% and 224.6%, and those are 43.4% and 94.2% for FixMatch.
FlexMatch shows the best performance in CIFAR-10 with 20 and 40 labeled samples, but it shows the worst or the almost worst performance in the other cases. Co-Match shows similar performance patterns to FlexMatch.
FlexMatch and CoMatch showed good performances with enough labeled samples, but they shows bad performances in extremely scarce label scenarios.
We also perform experiments with higher resolution datasets, STL-10, which has 96 × 96 images. Table 5 shows the experimental results. FixMatch+Sel+Reg shows 23%, 39.8%, and 22.2% performance improvement over
FixMatch, confirming that our methods are also effective in STL-10 dataset.
Through the experiments, we verify that our propaga-tion regularizer and model selection are very effective to improve the performance of SSL approaches in extremely label-scarce scenarios. 4.4. Results with More Examples
Table 6 shows the performance of the propagation regu-larization and model selection methods for FixMatch with more labeled examples. We can notice that our approaches are still valid with large labeled datasets.
In the cases with 10 and 25 labeled samples per class, the performance gain is reduced because confirmation bias will also reduce if there are many labeled samples. When the number of labeled samples is large enough, confirmation bias can be easily suppressed and the model can be trained stably. Also, in an environment with enough labeled exam-ples, the propagation regularizer will lost its influence. Its
value will be zero because there is little confirmation bias.
Through these experiments, we can conclude that our ap-proaches effectively suppress confirmation bias regardless of the number of labeled samples and are generally applica-ble. 4.5. Computational Cost
Since the proposed model selection method is performed after training, the training time does not increase. The pro-posed propagation regularizer is performed on each training batch, but it requires only a very small computation cost.
When comparing the actual training time, it increases only by 0.64%. This makes practically no difference. 5.