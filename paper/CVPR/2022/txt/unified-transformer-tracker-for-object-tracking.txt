Abstract (cid:54)(cid:50)(cid:55)(cid:3)(cid:3) (cid:56)(cid:86)(cid:72)(cid:85)(cid:16)(cid:54)(cid:83)(cid:72)(cid:70)(cid:76)(cid:73)(cid:76)(cid:72)(cid:71) (cid:48)(cid:50)(cid:55)(cid:3)(cid:3) (cid:39)(cid:72)(cid:87)(cid:72)(cid:70)(cid:87)(cid:82)(cid:85)(cid:16)(cid:86)(cid:83)(cid:72)(cid:70)(cid:76)(cid:73)(cid:76)(cid:72)(cid:71)(cid:3) (cid:55)(cid:85)(cid:68)(cid:70)(cid:78)(cid:3)(cid:55)(cid:85)(cid:68)(cid:81)(cid:86)(cid:73)(cid:82)(cid:85)(cid:80)(cid:72)(cid:85) (cid:54)(cid:50)(cid:55)(cid:3)(cid:53)(cid:72)(cid:86)(cid:88)(cid:79)(cid:87)(cid:86)
As an important area in computer vision, object track-ing has formed two separate communities that respectively study Single Object Tracking (SOT) and Multiple Object
Tracking (MOT). However, current methods in one tracking scenario are not easily adapted to the other due to the diver-gent training datasets and tracking objects of both tasks. Al-though UniTrack [45] demonstrates that a shared appear-ance model with multiple heads can be used to tackle indi-vidual tracking tasks, it fails to exploit the large-scale track-ing datasets for training and performs poorly on the single object tracking. In this work, we present the Uniﬁed Trans-former Tracker (UTT) to address tracking problems in dif-ferent scenarios with one paradigm. A track transformer is developed in our UTT to track the target in both SOT and
MOT where the correlation between the target feature and the tracking frame feature is exploited to localize the target.
We demonstrate that both SOT and MOT tasks can be solved within this framework, and the model can be simultaneously end-to-end trained by alternatively optimizing the SOT and
MOT objectives on the datasets of individual tasks. Exten-sive experiments are conducted on several benchmarks with a uniﬁed model trained on both SOT and MOT datasets. 1.

Introduction
Visual object tracking is one of the fundamental com-puter vision tasks with numerous applications [10, 25, 40, 55]. Unlike clearly deﬁned object classiﬁcation and detec-tion problems [18, 19, 46, 50], object tracking is considered in different scenarios and can be categorized into two main paradigms 1) Single Object Tracking (SOT) is to track an annotated target from any object category in the ﬁrst frame throughout a video [13, 25]; 2) Multiple Object Tracking (MOT) aims at estimating bounding boxes and identities of objects in videos where categories of targets are known and
*This work was done during Fan’s internship at Meta AI.
†Correspondence author. (cid:37) (cid:68) (cid:70) (cid:78) (cid:69) (cid:82) (cid:81) (cid:72) (cid:38)(cid:82)(cid:85)(cid:85)(cid:72)(cid:79)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81) (cid:37) (cid:68) (cid:70) (cid:78) (cid:69) (cid:82) (cid:81) (cid:72) (cid:55)(cid:85)(cid:68)(cid:70)(cid:78)(cid:76)(cid:81)(cid:74)(cid:3)(cid:41)(cid:85)(cid:68)(cid:80)(cid:72)(cid:86) (cid:48)(cid:50)(cid:55)(cid:3)(cid:53)(cid:72)(cid:86)(cid:88)(cid:79)(cid:87)(cid:86) (cid:55)(cid:68)(cid:85)(cid:74)(cid:72)(cid:87)(cid:3)(cid:69)(cid:82)(cid:91) (cid:38)(cid:68)(cid:87)(cid:72)(cid:74)(cid:82)(cid:85)(cid:92) (cid:39)(cid:68)(cid:87)(cid:68)(cid:86)(cid:72)(cid:87) (cid:54)(cid:50)(cid:55) (cid:56)(cid:86)(cid:72)(cid:85)(cid:16)(cid:86)(cid:83)(cid:72)(cid:70)(cid:76)(cid:73)(cid:76)(cid:72)(cid:71) (cid:49)(cid:82)(cid:87)(cid:3)(cid:79)(cid:76)(cid:80)(cid:76)(cid:87)(cid:72)(cid:71) (cid:47)(cid:68)(cid:54)(cid:50)(cid:55)(cid:15)(cid:3)(cid:42)(cid:50)(cid:55)(cid:20)(cid:19)(cid:78)(cid:15)(cid:3)(cid:55)(cid:85)(cid:68)(cid:70)(cid:78)(cid:76)(cid:81)(cid:74)(cid:49)(cid:72)(cid:87) (cid:48)(cid:50)(cid:55) (cid:39)(cid:72)(cid:87)(cid:72)(cid:70)(cid:87)(cid:72)(cid:71) (cid:47)(cid:76)(cid:80)(cid:76)(cid:87)(cid:72)(cid:71) (cid:38)(cid:85)(cid:82)(cid:90)(cid:71)(cid:75)(cid:88)(cid:80)(cid:68)(cid:81)(cid:15)(cid:3)(cid:48)(cid:50)(cid:55)(cid:20)(cid:25)
Figure 1. Uniﬁed transformer tracker for both single object tracking (SOT) and multiple object tracking (MOT) task. The target box in SOT is speciﬁed in the ﬁrst frame while all boxes in reference frames are from the detection model in MOT. We use one tracking model to predict target localization in tracking frames for both tasks. objects could appear or disappear [53,55]. Current methods in the tracking community solve individual tasks separately by training models on the individual datasets for either SOT or MOT.
Siamese architecture is widely applied in SOT where various designs focus on improving the discriminative rep-resentation of objects [10,13,25,49]. For MOT, tracking by detection is the most popular paradigm and achieves high-est tracking performance on several benchmarks [2, 37, 55].
This paradigm is not applicable for SOT as the model would fail to detect objects of unseen categories in SOT. Some
MOT methods [24, 60] use the Siamese tracker in SOT [4] to predict the location of the targets in tracking frames and fuse the predicted boxes with the detection boxes to enhance the detection results. However, these methods are not com-petitive to the tracking-by-detection methods in MOT. Al-beit the Siamese trackers have been applied in both tracking tasks, none of these works are able to address two tracking
tasks with a uniﬁed paradigm. In practice, a uniﬁed tracking system is signiﬁcant in many ﬁelds. For the AR/VR appli-cations, tracking speciﬁc or unseen instances like personal cups is related to SOT while perceiving the environment of general classes like people is related to MOT. It is expen-sive and inefﬁcient to maintain two separate tracking sys-tems. The uniﬁed tracking system, which can easily switch tracking mode by demands, becomes more essential in real world deployment.
UniTrack [45] ﬁrstly attempts to address SOT and MOT concurrently by sharing the backbone and fusing multiple tracking heads. However, It fails to exploit large scale track-ing datasets for training due to the head design and the di-vergent training datasets in different tasks. As shown in the
Fig. 1, the training data in SOT and MOT are from vari-ous sources. Datasets in SOT only provide annotations of a single target in one video while dense object annotations are available in MOT datasets although the object categories are
ﬁxed. The tracking capacity of UniTrack [45] is thus lim-ited, and the model would fail to track objects in complex scenarios.
In this paper, we introduce a Uniﬁed Transformer
Tracker (UTT) as shown in Fig. 1 for solving both tracking tasks. For the tracking object in the reference frame, which is speciﬁed in SOT or detected in MOT, we provide a small feature map proposal in the tracking frame based on the pre-vious localization. The target feature then correlates with the feature map proposal to update target representation and output the target localization. This enables our UTT to track objects in both SOT and MOT with the same design. The updated target features are further correlated with the new search feature proposal, which is cropped based on the pro-duced target localization. This process is repeated several times to reﬁne the localization of tracking targets. To en-able exploiting training samples from both tasks, we alter-natively train the network with datasets in each task. Ex-periments demonstrate that our tracker can be well learned on both tasks. Our main contributions are summarized as follows:
• We propose a novel Uniﬁed Transformer Tracker (UTT) for both single and multiple object tracking. To the best of our knowledge, this is the ﬁrst work that the tracking model is end-to-end trained on both tasks.
• We develop a novel and effective track transformer to localize targets via the correlation between target fea-ture and the tracking frame feature. The target features are well encoded through our transformer design.
• To verify the uniﬁed object tracking capability, we evaluate our UTT on both SOT and MOT bench-marks. Our proposed method achieves comparable performance to the state-of-the-art algorithms not only on LaSOT [16], TrackingNet [32], and GOT-10k [22] in the SOT setting, but also on MOT16 [31] in the
MOT setting. 2.