Abstract
Unsupervised image-to-image translation has gained considerable attention due to recent impressive advances in generative adversarial networks (GANs). This paper presents a simple but effective regularization technique for improving GAN-based image-to-image translation. To gen-erate images with realistic local semantics and structures, we propose an auxiliary self-supervision loss that enforces point-wise consistency of the overlapping region between a pair of patches cropped from a single real image dur-ing training the discriminator of a GAN. Our experiment shows that the proposed dense consistency regularization improves performance substantially on various image-to-image translation scenarios. It also leads to extra perfor-mance gains through the combination with instance-level regularization methods. Furthermore, we verify that the proposed model captures domain-specific characteristics more effectively with only a small fraction of training data. 1.

Introduction
Generative adversarial network (GAN) [8] is an inno-vative framework for generative modeling, i.e., generating images that follow the same distribution as training data.
The performance of the state-of-the-art GAN models de-pends highly on the quality of discriminators, which dis-tinguish real images from fake ones while maintaining the balance with matching generators for the joint optimization.
Since discriminators are prone to overfit the training dataset and often lead to the mode collapse of generated outputs, learning robust discriminators is critical to accomplish high-performance generators.
To this end, self-supervised learning methods have been actively used for regularizing discriminators in the GAN
*These authors contributed equally.
Figure 1. Qualitative comparison of output feature maps given by three different variation of the baseline mode, CUT [26], for an example in the Horse → Zebra dataset. The activations from DCR highlight foreground accurately while suppressing background ef-fectively, which helps the image-to-image translation task focused more on target objects. framework [15, 16, 32, 34]. The goal of the regulariza-tion is to obtain robust representations of images for better discrimination of real and fake images [17]. The existing methods often rely on contrastive learning in an instance-level [3, 15, 16, 32], where a pair of augmented instances from an image are encouraged to have consistent features with respect to predefined global transforms while negative images are optionally considered to achieve better repre-sentation learning in discriminators. However, the regular-ization based only on such global representations may be limited to imposing loose constraints on discriminators and may allow generators to deceive the discriminator despite local structural or semantic inconsistency in output images.
To alleviate the drawback, we propose a dense consis-tency regularization (DCR) approach applicable to the dis-criminator of a GAN. DCR provides stronger constraints to the learned representations given by discriminators through their point-wise consistency between a pair of patches cropped from the same image. Our work is motivated by the hypothesis that image generation requires pixel-level pre-diction [14] and a dense regularization of representations is an effective way to improve the supervision quality of a discriminator. The goal of the proposed dense consistency regularization is to generate images with both semantic con-Figure 2. Illustration of the proposed DCR method. When updating a discriminator, two augmented views are randomly cropped from a single real image. The two views are then processed by the intermediate feature extraction network D0, where D0 is the first part of he discriminator while the remaining part is denoted by D1. Note that D1 is not used in our work. The DCR module is applied to one of the branches, and a stop-gradient operation is employed in the other one. The loss LDCR is given by the similarity between the representations of two branches over the overlapping region Ω while ˜Ω denotes the binary map indicating matching pairs of pixels. sistency and visual harmony in spatial neighborhoods. This is achieved if the discriminator focuses on important fea-tures or regions for image-to-image translation instead of the background, as shown in Figure 1. Our main idea is il-lustrated in Figure 2, where the dense correspondence reg-ularization is imposed on the intermediate layers of the dis-criminator.
We evaluate the proposed approach on various image-to-image translation scenarios such as CycleGAN [37], MU-NIT [13], StarGANv2 [5], CUT [26], and FSeSim [36]. Ac-cording to our experiments on the Horse → Zebra, Winter
→ Summer, Cat → Dog, and AFHQ datasets, the mod-els with DCR consistently improve the FID scores com-pared to the models without DCR, which confirms that DCR indeed captures domain-specific characteristics effectively.
For example, we manage to improve the FID score of Cycle-GAN [37] from 78.2 to 54.4, and that of MUNIT [13] from 102.3 to 59.9 on the Horse → Zebra dataset. Moreover, we also find out that DCR is particularly powerful with a small number of training data. Specifically, StarGANv2 [5] with
DCR achieves the best FID score of 17.15 even if only 10% of a specific domain in the AFHQ dataset is used for train-ing, while the best FID scores of StarGANv2 [5] are 22.63 and 17.86 with 10% and 100% of the examples in AFHQ.
We summarize our contributions as follows:
• We introduce a novel dense consistency regularization technique, referred to as DCR, for the discriminators of
GANs, which facilitates high-fidelity image generation and translation.
• We show that DCR is effective to maintain structural and semantic consistency in the spatial neighborhoods of generated images.
• We empirically demonstrate that DCR achieves out-standing performance in various image-to-image trans-lation scenarios.
In the rest of this paper, we first discuss closely related works to our approach in Section 2, and present our algo-rithm and implementation details in Section 3. Section 4 demonstrates the results from our experiments with their analysis, and Section 5 concludes this paper. 2.