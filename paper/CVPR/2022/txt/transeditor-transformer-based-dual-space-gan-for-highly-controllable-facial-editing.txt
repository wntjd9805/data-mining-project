Abstract
Recent advances like StyleGAN have promoted the growth of controllable facial editing. To address its core challenge of attribute decoupling in a single latent space, attempts have been made to adopt dual-space GAN for bet-ter disentanglement of style and content representations.
Nonetheless, these methods are still incompetent to ob-tain plausible editing results with high controllability, es-pecially for complicated attributes. In this study, we high-light the importance of interaction in a dual-space GAN for more controllable editing. We propose TransEditor, a novel
Transformer-based framework to enhance such interaction.
Besides, we develop a new dual-space editing and inversion strategy to provide additional editing flexibility. Extensive experiments demonstrate the superiority of the proposed framework in image quality and editing capability, suggest-ing the effectiveness of TransEditor for highly controllable facial editing. Code and models are publicly available at https://github.com/BillyXYB/TransEditor.
∗ Two authors have equal contributions, ordered alphabetically. This work was done during an internship at Shanghai AI Laboratory.
1.

Introduction
High-fidelity generative modeling has made a dramatic leap thanks to the progress of Generative Adversarial Net-works (GANs) [13, 22, 23]. These efforts have expedited the advancement in facial editing [14, 16, 40], an important downstream task with many practical applications, circum-venting the cumbersome manual editing processes. Never-theless, highly controllable facial attribute editing remains challenging when applying current approaches [23–25].
The main challenge of highly controllable facial editing lies in the clean disentanglement of attributes in the latent space. For instance, it is expected to maintain the consis-tent face characteristic when manipulating the head pose of a portrait. Representative GAN-based methods [23, 24] ex-plored the single latent space representation and the style modulation technique of generators for better image gen-eration and semantic editing. On another note, some stud-ies [6,39] focus on the local-region editing for specific facial parts in a fine-grained manner. However, under the single latent space design, these works still suffer from the latent entanglement of certain complicated attributes, such as fa-cial pose.
Working towards these challenges, recent studies [4, 25] have presented preliminary attempts that leverage the idea of dual latent spaces for style and content disentanglement within the StyleGAN [22] architecture, achieving plausible semantic separation in each space. However, these attempts may not be adequate to take full advantage of the potential of a dual-space GAN. We empirically observed that these methods fail to obtain decent facial editing results, espe-cially in complicated attributes, as shown in Fig. 6. Altering the pose of a face may produce dissatisfactory artifacts, and a drastic change in hue easily occurs when interpolating the style code of DAT [25] by fixing the content code. Accord-ingly, we conjecture that the underlying cause is the lack of interaction between the two latent spaces, and thus edit-ing one space may distort the other, making the controllable facial editing infeasible.
In this paper, we show that the latent space interaction in a dual-space GAN plays a significant role in facial edit-ing.
Inspired by recent immense success of Transform-ers [5, 9, 20, 27, 38] in vision tasks, we propose a novel
Transformer-based dual-space GAN, named TransEditor, to strengthen the dual latent space interaction. The two latent spaces in our generator, i.e., P-space and Z-space, serve as the initial input feature map of the generator and the style modulation [23] for all layers, respectively. Specif-ically, we propose a Transformer-Based Cross-Space Inter-action module, where we incorporate a cross-space atten-tion mechanism based on the Transformer to enhance the in-teraction between these two spaces, facilitating highly con-trollable facial editing. The design of interaction is non-trivial since potential entanglement might be aggravated via the interaction. We propose to use the P-space as the query and the Z-space as the key and value. Therefore, P-space is only exploited to re-weight the value matrix from Z-space through the Cross-Space Interaction module, hence still being disentangled with the refined code from Z-space.
Thanks to the design of the interaction module, our dual spaces allow flexible editing while maintaining disentangle-ment semantically in an expressive manner, with P-space mainly controlling the structural information and Z-space determining the texture representation. Then, different from previous methods [32–34] that perform editing in a single space, we devise a novel Dual-Space Editing strategy to leverage the controllability brought by TransEditor (Fig. 1).
Further, to enable the real image editing, we also extend existing inversion techniques [31] to fit the proposed dual-space design.
The contributions of this work can be summarized as fol-lows. We propose TransEditor, a novel Transformer-based dual-space GAN for highly controllable facial attribute edit-ing. Through the introduced cross-space attention mech-anism, the two latent spaces can establish meaningful in-teraction in a disentangled manner. Additionally, we de-velop a novel flexible dual-space image editing and inver-sion strategy to leverage the improved controllability pro-vided by TransEditor. Extensive experiments demonstrate the effectiveness of TransEditor in highly controllable and stable facial attribute editing, outperforming state-of-the-art approaches especially for complicated attributes. 2.