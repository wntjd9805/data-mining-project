Abstract
The mainstream paradigm behind continual learning has been to adapt the model parameters to non-stationary data distributions, where catastrophic forgetting is the central challenge. Typical methods rely on a rehearsal buffer or known task identity at test time to retrieve learned knowl-edge and address forgetting, while this work presents a new paradigm for continual learning that aims to train a more succinct memory system without accessing task identity at test time. Our method learns to dynamically prompt (L2P) a pre-trained model to learn tasks sequen-tially under different task transitions.
In our proposed framework, prompts are small learnable parameters, which are maintained in a memory space. The objective is to optimize prompts to instruct the model prediction and ex-plicitly manage task-invariant and task-speciﬁc knowledge while maintaining model plasticity. We conduct comprehen-sive experiments under popular image classiﬁcation bench-marks with different challenging continual learning set-tings, where L2P consistently outperforms prior state-of-the-art methods. Surprisingly, L2P achieves competitive results against rehearsal-based methods even without a re-hearsal buffer and is directly applicable to challenging task-agnostic continual learning.
Source code is available at https://github.com/google- research/l2p. 1.

Introduction
Contrary to ordinary supervised learning that trains on independent and identically distributed (i.i.d.) data, contin-ual learning tackles the problem of training a single model on non-stationary data distributions where different classiﬁ-cation tasks are presented sequentially. However, since the model only has access to the current data in an individual phase of the learning cycle, it is prone to overﬁt on the cur-rently available data and suffers from performance deterio-ration on previous data due to catastrophic forgetting [37].
A major body of work in continual learning follows the
⇤Work done during internship at Google Cloud AI Research.
Figure 1. Overview of the L2P framework. Compared with typical methods that adapt entire or partial model weights to tasks sequen-tially with a rehearsal buffer to avoid forgetting, L2P uses a single backbone model and learns a prompt pool to instruct the model conditionally. Task-speciﬁc knowledge is stored inside a prompt pool, thus a rehearsal buffer is no longer mandatory to mitigate forgetting. L2P automatically selects and updates prompts from the pool in an instance-wise fashion, thus task identity is not re-quired at test time. Notably, our largest prompt space is smaller 224 image. than the size of one 224
⇥ learning paradigm by adapting the entire or partial model weights continually as the data distribution shifts, with a focus on preserving past knowledge [9,34]. Although many types of methods attain good results, there are still critical limitations that need to be addressed. First, motivated by the episodic memory in the hippocampus according to the
Complementary Learning Systems (CLS) theory [23, 36], many state-of-the-art methods [3, 4, 8] rely on a rehearsal buffer to re-train a portion of past examples. However, they suffer from substantial performance deterioration with smaller buffer size [4] and become ineffective when a re-hearsal buffer is not allowed – for example, in real-world scenarios where data privacy matters [54]. This suggests that simply buffering past data and re-train the model may not be the best approach to retrieve past knowledge. With-out accessing a rehearsal buffer, another branch of works
[19, 26, 45] bypass the forgetting issue by assuming known task identity at test time, so that they are able to attach task-independent modules to the shared model for infer-ence. However, knowing task identity at test time restricts practical usage.
The limitations of prior work bring up critical questions (1) Whether the form of in continual learning [13, 16]: episodic memory can go beyond buffering past data to more intelligent and succinct episodic memory system? (2) How
to automatically select relevant knowledge component for arbitrary sample without knowing its task identity?
To answer the ﬁrst question, we draw inspiration from recent advances in prompt-based learning (prompting) [29], a new transfer learning technique in the ﬁeld of natural language processing (NLP). Prompting techniques design model textual inputs with templated or learnable prompt to-kens containing additional task-speciﬁc information, such that the pre-trained language model can process parame-terized inputs in order to perform prompt-speciﬁc predic-tion [25, 27, 53].
Intuitively, prompt-based learning re-formulates learning downstream tasks from directly adapt-ing model weights to designing prompts that “instruct” the model to perform tasks conditionally. A prompt encodes task-speciﬁc knowledge and has the ability to utilize pre-trained frozen models more effectively than ordinary ﬁne-tuning [25, 47]. Thus, it is promising to leverage prompts to learn knowledge, and further store learned knowledge, in the continual learning context.
Nevertheless, it is not clear how to apply prompting to address the aforementioned second question in continual learning directly: On one hand, if we train different prompts for different tasks sequentially, test-time task identity is still required for making predictions using an appropriate task-speciﬁc prompt. On the other hand, as a transfer learning technique, the target of prompting is to make frozen pre-trained models perform well on downstream tasks individ-ually, not sequentially. Therefore, if we instead maintain a single shared prompt for all tasks, the problem of catas-trophic forgetting may still exist (see Section 5.4).
To this end, we propose a new continual learning method called Learning to Prompt for Continual Learning (L2P), which is orthogonal to popular rehearsal-based methods and applicable to practical continual learning scenarios with-out known task identity or boundaries. Figure 1 gives an overview of our method in contrast to typical continual learning methods. L2P leverages the representative features from pre-trained models; however, instead of tuning the pa-rameters during the continual learning process, L2P keeps the pre-trained model untouched, and instead learns a set of prompts that dynamically instruct models to solve corre-sponding tasks. Speciﬁcally, the prompts are structured in a key-value shared memory space called the prompt pool, and we design a query mechanism to dynamically lookup a subset of task-relevant prompts based on the instance-wise input features. The prompt pool, which is optimized jointly with the supervised loss, ensures that shared prompts encode shared knowledge for knowledge transfer, and un-shared prompts encode task-speciﬁc knowledge that help maintain model plasticity. Our design explicitly decou-ples shared and task-speciﬁc knowledge, thus largely reduc-ing the interference between task-speciﬁc knowledge dur-ing optimization, leading to minimal catastrophic forgetting without the necessity of a rehearsal buffer. The instance-wise query mechanism removes the necessity of knowing the task identity or boundaries, enabling the most challeng-ing, yet under-investigated task-agnostic continual learning.
The selected prompts are then prepended to the input em-beddings (Figure 2), which implicitly add task-relevant in-struction to pre-trained models, so that the model recalls the most relevant features to conduct corresponding tasks.
In summary, this work makes the following contributions: 1. We propose L2P, a novel continual learning frame-work based on prompts for continual learning, provid-ing a new mechanism to tackle continual learning chal-lenges through learning a prompt pool memory space, which are served as parameterized “instructions” for pre-trained models to learn tasks sequentially. The method is applicable to handle the most challenging task-agnostic continual learning. 2. We conduct comprehensive experiments to demon-strate the effectiveness of L2P on multiple continual learning benchmarks, including class- and domain-incremental, and task-agnostic settings. The proposed
L2P outperforms previous state-of-the-art methods consistently on all benchmarks. Surprisingly, even when a rehearsal buffer is not used, L2P still achieves competitive results against rehearsal-based methods, which is ideal in real-world scenarios when rehearsal buffer is prohibited. 3. To the best of our knowledge, we are the ﬁrst to intro-duce the idea of prompting in continual learning. We expect that our method provides a different perspective for solving frontier challenges in continual learning. 2.