Abstract
Self-explaining deep models are designed to learn the la-tent concept-based explanations implicitly during training, which eliminates the requirement of any post-hoc explana-tion generation technique.
In this work, we propose one such model that appends an explanation generation mod-ule on top of any basic network and jointly trains the whole module that shows high predictive performance and gen-erates meaningful explanations in terms of concepts. Our training strategy is suitable for unsupervised concept learn-ing with much lesser parameter space requirements com-pared to baseline methods. Our proposed model also has provision for leveraging self-supervision on concepts to ex-tract better explanations. However, with full concept su-pervision, we achieve the best predictive performance com-pared to recently proposed concept-based explainable mod-els. We report both qualitative and quantitative results with our method, which shows better performance than recently proposed concept-based explainability methods. We re-ported exhaustive results with two datasets without ground truth concepts, i.e., CIFAR10, ImageNet, and two datasets with ground truth concepts, i.e., AwA2, CUB-200, to show the effectiveness of our method for both cases. To the best of our knowledge, we are the first ante-hoc explanation gener-ation method to show results with a large-scale dataset such as ImageNet. 1.

Introduction
Recent years have seen an exponentially increasing in-terest in explainability of decisions of Deep Neural Net-work (DNN) models across domains including biometrics, healthcare, autonomous navigation and many more. Exist-ing efforts in computer vision including occlusion-based,
Figure 1. Illustration of the proposed framework. Our framework offers a way to train models that can not only predict but also ex-plain their predictions.
It can be easily integrated with existing backbone networks. Compared to existing techniques, it provides the flexibility to incorporate different forms of supervision (includ-ing weaker forms of supervision like self-supervision) whenever available or feasible. gradient-based and Shapley value-based efforts largely per-form post hoc analysis [23, 27, 36], of an already trained model to identify what a DNN model looked at in an in-put image while making a prediction. While this is useful, the separation of explanation from prediction is not ideal.
When an explanation goes wrong, it is not trivial to under-stand if the explanation method is incorrect, or if the model itself relied on spurious correlations to make a prediction.
This has paved the need for ante hoc methods that jointly learn to explain and predict, and thus learn inherently inter-pretable models.
Efforts on envisioning interpretable learning models by
Rudin [25] and Lipton [18] have stressed on the importance of implicitly interpretable methods over post hoc explana-tions in elaborate terms. In a more recent exposition, Rudin et al [26] identified ten challenges of interpretable machine
learning, which also highlighted the need for placing con-straints into models to learn with better interpretability dur-ing training itself. The last few years have seen a few efforts in ante hoc methods that explain through concepts, which are learned during the training of the DNN itself such as
Self-Explaining Neural Networks [1], Concept Bottleneck
Models [14], Concept-based Model Extraction [12], and
Concept Whitening [2]. Learning concepts during training provides a natural pathway for ante hoc explanations that are global (concepts that are most activated on a dataset or a class) or local (concepts that are most activated for pre-diction on given input image). Existing methods however either require concept-level supervision to train the model
[14], or require a significant number of additional parame-ters in the network [1], which prohibits their use in deeper models more commonly used in practice.
In this work, we propose a new method towards learn-(i) can be ing ante-hoc explanations via concepts, that: added easily to existing backbone classification architec-tures with minimal additional parameters; (ii) can provide explanations for model decisions in terms of concepts for an individual input image or for groups of images; and (iii) can work with different levels of supervision, including no concept-level supervision at all. This is achieved by an ar-chitectural modification added to a backbone network along with additional loss terms that allow such ante hoc learning.
Importantly, we show that our framework allows learning of concepts with no supervision, self-supervision as well as full supervision at a concept-level. An overview of our pro-posed model is shown in fig.1.
Our key contributions in this work can be summarized as follows:
• We propose a simple and effective method that jointly learns to predict and explain (through concepts) in an ante hoc manner (i.e. learning to explain during training itself, as opposed to post hoc explainability methods popularly used today). levels of supervision:
• Our method can learn to explain through concepts with different (i) with no concept-level supervision; (ii) through weak supervision (self-supervised learning of concepts); as well as (iii) with concept-level supervision.
• We perform a comprehensive suite of experiments to study accuracy and explainability of our method on mul-tiple benchmark datasets quantatively and qualitatively, and show ablation studies on different choices made in the method. In this context, we introduce a metric based on concept intervention for ante hoc explainable models such as ours.
• Our method outperforms existing methods on accuracy and explainability metrics, and achieves these results with negligible computational overhead over baseline models with no explanation component. 2.