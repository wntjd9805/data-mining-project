Abstract
Visual recognition is recently learned via either super-vised learning on human-annotated image-label data or language-image contrastive learning with webly-crawled image-text pairs. While supervised learning may result in a more discriminative representation, language-image pretraining shows unprecedented zero-shot recognition ca-pability, largely due to the different properties of data sources and learning objectives.
In this work, we intro-duce a new formulation by combining the two data sources into a common image-text-label space. In this space, we propose a new learning paradigm, called Uniﬁed Con-trastive Learning (UniCL) with a single learning objective to seamlessly prompt the synergy of two data types. Ex-tensive experiments show that our UniCL is an effective way of learning semantically rich yet discriminative repre-sentations, universally for image recognition in zero-shot, linear-probing, fully ﬁnetuning and transfer learning sce-narios. Particularly, it attains gains up to 9.2% and 14.5% in average on zero-shot recognition benchmarks over the language-image contrastive learning and supervised learn-ing methods, respectively. In linear probe setting, it also boosts the performance over the two methods by 7.3% and 3.4%, respectively. Our study also indicates that UniCL stand-alone is a good learner on pure image-label data, rivaling the supervised learning methods across three im-age classiﬁcation datasets and two types of vision back-bones, ResNet and Swin Transformer. Code is available at: https://github.com/microsoft/UniCL. 1.

Introduction
Learning to recognize visual concepts in an image has been a fundamental and long-standing research problem.
Typically, this can be tackled via either supervised learning on human-annotated image-label pairs [10] or contrastive learning on webly-crawed image-text pairs [29, 47]. When
Figure 1. Uniﬁed contrastive learning paradigm in the image-text-label space, which recovers the supervised learning (e.g.,
Cross-Entropy (CE) [46] or Supervised Contrastive Learning (Sup-Con) [30]) on image-label data, and language-image contrastive learning (e.g., CLIP [47] or ALIGN [29]) on image-text data. fueled with clean and large-scale human-annotated image-label data, e.g., ImageNet [10], supervised learning can attain decent visual recognition capacities over the given categories [23, 34, 53] and also powerful transfer learning abilities [14, 32]. Nevertheless, collecting precise image-label data can be a laborious and expensive process, not to say its difﬁculty to scale up to numerous visual concepts1.
On the other hand, language-image contrastive learning has recently emerged as a promising approach by leveraging huge amounts of webly-crawled image-text pairs. These pairs are usually noisy, free-form but cover lots of visual concepts. As demonstrated in CLIP [47] and ALIGN [29], models learned from hundreds of millions of image-text pairs can attain impressive low-shot recognition performance for a wide range of visual understanding scenarios. Though these image-text models show a broad coverage of visual concepts, we ﬁnd in our experiments that they usually lack the strong
*equal contribution 1The largest scale but private JFT-300M covers 18,291 concepts.
discriminative ability required by transfer learning. A natural question is: can we have one model for both discriminative representations and broad visual concept coverage?
In this work, we take the ﬁrst step to answer this question.
We start with a new perspective, illustrated in Fig. 1. Instead of isolating image-label and image-text data, we deﬁne an image-text-label space and show how we can eliminate the boundary between two data types. As shown in Fig. 1 left part, supervised learning [30] on image-label data typically aims at mapping images to discrete labels, and completely ignores the textual concept associated with each label during the training. In contrast, language-image contrastive learn-ing [47] aims at learning a pair of visual and textual encoders to align images and texts as shown in Fig. 1 right part. This learning method implicitly assumes that each image-text pair has a unique label. Comparing these two learning paradigms side by side, we can see that both of them actually reside in the common image-text-label space, which is constructed by mapping each label to a textual concept for supervised learn-ing, and assigning each textual description a unique label for language-image pretraining, as shown in Fig. 1 bottom.
Based on this new perspective, we can simply use a visual encoder and a language encoder to encode the images and texts, and align the visual and textual features with the guide of labels (unique labels for image-text pairs and manual la-bels for image-label data). However, learning from these combined labels cannot be supported in existing supervised learning and language-image contrastive learning paradigms.
For this purpose, we propose a uniﬁed contrastive learning method, called UniCL to seamlessly accommodate both data types for visual-semantic representation learning. It takes images, texts as input and compute the loss with softened targets derived from the labels. With UniCL, we combine image-label and image-text data together to learn discrimina-tive and semantic-rich representations, which are beneﬁcial to a variety of downstream tasks. To summarize, our main contributions are:
• We introduce a new perspective of image-text-label space, which can seamlessly unify the commonly used image-label and image-text data.
• We propose a uniﬁed contrastive learning method called
UniCL in the image-text-label space, that can learn from either of the image-label and image-text data, or both.
• Extensive experiments show that our UniCL can leverage both types of data effectively and achieve superior per-formance universally on standard zero-shot, linear probe, fully-ﬁnetuning and transfer learning settings. 2.