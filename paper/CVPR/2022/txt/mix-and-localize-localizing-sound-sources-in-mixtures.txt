Abstract
We present a method for simultaneously localizing multi-ple sound sources within a visual scene. This task requires a model to both group a sound mixture into individual sources, and to associate them with a visual signal. Our method jointly solves both tasks at once, using a formulation inspired by the contrastive random walk of Jabri et al. We create a graph in which images and separated sounds correspond to nodes, and train a random walker to transition between nodes from different modalities with high return probabil-ity. The transition probabilities for this walk are determined by an audio-visual similarity metric that is learned by our model. We show through experiments with musical instru-ments and human speech that our model can successfully lo-calize multiple sounds, outperforming other self-supervised methods. 1.

Introduction
Humans have the remarkable ability to localize many sounds at once [20]. Existing audio-visual sound localiza-tion methods, by contrast, are generally trained with the assumption that only a single sound source is present at a time, and largely lack mechanisms for grouping the contents of a scene into multiple audio-visual events.
This problem is often addressed through contrastive learn-ing [2–4, 32, 39]. These methods generally produce a single embedding for the audio, representing the sound source, and an embedding for each image patch, representing the sound’s possible locations. They then learn cross-modal correspondences, such that image patches and sounds that co-occur within the same scene are brought close together, and pairings that do not co-occur are pulled apart. Extending this approach to multiple sound sources seemingly requires solving two different problems: separating the sources from a sound mixture, and localizing them within an image.
We propose a simple model that jointly addresses both of these problems. Our model uses cycle consistency to group a scene into sound sources, inspired by the contrastive
* Indicates equal contribution.
Figure 1. Cycle-consistent multi-source localization. Our model jointly learns to separate a sound mixture into sound sources, and to localize these sources within an image. To do this, we use a self-supervised grouping method based on cycle-consistent random walks. We show sound source localization results from our model. random walk [25]. It produces multiple embedding vectors for a sound mixture, each representing a different sound source, and an audio-visual similarity metric that associates them with their corresponding image content. This similarity metric defines the transition probabilities for a random walk on a graph whose nodes correspond to images and predicted sound sources. Our model performs a random walk that transitions from the audio to images, and then back. We learn a similarity metric that maximizes the probability of cycle-consistency (i.e., return probability) for the walk. After training, we create an attention map between an image and each sound source by estimating the similarity score between the audio and visual embeddings.
Obtaining a cycle-consistent walk requires extracting sound sources from the mixture, and associating each one with distinct image content. Consequently, this formula-tion has several advantages over other self-supervised audio-visual localization methods. It separates sounds from a mix-ture, and explicitly groups the scene into discrete sound-making objects. The model is also simple, and can be imple-mented as a straightforward extension of previous contrastive methods.
We evaluate our model on synthetic and real-world sound mixtures containing musical instruments [10, 53] and human speakers [13]. We find that, in comparison to other self-supervised localization methods, our model is more accurate in localizing sounds in multi-source mixtures. 2.