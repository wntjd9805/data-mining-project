Abstract
Sketch-based image manipulation is an interactive image editing task to modify an image based on input sketches from users. Existing methods typically formulate this task as a conditional inpainting problem, which requires users to draw an extra mask indicating the region to modify in addition to sketches. The masked regions are regarded as holes and ﬁlled by an inpainting model conditioned on the sketch. With this formulation, paired training data can be easily obtained by randomly creating masks and extracting edges or contours.
Although this setup simpliﬁes data preparation and model design, it complicates user interaction and discards useful information in masked regions. To this end, we investigate a new paradigm of sketch-based image manipulation: mask-free local image manipulation, which only requires sketch in-puts from users and utilizes the entire original image. Given an image and sketch, our model automatically predicts the target modiﬁcation region and encodes it into a structure agnostic style vector. A generator then synthesizes the new image content based on the style vector and sketch. The manipulated image is ﬁnally produced by blending the gener-ator output into the modiﬁcation region of the original image.
Our model can be trained in a self-supervised fashion by learning the reconstruction of an image region from the style vector and sketch. The proposed method offers simpler and more intuitive user workﬂows for sketch-based image manip-ulation and provides better results than previous approaches.
More results, code and interactive demo will be available at https://zengxianyu.github.io/sketchedit. 1.

Introduction
Recently there have been increasing efforts and demand for building interactive photo editing tools on devices with touch interfaces. Being expressive and easily editable, sketching is one of the most straightforward ways that people illustrate their creative ideas and interact with the apps [2, 10, 25, 44]. Sketch-based image editing is an emerg-ing research topic where the goal is to build models which can manipulate holistic or local structures of an image ac-cording to the user-drawn sketches. It has made a signiﬁcant progress in the last few years with advancements in deep learning and generative models, e.g., GANs [11].
There are two challenges in sketch-based image manip-ulation: (1) The input sketch roughly indicates where to modify, but the precise modiﬁcation region is unknown, and
based image manipulation, which requires only sketch inputs from users while leveraging the entire original image. Our system provides a more straightforward and user friendly interface for image manipulation: to solely sketch directly on top of the original image, as illustrated in Fig. 2 (b).
Moreover, as the system takes the entire image as input, information in the modiﬁcation region can be reserved, re-sulting in more consistent appearance (e.g. color, texture) with the original content in the modiﬁcation region.
For local editing, it is desired to preserve most of the original image content and only modify the relevant image region surrounding the sketch input. Therefore, we ﬁrst predict the modiﬁcation region with a mask estimator and then synthesize new content inside with a generator. The manipulated image is produced by blending the generator output into the original image using the predicted mask.
To encourage the structure of the synthesized content to follow the sketch while only keeping the style of the original content, we encode the modiﬁcation region of an input image into a structure agnostic style vector with a style encoder.
The system can be trained in a self-supervised fashion by learning to reconstruct the target modiﬁcation region based on the style vectors and sketches.
We evaluate our method on multiple datasets, including
CelebAHQ, Places2, and newly constructed datasets Sketch-Face and SketchImg containing sketches and masks drawn by users. Extensive experiments demonstrate that the pro-posed method outperforms the state-of-the-art approaches.
To show the advantage of our new framework in terms of user interaction, we include an interactive demo in the sup-plementary material. Our contributions are as follow,
• Investigation of a new paradigm of sketch-based image manipulation: mask-free local image manipulation that requires only partial sketch inputs from users.
• The ﬁrst system for mask-free sketch-based local image manipulation, including the network architecture, data acquisition, and training strategy.
• Extensive experiments are conducted on multiple datasets to demonstrate the superiority of our approach over the related methods. 2.