Abstract
In this paper, we address the problem of texture rep-resentation for 3D shapes for the challenging and under-explored tasks of texture transfer and synthesis. Previous works either apply spherical texture maps which may lead to large distortions, or use continuous texture fields that yield smooth outputs lacking details. We argue that the tra-ditional way of representing textures with images and link-ing them to a 3D mesh via UV mapping is more desirable, since synthesizing 2D images is a well-studied problem. We propose AUV-Net which learns to embed 3D surfaces into a 2D aligned UV space, by mapping the corresponding se-mantic parts of different 3D shapes to the same location in the UV space. As a result, textures are aligned across objects, and can thus be easily synthesized by generative models of images. Texture alignment is learned in an un-supervised manner by a simple yet effective texture align-ment module, taking inspiration from traditional works on linear subspace learning. The learned UV mapping and aligned texture representations enable a variety of applica-tions including texture transfer, texture synthesis, and tex-tured single view 3D reconstruction. We conduct experi-ments on multiple datasets to demonstrate the effectiveness of our method. 1.

Introduction
The field of 3D shape reconstruction and synthesis has witnessed significant advancements in the past few years.
By utilizing the power of deep learning, several works re-construct 3D shapes from voxels, point clouds, single and multi-view images, with a variety of output shape repre-sentations [12, 14, 15, 20, 24, 30, 51]. 3D generative mod-els have also been proposed to synthesize new shapes
[4, 11, 19, 34, 39], with the aim of democratizing 3D con-tent creation. However, despite the importance of textures in bringing 3D shapes to life, very few methods have tackled semantic-aware texture transfer or synthesis for 3D shapes
[5, 10, 18, 22, 35, 36, 50].
Previous work on texture generation mostly relies on warping a spherical mesh template to the target shape
[5, 10, 22, 35], therefore obtaining a texture map defined on
Figure 1. AUV-Net learns aligned UV maps for a set of 3D shapes, enabling us to easily transfer textures between shapes. the sphereâ€™s surface, which can be re-projected into a square image for the goal of texture synthesis. NeuTex [47] gen-erates 3D shapes with a neural implicit representation for arbitrary surface topology, yet embeds the surface of the shape onto a sphere, which also results in a spherical tex-ture map. Spherical texture maps can only support limited topology, and may introduce severe distortions for thin parts such as animal limbs [27, 43]. Another line of work uses implicit texture fields for texture synthesis [32], without re-lying on explicit texture mapping. Although texture fields were successfully applied to multi-view image reconstruc-tion [31], they have primarily been used for fitting a sin-gle object or scene. Generative models usually suffer from overly smoothed synthesized textures [37, 48].
In contrast, the traditional UV mapping in computer graphics handles arbitrary shape topology and avoids heavy distortions by cutting the surface into pieces and mapping different pieces to different regions on the 2D UV plane. It further preserves texture details by storing the texture in a
high-resolution texture image. However, the UV mappings are usually created by 3D artists, and thus are inconsistent across different shapes. Therefore, using such representa-tion for texture synthesis and transfer would require dense shape correspondences.
In this paper, we propose to train a neural network to pre-dict the UV mapping and the texture image jointly, aiming at high-quality texture transfer and synthesis without need-ing to conform to a pre-defined shape topology. Specifi-cally, our network learns to embed 3D coordinates on mesh surfaces into a 2D aligned UV space, where correspond-ing parts of different 3D shapes are mapped to the same locations in the texture image, as shown in Figure 1. Such alignment is enabled by a simple yet effective texture align-ment module inspired by traditional linear subspace learn-ing methods such as Principal Component Analysis (PCA), as shown in Figure 3. The network generates a basis shared by all shape textures, and predicts input-specific coefficients to construct the texture image for each shape as a linear combination of the basis images. This forces the texture images to be aligned so that they can be effectively decom-posed into combinations of basis images, as visualized in
Figure 2. Afterwards, the network reconstructs the colors of the input shape by learning a UV mapping to index the aligned texture image. To unwrap 3D shapes of complex structure or topology, we further introduce a masking net-work that cuts the shape into multiple pieces to reduce the distortion in the UV mapping.
Our method effectively aligns textures across all shapes, allowing us to swap textures between different objects, by simply replacing the texture image from one object with an-other. The aligned high-quality texture images produced by our method make it significantly easier to train generative models of textures, since they are aligned and disentangled from geometry. They also enable textured 3D shape recon-struction from single images. We perform extensive exper-iments on multiple categories including human heads, hu-man bodies, mammals, cars, and chairs, to demonstrate the efficacy of our approach. 2.