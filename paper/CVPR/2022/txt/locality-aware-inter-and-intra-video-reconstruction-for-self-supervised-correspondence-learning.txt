Abstract
Our target is to learn visual correspondence from unla-beled videos. We develop LIIR, a locality-aware inter-and intra-video reconstruction method that ﬁlls in three miss-ing pieces, i.e., instance discrimination, location awareness, and spatial compactness, of self-supervised correspondence learning puzzle. First, instead of most existing efforts focu-sing on intra-video self-supervision only, we exploit cross-video afﬁnities as extra negative samples within a uniﬁed, inter-and intra-video reconstruction scheme.This enables in-stance discriminative representation learning by contrasting desired intra-video pixel association against negative inter-video correspondence. Second, we merge position informa-tion into correspondence matching, and design a position shifting strategy to remove the side-effect of position encod-ing during inter-video afﬁnity computation, making our LIIR location-sensitive. Third, to make full use of the spatial conti-nuity nature of video data, we impose a compactness-based constraint on correspondence matching, yielding more spar-se and reliable solutions. The learned representation sur-passes self-supervised state-of-the-arts on label propaga-tion tasks including objects, semantic parts, and keypoints. 1.

Introduction
As a fundamental problem in computer vision, correspon-dence matching facilitates many applications, such as scene understanding [71], object dynamics modeling [27], and 3D reconstruction [19]. However, supervising representation for visual correspondence is not trivial, as obtaining pixel-level manual annotations is costly, and sometimes even prohibi-tive (due to occlusions and free-form object deformations).
Although synthetic data would serve an alternative in some low-level visual correspondence tasks (e.g., optical ﬂow es-timation [2]), they limit the generalization to real scenes.
Using natural videos as a source of free supervision, i.e.,
*Work done during an internship at Baidu Research.
†Corresponding author: Wenguan Wang.
[10]
[50]
[61]
[55]
[28]
[100]
[75]
[45]
[60]
[44]
[36]
[43]
[82]
LIIR (ours)
[34]
[31]
[98]
[39]
[87] 0
[3]
[54]
[94]
Figure 1. Performance comparison over DAVIS17 val. Our LIIR surpasses all existing self-supervised methods, and is on par with many fully-supervised ones trained with massive annotations. self-supervised temporal correspondence learning, is consi-dered as appealing [40]. This is because videos contain rich realistic appearance and shape variations with almost inﬁ-nite supply, and deliver valuable supervisory signals from the intrinsic coherence, i.e., correlations among frames.
Along this direction, existing solutions are typically built upon a reconstruction scheme (i.e., each pixel from a ‘query’ frame is reconstructed by ﬁnding and assembling relevant pixels from adjacent frame(s)) [39, 40, 84], and/or adopt a cycle-consistent tracking paradigm (i.e., pixels/patches are encouraged to fall into the same location after one cycle of forward and backward tracking) [31, 42, 52, 86, 90].
Unfortunately, these successful approaches largely ne-glect three crucial abilities for robust temporal correspon-dence learning, namely instance discrimination, location awareness, and spatial compactness. First, many of them share a narrow view that only considers intra-video context for correspondence learning. As it is hard to derive a free signal from a single video for identifying different object instances, the learned features are inevitably less instance-discriminative. Second, existing methods are typically built without explicit position representation. Such design seems
Figure 2. Illustration of different self-supervised architectures for temporal correspondence learning (§2): (a) reconstruction based, (b) cycle-consistency based, and (c) our LIIR that addresses instance discrimination, location awareness, and spatial compactness. counter-intuitive, given the extensive evidence that spatial position is encoded in human visual system [25] and plays a vital role when human track objects [62]. Third, as the visual world is continuous and smoothly-varying, both spa-tial and temporal coherence naturally exist in videos. While numerous strategies are raised to address smoothness on the time axis, far less attention has been paid to the spatial case.
To ﬁll in these three missing pieces to the puzzle of self-supervised correspondence learning, we present a locality-aware inter-and intra-video reconstruction framework – LIIR.
First, we augment existing intra-video analysis based corre-spondence learning strategy with inter-video context, which is informative for instance-level separation. This leads to an inter-and intra-video reconstruction based training objective, that inspires intra-video positive correspondence matching, but penalizes unreliable pixel associations within and cross videos. We empirically verify that our inter-and intra-video reconstruction strategy can yield more discriminative fea-tures, that encode higher-level semantics beyond low-level intra-instance invariance modeled by previous algorithms.
Second, to make our LIIR more location-sensitive, we learn to encode position information into the representation. Al-though position bias is favored for intra-video correspon-dence matching, it is undesired in the inter-video case. We thus devise a position shifting strategy to foster the strength and circumvent the weaknesses of position encoding. We ex-perimentally show that, explicit position embedding beneﬁts correspondence matching. Third, we involve a spatial com-pactness prior in intra-video pixel-wise afﬁnity estimation, resulting in sparse yet compact associations. For each query pixel, the distribution of related pixels is ﬁt by a mixture of Gaussians. This enforces each query pixel to match only a handful of spatially close pixels in an adjacent frame. Our experiments show that such compactness prior not only reg-ularizes training, but also removes outliers during inference.
These three contributions together make LIIR a power-ful framework for self-supervised correspondence learning.
Without any adaptation, the learned representation is effec-tive for various correspondence-related tasks, i.e., video ob-ject segmentation, semantic part propagation, pose tracking.
On these tasks, LIIR consistently outperforms unsupervised state-of-the-arts and is comparable to, or even better than, some task-speciﬁc fully-supervised methods (e.g., Fig. 1). 2.