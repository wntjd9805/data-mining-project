Abstract
The limited availability of annotated data often hinders real-world applications of machine learning. To efficiently learn from small quantities of multimodal data, we lever-age the linguistic knowledge from a large pre-trained lan-guage model (PLM) and quickly adapt it to new domains of image captioning. To effectively utilize a pretrained model, it is critical to balance the visual input and prior linguistic knowledge from pretraining. We propose Visu-alGPT, which employs a novel self-resurrecting encoder-decoder attention mechanism to quickly adapt the PLM with a small amount of in-domain image-text data. The proposed self-resurrecting activation unit produces sparse activations that prevent accidental overwriting of linguis-tic knowledge. When trained on 0.1%, 0.5% and 1% of the respective training sets, VisualGPT surpasses the best baseline by up to 10.0% CIDEr on MS COCO [43] and 17.9% CIDEr on Conceptual Captions [63]. Furthermore,
VisualGPT achieves the state-of-the-art result on IU X-ray
[15], a medical report generation dataset. Our code is available at https://github.com/Vision-CAIR/
VisualGPT. 1.

Introduction
Recent performance gains in image captioning [13, 24, 28, 31, 75] are achieved on top of large-scale data corpora such as MS COCO [43] or Conceptual Captions [63], each containing hundreds of thousands of captions. Manual an-notation of captions requires considerable time and effort.
On the other hand, semi-automatic collection of image-caption pairs from the Internet, as used by Conceptual Cap-tions [63], may generate incorrect or undesirable training data even after multiple rounds of cleaning. Data for spe-cialized domains like medical report generation [15,40] and low-resource language captioning [18, 74] cannot be easily
Figure 1. Our VisualGPT model transfers the knowledge from a pre-trained language model to the caption decoder. A self-resurrecting encoder-decoder attention is designed to connect the multi-level visual features and caption decoder. scaled. Improving the data efficiency of image captioning networks would enable quick data curation, description of rare objects, and applications in specialized domains.
In this paper, we investigate the data efficiency prob-lem for image captioning. This problem is distinct from the novel object captioning problem [1, 23], which relies on abundant in-domain data but zero out-of-domain data. In-stead, we aim to improve the performance of image caption-ing systems trained on a small subset of in-domain data.
We propose to improve data efficiency by leveraging pre-trained language models (PLMs) [17, 34, 46, 60], such as
BERT [16], XLNet [77], and GPT [6, 58, 59]. Via self-supervised learning, these models acquire rich linguistic and semantic knowledge, which has been shown to inform downstream tasks in NLP [7, 21]. However, the adapta-tion of PLMs pretrained on unimodal textual data for mul-timodal tasks remain under-investigated.
image captioning. To our knowledge, this is the first work that focuses on efficiently adapting large pre-trained language models for image captioning.
â€¢ We propose a novel encoder-decoder attention with self-resurrecting activation units (SRAUs), which can balance features from the visual and textual modalities.
SRAU produces sparse activations that reduce acciden-tal overwriting of pretrained weights. 2.