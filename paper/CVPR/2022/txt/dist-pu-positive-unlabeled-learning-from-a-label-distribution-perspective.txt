Abstract
Positive-Unlabeled (PU) learning tries to learn binary classifiers from a few labeled positive examples with many unlabeled ones. Compared with ordinary semi-supervised learning, this task is much more challenging due to the ab-sence of any known negative labels. While existing cost-sensitive-based methods have achieved state-of-the-art per-formances, they explicitly minimize the risk of classifying unlabeled data as negative samples, which might result in a negative-prediction preference of the classifier. To allevi-ate this issue, we resort to a label distribution perspective for PU learning in this paper. Noticing that the label dis-tribution of unlabeled data is fixed when the class prior is known, it can be naturally used as learning supervision for the model. Motivated by this, we propose to pursue the la-bel distribution consistency between predicted and ground-truth label distributions, which is formulated by aligning their expectations. Moreover, we further adopt the entropy minimization and Mixup regularization to avoid the trivial solution of the label distribution consistency on unlabeled data and mitigate the consequent confirmation bias. Exper-iments on three benchmark datasets validate the effective-ness of the proposed method. 1.

Introduction
With the advent of big data, deep neural networks have attracted extensive attention, since their performance has reached or even surpassed the human level in various tasks
[21, 35, 33]. Specifically, such great success usually relies on supervision by a large amount of labeled data. However, it is hard to obtain intact label information in many real-world applications, even those with only binary options. For example, observed interactions between users and items in
Figure 1. Predicted label distributions of uPU [12], nnPU [27], and Ours on the training data of CIFAR-10 with 5 repeats and
Vehicles as the positive class. Compared with uPU and nnPU, the proposed label distribution alignment scheme ( ˆRlab) rectifies the negative-prediction preference, and achieves a predicted class prior consistent with the ground-truth (black dashed lines). recommendation systems are labeled positives. Since many unconcerned factors like lack of exposure or other coinci-dences could account for missing interactions, we cannot view all the unobserved interactions as negatives. Simi-lar scenarios include Alzheimer’s disease recognition [11], malicious URL detection [45] and particle picking in cryo-electron micrographs [4], where we only have access to a few labeled positives with plenty of unlabeled data. Such a great demand motivates us to learn from positive and unla-beled data, also known as PU learning.
Researchers have developed numerous PU algorithms over the past decades. One prevalent research line would be cost-sensitive PU learning. These methods [12, 14, 27, 38] often assume the availability of class prior and minimize the risk of classifying unlabeled data as negative instances. By reweighting the importance of the positive and the negative risks, they could get an either unbiased or consistent risk estimator for PU learning.
Despite the great success the cost-sensitive methods have 1
achieved, explicitly optimizing the risk that classifies un-labeled data to the negative class would essentially lead a flexible model (e.g., deep neural networks) to overfit, and further result in a negative-prediction preference of the clas-sifier, as illustrated in the left half of Fig. 1. As the train-ing epoch increases, the predicted class prior probability
Pr(ˆy = 1), i.e., the expectation of predicted label distri-bution, tends to decrease from the ground-truth class prior.
On the other hand, we notice that given the class prior, the underlying label distribution of data is immediately deter-mined. Such a label distribution could be a natural super-vision for the model. More specifically, the distribution of the model’s predicted labels is supposed to be consistent with that of ground-truth ones. Following this intuition, we propose the label distribution alignment for PU learn-ing, which aligns the expectations of the predicted and the ground-truth labels to ensure the label distribution consis-tency. In particular, the expectation of predicted labels is es-timated by sigmoid outputs from a deep network, enabling the end-to-end learning of the proposed framework. Com-pared with the cost-sensitive methods, the proposed label distribution alignment scheme could rectify the negative-prediction preference (see Fig. 1).
Nevertheless, merely pursuing the label distribution con-sistency might suffer from a trivial solution that all the pre-dicted scores of unlabeled data are equal to the class prior.
To avoid this issue, we employ the entropy minimization technique, which encourages the model to produce scores much closer to zero or one. Meanwhile, Mixup [44] is fur-ther adopted to alleviate the confirmation bias caused by the model’s overfitting on its early predictions. To summarize, the contributions of this paper are three-fold:
• We propose a PU learning framework based on label dis-tribution alignment called Dist-PU. Different with ex-isting methods, Dist-PU aligns the expectation of the model’s predicted labels with that of ground-truth ones, and thus mitigates the negative-prediction preference. We also present a generalization bound for label distribution alignment as the theoretical guarantee.
• We further incorporate entropy minimization and Mixup to avoid the trivial solution of label distribution alignment and the consequent confirmation bias.
• Extensive experiments are conducted on Fashion-MNIST,
CIFAR-10 and Alzheimer datasets, where Dist-PU out-performs existing state-of-the-art models in most cases. 2.