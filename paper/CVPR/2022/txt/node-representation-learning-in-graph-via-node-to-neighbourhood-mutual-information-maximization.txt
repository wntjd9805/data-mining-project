Abstract
The key towards learning informative node representa-tions in graphs lies in how to gain contextual information from the neighbourhood. In this work, we present a simple-yet-effective self-supervised node representation learning strategy via directly maximizing the mutual information be-tween the hidden representations of nodes and their neigh-bourhood, which can be theoretically justified by its link to graph smoothing. Following InfoNCE, our framework is optimized via a surrogate contrastive loss, where the pos-itive selection underpins the quality and efficiency of rep-resentation learning. To this end, we propose a topology-aware positive sampling strategy, which samples positives from the neighbourhood by considering the structural de-pendencies between nodes and thus enables positive selec-tion upfront. In the extreme case when only one positive is sampled, we fully avoid expensive neighbourhood aggrega-tion. Our methods achieve promising performance on vari-ous node classification datasets. It is also worth mentioning by applying our loss function to MLP based node encoders, our methods can be orders of faster than existing solutions.
Our codes and supplementary materials are available at https://github.com/dongwei156/n2n. 1.

Introduction
Graph-structured data is ubiquitous because almost noth-ing in the world exists in isolation. As an effective graph modeling tool, Graph Neural Networks (GNNs) have gained increasing popularity in a wide range of domains such as computer vision [6, 26, 29], natural language pro-cessing [35], knowledge representation [9], social net-works [15], and molecular property prediction [33], just name a few.
*Wei Dong’s email is dw156@mail.nwpu.edu.cn. Corresponding authors: Peng Wang (pengw@uow.edu.au) and Junsheng Wu (wujun-sheng@nwpu.edu.cn)
In this work, we focus on the node classification task in graphs where the key is to learn informative structure-aware node representations by gaining contex-tual information from the neighbourhood. This motivates a massive proliferation of message passing techniques in
GNNs. Among these methods, a dominant idea follows an AGGREGATION-COMBINE-PREDICTION pipeline, where AGGREGATION step aggregates the neighbour-ing information into vectorized representations via various neighbourhood aggregators such as mean [15], max [15], at-tention [31], and ensemble [8], which are COMBINED with the node representations via sum or concatenation to real-ize neighbourhood information fusion. To model multi-hop message passing, the AGGREGATION and COMBINE op-erations tend to be repeated before the ultimate node repre-sentations are obtained for label prediction. In other words, the information exchange in this pipeline is driven by a node classification loss in the PREDICTION phase. Under the umbrella of supervised learning, this line of methods uni-fies node representation learning and classification. A po-tential problem is that they may suffer from the scalability issue due to the expensive labeling cost for large-scale graph data.
As a remedy to expensive human annotation, Self-Supervised Learning (SSL) has shown proven success in computer vision [4, 5] and natural language processing [2, 12], which, however, is less sufficiently explored in graph modeling. The key challenge thereof lies in how to design suitable pretext task from non-Euclidean graph-structured data to learn informative node representations. Inheriting the idea from computer vision, many recent attempts ap-proach graph-based SSL by designing topological graph augmentations to generate multi-view graphs for contrastive loss. Off-the-shelf GNNs [20, 37] tend to be used as default options for node/graph encoding.
In this work, we propose a simple-yet-effective SSL alternative to learn node representations by adopting aggregation-free Multi-layer Perceptron (MLP) as node en-coder and directly maximizing the mutual information be-tween the hidden representations of nodes and their neigh-bourhood. Intuitively, by aligning the node representation to the neighbourhood representation, we can distill useful contextual information from the surrounding, analogous to knowledge distillation [7, 23]. Theoretically, the proposed
Node-to-Neighbourhood (N2N) mutual information max-imization essentially encourages graph smoothing based on a quantifiable graph smoothness metric. Following In-foNCE [22], the mutual information can be optimized by a surrogate contrastive loss, where the key boils down to positive sample definition and selection.
To further improve the efficiency and scalability of our
N2N network as well as ensuring the quality of selected positives, we propose a Topology-Aware Positive Sampling (TAPS) strategy, which samples positives for a node from the neighbourhood by considering the structural dependen-cies between nodes. This enables us to select the positives upfront. In the extreme case when only one positive is used for contrastive learning, we can avoid the time-consuming neighbourhood aggregation step, but still achieve promising node classification performance.
We conduct experiments on six graph-based node clas-sification datasets and the results show the promising po-tential of the proposed node representation strategy. The contributions of this work can be summarized as follows:
• We propose a simple-yet-effective MLP-based self-supervised node representation learning strategy with the idea of maximizing the mutual information be-tween nodes and surrounding neighbourhood. We re-veal our N2N mutual information maximization strat-egy essentially encourages graph smoothing by resort-ing to a quantifiable smoothness metric.
• The mutual information maximization problem is opti-mized by a surrogate contrastive loss. A scalable TAPS strategy is proposed which enables the positives to be selected upfront. In the case when only one positive is considered, we avoid expensive neighbourhood aggre-gation but still obtain satisfactory node classification performance.
• Experiments on six graph-based node classification datasets show our methods not only achieve compet-itive performance but have other appealing properties as well. For example, our methods can be orders of faster than existing solutions. 2.