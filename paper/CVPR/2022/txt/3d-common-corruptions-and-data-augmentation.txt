Abstract
We introduce a set of image transformations that can be used as corruptions to evaluate the robustness of models as well as data augmentation mechanisms for training neural networks. The primary distinction of the proposed transfor-mations is that, unlike existing approaches such as Common
Corruptions [27], the geometry of the scene is incorporated in the transformations – thus leading to corruptions that are more likely to occur in the real world. We also introduce a set of semantic corruptions (e.g. natural object occlusions.
See Fig. 1).
We show these transformations are ‘efﬁcient’ (can be computed on-the-ﬂy), ‘extendable’ (can be applied on most image datasets), expose vulnerability of existing models, and can effectively make models more robust when em-ployed as ‘3D data augmentation’ mechanisms. The evalu-ations on several tasks and datasets suggest incorporating 3D information into benchmarking and training opens up a promising direction for robustness research. 1.

Introduction
Computer vision models deployed in the real world will encounter naturally occurring distribution shifts from their training data. These shifts range from lower-level distor-tions, such as motion blur and illumination changes, to semantic ones, like object occlusion. Each of them rep-resents a possible failure mode of a model and has been frequently shown to result in profoundly unreliable predic-tions [15, 23, 27, 31, 66]. Thus, a systematic testing of vul-nerabilities to these shifts is critical before deploying these models in the real world.
This work presents a set of distribution shifts in order to test models’ robustness. In contrast to previously proposed shifts which perform uniform 2D modiﬁcations over the im-age, such as Common Corruptions (2DCC) [27], our shifts incorporate 3D information to generate corruptions that are consistent with the scene geometry. This leads to shifts that are more likely to occur in the real world (See Fig. 1). The resulting set includes 20 corruptions, each representing a
Figure 1. Using 3D information to generate real-world corruptions.
The top row shows sample 2D corruptions applied uniformly over the im-age, e.g. as in Common Corruptions [27], disregarding 3D information.
This leads to corruptions that are unlikely to happen in the real world, e.g. having the same motion blur over the entire image irrespective of the dis-tance to camera (top left). Middle row shows their 3D counterparts from 3D Common Corruptions (3DCC). The circled regions highlight the effect of incorporating 3D information. More speciﬁcally, in 3DCC, 1. motion blur has a motion parallax effect where objects further away from the cam-era seem to move less, 2. defocus blur has a depth of ﬁeld effect, akin to a large aperture effect in real cameras, where certain regions of the im-age can be selected to be in focus, 3. lighting takes the scene geometry into account when illuminating the scene and casts shadows on objects, 4. fog gets denser further away from the camera, 5. occlusions of a tar-get object, e.g. fridge (blue mask), are created by changing the camera’s viewpoint and having its view naturally obscured by another object, e.g. the plant (red mask). This is in contrast to its 2D counterpart that randomly discards patches [13]. See the project page for a video version of the ﬁgure. distribution shift from training data, which we denote as 3D
Common Corruptions (3DCC). 3DCC addresses several as-pects of the real world, such as camera motion, weather, occlusions, depth of ﬁeld, and lighting. Figure 2 provides an overview of all corruptions. As shown in Fig. 1, the cor-ruptions in 3DCC are more diverse and realistic compared to 2D-only approaches.
We show in Sec. 5 that the performance of the meth-ods aiming to improve robustness, including those with di-verse data augmentation, reduce drastically under 3DCC.
Furthermore, we observe that the robustness issues exposed by 3DCC well correlate with corruptions generated via pho-torealistic synthesis. Thus, 3DCC can serve as a challeng-Figure 2. The new corruptions. We propose a diverse set of new corruption operations ranging from defocusing (near/far focus) to lighting changes and 3D-semantic ones, e.g. object occlusion. These corruptions are all automatically generated, efﬁcient to compute, and can be applied to most datasets (Sec. 3.3). We show that they expose vulnerabilities in models (Sec. 5.2.1) and are a good approximation of realistic corruptions (Sec. 5.2.3).
A subset of the corruptions marked in the last column are novel and commonly faced in the real world, but are not 3D based. We include them in our benchmark. For occlusion and scale corruptions, the blue and red masks denote the amodal visible and occluded parts of an object, e.g. the fridge. ing testbed for real-world corruptions, especially those that depend on scene geometry.
Motivated by this, our framework also introduces new 3D data augmentations. They take the scene geometry into account, as opposed to 2D augmentations, thus enabling models to build invariances against more realistic corrup-tions. We show in Sec. 5.3 that they signiﬁcantly boost model robustness against such corruptions, including the ones that cannot be addressed by the 2D augmentations.
The proposed corruptions are generated programmati-cally with exposed parameters, enabling ﬁne-grained anal-ysis of robustness, e.g. by continuously increasing the 3D motion blur. They are efﬁcient to compute and can be com-puted on-the-ﬂy during training as data augmentation with a small increase in computational cost. They are also extend-able, i.e. they can be applied to standard vision datasets, e.g. ImageNet [12], that do not come with 3D labels. 2.