Abstract
Implicit neural representations (INRs) have recently emerged as a promising alternative to classical discretized representations of signals. Nevertheless, despite their prac-tical success, we still do not understand how INRs represent signals. We propose a novel unified perspective to theoreti-cally analyse INRs. Leveraging results from harmonic anal-ysis and deep learning theory, we show that most INR fam-ilies are analogous to structured signal dictionaries whose atoms are integer harmonics of the set of initial mapping frequencies. This structure allows INRs to express signals with an exponentially increasing frequency support using a number of parameters that only grows linearly with depth.
We also explore the inductive bias of INRs exploiting recent results about the empirical neural tangent kernel (NTK).
Specifically, we show that the eigenfunctions of the NTK can be seen as dictionary atoms whose inner product with the target signal determines the final performance of their reconstruction. In this regard, we reveal that meta-learning has a reshaping effect on the NTK analogous to dictionary learning, building dictionary atoms as a combination of the examples seen during meta-training. Our results permit to design and tune novel INR architectures, but can also be of interest for the wider deep learning theory community. 1.

Introduction
Implicit neural representations (INRs) have recently emerged as a powerful alternative to classical, discretized, representations of multimedia signals [9, 13, 30, 31, 39, 47, 48,52,53]. In contrast to traditional methods, INRs parame-terize the continuous mapping between coordinates and sig-nal values using neural networks. This allows for an effi-cient and compact representation of signals that can be eas-ily integrated into modern differentiable learning pipelines.
The recent success of INRs in many applications, such as surface representation [47], volume rendering [27,31,40,
Figure 1. Conceptual illustration of our main theoretical contribu-tions: i) Each layer of an INR increases the frequency support of the representation by splitting a signal into higher order harmon-ics. ii) INRs can be interpreted as signal dictionaries whose atoms are the eigenfunctions of their NTK at initialization. 50] or generative modelling [7, 14] can be largely attributed to the development of new periodic representations that can circumvent the spectral bias of standard neural networks.
Indeed, there is ample evidence that the use of periodic rep-resentations [22,31,48,52] can mitigate the bias of standard architectures towards low frequency [43].
Nevertheless, even if INRs have become widely adopted in practice, the theoretical understanding of their principles and properties is rather limited. For example, there is no clear criterion to select between different INR families, their parameters are mostly based on heuristics, and their limita-tions are not well understood. These shortcomings are slow-ing down further research developments. In this work, we therefore take a step back and focus on understanding the mechanisms behind the success of modern INRs, but also their failure modes, in order to develop more informed de-sign strategies. We provide a unified perspective with the aim to answer the following important questions:
*The first two authors contributed equally to this work. 1. What is the expressive power of INRs?
2. How does initialization affect their inductive bias?
Specifically, we first leverage results from harmonic analysis and deep learning theory, and we discover that the expressive power of most INRs is equivalent to that of a structured signal dictionary whose atoms are integer har-monics of the frequencies that define their initial input map-ping (see Fig. 1). This unifies many INR architectures under a single perspective, and can serve to understand them bet-ter and mitigate some of their common problems.
Then, we delve deeper on the inductive bias of INRs. We build upon the foundational work in [52], and exploit recent results in deep learning theory to give a new unifying frame-work to analyse the inductive bias of any INR architecture in terms of its empirical neural tangent kernel (NTK) [20].
In particular, we reveal the existence of a close analogy between the eigenfunctions of the empirical NTK and the atoms of a signal dictionary, and show that the difficulty of learning a signal with an INR is intimately connected to how efficiently it can be encoded by this dictionary.
Finally, we use our novel perspective to explain the role of meta-learning in improving the performance of INRs.
INRs are known to be notoriously inefficient, requiring long training times, and a large sample exposure to achieve good results, especially in 3D settings [17, 18, 45]. However, re-cent works have shown that using meta-learning algorithms to initialize INRs can greatly improve their speed of con-vergence and sample complexity [46, 51]. In this work, we show that meta-learning works as a dictionary learning al-gorithm, transforming the NTK of an INR into a rich signal dictionary whose atoms are formed by combinations of the examples seen during meta-training. This increases the rep-resentation efficiency of the target signals by the NTK [37], thus improving performance and training speed.
In summary, the main contributions of our work are:
• We provide a unified perspective to theoretically ana-lyze the expressive power and inductive bias of INRs.
• We show that the frequency support of INRs grows ex-ponentially with depth, as each layer splits its input into higher order harmonics, demonstrating their effi-ciency in representing wide spectrum signals.
• We use this theory to explain the main failure modes of INRs: imperfect recovery and aliasing.
• We show that the inductive bias of INRs can be charac-terized by the ability of their empirical NTKs to encode different target signals efficiently.
• Finally, we discover that meta-learning greatly in-creases the encoding efficiency of the NTK by con-structing a rich signal dictionary using different com-binations of the meta-training tasks.
Overall, we believe that our findings can impact the fu-ture research in INRs and their applications, and contribute to speeding up the development of new principled algo-rithms in the field. It gives a fresh perspective to understand and alleviate the drawbacks of the current architectures, as well as new intuitions to design better INR algorithms. Fi-nally, our analysis on the effect of meta-learning can also be of broader interest for the deep learning theory community1. 2. Implicit Neural Representations
The goal of an implicit neural representation is to en-code a continuous target signal g : RD → RC using a neural network fθ : RD → RC, parameterized by a set of weights θ ∈ RN , by representing the mapping between input coordinates r ∈ RD, e.g., pixels, and signal values g(r) ∈ RC, e.g., RGB colors. This is achieved minimizing a distortion measure, like mean-squared error, during train-ing using some form of (stochastic) gradient descent (SGD).
The continuous parameterization of INRs allows to store signals at a constant memory cost regardless of the spa-tial resolution, which makes INRs standout for reconstruct-ing high-dimensional signals, such as videos or 3D scenes.
The main challenge for INRs, though, is to reconstruct high frequency details present in most multimedia signals, e.g., textures in images. Classical neural network architectures are well-known for their strong spectral bias towards lower frequencies [43], and this has made them traditionally use-less for implicit representation tasks. Recently, however, few works [47, 52] have come up with different solutions to circumvent the spectral bias of neural networks, allowing faster convergence and greater fidelity of INRs.
In what follows, we provide an overview of the main so-lutions under a unified architecture formulation. Specif-ically, we note that most INR architectures can be de-composed into a mapping function γ : RD → RT fol-lowed by a multilayer perceptron (MLP), with weights
W (ℓ) ∈ RFℓ−1×Fℓ, bias b(ℓ) ∈ RFℓ, and activation func-: R → R, applied elementwise; at each layer tion ρ(ℓ)
ℓ = 1, . . . , L − 1. That is, if we denote by z(ℓ) each layers post activation, most INR architectures compute z(0) = γ(r), z(ℓ) = ρ(ℓ) (cid:16) fθ(r) = W (L)z(L−1) + b(L).
W (ℓ)z(ℓ−1) + b(ℓ)(cid:17) (1)
, ℓ = 1, . . . , L − 1
We now examine the two most popular INR architectures:
In [52], Tancik et al.
Fourier feature networks (FFNs) proposed to use a Fourier mapping γ(r) = sin(Ωr + ϕ), with parameters Ω ∈ RT ×D and ϕ ∈ RT followed by an MLP with ρ(ℓ) = ReLU. Specifically, they showed that initializing Ωi,j ∼ N (0, σ2) with random Fourier fea-tures [44] can modulate the spectral bias of an FFN, with 1Code to reproduce this work: github.com/gortizji/inr dictionaries
larger values of σ biasing these networks towards higher frequencies. Alternative formulations with deterministic initialization, commonly used for neural rendering algo-rithms [31] can be considered as a special category of these networks where the frequencies in Ω are taken to be powers of 2 and the frequencies in ϕ alternate between {0, π/2}.
Sinusoidal representation networks (SIRENs)
In [47],
Sitzmann et al. proposed to use MLP with sinusoidal ac-tivations, i.e., ρ(ℓ) = sin, where the first layer post acti-vation, z(0) = sin (cid:0)ω0(W (0)r + b(0))(cid:1) can be interpreted as γ(r) = sin(Ωr + ϕ). They showed that, by rescaling the parameters at initialization by the constant factor ω0,
SIRENs can also modulate the spectral bias, with larger ω0 biasing these networks towards higher frequencies.
Nonetheless, despite the ample empirical evidence that shows that these architectures are effective at representing natural images or other visual signals, there is little theoret-ical understanding of how they do so. Moreover, since the design of each of these networks is guided by very differ-ent principles, the sheer diversity in the structure of these architectures makes their analysis very involved.
In the next sections, we provide a unified perspective to analyze the expressive power and inductive bias of INRs and show that all modern INRs are intrinsically guided by the same fundamental principles, which let them express a wide range of signals. However, it also makes them prone to the same type of failure modes. Our novel framework can be used to design new principled solutions to address these shortcomings, but also simplify the tuning of current INRs. 3. Expressive Power of INRs
We now provide an integrated analysis of the expressive power of INRs. To that end, we will follow the formulation in Eq. (1), where, to simplify our derivations, we will re-strict ourselves to polynomial activation functions, i.e., non-linearities of the form ρ(x) = (cid:80)K k=0 αkxk. Note that this is a very mild assumption, as all analytic activation func-tions, e.g., sinusoids, can be approximated using polyno-mials with a na¨ıve Taylor expansion; and that even the non-differentiable ReLUs can be effectively approximated using
Chebyshev polynomials [28]. Note, also, that the sequence of coefficients of the polynomial expansion of most activa-tion functions used in practice decays very rapidly [28].
Now, without loss of generality, let D = 1 and con-sider what happens when a single-frequency mapping, i.e.
γ(r) = ejωr, goes through such a polynomial activation:
The output of the activation consists of a linear combina-tion of the integer harmonics of the input frequency, i.e.,
ρ (γ(r)) = ρ (cid:0)ejωr(cid:1) =
K (cid:88) k=0
αkejkωr. (2)
This harmonic expansion is precisely the mechanism that controls the frequency representation in INRs. More gen-erally, the mapping γ(r) acts as a collection of single fre-quency basis, whose spectral support is expanded after each non-linear activation into a collection of higher order har-monics. This particular structure is shared among all FFNs and SIRENs and it gives rise to the following result regard-ing their expressive power, i.e. the class of functions that can be represented with these architectures.
Theorem 1. Let fθ : RD → R be an INR of the form of
Eq. (1) with ρ(ℓ)(z) = (cid:80)K k=0 αkzk for ℓ > 1. Further-more, let Ω = [Ω0, . . . , ΩT −1]⊤ ∈ RT ×D and ϕ ∈ RT denote the matrix of frequencies and vector of phases, re-spectively, used to map the input coordinate r ∈ RD to
γ(r) = sin(Ωr + ϕ). This architecture can only represent functions of the form fθ(r) = (cid:88)
ω′∈H(Ω) cω′ sin (⟨ω′, r⟩ + ϕw′), (3) where
H(Ω) ⊆ (cid:40)T −1 (cid:88) t=0 stΩt (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) st ∈ Z ∧
T −1 (cid:88) t=0 (cid:41)
|st| ≤ K L−1
. (4)
Proof. See Appendix.
Thm. 1 shows that the expressive power of FFNs and
SIRENs is restricted to functions that can be expressed as a linear combination of certain harmonics of the feature mapping γ(r). That is, these architectures have the same expressive power as a structured signal dictionary whose atoms are sinusoids with frequencies equal to sums and dif-ferences of the integer harmonics of the mapping frequen-cies2. Interestingly, an analogous result was also proven for the Multiplicative Filter Networks (MFNs) [15], a proof-of-concept architecture based on a multiplicative connection between layers instead of the usual compositional structure of MLPs. In particular, it can be shown that MFNs, although very different in structure, are also only able to express lin-ear combinations of certain harmonics of their sinusoidal filters [15], which means that they have the same expressive power as FFNs and SIRENs.
Besides this unification, Thm. 1 also highlights that the way all these architectures encode different signals is very similar. Indeed, instead of representing a signal by directly learning the coefficients of the linear combination, which would require to store O(T K L) coefficients cω′; the multi-layer structure of all INRs imposes a certain low rank struc-ture over the coefficients – akin to the sparsity assumption in classical dictionaries [54] – which can greatly save on mem-ory as it only requires to store O(T 2L) parameters. This is better understood through an illustrative example3. 2We will refer to these components as the harmonics of γ(r). 3Similar examples for other architectures can be found in the Appendix.
Example. Let fθ be a three-layer SIREN defined as fθ(r) = w(2)⊤ sin (cid:16)
W (1) sin (Ωr) (cid:17)
, (5) where Ω ∈ RT , W (1) ∈ RF ×T , and w(2) ∈ RF . The output of this network can equivalently be represented as
F −1 (cid:88)
∞ (cid:88) fθ(r) = m=0 s1,...,sT =−∞ cm,s1,...,sT sin (cid:33)
, stωtr (cid:32)T −1 (cid:88) t=0 where cm,s1,...,sT = (cid:33) (cid:17) (cid:16)
W (1) m,t
Jst w(2) m , (cid:32)T −1 (cid:89) t=0 (6) (7) and Js denotes the Bessel function of first kind of order s.
Proof. See Appendix
As we can see, the harmonic expansion introduced by the nested sinusoids of this simple SIREN can be developed into a signal with a very large bandwidth. Indeed, the few coefficients of this network are enough to represent a signal supported by an infinite number of frequency harmonics.
On the other hand, note that composing sinusoids is a common operation in communication theory as it defines the basis of frequency modulation (FM) technology [42].
Interestingly, drawing analogies between FM signals and
SIRENs is a good source of inspiration to intuitively un-derstand how these networks modulate their spectral bias:
Recall that for FM signals, such as sin(β sin(ω0r)), the pa-rameter β controls the bandwidth of the modulation, which is generally limited by the decreasing nature of the Bessel coefficients Jn(β) in n. Increasing β has the effect of ex-panding the spectral support of the modulation, as the argu-ments of the Bessel functions increase.
The analogous phenomenon can be observed in Eq. (6) for this simple SIREN, but can be extended to more general architectures. In general, we see that due to the decreasing nature of the Bessel functions Jst(W (1) m,t), the high order harmonics in Eq. (6) tend to have smaller weights than the lower ones. This specific parameterization acts as an im-plicit bias mechanism, which focuses most of the energy of the output signal in a narrow band around the input frequen-cies Ω. Nevertheless, we can also see that increasing the scale of the coefficients in the inner layer, e.g., W (1), makes the coefficients of higher order terms in Eq. (7) larger, thus increasing the power of the higher order harmonics, and al-lowing the network to learn a wider range of frequencies.
The fact that all modern INRs encode information in a similar way can explain why all these architectures are as powerful, in practice. However, it may also explain why they all suffer from the same failure modes. In Sec. 4, we study these in more detail. h t u r
T d n u o r
G g n i p p a m y c n e u q e r f e l g n i
S
) 1
= 0 f (
) 5
. 0
= 0 f (
N
F
F
) 0 1
=
σ (
Figure 2. Left Image reconstruction with different mappings of the input coordinates. Right: Magnitude of the DFT of the re-construction. The FFN uses random Fourier encodings as de-fined in Sec. 2, and the single frequency mappings correspond to γ(r) = [cos(2πf0r), sin(2πf0r)]T . 4. Failure modes of INRs
We now move on to study of the main failure modes of
INRs. In particular, we will see how the specific harmonic expansion from Thm. 1 can sometimes lead to very recog-nizable artifacts in the learned reconstructions. Specifically, imperfect signal recovery and aliasing. 4.1. Imperfect recovery
One of the main consequences of Thm. 1 is that the set of frequencies that define the base embedding γ(r) com-pletely determines the frequency support of the reconstruc-tion fθ(r).
In this sense, it is fundamental to guarantee that the set H(Ω) permits to properly cover the spectrum of g(r). When this is not the case, the reconstructed repre-sentations can exhibit severe artifacts in the spatial domain stemming from an incorrect choice of fundamental frequen-cies determined by the INR mapping in Eq. (1).
Let us illustrate this phenomenon for FFNs4, but note that other types of architectures, such as SIRENs, also can suffer from spatial artifacts5. To that end, let us take the extreme case of an FFN fθ : R2 → R3, with a deterministic single frequency Fourier encoding γ(r) =
[sin(2πf0r), cos(2πf0r)]⊤, reconstructing an image f :
[−1, 1]2 → R3, from samples in a grid of N × N . 4The details of all our experiments can be found in the Appendix. 5We replicate our experiments for other networks in the Appendix.
Now, note that, in light of Thm. 1, this network can only represent signals with a frequency support in H(Ω) ⊆ {2k ·
πf0|k ∈ Z}, i.e., containing only even multiples of πf0.
This means that if we choose f0 = 1, the discrete Fourier transform (DFT) of the reconstruction will only have non-zero coefficients at frequencies corresponding to 2k · 2π/N , for k = 0, . . . , ⌊(N − 1)/2⌋. This frequency covering is certainly not enough to completely represent images, as it misses all odd multiples of 2π/N .
As shown in Fig. 2, reconstructing an image with such network produces severe artifacts. The learned representa-tion with f0 = 1 is highly distorted. That is, we see mul-tiple displaced versions of the target image imposed over each other. The nature of this artifact is much more clear when we inspect the DFT of the reconstruction, which is supported on a perfect grid in the spectral domain, missing all the values of the spectrum at the odd coefficients.
Strikingly, setting f0 = 0.5 is enough to completely get rid of this type of artifact. Indeed, when f = 0.5 the set
H(Ω) ⊆ {πk|k ∈ Z}, which means that the DFT of the reconstruction can have energy in all spectral coefficients.
Nonetheless, we also observe that the resulting image is quite blurry. As we will see, this is due to the fast de-cay of the polynomial coefficients in Eq. (2) for most ac-tivation functions, including ReLUs [28], which causes the weights of the high frequency harmonics in Eq. (3) to be very small. This phenomenon can be greatly alleviated, however, by increasing the frequency cover of the initial mapping γ(r) = sin(Ωr + ϕ) and sampling Ω ∈ RD×T using Ωi,j ∼ N (0, σ2).
Indeed, using a large T with a large σ can reduce the probability of having a limited rep-resentation of the frequency spectrum of the target signal.
Nevertheless, as we will see in Sec. 4.2, setting σ too large can introduce other problems. 4.2. Aliasing
It has been empirically shown that INRs with high fun-damental frequencies in γ(r) converge faster, and achieve higher performances in the training set [47,52]; even for tar-gets with high frequency details. Nevertheless, it has also been reported that initializing these frequencies too high leads to poor performance outside the exact support of the training set, and produces aliasing artifacts [5]. To the best of our knowledge, this behavior is still poorly understood.
Thm. 1 can, however, shed new light on this phe-nomenon. To that end, it is useful to see INRs as digital-to-analog converters (DAC), since INRs do little more than reconstruct a continuous signal from a set of discrete train-ing samples. Classical sampling theory [36] guarantees that one can reconstruct a bandlimited signal from its samples provided the sampling frequency is above the Nyquist rate.
Nevertheless, it also states that without this prior knowl-edge, the problem of reconstructing a continuous signal 0 0 3
= 0 w 0 3
= 0 w
Sampling frequency fs = 128
Sampling frequency fs = 256
Figure 3. Magnitude of the spectrum of g(r) = sin(2π · 23r) and its SIREN reconstruction trained at fs = 128 Hz. Top row shows
ω0 = 300, and bottom row ω0 = 30. On the left the signals are sampled at fs = 128 Hz and on the right at fs = 256 Hz. from its samples is, in general, ill-posed – there are many continuous functions that can lead to the exact same sam-ples. Since INRs do not have an explicit knowledge of the bandwidth of the target, only their implicit bias can deter-mine which of all these functions they reconstruct.
When the implicit bias does not match the nature of the signals, this can lead to reconstruction artifacts. Take for instance the problem of reconstructing a single-frequency signal g(r) = sin(2π · 23r) using a SIREN (ω0 = 300 rad/s) trained on 128 evenly spaced samples in the range
[0, 1], i.e., sampled with a frequency of fs = 128 Hz. As we can see in Fig. 3, the discrete-time Fourier transform of the reconstruction at the training points perfectly matches the target signal, i.e., the training loss is zero. Surprisingly, though, if one reconstructs the signal on a finer grid, e.g., fs = 256 Hz, which contains coordinates not seen during training, one can see that the spectrum of the reconstruction has an additional peak at 105 Hz that is not present in the target signal. That is, the implicit bias of the network has
“chosen” to reconstruct the signal using an aliased higher frequency component, as it had no way to discard this fea-sible solution.
Interestingly, if one initializes the SIREN using ω0 = 30 rad/s, instead, this aliased copy disappears.
Thm. 1 gives the key to understand this behaviour.
Specifically, note that most non-linearities used in INRs, e.g., ReLU or sin, can be effectively approximated by poly-nomials of small order, or with rapidly decaying coeffi-cients. As a result, even if the frequency support of the
INRs can include harmonics of very high frequencies, theo-retically, those components tend to be weighted with much smaller coefficients in practice. Increasing the value of the fundamental frequencies does help to include higher fre-quency components without relying in very high order har-monics. However, it does so, at the cost of introducing high frequency components with large weights in Eq. (3), thus increasing the chances of yielding aliased reconstructions.
Reconstructing signals at low sampling rates makes the aliased high frequency components in Eq. (3) indistinguish-able from lower frequency components. As we have seen this phenomenon stems from the underspecification [11] of the reconstruction of the reconstruction problem in INRs, which can yield aliasing artifacts when testing at higher sampling rates. Solving this issues is crucial in application where a certain degree of generalization is required from the INRs. Applications such as super-resolution [8, 21] or scene reconstruction [47] cannot rely on pure overfitting, and require INRs to generalize outside of their training sup-port. Overall, we hope that our new insights can support the design of a new generation of INR architectures and algo-rithms that can mitigate this underspecification. 5. Inductive bias of INRs
All our results, so far, have only dealt with expressive power, i.e., the type of functions that can be represented by INRs. However, even if a network can express a sig-nal, it does not mean that it can learn to represent it effi-ciently. MLPs, for instance, are widely known to be univer-sal function approximators [10], but still they have a hard time learning to high frequency functions [43]. To the best of our knowledge, the inductive bias of INRs is a largely un-explored topic. Besides the fact that INRs can circumvent the spectral bias [47,52], little is known of how different de-sign choices influence the learnability of different signals.
In what follows, we will try to narrow this knowledge gap, as we will leverage recent results from deep learning theory to shed new light on the inductive bias of INRs, and how their initialization has a crucial role on what they learn. 5.1. Overview of NTK theory
Studying the inductive bias of deep learning is hard. This is mostly due to the non-linear nature of the mapping be-tween parameters and functions specified by neural net-works. Recent studies, however, have started arguing that studying learnability approximately is much more tractable.
Notably, the neural tangent kernel (NTK) framework [20] proposes to approximate any neural network by its first or-der Taylor decomposition around the initialization θ0, i.e., fθ(r) ≈ fθ0(r) + (θ − θ0)⊤∇θfθ0(r), since using this approximation, the network is reduced to a simple linear predictor defined by the kernel (8)
Θ(r1, r2) = ⟨∇θfθ0(r1), ∇θfθ0 (r2)⟩. (9)
Remarkably, while the understanding of deep learning is still in its infancy, the learning theory of kernels is much more developed [49]. Specifically, it can be shown that us-ing the kernel in Eq. (9), the sample complexity, and op-timization difficulty, of learning a target function g grows proportionally to its kernel norm [6], i.e.,
Figure 4. Average energy concentration of 100 validation im-ages from CelebA on subspaces spanned by the eigenfunctions of the empirical NTK associated to eigenvalues greater than a given threshold. Legend shows the average test PSNR after training to reconstruct those images from 50% randomly selected pixels.
∞ (cid:88)
∥g∥2
Θ = 1
λi
|⟨ϕi, g⟩|2 , (10) i=0 where ⟨ϕi, g⟩ = Er[ϕi(r)g(r)], and {λi, ϕi}∞ i=0 de-note the eigenvalue, eigenfunction pairs of the kernel given by its Mercer’s decomposition, i.e., Θ(r1, r2) = (cid:80)∞ i=0 λiϕi(r1)ϕi(r2). That is, those targets that are more concentrated in the span of the eigenfunctions associated with the largest eigenvalues of the kernel are easier to learn.
Eq. (8) holds with equality only if the neural network fθ is infinitely wide and has a specific structure [1, 20].
For the finite-size neural networks used in practice, it only provides a rough approximation. Fortunately, recent stud-ies have shown that even if finite-size neural networks and their kernel approximations do not have exactly the same dynamics, their sample complexity when learning a target g scales in both cases with its kernel norm [37], which makes
Eq. (10) a good proxy for learnability in deep learning. 5.2. NTK eigenfunctions as dictionary atoms
The fact that the empirical NTK can approximately cap-ture learnability in deep learning leads to a new interpre-tation of INRs: we can view INRs as signal dictionaries whose atoms are given by the eigenfunctions of the NTK at initialization. In this view, the study of the inductive bias of an INR is equivalent to the study of the representation capa-bilities of its NTK dictionary, in the sense that the functions that can be efficiently encoded by this dictionary are the ones that will be easier to learn.
The simplicity of this analogy allows us to investigate phenomena that appear complex otherwise. For example, we can use this perspective to constructively characterize the effect of the parameter ω0 in the inductive bias of a
SIREN, and compare different networks, or initializations.
To that end, we measure the average energy concentration6 of N = 100 validation images {gn}N n=1 from the CelebA 6Details of the experiments can be found in the Appendix.
ReLU
FFN (σ = 1)
FFN (σ = 10)
Learned
Initialization
SIREN (ω0 = 5)
SIREN (ω0 = 30)
SIREN (ω0 = 100)
Figure 5. First eigenfunctions of the empirical NTK of different INRs at initialization. The first six architectures are initialized as described in Sec. 2. The learned initialization row shows the eigenfunctions of a SIREN initialized after meta-learning on 1, 000 training images from the CelebA dataset [25] following the procedure described in [51]. Details of this experiment can be found in the Appendix. dataset [25] on the span of the eigenfunctions of the NTK associated to eigenvalues greater than a given treshold, i.e.,
E(λ) = 1
N
N (cid:88) (cid:88) n=1
λi/λ0≥λ
|⟨ϕi, gn⟩|2
|⟨gn, gn⟩|2. (11)
This metric is intimately connected to the kernel norm in
Eq. (10), and it can give us a convenient perspective of the region of the NTK spectrum that will represent an image.
The results of this procedure applied to different networks are shown in Fig. 4. Remarkably, for very low values of
ω0, most of the energy of these images is concentrated on the eigenfuctions corresponding to small eigenvalues. How-ever, as we increase ω0, the energy concentration gets more skewed towards the eigenfunctions associated with large eigenvalues. Interestingly, after some point (ω0 > 40), the energy profile starts receding to the right, again.
Comparing the energy profiles with the generalization performance of these networks, we observe a clear pat-tern: the more energy is concentrated on the eigenfunc-tions associated with larger eigenvalues, the better the test peak signal-to-noise ratio (PSNR)7. To understand this phe-nomenon, we can inspect the eigenfunctions of the NTK.
As it is shown in Fig. 5, the eigenfunctions of the SIRENs with larger ω0 have higher frequency content. This means that increasing ω0 can have a positive effect in generaliza-tion as it yields a dictionary that better spans the medium-high frequency spectrum of natural images. Increasing ω0 too much, on the other hand, yields atoms with an overly high frequency content that cannot span the space of natural images efficiently, which explains their poor reconstruction performance of these networks.
Overall, we see how interpreting learnability as encoding efficiency of the NTK dictionary is a powerful analogy that 7Correlations with other training metrics are shown in the Appendix. can explain diverse phenomena, and lets us study under a single framework all sorts of INR questions, including those which might not be readily understood from Thm. 1. This is a very powerful tool that we further exploit in Sec. 5.3 to provide novel insights on the role of meta-learning in INRs. 5.3. Meta-learning as dictionary learning
Prior work has shown that a correct initialization is key to ensure a good performance for INRs [47, 52]. In this sense, recent studies [46, 51] have shown that the use of learned initialization, such as the ones obtained from meta-learning algorithms [16], can significantly boost the performance of
INRs. Indeed, initializing with meta-learned weights is one of the most effective remedies against the slow speed of convergence, and high sample complexity of INRs. How-ever, while there has been recently great progress in under-standing traditional forms of deep learning, the role of meta-learning on the inductive bias of deep neural networks re-mains largely overlooked. Interestingly, we now show how using the connections between INRs and signal dictionaries can help us understand meta-learning in general.
To do so, we follow the same experimental protocol as in Sec. 5.2, where instead of computing the eigenfunctions of the NTK at a random initialization point, we linearize the INRs using Eq. (8) at the meta-learned weights, after pre-training on 1, 000 training images from CelebA using model agnostic meta-learning (MAML) [16, 51].
As it is shown in Fig. 4, the meta-learned weights yield an eigenstructure that concentrates most of the energy of the target images on a subspace spanned by the eigenfunctions of the NTK with the largest eigenvalues, with almost no energy concentrated on the eigenfunctions corresponding to smaller eigenvalues. Therefore, training this INR starting from the meta-learned weights, results in a very fast speed of convergence and superior generalization capacity.
As it happened with the role of ω0 in Sec. 5.2, visually in-specting the eigenfunctions of the NTK can help to build an intuition around this phenomenon. In this regard, recall that the CelebA dataset consists of a collection of face images.
Strikingly, as illustrated in Fig. 5, the first eigenfunctions of the meta-learned NTK also look like faces. Clearly, meta-learning has reshaped the NTK so that the eigenfunctions have a large correlation with the target images.
To the best of our knowledge, we are the first to re-port the NTK reshaping behavior of meta-learning, which cannot be obviously explained by first order approximation theories (cf. Eq. (8)). This result is remarkable for deep learning theory, as it helps us undertand the high-order dy-namics of the NTK during training, which remains one of the main open questions of the field. Prior work had ob-served that standard training procedures change the first few eigenfunctions of the NTK so that they look like the target task [4, 23, 37, 38], but our observations in Fig. 4 and Fig. 5 go one step further, and show that meta-learning has the potential to reshape a much larger space of the NTK dic-tionary by combining many tasks together, thus increasing the capacity of the NTK to efficiently encode a full meta-distribution of signals8. In this sense, we believe that that drawing parallels between classical dictionary learning al-gorithms [54] and meta-learning can be a strong abstrac-tion which can simplify the complexity of this problem, thus leading to a promising avenue for future research. Delving deeper in this connection will not only improve our under-standing of meta-learning as a whole, but it can also provide new insights for the design of more efficient INRs by lever-aging data to construct richer dictionaries. 6.