Abstract
Space-time video super-resolution (STVSR) is the task of interpolating videos with both Low Frame Rate (LFR) and
Low Resolution (LR) to produce High-Frame-Rate (HFR) and also High-Resolution (HR) counterparts. The existing methods based on Convolutional Neural Network (CNN) succeed in achieving visually satisfied results while suf-fer from slow inference speed due to their heavy architec-tures. We propose to resolve this issue by using a spatial-temporal transformer that naturally incorporates the spa-tial and temporal super resolution modules into a single model. Unlike CNN-based methods, we do not explic-itly use separated building blocks for temporal interpola-tions and spatial super-resolutions; instead, we only use a single end-to-end transformer architecture. Specifically, a reusable dictionary is built by encoders based on the in-put LFR and LR frames, which is then utilized in the de-coder part to synthesize the HFR and HR frames. Com-pared with the state-of-the-art TMNet [54], our network is 60% smaller (4.5M vs 12.3M parameters) and 80% faster (26.2fps vs 14.3fps on 720 × 576 frames) without sacri-ficing much performance. The source code is available at https://github.com/llmpass/RSTT. 1.

Introduction
Space-time video super-resolution (STVSR) refers to the task of simultaneously increasing spatial and temporal res-olutions of low-frame-rate (LFR) and low-resolution (LR) videos, which appears in a wide variety of multimedia ap-plications such as video compression [36], video stream-ing [51], video conferencing [45] and so on. In the stage of deployment, many of them have stringent requirements for the computational efficiency, and only LFR and LR frames can be transferred in real-time. STVSR becomes a natural
∗Equal contribution. This work was done when Zhicheng Geng was an intern at Applied Sciences Group, Microsoft. † Corresponding author, llmpass@gmail.com.
Figure 1. Performance of RSTT on Vid4 dataset [21] using small (S), medium (M) and large (L) architectures compared to other baseline models. In the top, we plot FPS versus PSNR.
Note that 24 FPS is the standard cinematic frame rate [42].
In the bottom, we plot the number of parameters (in millions) versus
PSNR. We omit the STARnet here since it is significantly larger than others while performs the worst; see Table 1 for details. remedy in this scenario for recovering the high-frame-rate (HFR) and high-resolution (HR) videos. However, the per-formance of existing STVSR approaches are far from being real-time, and a fast method without sacrificing much visual quality is crucial for practical applications.
The success of traditional STVSR approaches usually relies on strong illumination consistency assumptions [29, 39], which can be easily violated in real world dynamic pat-terns. Ever since the era of deep neural network (DNN), convolutional neural network (CNN) exhibits promising results in many video restoration tasks, e.g., video de-noising [44, 8], video inpainting [18, 47], video super-resolution (VSR) [21, 17, 59, 49, 14] and video frame inter-polation (VFI) [16, 1, 6, 33, 32, 30, 31, 11]. One straightfor-ward way to tackle the STVSR problem is that by treating it as a composite task of VFI and VSR one can sequen-tially apply VFI, e.g., SepConv [33], DAIN [1], CDFI [11], and VSR, e.g., DUF [17], RBPN [14], EDVR [49], on the input LFR and LR video. Nevertheless, this simple strat-egy has two major drawbacks: first, it fails to utilize the in-ner connection between the temporal interpolation and spa-tial super-resolution [52, 54]; second, both VFI and VSR models need to extract and utilize features from nearby LR frames, which results in duplication of work. Additionally, such two-stage models usually suffer from slow inference speed due to the large amount of parameters, hence pro-hibits from being deployed on real-time applications.
To alleviate the above problems, recent learning based methods train a single end-to-end model [15, 52, 53, 54], where features are extracted from the input LFR and LR frames only once and then are upsampled in time and space sequentially inside the network. However, researches in this line still consist of two sub-modules: a temporal interpo-lation network, e.g., Deformable ConvLSTM [52, 53] and
Temporal Modulation Block [54], and a spatial reconstruc-tion network, e.g., residual blocks used in both Zooming
SloMo [52, 53] and TMnet [54]. One natural question to ask is that whether we can have a holistic design such that it increases the spatial and temporal resolutions simultane-ously without separating out the two tasks.
In this paper, we propose a single spatial temporal trans-former that incorporates the temporal interpolation and spa-tial super resolution modules for the STVSR task. This ap-proach leads to a much smaller network compared with the existing methods, and is able to achieve a real-time infer-ence speed without sacrificing much performance. Specifi-cally, we make the following contributions:
• We propose a Real-time Spatial Temporal Trans-former (RSTT) to increase the spatial and temporal res-olutions without explicitly modeling it as two sub-tasks.
To the best of our knowledge, this is the first time that a transformer is utilized to solve the STVSR problem.
• Inside RSTT, we design a cascaded UNet-style architec-ture to effectively incorporate all the spatial and temporal information for synthesizing HFR and HR videos. In par-ticular, the encoder part of RSTT builds dictionaries at multi-resolutions, which are then queried in the decoder part for directly reconstructing HFR and HR frames.
• We propose three RSTT models with different number of encoder and decoder pairs, resulting in small (S), medium (M) and larger (L) architectures. Experiments show that
RSTT is significantly smaller and faster than the state-of-the-art STVSR methods while maintains similar per-formance: (i) RSTT-L performs similarly as TMNet [54] with 40% less parameters, RSTT-M outperforms Zoom-ing SlowMo [52] with 50% less parameters and RSTT-S outperforms STARNet [15] with 96% less parameters; (ii)
RSTT-S achieves a frame rate of more than 24 per second (the standard cinematic frame rate) on 720 × 576 frames.
It achieves the performance of Zooming SlowMo [52] with a 75% speedup, and outperforms STARNet [15] with around 700% speedup (see Figure 1). 2.