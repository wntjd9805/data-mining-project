Abstract
We consider the problem of recovering a single person’s 3D human mesh from in-the-wild crowded scenes. While much progress has been in 3D human mesh estimation, ex-isting methods struggle when test input has crowded scenes.
The first reason for the failure is a domain gap between training and testing data. A motion capture dataset, which provides accurate 3D labels for training, lacks crowd data and impedes a network from learning crowded scene-robust image features of a target person. The second reason is a feature processing that spatially averages the feature map of a localized bounding box containing multiple people. Aver-aging the whole feature map makes a target person’s feature indistinguishable from others. We present 3DCrowdNet that firstly explicitly targets in-the-wild crowded scenes and es-timates a robust 3D human mesh by addressing the above issues. First, we leverage 2D human pose estimation that does not require a motion capture dataset with 3D labels for training and does not suffer from the domain gap. Sec-ond, we propose a joint-based regressor that distinguishes a target person’s feature from others. Our joint-based regres-sor preserves the spatial activation of a target by sampling features from the target’s joint locations and regresses hu-man model parameters. As a result, 3DCrowdNet learns target-focused features and effectively excludes the irrele-vant features of nearby persons. We conduct experiments on various benchmarks and prove the robustness of 3DCrowd-Net to the in-the-wild crowded scenes both quantitatively and qualitatively. Codes are available here 1. 1.

Introduction
Extensive research has been committed to reconstructing an accurate 3D human mesh, which represent both the pose and shape of a human, from a single image. However, 3D 1https : / / github . com / hongsukchoi / 3DCrowdNet _
RELEASE (a) Domain gap (occlusion/pose/appearance, etc) (b) Qualitative comparison
Figure 1. 3DCrowdNet resolves (a) a domain gap issue in estimat-ing a 3D human mesh from in-the-wild crowded scenes. Due to the large domain gap between motion capture data and in-the-wild crowd data, (b) existing state-of-the-art methods such as SPIN [19] produce inaccurate results, while 3DCrowdNet gives an accurate 3D human mesh despite severe inter-person occlusion. We conceal a person’s face in this paper to abide by the ethical policy. human mesh estimation from in-the-wild crowded scenes has been barely studied, despite their common presence.
Consequently, most of the previous works show results on scenes without inter-person occlusion and provide inaccu-rate results on crowded scenes. The inter-person occlusion is the essential challenge of in-the-wild crowded scenes, and many practical applications including abnormal behav-ior detection [8] and person re-identification [35] encounter such situations. This paper investigates the limitation of the current literature and proposes a novel method for robust 3D human mesh estimation from in-the-wild crowded scenes.
The currently dominant training strategy for human mesh recovery is mixed-batch training. It composes a mini-batch with one-half data from a motion capture (MoCap) 3D dataset [13, 26] and the other from an in-the-wild 2D dataset [22]. To use the 2D dataset for supervision, 3D joints regressed from a predicted mesh are projected onto the image plane, and the distance with 2D annotations is computed. This way of mixing 3D and 2D data is well known to improve accuracy and generalization [17, 19] by implicitly inducing a neural network to benefit from accu-rate 3D annotations of the 3D data and diverse image ap-pearances in the 2D data. The dominant approach of recent works [5, 9, 19] is a model-based approach using a global feature vector, which obtains the feature vector with a deep convolutional neural network (CNN) and regresses the hu-man model parameters (e.g. SMPL [24]) from it. First, they crop an image using a bounding box of a target person de-tected from off-the-shelf human detectors [10]. Then they process the target’s cropped image with a deep CNN and perform a global average pooling to obtain the global fea-ture vector. The global feature vector is fed to a Multi-Layer
Perceptron (MLP)-based regressor that regresses the mesh parameters. The 3D meshes are obtained by forwarding the parameters to the human model layers.
While the recent works have shown reasonable results on standard benchmarks [13,46] based on the two wheels of the current literature, in-the-wild crowded scenes remain insur-mountable due to the following two reasons. First, a large domain gap between training data from MoCap datasets and testing data from in-the-wild crowded scenes hinders a deep
CNN from extracting proper image features of a target per-son. The domain gap arises from the presence of a human crowd, which entails diverse inter-person occlusion, inter-acting body poses, and indistinguishable cloth appearances (Figure 1a). The mixed-batch training alone is insufficient to overcome the domain gap, and existing methods struggle to acquire robust image features from in-the-wild crowded scenes, and produce inaccurate meshes (Figure 1b). Intu-itively, this tells us that external guidance robust to the do-main gap is required for a crowded scene-robust image fea-ture, in addition to the mixed-batch training.
Next, the global average pooling on a deep CNN fea-ture collapses the spatial information that distinguishes a target person’s feature from others.
In-the-wild crowded scenes often involve overlapping people and inaccurate hu-man bounding boxes. Thus, a bounding box of a target in-evitably includes non-target people. A deep CNN feature retains features of these non-target people, and the global average pooling makes a target person’s feature indistin-guishable from others. This confuses a regressor and makes it difficult to capture an accurate 3D pose of a target person.
For instance, the regressor may miss human parts occluded by another person or predict a different person’s pose.
In this regard, we present 3DCrowdNet, a novel network that learns to estimate a single person’s robust 3D human mesh from in-the-wild crowded scenes. This study is one of the earliest works that explicitly tackle 3D human mesh estimation of a target person in a crowd. 3DCrowdNet ad-dresses the two issues of previous works in two folds. First, we resolve the domain gap by explicitly guiding a deep
CNN to extract a crowded scene-robust image feature using an off-the-shelf 2D pose estimator. Unlike methods target-ing 3D geometry, the 2D pose estimator does not require depth supervision and is not trained on a MoCap dataset.
Instead, it is trained only on in-the-wild datasets [21, 41] that have images containing human crowds and suffers less from a domain gap regarding the inference on crowded scenes. Consequently, the 2D pose estimator’s outputs pro-vide strong evidence of a target person and help 3DCrowd-Net pay attention to a target’s feature despite the challenges in in-the-wild crowded scenes.
Second, we propose a joint-based regressor that does not blow away the spatial activation of a target person in a fea-ture map with the global average pooling. The joint-based regressor first predicts the spatial locations of joints. Then, it samples image features from a deep CNN feature map with the locations. In particular, we keep the sampling area small to exclude features of non-target people. The target person’s feature is distinguished from others, and human model parameters are regressed from the sampled image features. The joint-based regressor differs from the previous regressors that evenly aggregate people’s features regardless of the target. Figure 2 depicts the overview of 3DCrowdNet.
Note that 3DCrowdNet substantially differs from prior works [6, 25] that directly lift 2D estimation outputs to 3D-(a) we focus on producing and leveraging image features of a target person in human crowds, and (b) such image fea-tures help 3DCrowdNet to resolve the depth and shape am-biguity of a target person, from which the 2D estimation outputs inherently suffer. Thus, we argue that this work takes a step towards accurate 3D human mesh estimation from in-the-wild crowded scenes by distinguishing image features of a target person in densely interacting crowds, which is highly challenging but important. The experiments show that 3DCrowdNet significantly outperforms the pre-vious 3D human mesh estimation methods on in-the-wild crowded scenes. Also, it achieves state-of-the-art accuracy in multiple 3D benchmarks [16, 28, 46]. Extensive qualita-tive results are presented in the main manuscript and sup-plementary material. Our contributions can be summarized as follows:
• We present 3DCrowdNet, the first approach to 3D hu-man mesh recovery from in-the-wild crowded scenes.
It effectively processes image features of a target per-son in a crowd, which is essential for accurate 3D pose and shape reconstruction.
• It extracts crowded scene-robust image features by re-solving the domain gap with a 2D pose estimator.
• It distinguishes a target person’s image features from others using a joint-based regressor.
• 3DCrowdNet significantly outperforms previous meth-ods on in-the-wild crowded scenes both quantitatively and qualitatively, and achieves state-of-the-art 3D pose and shape accuracy on multiple 3D benchmarks. 2.