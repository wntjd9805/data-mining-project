Abstract
Positional encodings have enabled recent works to train a single adversarial network that can generate images of different scales. However, these approaches are either lim-ited to a set of discrete scales or struggle to maintain good perceptual quality at the scales for which the model is not trained explicitly. We propose the design of scale-consistent positional encodings invariant to our generator’s layers transformations. This enables the generation of arbitrary-scale images even at scales unseen during training. More-over, we incorporate novel inter-scale augmentations into our pipeline and partial generation training to facilitate the synthesis of consistent images at arbitrary scales. Lastly, we show competitive results for a continuum of scales on various commonly used datasets for image synthesis. 1.

Introduction
Generative adversarial networks (GANs) [9] are the most commonly used paradigm for generating and manipulating images and videos [12,24,25,29,30,35,36]. The promising results obtained by GANs have motivated several applica-tions of computer graphics and visual content generation.
Ideally, a GAN model is not only capable of generating im-ages similar to the training data but also provides the flex-ibility to manipulate and control the generation process for the target application [13, 27]. For instance, a GAN model used for animations and videos should be able to generate objects in different positions, scales, and viewpoints while maintaining consistency over other attributes of the object.
Having a single model that provides control over different object attributes has received substantial attention from the research community [7, 18, 20]. However, most of the ex-isting GAN models are limited to the positional priors of their training data, making them unable to generate unseen translations and scales.
Xu et al. [37] recently revealed that convolutional GANs learn the positional priors of their training data by using the zero paddings in the convolutions as an imperfect and im-plicit positional encoding. Motivated by such discovery, ex-plicit positional encodings have been proposed to make the
GAN models equivariant to different translations, scales,
and resolutions [2,6,32,37]. Positional encodings have cre-ated the possibility of obtaining a single GAN model, that can generate images with different resolutions, as well as different object scales and positions. However, despite the new opportunities brought about by the recent works, the existing methods are still limited to multi-scale generation only in discrete resolutions. They suffer from object incon-sistency between different scales and resolutions.
To address the aforementioned limitations, we aim to extend the task of multi-scale generation, using a single generator, to arbitrary continuous scales. To this end, we first propose a more suitable positional encoding formu-lation. While this leads to arbitrary-scale generation, this strategy alone does not guarantee consistency across scales.
We therefore further propose a means of enforcing consis-tency between different scales and resolutions using inter-scale augmentations in the discriminator. Specifically, we generate images at different scales from the same latent code. Then, pairs of generated images at different scales go through channel-mix and cut-mix augmentations. Finally, the discriminator classifies the augmented images as real or fake. Such an approach encourages the generator to gen-erate scale-consistent images so that the images still look realistic after inter-scale augmentations. Lastly, our method can also generate parts of the image in arbitrary resolutions with scale consistency, as visualized in Figure 1.
To summarize our contributions:
• We design a scale-consistent positional encoding scheme that enables fully convolutional and pad-free generators to generate images of arbitrary scales.
• We introduce a set of inter-scale augmentations that images pushes the generator to create consistent among scales.
• We further facilitate the consistency among arbitrary scales by incorporating partial generation in our train-ing pipeline.
We perform experiments on various commonly used datasets characterized by diverse positional priors. Our re-sults indicate that the introduced pipeline permits the con-sistent generation of images of arbitrary scales while pre-serving high visual quality. 2.