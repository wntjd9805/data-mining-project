Abstract
Novel-view prediction of a scene has many applications.
Existing works mainly focus on generating novel-view im-ages via pixel-wise prediction in the image space, often re-sulting in severe ghosting and blurry artifacts. In this paper, we make the first attempt to explore novel-view prediction in the layout space, and introduce the new problem of novel-view scene layout generation. Given a single scene layout and the camera transformation as inputs, our goal is to generate a plausible scene layout for a specified viewpoint.
Such a problem is challenging as it involves accurate un-derstanding of the 3D geometry and semantics of the scene from as little as a single 2D scene layout. To tackle this challenging problem, we propose a deep model to capture contextualized object representation by explicitly modeling the object context transformation in the scene. The con-textualized object representation is essential in generating geometrically and semantically consistent scene layouts of different views. Experiments show that our model outper-forms several strong baselines on many indoor and outdoor scenes, both qualitatively and quantitatively. We also show that our model enables a wide range of applications, includ-ing novel-view image synthesis, novel-view image editing, and amodal object estimation. 1.

Introduction
Multi-view prediction of a scene is of great importance in 3D scene understanding and has been studied for a long time [6, 10], with potential applications such as robotics,
Virtual Reality (VR), and Augmented Reality (AR). There is a line of research on generating images of a scene from new viewpoints [5, 31, 42]. However, these works render a scene in the image space via pixel-wise prediction directly, often resulting in severe ghosting and blurry artifacts.
In this paper, we take a step towards novel-view pre-diction of a scene in the layout space. We introduce the new problem of novel-view scene layout generation. Hav-ing a robust novel-view scene layout model not only enables
Figure 1. Novel-view scene layout generation. Given a single lay-out of an indoor scene (a) or an outdoor scene (b) as input (1st column), our model can generate plausible novel-view scene lay-outs for different viewpoints (2nd and 3rd columns). generating consistent and sharp scene layouts from different views even with large camera movements, but also provides scene understanding priors for a wide range of applications, including novel-view image synthesis, image editing, and amodal object estimation. As shown in Figure 1, given a single layout of an indoor scene (a) or an outdoor scene (b) as input, our goal is to generate novel-view layouts of the scene for different viewpoints (2nd and 3rd columns).
However, generating plausible novel-view scene layouts is a highly challenging problem. It requires an accurate un-derstanding of the 3D geometry and semantics of the scene from just a single 2D scene layout. The sizes, positions, and shapes of the objects may change a lot in both ob-served regions and unseen regions across different view-points. Hence, this problem is highly under-determined, as a result of the ambiguity of the input single 2D scene layout.
Recall how the human cognitive system works in this task. Consider the input layout of an indoor scene shown in Figure 1 (a). Human beings may use the geometric pri-ors and semantic relations of the objects in the input layout to infer the scene layout of a different viewpoint. The ge-ometric priors contain the common object properties (e.g., the shape of the window), while semantic relations repre-sent interactions among different types of objects to indi-Figure 2. A brief comparison between existing novel-view layout-to-image synthesis methods [12, 14] (left) and the proposed novel-view scene layout generation method (right). Existing works directly map the input layout to the 3D scene representation (e.g., MPI [14] or hybrid representation [12]), and then perform pixel-by-pixel projection in the image space. In contrast, our approach considers spatial and semantic interactions among objects in the layout space without explicit 3D modeling. The novel-view scene layouts generated from the contextualized object representation are crucial for scene understanding and enable a variety of applications. cate how these objects (e.g., bed and table) should be com-posited in the target view. Inspired by this observation, we propose a learning-based model for novel-view scene lay-out generation, by explicitly modeling the object spatial and semantic interactions in the scene. Our approach contains three main stages. First, given an input scene layout, we propose an Object Context Transformation (OCT) module to extract the contextualized object representation that en-codes object shapes, positions, and sizes in the scene. The contextual relationships among objects in the target view are learned via a view-aware attention mechanism. Second, we propose an Object Layout Generation (OLG) module to produce the shape, size, and position for each object in the target view. Finally, we use an Object Layout Composition (OLC) module to composite all the predicted object layouts and generate a plausible novel-view scene layout as output.
To evaluate the effectiveness of our model, we con-duct extensive experiments on numerous indoor and out-door scenes. Results show that our model can generate geometrically and semantically more consistent novel-view scene layouts, compared with the baselines. In addition, we show a wide range of applications of our model, including novel-view image synthesis, novel-view image editing, and amodal object estimation.
In summary, the main contributions of this paper include:
• We make the first attempt to investigate the new prob-lem of novel-view scene layout generation by learning object context in the scene.
• We propose a new model that consists of an OCT mod-ule to capture the contextualized object representation, an OLG module to predict the layouts of individual ob-jects, and an OLC module to composite the predicted object layouts properly to an output scene layout.
• Experimental results demonstrate that our model can generate geometrically and semantically consistent novel-view scene layouts from a single input layout, enabling a wide range of applications. 2.