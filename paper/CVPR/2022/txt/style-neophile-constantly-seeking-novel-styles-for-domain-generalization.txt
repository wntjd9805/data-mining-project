Abstract
This paper studies domain generalization via domain-invariant representation learning. Existing methods in this direction suppose that a domain can be characterized by styles of its images, and train a network using style-augmented data so that the network is not biased to par-ticular style distributions. However, these methods are re-stricted to a finite set of styles since they obtain styles for augmentation from a fixed set of external images or by in-terpolating those of training data. To address this limitation and maximize the benefit of style augmentation, we propose a new method that synthesizes novel styles constantly dur-ing training. Our method manages multiple queues to store styles that have been observed so far, and synthesizes novel styles whose distribution is distinct from the distribution of styles in the queues. The style synthesis process is formu-lated as a monotone submodular optimization, thus can be conducted efficiently by a greedy algorithm. Extensive ex-periments on four public benchmarks demonstrate that the proposed method is capable of achieving state-of-the-art domain generalization performance. 1.

Introduction
Convolutional neural networks (CNNs) have driven re-markable advances in visual recognition for the last decade.
However, their performance is often degraded when train-ing and test data are drawn from different distributions [8, 21, 46]. As such a distribution shift appears frequently and significantly in the wild, it has been a major obstacle to ap-plying CNNs to real-world applications. The most common solution to this issue is domain adaptation [8, 30, 41, 42], which aims at adapting a model trained on source domains to a known target domain. However, domain adaptation models in general do not well generalize to unseen domains since they assume a single target domain.
Domain generalization (DG) [1, 2, 7, 22, 32] has been studied to resolve this limitation of domain adaptation.
The goal of DG is to improve the generalization capa-bility of a model on arbitrary domains unseen at training
Figure 1. The motivation of our method. We improve the gen-eralization ability of a model by adaptively synthesizing diverse, plausible, and novel styles that are distinct from both source do-main styles and previously synthesized novel styles, then injecting them into intermediate features of the model during training for learning style-invariant representation. time. DG has been achieved by learning domain-invariant features [14, 24, 26, 39, 48] that capture semantics relevant to the target task while not being biased towards domain-specific characteristics.
In this context, styles of images have been used to characterize their domains [27, 58]; it has been demonstrated that reducing model bias towards styles could improve the generalization ability [5,33]. As a simple yet effective realization of this idea, style augmentation has been investigated recently [16, 17, 51, 58]. It allows a model to be unbiased to particular style distributions by augment-ing training images with varying styles. Although they have been proven to be effective for domain generalization, how-ever, there is still room for further improvement in terms of the style diversity; existing style augmentation methods ob-tain styles for augmentation from a restricted set of external images [17, 51] or by interpolating styles of source domain images [58], both of which lead to a limited range of styles.
In this paper, we present a novel framework to further enlarge the benefit of style augmentation. The key idea is to constantly generate novel and plausible styles and augment training images with the synthetic styles. Specifically, to
be novel, synthetic styles generated by our method should be distinct not only from styles of source domain images but also from previously generated synthetic styles, as illus-trated in Fig. 1. To be plausible, on the other hand, they should not deviate too much from real image styles.
For efficient style synthesis, our framework begins with sampling a few prototypes that well represent the entire dis-tribution of source image styles. Then the source style pro-totypes and previously synthesized novel styles are used to approximate the distribution of styles that have been ob-served by the model. To synthesize novel styles, we first generate plausible candidates of novel styles by jittering source image styles with random noises, and then sample a subset of such candidates that are diverse and not well represented by the approximate distribution of observed styles. This sampling process is implemented efficiently using (1) style queues that store source image styles and previously synthesized novel styles, and (2) score functions that measure the quality of sampled source style prototypes and novel styles. In particular, we employ monotone sub-modular score functions so that near-optimal prototypes and novel styles can be efficiently estimated by a greedy algo-rithm.
Our method is evaluated and compared with previous work on four public benchmarks for DG: PACS [21], Of-ficeHome [45], and DomainNet [35] for image classifica-tion, and the other for cross-domain person re-ID [28, 54].
Extensive experiments on these benchmarks demonstrate that our method is capable of achieving state-of-the-art do-main generalization performance. The contribution of this paper is three-fold:
• We propose a novel approach to domain generalization that constantly synthesizes novel, diverse, and plausi-ble styles to maximize the generalization effect of style augmentation.
• We present a novel framework based on style queues and submodular optimization for maintaining and gen-erating styles effectively and efficiently.
• Our method outperforms existing DG techniques on four public benchmarks, in particular on those depict-ing large domain discrepancy. 2.