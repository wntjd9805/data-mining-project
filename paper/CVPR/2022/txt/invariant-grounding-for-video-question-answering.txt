Abstract track  - race
I
M
L
Video Question Answering (VideoQA) is the task of an-swering questions about a video. At its core is understand-ing the alignments between visual scenes in video and lin-guistic semantics in question to yield the answer. In lead-ing VideoQA models, the typical learning objective, empiri-cal risk minimization (ERM), latches on superﬁcial corre-lations between video-question pairs and answers as the alignments. However, ERM can be problematic, because it tends to over-exploit the spurious correlations between question-irrelevant scenes and answers, instead of inspect-ing the causal effect of question-critical scenes. As a result, the VideoQA models suffer from unreliable reasoning. answer (b)
In this work, we ﬁrst take a causal look at VideoQA and argue that invariant grounding is the key to ruling out the spurious correlations. Towards this end, we propose a new learning framework, Invariant Grounding for VideoQA (IGV), to ground the question-critical scene, whose causal relations with answers are invariant across different inter-ventions on the complement. With IGV, the VideoQA mod-els are forced to shield the answering process from the negative inﬂuence of spurious correlations, which signiﬁ-cantly improves the reasoning ability. Experiments on three benchmark datasets validate the superiority of IGV in terms of accuracy, visual explainability, and generalization abil-ity over the leading baselines. Our code is available at https://github.com/yl3800/IGV . 1.

Introduction
Video Question Answering (VideoQA) [9] is growing in popularity and importance to interactive AI, such as vision-language navigation for in-home robots and personal assis-tants [2, 35]. It is the task of multi-modal reasoning, which answers the natural language question about the content of a given video. Clearly, inferring a reliable answer requires a deep understanding of visual scenes, linguistic semantics,
*Corresponding author. This work is supported by the Sea-NExT Joint
Lab
I
M
L race track  - race answer (b)
I
M
L
Scene: track – Answer: race (a) Local mutual information (LMI) between the “track” scene and answers.
Answer Candidates
Complement Scene
Causal Scene
Question: What is the man doing?
Ground-Truth Answer: Jump
Predictive Answer: Race (b) Running example of how the “track” complement deviates the answering.
Figure 1. Running example. (a) Superﬁcial correlations between visual scenes and answers; (b) Suffering from the spurious corre-lations, VideoQA model fails to answer the question. 3 and more importantly, the visual-linguistic alignments.
Towards this end, a number of VideoQA models have emerged [8–10,18,38]. Scrutinizing these models, we sum-marize their common paradigm as a combination of two modules: (1) video-question encoder, which encapsulates the visual scenes of video and the linguistic semantics of question as representations; and (2) answer decoder, which exploits these representations to model the visual-linguistic alignment and yield an answer. Consequently, the criterion of empirical risk minimization (ERM) is widely adopted as the learning objective to optimize these modules — that is, minimizing the loss between the predictive answer and the ground-truth answer.
However, the ERM criterion is prone to over-exploiting the superﬁcial correlations between video-question pairs and answers. Speciﬁcally, we use the metric of local mutual information (LMI) [27] to quantify the correlations between
the “track” scene and answers. As Figure 1a shows, most videos with “track” scene are associated with the “race” an-swer. Instead of inspecting the visual-linguistic alignments (i.e. which scene is critical to answer the question), ERM blindly captures all statistical relations. As Figure 1b shows, it makes VideoQA model naively link the “track”-relevant videos with the strongly-correlated “race” answer, instead of the gold “jump” answer. Taking a causal look [23, 24] at VideoQA (see Section 3), we partition the visual scenes into two parts: (1) causal scene, which holds the question-critical information, and (2) its complement, which is irrel-evant to the answer. We scrutinize that the complement is spuriously correlated with the answer, thus ERM hardly dif-ferentiates the effects of causal and complement scenes on the answer. Worse still, the unsatisfactory reasoning obsta-cles the VideoQA model to own the intriguing properties:
• Visual-explainability to exhibit “Which visual scene are the right reasons for the right answering?” [6, 26]. Taking
Figure 1b as an example to answer “What is the man do-ing?”, the model should attend the “jump” event present in the last three clips, rather than referring to the “track” complement in the ﬁrst two clips. One straightforward so-lution is “learning to attend” [31, 36, 39] to ground some scenes via the attentive mechanism. Nonetheless, guided by ERM, such attentive grounding still suffers from the spurious correlations, thus making the highly-correlated complement grounded.
• Introspective learning to double-check “How would the predictive answer change if the causal scenes were ab-sent?”. On top of attentive grounding, the model needs to introspect whether the learned knowledge (i.e. attended scene) reliably and faithfully reﬂects the logic behind the answering. Brieﬂy put, it should fail to answer the ques-tion if the causal scenes were removed.
• Generalization ability to enquire “How would the pre-dictive answer response to the change of spurious corre-lations?”. As spurious correlations poorly generalize to open-world scenarios, the model should instead latch on the causal visual-linguistic relations that are stable across different environments.
Inspired by recent invariant learning [3, 16, 33], we con-jecture that invariant grounding is the key to distinguishing causal scenes from the complements and overcoming these limitations. By “invariant”, we mean that the relations be-tween question-critical scenes and answers are invariant re-gardless of changes in complements. Towards this end, we propose a new learning framework, Invariant Grounding for
VideoQA (IGV). Concretely, it integrates two additional modules with into the VideoQA backbone model: a ground-ing indicator, a scene intervener. Speciﬁcally, the grounding indicator learns to attend the causal scenes for a given ques-tion and leaves the rest as the complement. Then, we collect visual clips from other training videos to compose a mem-ory bank of complement stratiﬁcation. For the causal part of interest, the scene intervener conducts the causal interven-tions [23, 24] on its complement — that is, replace it with the stratiﬁcation sampled from the memory bank and com-pose the “intervened videos”. After pairing the casual, com-plement, and intervened scenes with the question, we feed them into the backbone model to obtain the correspond-ing predictions: (1) causal prediction, which approaches the gold answer, so as to achieve visual explainability; (2) com-plement prediction, which contains no critical clues to the ground-truth answer, thus enforces the backbone model to perform introspective reasoning; and (3) intervened predic-tion, which is consistent with the causal prediction across different intervened complements.
Jointly learning these predictions enables the backbone model to alleviate the neg-ative inﬂuence of multi-modal data bias. It is worthwhile emphasizing that IGV is a model-agnostic strategy, which trains the VideoQA backbones in a plug-and-play fashion.
Our contributions are summarized as follows:
• We highlight the importance of grounding causal scenes from the complements to visual-explainability, general-ization, and introspective learning of VideoQA models.
• We propose a new model-agnostic training scheme, IGV, which incorporates invariant grounding into the VideoQA models, to mitigate the negative inﬂuence of multi-modal data bias and enhance the multi-modal reasoning ability.
• On three benchmark datasets (i.e. MSRVTT-QA [38],
MSVD-QA [38], NExT-QA [37]), we conduct extensive experiments to justify the superiority of IGV in training the VideoQA backbones. In particular, IGV signiﬁcantly outperforms the state-of-the-art models. 2. Preliminaries
In this section, we summarize the common paradigm of
VideoQA models. Throughout the paper, we denote the ran-dom variables and their deterministic values by upper-cased (e.g. V ) and lower-cased (e.g. v) letters, respectively.
Modeling. Given the video-question pair (V, Q), the primer task of VideoQA is to generate an answer ˆA as:
ˆA = f ˆA(V, Q), (1) where f ˆA is the VideoQA model, which is typically com-posed of two modules: video-question encoder, and answer decoder. Speciﬁcally, the encoder includes two compo-nents: (1) a video encoder, which encodes visual scenes of the target video as a visual representation, such as motion-appearance memory design [9, 10], structural graph repre-sentation [12, 15, 20, 34], hierarchical architecture [8, 17]; and (2) a question encoder, which encapsulates linguis-tic semantics of the question into a linguistic represen-V
Q tation, such as global/local representation of textual con-tent [14, 32], graph representation of grammatical depen-dencies [20]. On top of these representations, the decoder learns the visual-linguistic alignments to generate the an-swer. In particular, the alignments are modeled via cross-modal interaction like graph alignment [20], cross-attention
[14, 15, 18, 40] and co-memory [10], etc.
A
C
T
Learning. To optimize these modules, most of the leading
VideoQA models [9,10,14,15,17] cast the multi-modal rea-soning problem as a supervised learning task and adopt the learning objective of empirical risk minimization (ERM) as:
ERM( ˆA, A), min h L (2)
L where
ERM is the risk function to measure the loss between the predictive answer ˆA and ground-truth answer A, which is usually set as cross-entropy loss [10, 17] or hinge loss
In essence, ERM encourages these VideoQA
[9, 15, 37]. modules to capture the statistical correlations between the video-question pairs and answers. 3. Causal Look at VideoQA
From the perspective of causal theory [23,24], we revisit the VideoQA scenario to show superﬁcial correlations be-tween video-question pairs and answers. We then analyze
ERM’s suffering from the spurious correlations. 3.1. Causal Graph of VideoQA
In general, multiple visual scenes are present in a video.
But only part of the scenes are critical to answering the question of interest, while the rest hardly offers informa-tion relevant to the question. Moreover, the linguistic varia-tions in different questions should activate different scenes of a video. These facts inspire us to split the video into the causal and complement parts in terms of the question. Here we use a causal graph [23, 24] to exhibit the relationships input video V , input question Q, among ﬁve variables: causal scene C, complement scene T , ground-truth answer
A. Figure 2 illustrates the causal graph, where each link is a cause-and-effect relationship between two variables:
• C
• V
V
!
T . The input video V consists of C and T .
For example, the video in Figure 1b is the combination of the ﬁrst two clips (i.e. C) and the last three clips (i.e. T ).
Q. The causal scene C is conditional upon the video-question pair (V, Q), which distills Q-relevant information from V . For a given V , the variations in Q result in different C.
!
C
• Q
A
!
C. The answer A is determined by the ques-tion Q and causal scene C, reﬂecting the visual-linguistic alignments. Considering the example in Figure 1b again,
C is the oracle scene that perfectly explains why “jump” is labeled as the ground truth to answer the question.
"
#
!: Video
$: Question
%: Answer
$
%
&
": Complement Scene
#: Causal Scene
Figure 2. Causal graph of VideoQA
B
• T L9999K C. The dashed arrow summarizes the addi-tional probabilistic dependencies [21, 22] between C and
T . Such dependencies are usually caused by the selection bias or inductive bias during the process of data collection or annotation [5, 30]. For example, one mostly collects
Mediated Interactive Effect
Pure Indirect Effect the videos with the “jump” events on the “track”. Here we list three typical scenarios: (1) C is independent of T (i.e. T
T ),
C); (2) C is the direct cause of T (i.e. C
T ); (3) C and T have a common or vise versa (i.e. C
T ). See Appendix A for details.
E cause E (i.e. C
!
!
? 3.2. Spurious Correlations
!
V
C
Taking a closer look at the causal graph, we ﬁnd that the complement scene T and the ground-truth answer A can be spuriously correlated. Speciﬁcally, as the confounder [22– 24] between T and A, Q and V open the backdoor paths:
T
A, which make T and A spuriously correlated even though there is no direct causal path from T to A. Worse still, T L9999K C can amplify this issue. Assuming C
T , C becomes an additional confounder to yield another backdoor path T
C the probabilistic dependence: A
A. Such spurious correlations can be summarized as
A and T
T .
!
!
!
!
!
!
Q
C
V
As ERM naively captures the statistical correlations be-tween video-question pairs and answers, it fails to distin-guish the causal scene C and complement scene T , thus failing to mitigate the negative inﬂuence of spurious cor-relations. As a result, it limits the reasoning ability of
VideoQA models, especially in the following aspects: (1) visual-explainability to reason about “Which visual scenes are the supporting evidence to answer the question?”; (2) introspective learning to answer “How would the answer change if the causal scenes were absent?”; and (3) general-ization ability to enquire “How would the answer response to the change of spurious correlations?”. 6 ? 4. Methodology
We get inspiration from invariant learning [3, 16, 33] and argue that invariant grounding of causal scenes is the key to reducing the spurious correlations and overcoming the fore-going limitations. We then present a new learning frame-work, Invariant Grounding for VideoQA (IGV). 4.1. Invariant Grounding for VideoQA
Upon closer inspection on the causal graph, we notice that the ground-truth answer A is independent of the visual                  
complement T , only when conditioned on the question Q and the causal scene C, more formally:
A
T
C, Q. (3)
A
?
!
|
This probabilistic independence indicates the invariance — that is, the relations between the (C, Q) pair and A are in-variant regardless of changes in T . The causal relationship
Q
C is invariant across different T . Taking Figure 1b as an example, if the question and the causal scene (i.e. the last three clips) remain unchanged, the answer should arrive at “jump”, no matter how the complement varies1 (e.g. substitute the “track” clips by the “cloud”- or “sea”-relevant ones). This highlights that the (C, Q) pair is the key to shielding A from the inﬂuence of T .
Modeling. However, only the (V, Q) pair and A are avail-able in the training set, while neither C nor the grounding function towards C is known. This motivates us to incor-porate visual grounding into the VideoQA modeling, where the grounded scene ˆC aims to estimate the oracle C and guide the prediction of answer ˆA. More formally, instead of the conventional modeling (cf . Equation (1)), we system-atize the modeling process as:
ˆC = f ˆC(V, Q),
ˆA = f ˆA( ˆC, Q), (4) where f ˆC is the grounding model, and f ˆA is the VideoQA model that relies on the ( ˆC, Q) pair instead. See Section 4.2 for our implementations of f ˆC and f ˆA.
Learning. Nonetheless, simply integrating visual ground-ing with the VideoQA model falls into the “learning to at-tend” paradigm, which still suffers from the spurious cor-relations and erroneously attends to the complement scenes as ˆC. To this end, we exploit the invariance property of C (cf . Equation (3)) and reformulate the learning objective of invariant grounding as: min f ˆA,f ˆC L
IGV( ˆA, A), s.t. A
ˆT
?
|
ˆC, Q, (5)
ˆC is
IGV is the loss function to our IGV; ˆT = V where the complement of ˆC. In the next section, we will elaborate how to implement
IGV and achieve invariant grounding.
L
\
L 4.2. IGV Framework
Figure 3 displays our IGV framework, which involves two additional modules, the grounding indicator and scene intervener, beyond the VideoQA backbone model f ˆA. 4.2.1 Grounding Indicator
For a video-question pair instance (v, q), at the core of the grounding indicator is to split the video instance v into two
Intervened Video (+∗, /)
Scene
Intervener ($%, /)
Complement Scene ($-, /)
Causal Scene
VideoQA
Backbone
!!"
Intervened
Prediction
Complement
Prediction
Causal
Prediction (+, /)
Grounding
Indicator
Video-Question Pair
Figure 3. Overview of our IGV framework. parts, ˆc and ˆt, according to the question q. Towards this end, it ﬁrst employs two independent LSTMs [11] to encode the visual and linguistic characteristics of v and q, respectively: vg, vl = LSTM1(v), qg, ql = LSTM2(q), (6)
⇥ where the features of v are K ﬁxed visual clips, while q is associated with L language tokens; LSTM1 outputs vl 2 d as the local representations of clips, and yields the
RK
Rd as the global representations of the last hidden state vg 2 d holistic video. Analogously, LSTM2 generates ql 2
⇥ as the local representations of tokens, and makes the last
Rd represent the question holistically, hidden state qg 2 here d is the hidden dimension.
RL
Upon these representations, the attention scores are con-structed to indicate the importance of each visual clip. Here
RK to exhibit the probability of each clip we devise pˆc 2
RK is in con-belonging to the causal scene ˆc, while pˆt 2 trast to pˆc to show how likely each clip composes the com-plement ˆt. The formulations are as follows: pˆc = Softmax(MLP1(vl) pˆt = Softmax(MLP3(vl)
MLP2(qg)>),
MLP4(qg)>),
·
· (7) (8)
⇥ 2 2
RK where four multilayer perceptrons (MLPs) are employed to d0 , distill useful information: MLP1(vl), MLP3(vl)
Rd0 ; d0 is the feature dimension.
MLP2(qg), MLP4(qg)
However, as the soft masks make ˆc and ˆt overlapped, the attentive mechanism cannot shield the answering from the inﬂuence of the complement. Hence, the grounding indi-cator produces discrete selections instead to make ˆc and ˆt disjoint. Nonetheless, simple sampling or selection is not differentiable. To achieve differentiable discrete selection, we apply Gumbel-Softmax [13]:
I = Gumbel-Softmax([pˆc, pˆt]), (9)
⇥
⇥ 2
RK
RK where Gumbel-Softmax is built upon the concatenation of 2), and outputs the indicator pˆc and pˆt (i.e. [pˆc, pˆt] 2 whose ﬁrst and second column indexes ˆc vector I 2 and ˆt over k clips, respectively. As such, we can devise ˆc and ˆt as follows: vk|
Ik0 ·
,
Ik1 = 1
}
,
Ik0 = 1
}
Ik1 ·
{ vk|
ˆt = (10)
ˆc =
{ 1Note that the complement substitutes will not involve the question-relevant scenes, in order to avoid creating additional paths from T to A. where I0k and I1k suggests that the k-th clip belongs to the causal and complement scenes, respectively.  
Complement Scene
Causal Scene
Intervened Complement
Causal Scene
!"($% =
!"($% =
)
)
Intervened Complement
Causal Scene
Figure 4. Illustration of interventional distribution. 4.2.2 Scene Intervener
It is challenging to learn the grounding indicator, owing to the lack of supervisory signals of clip-level importance. To remedy this issue, we propose the scene intervener, which preserves the estimated causal scene ˆc but intervenes the estimated complement ˆt to create the “intervened videos”, as Figure 4 shows.
Speciﬁcally, for the observed video-question pairs dur-ing training, the scene intervener ﬁrst collects visual clips from other training videos as a memory bank of comple-ment stratiﬁcation, ˆ
ˆt
. Then, for the video of in-}
{
T
ˆt, the intervener conducts causal interven-terest v = ˆc tions [23, 24] on its ˆt — that is, random sample a comple-ment stratiﬁcation ˆt⇤ to replace ˆt and combine it with
ˆc at hand as a new video v⇤ = ˆc
ˆ
T
ˆt⇤.
= 2
[
It is worthwhile mentioning that, distinct from the cur-rent invariant learning studies [3, 16, 33] that only parti-tion the training set into different environments, our scene intervener exploits the interventional distributions [29] in-stead. The interventional distribution (i.e., the videos with the same interventions) can be viewed as one environment.
[ 4.2.3 VideoQA Backbone Model
Inspired by [15], we design a simple yet effective architec-ture as our backbone predictor, where the video encoder is shared with the grounding indicator.
It embodies convo-lutional graph networks (GCN) to propagate clip-level vi-sual messages, then integrates cross-modal fused local and global representations via BLOCK fusion [4]. See Ap-pendix B for the detailed architecture. 4.2.4 Joint Training
For a video-question pair instance (v, q), we have estab-lished the causal scene ˆc, complement scene ˆt, and inter-vened video v⇤ via the grounding indicator and scene inter-vener. Pairing them with q synthesizes three new instances: (ˆc, q), (ˆt, q), (v⇤, q). We next feed these instances into the backbone VideoQA model f ˆA to obtain three predictions:
• Causal prediction. As the causal scene ˆc is expected to be sufﬁcient and necessary to answer the question q, we leverage its predictive answer f ˆA(ˆc, q) to approach the ground-truth answer a solely:
ˆc = XE(f ˆA(ˆc, q), a),
L (11) where XE denotes the cross-entropy loss.
• Complement prediction. As no critical clues should ex-ist in the complement scene ˆt to answer the question q, we encourage its predictive answer f ˆA(ˆt, q) to evenly predict all answers. This uniform loss is formulated as:
ˆt = KL(f ˆA(ˆt, q), u),
L (12) where KL denotes KL-divergence, and u is the uniform distribution over all answer candidates.
• Intervened prediction. According to the invariant con-straint (cf . Equation (3)), the causal relationship between the causal scene and the answer is stable across different complements. To parameterize this constraint, we enforce all v’s intervened versions to hold the consistent predic-tions: v⇤ = Eˆt⇤2T (KL(f ˆA(v⇤, q), f ˆA(ˆc, q))). (13)
L
Aggregating the foregoing risks, we attain the learning ob-jective of IGV: (14)
IGV = E(v,q,a)
+
ˆc +  1
ˆt +  2 v⇤ ,
L
L
L
O 2O
L
+ is the training set of the video-question pair where (v, q) and the ground-truth answer a;  1 and  2 are the hyper-parameters to control the strengths of invariant learn-ing. Jointly learning these predictions enables the VideoQA backbone model to uncover the question-critical scene, so as to mitigate the negative inﬂuence of spurious correlations between the question-irrelevant complement scene and an-swer. In the inference phase, we use the causal prediction f ˆA(ˆc, q) to answer the question. 5. Experiments
We conduct extensive experiments to answer the follow-ing research questions:
• RQ1: How effect is IGV in training VideoQA backbones as compared with the State-of-the-Art (SoTA) models?
• RQ2: How do the loss component and feature setting af-fect the performance?
• RQ3: What are the learning patterns and insights of IGV training?
Settings: We compare IGV with seven baselines from fami-lies of Memory, GNN and Hierarchy (Appendix C) on three
VideoQA datasets: NExT-QA [37] which features causal
Table 1. Comparison of accuracy on NExT-QA test set. The best and second-best results are highlighted.
Models
Causal
Temp
Descrip
All
Co-Mem [10]
HCRN [17]
HME [9]
HGA [15]
IGV(Ours)
Abs. Improve 45.85 47.07 46.76 48.13 48.56
+0.43 50.02 49.27 48.89 49.08 51.67
+1.65 54.38 54.02 57.37 57.79 59.64
+1.85 48.54 48.82 49.16 50.01 51.34
+1.33 and temporal action interactions among multiple objects.
It contains about 47.7K manually annotated questions for multi-choice QA collected from 5.4K videos with an aver-age length of 44s. MSVD-QA [38] and MSRVTT-QA [38] are two prevailing datasets that focus on the description of video elements. They respectively contain 50K and 243K
QA pairs with open answer space over 1.6K and 6K. For all three datasets, we follow their ofﬁcial data splits for exper-iments and report accuracy as evaluation metric.
Implementation Details: For the visual feature, we follow previous works [15, 17, 37] and extract video feature as a combination of motion and appearance representations by using the pre-trained 3D ResNeXt-101 and ResNet-101, re-spectively. Speciﬁcally, each video is uniformly sampled into K=16 clips, where each clip is represented by a com-bined feature vector vdv k , where dv equals 4096. Similar to [37], we obtain the contextualized word representation from the ﬁnetuned BERT model, and the feature dim dq is 768. For our model, the dimension of the hidden states are set to d = 512, and the number of graph layers in IGV back-bone predictor is 2. During training, IGV is optimized by
Adam optimizer with the initial learning rate of 1e-4, which will be halved if no validation improvements in 5 epochs.
We set the batch size to 256 and a maximum of 60 epochs. (See Appendix D for more details and complexity analysis). 5.1. Main Results (RQ1) 5.1.1 Comparisons with SoTA Methods
As shown in Table 1 and Table 2, our method outperforms
SoTAs with questions of all sub-types surpassing their com-petitors. Speciﬁcally, we have two major observations:
First, on NExT-QA, IGV gains remarkable improvement on temporal type (+1.65%), the underlying explanation are: 1) temporal question generally corresponds to video content with a longer time span, which requires more introspective grounding of the causal scene. Fortunately, IGV’s design philosophy comfort such demand by wiping out the trivial scenes, which takes up a huge proportion in temporal type, thus making the predicting faithful. 2) temporal questions tend to include a temporal indicative phase (e.g. ”at the end of the video”) that serves as a strong signal for grounding
Table 2. Comparison of accuracy on MSVD-QA and MSRVTT-QA test set. ”
” indicates the result is re-implementation with the
† publicly available code
Models
MSVD-QA MSRVTT-QA
Memory
GNN
Hierarchy
Causal view
AMU [38]
HME [9]
Co-Mem
†
[15]
HGA
†
B2A [20]
[10]
HCRN [17]
HOSTR [8]
IGV (Ours)
Abs. Improve 32.0 33.7 34.6 35.4 37.2 36.1 39.4 40.8
+1.4 32.0 33.0 35.3 36.1 36.9 35.6 35.9 38.3
+1.4 indicator to locate the target window.
Second, along with descriptive questions on NExT-QA, the result on MSRVTT-QA and MSVD-QA (both emphases on question of descriptive type) demonstrate the superior-ity in descriptive question across all three datasets (+1.85% on NExT-QA, +1.4% on MSRVTT-QA and MSVD-QA).
Such improvement is underpinned by logic that answering descriptive questions requires scrutiny on the scene of in-terest, instead of a holistic view of the entire sequence. Ac-cordingly, targeted prediction inducted by IGV concentrates reasoning on keyframes, thus achieving better performance.
As a consequence, such improvement strongly validates that
IGV generalizes better over various environments. 5.1.2 Backbone Agnostic
By nature, our IGV principle is orthogonal to backbone de-sign, thus helping to boost any off-the-shelf SoTAs with-out compromising the underlying architecture. We there-fore experimentally testify the generality and effectiveness of our learning strategy by marrying the IVG principle with methods from two different categories: Co-Mem [10] from memory-based architecture and HGA [15] from Graph-based method. Table 3 shows the results on three backbone predictors (including ours). Our ﬁndings are:
⇠
⇠ 1. Better improvement for severe bias. We notice 4.7%) is con-that the improvement on MSVD-QA (+3.1% 2%). siderably larger than that on MSRVTT-QA (+1.4%
Such expected discrepancy is caused by the fact that, al-though identical in question type, MSRVTT-QA is almost 5 times larger than MSVD-QA (#QA pairs 243K vs 50K).
As a result, the baseline model trained on MSRVTT-QA is gifted with better generalization ability, whereas the model on MSVD-QA still suffers from severe shortcut correla-tion. For the same reason, the IGV framework achieves much better improvement in the severe-shortcut situation (e.g. MSVD-QA). Such discrepancy validates our motiva-tion of eliminating statistic dependency. 2. Constant improvement for each method. Through
Table 3.
”+IGV” denoted our strategy is incorporated.
IGV strategy is applied to different SoTAs methods.
Models
MSVD-QA
MSRVTT-QA
Baseline
+IGV
Baseline
+IGV
Co-Mem [10]
HGA [15]
Our Backbone 34.6 35.4 36.1 37.7 38.8 40.8 35.3 36.1 36.3 37.3 37.5 38.3
Table 4. Study of IGV loss components
Variants
Baseline
ˆc
ˆc +
ˆc +
ˆc +
L
L
L
L
ˆt v⇤
ˆt +
L
L
L v⇤
L
MSVD-QA
MSRVTT-QA
Our Backbone
Co-Mem [10]
Our Backbone
Co-Mem [10] 36.1 36.0 37.4 38.2 40.8 34.6 33.3 36.1 36.3 37.7 36.3 36.7 37.8 37.4 38.3 35.3 36.0 36.8 36.2 37.3 row-wise inspection, we notice that for each bench-mark, IGV can bring considerable improvement across different backbone models (+3.1% 4.7% for MSVD-QA,
+1.4% 2% for MSRVTT-QA). Such stable enhancement strongly veriﬁes our modal-agnostic statement.
⇠
⇠ 5.2. In-Depth Study (RQ2) 5.2.1 Contributions of Different Loss Components
An in-depth comprehension of IGV framework requires careful scrutiny on its components. Alone this line, we ex-haust the combination of IGV loss components and design three variants: v⇤ . Table 4 shows
ˆt and the result of the above variants on two benchmarks across two backbone predictors. Our observations are as follow:
ˆc +
ˆc +
ˆc,
L
L
L
L
L
• Using
L
ˆc solely, which can be viewed as a special case of
ERM-guided attention, hardly outperforms the baseline, because grounding indicators can not identify the causal scene without clip-level supervision. Such an expected result reﬂects our motivation in interventional design.
•
L
L
L
ˆc +
ˆt and
ˆc + v⇤ matched equally in accuracy that
L
ˆc in all cases. Such consistently surpass baseline and progress shows the effectiveness of intervention strategy and introspective regularization imposed on complement. v⇤ further boosts the performance
L signiﬁcantly, which shows v⇤ contribute in dif-L ferent aspects and their beneﬁts are mutually reinforcing.
ˆt and
ˆt +
ˆc +
L
L
L
L
• In all cases, 5.2.2 Study of Feature
By convention, we study the effect of the input condition by ablation on the visual feature. Particularly, we denote
APP for tests that adopt only appearance feature as input and MOT for tests that utilize motion feature alone. Figure 6a delivers results on two benchmarks, where we observe:
First, IGV can improve the performance signiﬁcantly for all input conditions, which generalizes the effectiveness of our framework. Similar to Table 3, the improvement on
MSVD-QA is larger than that on MSRVTT-QA, which so-lidiﬁes our ﬁnding in Section 5.1.2.
Second, compared to motion feature, IGV brings distinc-tively larger improvements using appearance feature. Con-sidering the causal nature of IGV, we conclude that static correlation tends to bias more in appearance feature. 5.2.3 Study of Hyper-parameter the hyper-To validate the sensitivity of IGV against parameters, we conduct experiments with variations of  1 and  2 on two datasets. Without loss of generalization, we tune  1 ( 2) as sample of
, while keeping the  2 ( 1) as 1. According to Figure 6b, we
  have follow observations: 10, i 1.3i
|  10


Z 2 i
For MSVD-QA, we observe consistent peaks around 0.8 for both  1 and  2. Comparatively, ﬂuctuation on
MSRVTT-QA is more moderate, where tuning on   only causes a 1.5% difference in their accuracy. It’s noteworthy that IGV outperforms the baseline by a large margin (+3%) under all tests, which indicates IGV’s robustness against variation of hyper-parameters. Additionally, comparing to
 2, IGV is more sensitive to  1. Typically, the performance suffers a drastic degradation for  1 larger than 5 on both datasets. Whereas  2 maintain above 39% (MSVD-QA) and 37.5% (MSRVTT-QA) for all tests. 5.3. Qualitative analysis (RQ3)
As mentioned in Section 1 , IGV is empowered with visual-explainability, and is apt to account for the right scene for its prediction. Following this essence, we grasp the learning insight of IGV by inspecting some correct ex-amples from the NExT-QA dataset and show the visualiza-tion in Figure 5. Concretely, each video comes with two questions that emphasize different parts of the video. We notice that, even for the same video, our grounding window is question-sensitive to enclose the explainable content with correct prediction. Nonetheless, we also observe results of insufﬁcient-grounding on the third row Q2, where the girl starts to bend down before the last two frames, even though the most informative last two frames are encompassed. 6.