Abstract
The images captured by under-display cameras (UDCs) are degraded by the screen in front of them. We model this degradation in terms of a) diffraction by the pixel grid, which attenuates high-spatial-frequency components of the image; and b) diffuse intensity and color changes caused by the multiple thin-film layers in an OLED, which modu-late the low-spatial-frequency components of the image. We introduce a deep neural network with two branches to re-verse each type of degradation, which is more effective than performing both restorations in a single forward network.
We also propose an affine transform connection to replace the skip connection used in most existing DNNs for restor-ing UDC images. Confining the solution space to the linear transform domain reduces the blurring caused by convolu-tion; and any gross color shift in the training images is elim-inated by inverse color filtering. Trained on three datasets of
UDC images, our network outperformed existing methods in terms of measures of distortion and of perceived image quality. 1.

Introduction
Under-display cameras (UDCs) are a key technology for realizing full-screen smartphones. Unfortunately, the qual-ity of the images captured by a UDC is considerably re-duced by the light loss and diffraction introduced by the display panel which is in front of it [8, 19, 40]. One way of addressing this problem is to increase the proportion of the panel which is transparent by reducing the pixel density in the region of the display immediately above the camera, and by modifying the layout of the RGB sub-pixels [26, 32, 37].
Increasing the transparent area reduces the resolution of the screen, and therefore distortion of the light reaching the
*Correspondence to: Sungroh Yoon (sryoon@snu.ac.kr).
Figure 1. Physical model of the degradation of the image received by a UDC, simplified by two degradation processes: (1) color fil-tering and spatially variant attenuation Ψ by the thin-film layers of the OLEDs, and (2) diffraction Φ by the pixel definition layer (PDL), which can be represented by a point spread function (PSF).
A UDC image y can be simulated by compositing the effect of Ψ and Φ on a latent image x.
UDC is unavoidable with a screen of any acceptable res-olution; and the presence of the wires required to drive the pixels make this distortion worse. The natural limits on the extent to which an OLED display can be rearranged mean that restoration is required to make the images received by a UDC look like the images that would be received if the display was not in front of the UDC. Methods have recently been proposed based on paired image datasets [8, 21, 40] and image restoration [8,16,19,27,29,33,36,39] which use a learning approach with a neural network.
Image restoration methods using deep neural networks (DNNs) have progressed substantially over the years: spe-cific DNN architectures have been developed for many
restoration tasks, such as super-resolution [22], deblur-ring [18], dehazing [10], and deraining [20]. The UDC presents a new image restoration task [39], which has re-ceived considerable attention, but most of the methods in-troduced so far use networks developed for other restoration tasks. Some methods [8, 19] do consider the physical pro-cesses that affect the image received by a UDC, in which the angle of the incident light is included in the inference pro-cess. However, in the same way that incorrect kernel estima-tion produces a severe artifact in deblurring techniques [18], errors in predicting the angle of incidence make this ap-proach to restoring UDC images problematic.
In pioneering work on this topic [8, 19, 40], UDC degra-dation was modeled in terms of the diffraction and reduced intensity caused by display pixels. This model is effective in addressing the process that dominates image degradation, it only represents the reduction in high-spatial-frequency components of the image by diffraction, and does not con-sider the different transmission of wavelengths by the thin-film layers of an OLED, and long-range degradation caused by the non-uniformity of those layers, which is associated with the modulation of low-spatial-frequency components (see Fig. 1). We propose a physical model of UDC image degradation which includes low-spatial-frequency degrada-tion processes, such as color shifts and spatial attenuation, and we introduce a DNN architecture to reverse the changes in the image predicted by our model. Whereas existing methods of UDC image restoration [8, 16, 19, 27, 29, 33, 36, 39] mainly involve a network performing a single deblur-ring task, we separate this task into high- and low-spatial-frequency reconstruction with two network branches. We induce each branch to deal with a difference range of fre-quencies. This branched network for UDC image restora-tion (BNUDC) effectively removes high-spatial-frequency noise such as ’flare’ [8] and degradation with low spatial frequency, such as color shift. We also propose an affine transform connection which reduces over-smoothing by the convolution operation and preserves the structure of the im-age by constraining the solution space to the linear trans-form domain of the input image. In addition, we introduce inverse color filtering, which is a pre-processing technique that improves the color fidelity of the restoration of im-ages with a severe color shift, such as those in the POLED dataset [40]. Simplifying the effect of the stacked thin films to that of a single color filter (see Fig. 1), allows inverse color filtering to be performed easily by inverting in the CIE
XYZ color space. This is equivalent to the data normaliza-tion processes widely used in deep learning.
Overall, our contributions can be summarized as follows:
• We present a new model of the UDC degradation found in images captured by a UDC, which is specific to the optical properties of OLEDs. The model includes changes with a low spatial frequency, such as color shift and spatially variant attenuation. We propose a
DNN architecture with one branch to restore high-frequency component, and a second branch to restore low-frequency components.
• We propose an affine transformation connection as an alternative to residual learning [11], This connection is specific to our model of image degradation. It removes noise introduced by restoration while preserving the structure of the image from the UDC.
• Our network achieves a-state-of-the-art performance in terms of both numerical and perceptual distortion metrics on three public datasets [8, 40]. Our network does not require a point-spread function (PSF) as a prior; but nevertheless it outperforms existing methods which are conditioned by a PSF. 2.