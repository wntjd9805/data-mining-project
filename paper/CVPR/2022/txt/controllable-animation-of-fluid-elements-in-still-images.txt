Abstract
We propose a method to interactively control the anima-tion of fluid elements in still images to generate cinema-graphs. Specifically, we focus on the animation of fluid el-ements like water, smoke, fire, which have the properties of repeating textures and continuous fluid motion. Taking in-spiration from prior works, we represent the motion of such fluid elements in the image in the form of a constant 2D op-tical flow map. To this end, we allow the user to provide any number of arrow directions and their associated speeds along with a mask of the regions the user wants to animate.
The user-provided input arrow directions, their correspond-ing speed values, and the mask are then converted into a dense flow map representing a constant optical flow map (FD). We observe that FD, obtained using simple exponen-tial operations can closely approximate the plausible mo-tion of elements in the image. We further refine computed dense optical flow map FD using a generative-adversarial network (GAN) to obtain a more realistic flow map. We devise a novel UNet based architecture to autoregressively generate future frames using the refined optical flow map by forward-warping the input image features at different res-olutions. We conduct extensive experiments on a publicly available dataset and show that our method is superior to the baselines in terms of qualitative and quantitative met-rics.
In addition, we show the qualitative animations of the objects in directions that did not exist in the training set and provide a way to synthesize videos that otherwise would not exist in the real world. Project url: https:
//controllable-cinemagraphs.github.io/ 1.

Introduction
It is widely perceived that animations capture human imagination more than still images. The effect of this can be seen in the proliferation of video content that is being uploaded on social media. Studies show that video-based ads and explainers are far more likely to gain trust and en-gagement than those based on other modalities, leading to a significant boost in sales. However, the required animations or videos are less easily available for the users to leverage than still images that exist in abundance in one’s collection.
Hence, it is desirable to empower the practitioners with con-trollable tools to convert the still images to videos of the required kind. This motivates us to consider the problem of animating images with user control to generate output videos that are generally called ‘cinemapgraphs’ in litera-ture. Similar to [11] we focus on the images that contain fluid elements like water, smoke, fire that have repeating textures and continuous fluid motion.
There has been a rich body of work [1,3,5,7,9–11,16,20,25]
that has focused on generating animations from still images.
While [3, 7, 11, 16, 20, 25] focus on uncontrollable image-to-video synthesis, attempts [1, 2, 5, 9, 10] have been made for controllable image-to-video synthesis with the user-provided direction of the motion of the objects in the im-ages. While these methods provide some control to the user, they suffer from certain drawbacks. Specifically, [1,2,5] ei-ther allow the user to poke at just a single pixel location or provide a single user direction. Halperin et. al [9] ob-tain a displacement field by exploiting self-similarity that exists in images of repeating structures like buildings, stair-cases. However, such a method is unsuitable for animating fluid objects that we are considering as such objects do not have specific structures that can manifest in self-similarity, leading to an erroneous displacement field. Hao et. al [10] proposed a method in which the user can provide sparse tra-jectories as input, defined by the direction of the motion at different locations. A dense optical flow map is estimated in an unsupervised manner and is warped with the input im-age to obtain the future frames. As shown later, the dense optical flow estimated thus is brittle and is prone to produce unrealistic video synthesis results. Motivated by this, we consider the problem of animating images given i) a single still image ii) a user-provided mask specifying the region to be animated and iii) a set of movement directions, called flow hints at different locations in the masked region.
To circumvent the problems associated with directly obtain-ing a flow map from user inputs, we propose a two-step ap-proach to estimate the flow map from a sparse set of arrow directions and their associated speeds. Firstly, we approxi-mate the dense optical flow using simple exponential oper-ations on the movement directions and speeds input by the user. Next, the thus estimated approximate flow map is fur-ther refined using a GAN-based network [8] to obtain the final estimate of the flow map representing the constant 2D flow map of the desired movement. The estimated flow map along with the input image is fed into a GAN-based image generator, similar to Holynski et al. [11] to obtain the future frames. The contributions of our paper are as follows.
• We propose a two-stage approach to interactively con-trol the animation of fluid elements from a still image.
• We propose a novel approach to approximate the con-stant flow map governing the motion using simple ex-ponential operations on the user-provided inputs in the form of speed and directions.
• Through qualitative and quantitative experiments, we show that our method beats all previous and other pro-posed baselines on a publicly available dataset of im-ages of fluid motion.
• We prove the generalizability of our method to any ar-bitrary set of user directions by showing the qualitative animations of fluid objects in directions that did not ex-ist in the training set. 2.