Abstract
This paper explores the feasibility of ﬁnding an opti-mal sub-model from a vision transformer and introduces a pure vision transformer slimming (ViT-Slim) framework.
It can search a sub-structure from the original model end-to-end across multiple dimensions, including the input to-kens, MHSA and MLP modules with state-of-the-art perfor-mance. Our method is based on a learnable and uniﬁed (cid:96)1 sparsity constraint with pre-deﬁned factors to reﬂect the global importance in the continuous searching space of dif-ferent dimensions. The searching process is highly efﬁcient through a single-shot training scheme. For instance, on
DeiT-S, ViT-Slim only takes ∼43 GPU hours for the search-ing process, and the searched structure is ﬂexible with di-verse dimensionalities in different modules. Then, a bud-get threshold is employed according to the requirements of accuracy-FLOPs trade-off on running devices, and a re-training process is performed to obtain the ﬁnal model. The extensive experiments show that our ViT-Slim can compress up to 40% of parameters and 40% FLOPs on various vi-sion transformers while increasing the accuracy by ∼0.6% on ImageNet. We also demonstrate the advantage of our searched models on several downstream datasets. Our code is available at https://github.com/Arnav0400/
ViT-Slim. 1.

Introduction
Transformer [49] has been a strong network model for various vision tasks, such as image classiﬁcation [14, 28, 38,46,53], object detection [3,43,64], segmentation [50,54, 61], etc. It is primarily composed of three underlying mod-ules: Multi-Head Self-Attention (MHSA), Multi-Layer Per-ceptron (MLP) and Image Patching Mechanism. The major limitation of ViT in practice is the enormous model sizes
*indicates equal contribution. This work was done when Arnav was a research assistant at MBZUAI, Zhiqiang Shen is the corresponding author. and excessive training and inference costs, which hinders its broader usage in real-world applications. Thus, a large body of recent studies has focused on compressing the vi-sion transformer network through searching a stronger and more efﬁcient architecture [6–8, 34, 42] for better deploy-ment on various hardware devices.
However, many commonly-used searching strategies are recourse-consuming, such as the popular reinforcement learning and evolutionary searching approaches. Single-path one-shot (SPOS) [15] is an efﬁcient searching strategy and promising solution for this task, while it still needs to train the supernet for hundreds of epochs and then evalu-ate thousands of subnets for ﬁnding out the optimal sub-architecture, which is still time-consuming, typically with more than tens of GPU days.
Recently, there are some works utilizing Batch Normal-ization (BN) scaling parameter as the indicator for the im-portance of operations to prune or search subnets, such as
Network Slimming [27], SCP [23], BN-NAS [5], etc., since
BN’s parameter is an existing factor and light-weight mea-sure metric in the network for the importance of subnets.
This searching method can offer 10× speedup of training than SPOS in general. But in practice, not all networks contain BN layers, such as the transformers. Also, there are many unique properties in a transformer design like the dependency of input tokens from shallow layers to deep lay-ers. Simply utilizing such a strategy is not necessarily prac-tical or optimal for the newer transformer models.
Consequently, the main remaining problem is that there is no BN layer involved in conventional transformer archi-tectures, so we cannot directly employ the scaling coef-ﬁcient in BN as the indicator for searching. To address this, in this work we propose to incorporate the explicitly soft masks to indicate the global importance of dimensions across different modules of a transformer. We consider jointly searching on all three dimensions in a transformer layerwise tokens/patches, MHSA end-to-end, including: and MLP dimensions. In particular, we design additional differentiable soft masks on different modules for the in-Method
GLiT [6]
S2ViTE [8]
Dynamic-ViT [34]
Patch-Slimming [45]
ViTAS [42]
AutoFormer [7]
ViT-Slim (Ours)
Table 1. Feature-by-feature comparison of compression and search approaches for vision transformers. “g-hrs” indicates GPU hours.
Searching Method
Two stage evolutionary
Iterative Prune & Grow
Layerwise Prediction module
Top-Down layerwise
Evolutionary
Evolutionary
One-shot w/ (cid:96)1-sparsity
Searching Time 200-ep 510 g-hrs (600-ep) 26 g-hrs (30-ep) 31 g-hrs (36-ep) 300-ep 500-ep 43 g-hrs (50-ep)
Searching Space
Discrete, Pre-deﬁned
Continous, limited
Dynamic Patch Sel.
Layerwise Patch Sel.
Discrete, Pre-deﬁned
Discrete, Pre-deﬁned
Target Arch
SA + 1D-CNN
ViT/DeiT family
ViT/DeiT family
ViT/DeiT family
Customized ViT
Customized ViT
Both
Both
FLOPs only
FLOPs only
Both
Both
Both
ViT/DeiT/Swin, etc. Continuous, all modules
Inherit Pre-train Reduce Params & FLOPs
No
Yes
Yes
Yes
No
No
Yes dividual dimension of a transformer, and the (cid:96)1-sparsity is also imposed to force the masks to be sparse during search-ing. We only need a few epochs to ﬁnetune these mask parameters (they are initialized to 1 so as to give equal im-portance to all dimensions at the beginning of search) to-gether with the original model’s parameters for completing the whole searching process, which is extremely efﬁcient.
For the token search part, we apply a tanh over masks to avoid exploding mask values which is observed empirically.
We call our method ViT-Slim, a joint sparse-mask based searching method with an implicit weight sharing mecha-nism for searching a better sub-transformer network. This is a more general and ﬂexible design than previous BN-based approaches since we have no requirement of BN layers in the networks. This is more friendly to transformers and a feature-by-feature comparison with other ViT compression methods is shown in Table 1. One core advantage of our method is the efﬁciency of searching, we can inherit the pre-trained parameters and conduct a fast search upon it. An-other advantage is the zero-cost subnet selection and high
ﬂexibility. In contrast to the SPOS searching that requires to evaluate thousands of subnets on validation data, once we complete the searching process, we can obtain countless subnetworks and the ﬁnal structure can be determined by the requirement of accuracy-FLOPs trade-off of the real de-vices that we deploy our model on, without any extra eval-uation. The last advantage is that we can search for a ﬁner-grained architecture such as the different dimensionalities in different self-attention heads, as our search space is con-tinuous in them. This characteristic allows us to ﬁnd the architecture with unique individual dimensions and shapes in different layers and modules, which would always ﬁnd out a better subnet than other counterparts.
Comprehensive experiments and ablation studies are conducted on ImageNet [13], which show that ViT-Slim can compress up to 40% of parameters and 40% FLOPs on various vision transformers like DeiT [47], Swin [28] with-out any compromising accuracy (in some circumstances our compressed model is even better than the original one). We also demonstrate the advantage of our searched models on several downstream datasets.
Our main contributions are:
• We introduce ViT-Slim, a framework that can jointly per-form an efﬁcient architecture search over all three mod-ules - MHSA, MLP and Patching Mechanism in vision transformers. We stress that our method searches for structured architectures which can bring practical efﬁ-ciency on modern hardware (e.g., GPUs).
• We empirically explore various structured slimming strategies by sharing weights in candidate structures, and provide the best performing structure by employing a continuous search space in contrast to a pre-deﬁned dis-crete search space in existing works.
• Our method can perform directly over pre-trained trans-formers by employing a single shot searching mechanism for all possible budgets, eliminating the need of search-speciﬁc pre-training of large models and performing re-peated searching for different modules/budgets.
• We achieve state-of-the-art performance at different bud-gets on ImageNet across a variety of ViT compression and search variants. Our proposed ViT-Slim can com-press up to 40% of parameters and 40% FLOPs while increasing the accuracy by ∼0.6%. 2.