Abstract
Given a small training data set and a learning algo-rithm, how much more data is necessary to reach a target validation or test performance? This question is of criti-cal importance in applications such as autonomous driving or medical imaging where collecting data is expensive and time-consuming. Overestimating or underestimating data requirements incurs substantial costs that could be avoided with an adequate budget. Prior work on neural scaling laws suggest that the power-law function can ﬁt the valida-tion performance curve and extrapolate it to larger data set sizes. We ﬁnd that this does not immediately translate to the more difﬁcult downstream task of estimating the required data set size to meet a target performance. In this work, we consider a broad class of computer vision tasks and system-atically investigate a family of functions that generalize the power-law function to allow for better estimation of data requirements. Finally, we show that incorporating a tuned correction factor and collecting over multiple rounds sig-niﬁcantly improves the performance of the data estimators.
Using our guidelines, practitioners can accurately estimate data requirements of machine learning systems to gain sav-ings in both development time and data acquisition costs. 1.

Introduction
Before deploying a deep learning model, designers may mandate that the model meet a baseline performance, such as a target metric over a held out validation or test set. For example, an object detector may require a minimum mean average precision before being deployed in a safety-critical application. One of the most effective ways of meeting the target performance is by collecting more training data for a given model. However, how much more data is needed?
Overestimating data requirements can incur costs from unnecessary collection, cleaning, and annotation. For ex-ample, annotating segmentation data sets may require 15 to 40 seconds per object [2], meaning annotating a driving
Figure 1. Extrapolating accuracy on ImageNet [7] as a function of data set size from 10% of the data set (125, 000 images; dotted) and 50% (600, 000 images; dashed) using four regression func-tions. The vertical dashed lines show how much data is needed to meet a target 67% validation accuracy according to each dashed curve. All the dashed curves can accurately extrapolate perfor-mance as they are given a sufﬁcient amount of images. Although the functions have an error of 1-6% from the ground truth (67% at 900, 000 images), they mis-estimate the data requirement by 120, 000 to 310, 000 images. data set of 100, 000 images with on average 10 cars per im-age can take between 170 and 460 days-equivalent of time.
On the other hand, underestimating means having to col-lect more data at a later stage, incurring future costs and workﬂow delays. For instance in autonomous vehicle ap-plications, each period of data collection requires managing a ﬂeet of drivers to record driving videos. Thus, accurately estimating how much data is needed for a given task can reduce both costs and delays in the deep learning workﬂow.
There is a growing body of literature on estimating the sample complexity of machine learning models [4, 11, 12].
Recently proposed neural scaling laws suggest that gener-alization scales with the data set size according to a power
law [3, 14, 15, 20, 26]. Rosenﬁeld et al. [26] propose ﬁtting a power law function using the performance statistics from a small data set to extrapolate the performance for larger data sets; while not a focus of their paper, they suggest this can be used to estimate the data requirements. However, the power law function is not the only possible choice. We pro-pose in this paper to use it with similar functions that can be more accurate in practice. Figure 1 illustrates the data col-lection process in image classiﬁcation with the ImageNet data set [7] for the power law function and several effec-tive alternatives. When using small data sets to extrapolate, the ﬁtted functions may diverge in different ways from the ground truth performance curve. More importantly, even a small error in extrapolating accuracy can lead to large errors in over or under-estimating the data requirements, which may present huge operational costs.
In this paper, we ask: given a small training data set and a model not yet meeting target performance in some metric, what is the least amount of data we should collect to meet the target? Generalizing the estimation of data requirements from power laws, we investigate several alternate regression functions and show that all of them are well-suited towards estimating model performance. Moreover, each function is almost always either overly optimistic (i.e. under-estimating the data requirement) or pessimistic (i.e. over-estimating), meaning that there is no unique best regression function for all situations, but using all of the different functions, we can approximately bound the true data requirement. Through a simulation of the data collection workﬂow, we show that in-crementally collecting data over multiple rounds is critical to meeting the requirement without signiﬁcantly exceeding it. Finally, we introduce a simple correction factor to help these functions meet data requirement more often; this fac-tor can be learned by simulating on prior tasks. We explore classiﬁcation, detection, and segmentation tasks with differ-ent data sets, models, and metrics to show that our results hold in every setting considered.
Altogether, our empirical ﬁndings and proposed en-hancements yield easy-to-implement guidelines for data collection in real-world applications: practitioners should allocate for up to ﬁve rounds of data collection and use the correction factor introduced in this paper to augment an op-timistic regression function (e.g. Power Law, Logarithmic,
Algebraic Root) in order to accurately estimate data require-ments and ultimately collect only a relatively small amount more than the minimum data required to meet the desired performance. We believe that this approach can improve workﬂows and yield large cost savings in the future. 2.