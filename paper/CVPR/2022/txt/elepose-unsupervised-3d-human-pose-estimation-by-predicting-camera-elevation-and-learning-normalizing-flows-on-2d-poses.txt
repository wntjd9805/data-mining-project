Abstract
Human pose estimation from single images is a challeng-ing problem that is typically solved by supervised learn-ing. Unfortunately, labeled training data does not yet ex-ist for many human activities since 3D annotation requires dedicated motion capture systems. Therefore, we propose an unsupervised approach that learns to predict a 3D hu-man pose from a single image while only being trained with 2D pose data, which can be crowd-sourced and is already widely available. To this end, we estimate the 3D pose that is most likely over random projections, with the likelihood estimated using normalizing flows on 2D poses. While pre-vious work requires strong priors on camera rotations in the training data set, we learn the distribution of camera angles which significantly improves the performance. Another part of our contribution is to stabilize training with normalizing flows on high-dimensional 3D pose data by first projecting the 2D poses to a linear subspace. We outperform the state-of-the-art unsupervised human pose estimation methods on the benchmark datasets Human3.6M and MPI-INF-3DHP in many metrics. 1.

Introduction
Human pose estimation from single images is an on-going research topic with many applications in medicine, sports, and human-computer interaction. Tremendous im-provements have been achieved in recent years via machine learning. However, many recent approaches rely on a large amount of data used to train a 3D pose estimator in a su-pervised fashion. Unfortunately, such training data is hard to record and rarely available for specialized domains. For this reason, recent work focuses on reducing the amount of labeled data by using weak supervision in the form of unpaired 2D-3D examples, sparse supervision with a small amount of labeled 3D data, or multi-view setups during training. In contrast, we propose a method that is trained only from 2D data, which is easy to annotate by clicking
Figure 1. The elevation sampling for the creation of virtual views performed by traditional methods and by our approach are shown from left to right. The commonly used prior distribution of the ran-domly sampled elevation angle leads to errors if it does not exactly match the distribution in the training dataset. ElePose solves this problem by learning this distribution and compensating for it be-fore applying other transformations which significantly increases the performance of our method. visible keypoints in readily available images and thereby alleviates the 3D labelling and multi-view capture steps re-quired by weakly- and fully-supervised approaches.
Given observations of 2D human joints in monocular im-agery, we train a neural network to recover the depth —the missing third coordinate of the 3D human pose. With the same goals, Chen et al. [4] and Yu et al. [63] train a 3D pose estimator in an adversarial setting [11]. Their gener-ator predicts a 3D pose that is randomly rotated and pro-jected to a virtual camera which is fed into an adversarial network over ’fake’ projected 3D poses and the ’real’ 2D pose distribution. The idea is that, for a correct 3D pose prediction, the rotated and projected 2D pose should also come from the distribution of 2D training poses. However, the predicted 3D pose is rotated randomly over a fixed prior distribution defined relative to the camera coordinate sys-tem. It is a reasonable assumption when the camera is close to parallel to the ground plane. However, even for small elevation angles and even when modeling variation by sam-pling from a predefined Gaussian distribution, this leads to
random projections that cannot be found in the training data as shown in Fig. 1.
We build upon this concept and improve the handling of varying camera angles. Our core contribution is to train a network that predicts the elevation for every 2D input. After correcting for the predicted elevation, 3D reconstructions are upright such that rotating around the y-coordinate cor-responds to rotating around the up-direction and uniformly sampling from all possible azimuth angles is meaningful as human poses are generally symmetric around the direction of gravity and the ground normal. While camera angles have been estimated in supervised and weakly supervised settings [13, 14, 19, 56, 57] we do it for the monocular case and without supervision.
The approach of projecting to random virtual cameras re-quires to know the distributions of camera poses. Tailored to this, we propose a method for estimating the distribution of elevation angles from multiple point estimates, which fur-ther improves the performance of our model.
Another major change compared to previous work is that we use normalizing flows to learn a prior distribution over 2D poses, which is subsequently used to infer the most likely up-to-scale 3D pose. By contrast to GANs, which at best give a surrogate to the likelihood of an outcome with the discriminator response, our probabilistic formula-tion via normalizing flows naturally gives a likelihood for a predicted pose during inference time. Besides gaining a sig-nificant improvement in accuracy and robustness over exist-ing methods, our approach is also able to provide a measure for its performance, which is very valuable information in practical applications.
We overcome several technical challenges to make train-ing and inference tractable. First, the bijectivity of nor-malizing flows is a useful property, which enables them to avoid mode collapse. However, their construction restricts their input and output dimensions to be equal. For high-dimensional data, such as human poses, this leads to non-optimal convergence and an incomplete latent space. Sec-ond, the normalizing flow is still an approximation to the true pose distribution and can predict a high likelihood for poses that are outside the training distribution. Optimiz-ing the depth estimation network to produce 3D poses with high likelihoods for their back projections causes conver-gence to non-optimal solutions. To avoid this, we propose to first project the 2D poses to a lower-dimensional space given by a Principal Components Analysis (PCA) on the training data. Additionally, we introduce a suitable prior for the relative bone lengths in the human body to predict anthropometrically valid 3D poses. sented in current motion capture datasets. Source code: https://github.com/bastianwandt/ElePose.
Pose estimators could be abused for unwanted surveil-lance and our method could be used for motion pattern anal-ysis. However, we believe this risk is low since it does not reconstruct any visual features. 2.