Abstract 1.

Introduction
With increasing focus on augmented and virtual reality (XR) applications comes the demand for algorithms that can lift objects from images into representations that are suitable for a wide variety of related 3D tasks. Large-scale deployment of XR devices and applications means that we cannot solely rely on supervised learning, as collecting and annotating data for the unlimited variety of objects in the real world is infeasible. We present a weakly supervised method that is able to decompose a single image of an ob-ject into shape (depth and normals), material (albedo, re-ﬂectivity and shininess) and global lighting parameters. For training, the method only relies on a rough initial shape es-timate of the training objects to bootstrap the learning pro-cess. This shape supervision can come for example from a pretrained depth network or—more generically—from a traditional structure-from-motion pipeline. In our experi-ments, we show that the method can successfully de-render 2D images into a decomposed 3D representation and gen-eralizes to unseen object categories. Since in-the-wild eval-uation is difﬁcult due to the lack of ground truth data, we also introduce a photo-realistic synthetic test set that allows for quantitative evaluation. Please ﬁnd our project page at: https://github.com/Brummi/derender3d
From a single 2D image, humans can easily reason about the underlying 3D properties of an object, such as the 3D shape, the surface material and its illumination properties.
Being able to infer “object intrinsics” from a single image has been a long standing goal in Computer Vision and is often referred to as “inverse rendering” or “de-rendering” as it reverses the well-known rendering step of Computer
Graphics, where an image is generated from a similar set of object and material descriptors.
De-rendering an image into its physical components, not only plays an important role for general image understand-ing, but is also key to many applications, such as Aug-mented/Virtual Reality (XR) and Visual Effects (VFX). In these applications, a decomposed 3D representation can be used to increase the realism by enabling post-processing steps, such as relighting or changing the texture or mate-rial properties, which further blurs the line between real and synthetic objects in these environments.
As XR is moving from research and commercial use to consumer devices, a de-rendering method should work on a wide variety of images in the wild to allow a broad adoption of these technologies. While the history of image decom-position literature is long [13,14], recent learning-based ap-1
proaches have demonstrated this capability on speciﬁc cate-gories, such as human portraits [38] and synthetic ShapeNet objects [49], by training on ground-truth data, often ob-tained using synthetic models or sophisticated light stage capturing systems. However, obtaining large-scale ground-truth material and illumination annotations for general ob-jects “in the wild” is much more challenging and infeasible to collect for all objects. Models trained on synthetic data often lack sufﬁcient realism, resulting in poor transfer to real images. Models trained on real data usually focus on a single category (e.g. faces or birds [11, 17, 24, 25, 55]) and do not generalize to new classes.
On the other hand, another line of research that has re-cently gained interest, aims to learn 3D objects in an unsu-pervised or weakly-supervised fashion, without relying on explicit 3D ground-truth [17, 19, 25, 36, 55]. Although im-pressive results have been demonstrated in reconstructing 3D shapes of simple objects, few of the methods have con-sidered also recovering specular surface materials as this in-troduces even more ambiguities to the model. Furthermore, they are generally restricted to a single category.
In this paper, we explore the problem of learning non-Lambertian intrinsic decomposition from in-the-wild im-ages without relying on explicit ground-truth annotations.
In particular, we introduce a method that capitalizes on the coarse 3D shape reconstructions obtained from unsuper-vised methods and learns to predict a reﬁned shape as well as further decomposes the material into albedo and specular components, given a collection of single-view images.
At the core of the method lies an image formation pro-cess that renders the image from its individual components.
The model then learns to decompose the image through a reconstruction objective. Since this formulation is highly ambiguous, the model relies on several additional cues to enable learning a meaningful decomposition. We bootstrap the training using a coarse estimate of the initial shape. This estimate can come from a variety of sources. For datasets such as Co3D [40], where multi-view information is avail-able, we rely on traditional structure-from-motion pipelines (e.g. COLMAP [44]). For speciﬁc categories such as faces, existing specialized unsupervised methods can be used to obtain a coarse initial shape estimate. We present a sim-ple method that estimates initial material and light proper-ties using the coarse shape, the input image and a simple lighting model. We can then facilitate learning by using the coarse estimates as initial supervisory signals, which avoids many degenerate solutions that would fulﬁll the reconstruc-tion objective alone. Finally, to further improve the quality of the decomposition, we introduce a third objective, where the image is rendered with randomized light parameters, and a discriminator helps to ensure realistic reconstructions.
While we do need (pseudo) supervision of the coarse shape during training, the ﬁnal model can directly decom-pose an input image without any other input. We show that our model produces accurate and convincing image decom-positions that improve the state of the art and even general-izes beyond the categories of objects it was trained on. In our experiments, we show that the model works on a wide variety of objects from different datasets. However, as this is the ﬁrst method to tackle de-rendering in the wild, there is currently no suitable benchmark to quantitatively evalu-ate the quality of the decomposition. We thus also introduce a synthetic benchmark dataset, using photo-realistic render-ing of 10 objects from several viewpoints. Each image is associated with ground truth per-pixel material properties and lighting information that allows us to directly evaluate the decomposition. The new dataset, code and trained mod-els will be published together with the paper. 2.