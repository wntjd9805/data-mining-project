Abstract
Existing approaches for learning local image descrip-tors have shown remarkable achievements in a wide range of geometric tasks. However, most of them require per-pixel correspondence-level supervision, which is difficult to ac-quire at scale and in high quality. In this paper, we propose to explicitly integrate two matching priors in a single loss in order to learn local descriptors without supervision. Given two images depicting the same scene, we extract pixel de-scriptors and build a correlation volume. The first prior enforces the local consistency of matches in this volume via a pyramidal structure iteratively constructed using a non-parametric module. The second prior exploits the fact that each descriptor should match with at most one descrip-tor from the other image. We combine our unsupervised loss with a standard self-supervised loss trained from syn-thetic image augmentations. Feature descriptors learned by the proposed approach outperform their fully- and self-supervised counterparts on various geometric benchmarks such as visual localization and image matching, achiev-ing state-of-the-art performance. Project webpage: https:
//europe.naverlabs.com/research/3d-vision/pump. 1.

Introduction
Local image descriptors, usually extracted sparsely as keypoints, are at the core of numerous computer vision tasks such as large-scale visual localization [60], pose estimation [25], Structure-from-Motion (SfM) [56, 70], dense 3D reconstruction [63] and SLAM [6]. Nowadays, learning-based approaches [1, 18, 26, 43, 48, 62, 68, 69, 76] significantly outperform the standard handcrafted keypoints such as SIFT [34] or ORB [53]. They are often trained as-suming that numerous ground-truth pixel correspondences between pairs of images are available. These correspon-dences are most of the time obtained by considering a large collection of images for a given landmark and building a
Structure-from-Motion (SfM) reconstruction, as done for instance for the MegaDepth dataset [31]. This SfM pipeline
Figure 1. Qualitative impact of PUMP, our novel unsupervised loss, on a challenging image pair with illumination changes. We match keypoints extracted with models trained without (top) and with it (bottom), showing only matches that pass the geometric verification. Our unsupervised model finds more than twice as many true matches compared to a model trained without PUMP. nevertheless fails in many cases, yielding an unfathomable bottleneck to the kind of ground-truth data that can be gen-erated. The question we try to answer in this work is the is it possible to exploit the sleeping potential following: of unsupervised image pairs, i.e. image pairs without any ground-truth pixel correspondences?
In the remainder of the paper, we follow Truong et al. [67] and adopt a practical definition of unsupervised learning w.r.t. the feature learning task. We denote a learn-ing formulation ‘unsupervised’ if it does not require any supervision other than pairs of images depicting the same visual content. Inspired by the success of self-supervised learning for representation learning [8, 17, 24, 72], depth re-gression [15] and point cloud registration [2, 3], pure self-supervised learning approaches for local descriptors have provided partial answers to this question. They are trained on synthetically generated image pairs, where the second image is obtained by applying known transformations to
the first image, such as a random homography, color jit-tering or even style transfer [41]. However, homographies and the like cannot model the full range of possible trans-formations between real image pairs. In parallel, weakly-supervised methods have been proposed and demonstrate the ability to train from e.g. known camera poses [71]. Yet, this is only achievable through the use of complex acqui-sition setups that require the deployment of sensors based on different modalities (IMU or GPS), or again, resorting to SfM reconstructions. Recently, unsupervised learning of local descriptors has been introduced in the form of cycle consistency constraints across multiple images [67, 79], ei-ther requiring more images to extract features for training, or at the cost of iterative training of descriptors and expen-sive model fitting [74].
In this paper, we introduce a novel method to learn lo-cal descriptors without supervision. It is based on jointly enforcing two key matching priors: local consistency and uniqueness of the matching. The former simply amounts to state that two neighboring pixels of one image will likely match with two pixels forming a similar neighboring pair in the other image, up to a small deformation. We as-sume that this holds in general at any scale, hence this prior can be efficiently enforced through a pyramidal structure.
Inspired by DeepMatching [49], we employ a pyramidal non-parametric module that extracts higher-level correspon-dences enforcing the local consistency matching prior by design. The uniqueness prior, for its part, simply means that one pixel from the first image can correspond to at most one pixel in the second image. We enforce this property on high-level correspondences output by the DeepMatching module, which gracefully back-propagates along the pyra-mid to low-level pixel correspondences, enabling an effec-tive training of local descriptors without supervision.
We coin our proposed approach PUMP for Pyramidal and Uniqueness Matching Priors. It is trained in conjunc-tion with a self-supervised loss applied on synthetic image pairs. We experiment with both sparse and dense match-ing, either relying on external sparse keypoint detectors, or, in the case of dense matching, leveraging DeepMatching once again at test time to further enforce the two match-ing priors dynamically. We show that our unsupervised loss results in a significant increase of performance compared to a model trained solely using self-supervision and signif-icantly outperforms the state of the art on several tasks and benchmarks. In short, we make the following contributions:
• We revisit the key notion of matching prior for descrip-tor learning, and show their unreasonable effectiveness at training and test time.
• We introduce a novel unsupervised loss derived from these priors, termed PUMP, for training deep descrip-tors at the pixel level.
• We present experimental evidence that our approach significantly outperforms state-of-the-art methods on both dense and sparse matching tasks in spite of re-quiring less supervision and training data. 2.