Abstract
Input (65536 pts)
SA-ConvONet
POCO (ours)
Implicit neural networks have been successfully used for surface reconstruction from point clouds. However, many of them face scalability issues as they encode the isosur-face function of a whole object or scene into a single latent vector. To overcome this limitation, a few approaches in-fer latent vectors on a coarse regular 3D grid or on 3D patches, and interpolate them to answer occupancy queries.
In doing so, they lose the direct connection with the in-put points sampled on the surface of objects, and they at-tach information uniformly in space rather than where it matters the most, i.e., near the surface. Besides, relying on fixed patch sizes may require discretization tuning. To address these issues, we propose to use point cloud con-volutions and compute latent vectors at each input point.
We then perform a learning-based interpolation on nearest neighbors using inferred weights. Experiments on both ob-ject and scene datasets show that our approach significantly outperforms other methods on most classical metrics, pro-ducing finer details and better reconstructing thinner vol-umes. The code is available at https://github.com/ valeoai/POCO. 1.

Introduction
Constructing a surface or volume representation from 3D points sampled at the surface of an object or scene has nu-merous applications, from digital twins processing to aug-mented and virtual reality. Cheaper sensors directly produc-ing 3D points (depth cameras, low-cost lidars) and mature multi-view stereo techniques [88, 89] operating on images offer increasing opportunities for such reconstructions.
Traditional 3D reconstruction approaches [4] generally express the target surface as the solution to an optimiza-tion problem under some prior constraints. Possibly lever-aging visibility or normal information, they are generally scalable to large scenes and offer a substantial robustness to noise and outliers [47, 51, 71, 81, 94, 103, 110, 123]. Al-though some try to cope with density variation [9, 42, 43], a common limitation of these approaches is their inability to properly complete parts of the scene that are less densely (a) Scene 1 56 min 40 s 10 min 19 s (b) Scene 2 1 h 38 min 17 min 22 s
Figure 1. MatterPort3D. POCO trains on Synthetic Rooms 10k. sampled or that are missing (typically due to occlusions). A variety of hand-crafted priors try to address this complete-ness issue: local or global smoothness [58], decomposition into geometric primitives [87] (in particular for piecewise-planar man-made environments [3, 6, 14, 28, 72]) and struc-tural regularities [53, 79]. Data-driven priors have also been explored, based on shape retrieval [30], possibly with de-Figure 2. Overview of our method (inference). Given 3D points sampled on a surface, we construct latent vectors at each input point.
Then, to estimate the occupancy of a given query point in space, we interpolate with inferred weights the relative occupancy scores in a neighborhood. Last, a mesh is reconstructed based on occupancy queries (white blur indicates uncertainty) using a form of Marching cubes. formations [73]. But it remains limited in applicability.
To use richer priors, learning-based methods have been proposed, using explicit shape representations. Voxel-based approaches leverage a regular grid structure, extending 2D image-based techniques to 3D, but suffer from resolution limitations due to large memory consumption [20, 68, 111].
Directly generating a mesh with a neural network remains difficult [33] and is limited in practice to template deforma-tion [35]. Some forms of implicit representations have been used for point cloud generation, but providing much weaker geometrical and topological information [29, 55, 119].
More success has been achieved with explicitly-designed implicit representations, where the network encodes a func-tion R3 → R expressing a volume occupancy [15, 69] or a distance to the surface [70,77]. Such models require no dis-cretization and can address arbitrary topologies. More pre-cisely, discretization only occurs at mesh generation stage, using an algorithm such as the Marching cubes [63]. Yet, due to fully-connected architectures that lack translational equivariance, most existing approaches only operate on a single object and cannot apply to arbitrary scenes.
A few recent methods [17, 18, 22, 45, 80, 102], however, obtain a form of translational equivariance via Convolu-tional Neural Networks (CNNs). At least in theory, they can thus scale to larger scenes, possibly benefiting both from lo-cal and non-local information. But they operate on a vox-elized discretization whose vertices may be far from the in-put point cloud. They thus lose the direct connection with points sampled on the surface of objects. They are also sub-optimal in that the features or latent vectors holding the oc-cupancy or distance information are more or less uniformly distributed in space rather than focused where difficult de-cisions have to be made, i.e., near the surface.
Our approach, based on point convolution, overcomes these issues. It is illustrated on Fig. 2. Our contributions are:
• We attach features representing the implicit function to input points. Not only does it preserve point positions until later processing stages, rather than abstract them away too soon, but it concentrates the information to learn where it matters the most: close to the surface.
• We compute features using point convolution, which yields a natural coverage and scalability to scenes of arbitrary size. (Rather than tailor yet another specific network architecture, we rely on a general point convo-lution backbone, which offers prospects for improve-ment when better point convolutions are designed.)
• Rather than relying on hand-designed forms of aver-aging, we extend prior learning to interpolation, which we apply to query-relative features rather than global features, as others do, as it leads to better results.
• We propose an efficient test-time augmentation to treat inputs of high density or large size.
• While simple, our approach outperforms other meth-ods both on object and scene datasets, yielding finer details.
It is robust to domain shift (training on ob-jects, testing on scenes) and faster than methods that overfit to a scene or infer from scratch for each query. 2.