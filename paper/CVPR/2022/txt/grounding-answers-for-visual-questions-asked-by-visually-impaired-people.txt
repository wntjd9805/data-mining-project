Abstract
Visual question answering is the task of answering questions about images. We introduce the VizWiz-VQA-Grounding dataset, the first dataset that visually grounds answers to visual questions asked by people with visual im-pairments. We analyze our dataset and compare it with five VQA-Grounding datasets to demonstrate what makes it similar and different. We then evaluate the SOTA VQA and VQA-Grounding models and demonstrate that current
SOTA algorithms often fail to identify the correct visual ev-idence where the answer is located. These models regu-larly struggle when the visual evidence occupies a small fraction of the image, for images that are higher quality, as well as for visual questions that require skills in text recognition. The dataset, evaluation server, and leader-board all can be found at the following link: https:
//vizwiz.org/tasks-and-datasets/answer-grounding-for-vqa/. 1.

Introduction
Visual question answering (VQA) is the task of provid-ing a natural language answer to a question about an image.
While most VQA services only return a natural language answer, our work is motivated by the belief that it is also valuable for a VQA service to return the region in the im-age used to arrive at the answer. We call this task of locating the relevant visual evidence answer grounding.
Numerous applications would be possible if answer groundings were provided in response to visual questions.
First, they enable assessment of whether a VQA model rea-sons based on the correct visual evidence. This is valuable as an explanation as well as to support developers in debug-ging models. Second, answer groundings enable segment-ing the relevant content from the background. This is a valu-able precursor for obfuscating the background to preserve privacy, given that photographers can inadvertently capture private information in the background of their images [14] (exemplified in Figure 1b). Third, users with low vision could more quickly find the desired information if a ser-vice instead magnified the relevant visual evidence. This is
Figure 1. (a) We introduce a new dataset challenge that supports the task of grounding the visual evidence needed to answer visual questions asked by people with vision impairments. This enables valuable use cases including (b) background obfuscation to limit inadvertent privacy leaks and (c) automatic magnification to expe-dite low vision users’ abilities to answer their questions. valuable in part because answers from VQA services can be insufficient, including because humans suffer from “report-ing bias” meaning they describe what they find interesting without understanding what a person/population is seeking.
This is exemplified in Figure 1c, where the most popular re-sponse from 10 answers is the generic answer ‘pasta’ rather than the specific flavor, ‘Creamy Tomato Basil Penne’.
While datasets have been introduced to encourage progress on the answer grounding problem, all proposed dataset challenges originate from contrived visual ques-tions [6, 9, 11, 17, 18, 22, 26, 37, 42]. This includes scrap-ing images from photo-sharing websites (e.g., Flickr) and then generating questions automatically [6], by using image annotations paired with question templates to create ques-tions about the images or (2) manually [9, 11, 18, 42], by asking crowdworkers to make up questions about an im-age that would stump a robot. Yet, prior work has shown that such contrived settings can manifest different char-acteristics from authentic VQA use cases [15, 39]. This can cause algorithms trained and evaluated on contrived
datasets to perform poorly when deployed for authentic use cases [15, 16]. Moreover, this can limit the designs of algo-rithms since developers are oblivious to the additional chal-lenges their algorithms must overcome.
We introduce the first answer grounding dataset that originates from an authentic use case. We focus on vi-sual questions originating from blind people who both took the pictures and asked the questions about them in order to overcome real visual challenges [5]. This use case has been shown to manifest different challenges than contrived set-tings, including that images are lower quality [8], questions are more conversational [15], and different vision skills are needed to arrive at answers [39]. For approximately 10,000 image-question pairs submitted by this population, we col-lected answer groundings. Then, we analyzed the answer groundings to reveal their characteristics and show how they relate/differ to five existing answer grounding datasets. Fi-nally, we benchmarked state-of-the-art VQA and answer grounding models on our dataset and demonstrate what makes this dataset difficult for them, including smaller an-swer groundings, images that are of higher quality, and vi-sual questions that require skills in text recognition.
We offer this work as a foundation for designing mod-els that are robust to a larger range of potential challenges that can arise in real-world VQA settings. Challenges ob-served in our dataset can generalize to other scenarios, such as robotics and lifelogging, which similarly encounter vary-ing image quality and textual information (e.g., grocery stores). To encourage community-wide progress on such challenges, we have organized a dataset challenge with pub-lic evaluation server and leaderboard. Details can be found at the following link: https://vizwiz.org/tasks-and-datasets/answer-grounding-for-vqa/. 2.