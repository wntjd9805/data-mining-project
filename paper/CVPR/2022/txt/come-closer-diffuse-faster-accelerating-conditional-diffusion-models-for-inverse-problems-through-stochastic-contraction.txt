Abstract
Diffusion models have recently attained significant interest within the community owing to their strong performance as generative models. Furthermore, its application to inverse problems have demonstrated state-of-the-art performance.
Unfortunately, diffusion models have a critical downside
- they are inherently slow to sample from, needing few thousand steps of iteration to generate images from pure
In this work, we show that starting
Gaussian noise. from Gaussian noise is unnecessary.
Instead, starting from a single forward diffusion with better initialization significantly reduces the number of sampling steps in the
This work was supported by Institute of Information & communica-tions Technology Planning & Evaluation (IITP) grant funded by the Ko-rea government(MSIT) (No.2019-0-00075, Artificial Intelligence Gradu-ate School Program(KAIST)), and by National Research Foundation(NRF) of Korea grant NRF-2021M3I1A1097938 reverse conditional diffusion. This phenomenon is formally explained by the contraction theory of the stochastic differ-ence equations like our conditional diffusion strategy - the alternating applications of reverse diffusion followed by a non-expansive data consistency step. The new sampling strategy, dubbed Come-Closer-Diffuse-Faster (CCDF), also reveals a new insight on how the existing feed-forward neural network approaches for inverse problems can be synergistically combined with the diffusion models. Ex-perimental results with super-resolution, image inpainting, and compressed sensing MRI demonstrate that our method can achieve state-of-the-art reconstruction performance at significantly reduced sampling steps. 1.

Introduction
Denoising diffusion models [8, 10, 15, 29] and score-based models [31,32,34] are new trending classes of gener-Figure 2. Plot of average error ¯ε vs. time t, using different approaches. (a) Conditional diffusion starts from Gaussian noise x(t) and uses full reverse diffusion. (b) CCDF with vanilla initialization: Corrupted data is forward-diffused with a single step up to t = t0, and reverse diffused. (c) CCDF with NN initialization: Initialization with reconstruction from pre-trained NN lets us use much smaller timestep t = t′ 0 < t0, and hence faster reverse diffusion. ative models, which have recently drawn signficant atten-tion amongst the community due to their state-of-the-art performance. Although inspired differently, both classes share very similar aspects, and can be cast as variants of each other [12, 15, 34], thus they are often called diffusion models.
In the forward diffusion process, a sampled data point x at time t = 0 is perturbed gradually with Gaussian noise until t = T , arriving approximately at spherical Gaussian distribution, which is easy to sample from. In the reverse diffusion process, starting from the sampled noise at t = T , one uses the trained score function to gradually denoise the data up to t = 0, arriving at a high quality data sample.
Interestingly, diffusion models can go beyond uncondi-tional image synthesis, and have been applied to conditional image generation, including super-resolution [5, 17, 25], in-painting [31, 34], MRI reconstruction [6, 13, 33], image translation [5, 19, 28], and so on. One line of works re-design the diffusion model specifically suitable for the task at hand, thereby achieving remarkable performance on the given task [17, 25, 28]. However, they compromise flexi-bility since the model cannot be used on other tasks. An-other line of works, on which we build our method on, keep the training procedure intact, and only modify the inference procedure such that one can sample from a conditional dis-tribution [5, 6, 13, 33, 34]. These methods can be thought of as leveraging the learnt score function as a generative prior of the data distribution, and can be flexibly used across dif-ferent tasks.
Unfortunately, a critical drawback of diffusion models is that they are very slow to sample from. To address this, for unconditional generative models, many works focused on either constructing deterministic sample paths from the stochastic counterparts [30, 34], searching for the optimal steps to take after the training of the score function [4, 38], or by retraining student networks that can take shortcuts via knowledge distillation [18, 26]. Orthogonal and com-plementary to these prior works, in this work, we focus on accelerating conditional diffusion models by studying the contraction property [21–23] of the reverse diffusion path.
Specifically, our method, which we call Come-Closer-Diffuse-Faster (CCDF), first perturbs the initial estimate via forward diffusion path up to t0 < T , where t0 de-notes the time where the reverse diffusion starts. This for-ward diffusion comes almost for free, without requiring any passes through the neural network. While the distribution of forward-diffused (noise-added) images increases the es-timation errors from the initialization as shown in Fig. 2(b), the key idea of the proposed CCDF is that the reverse con-ditional diffusion path reduces the error exponentially fast thanks to the contraction property of the stochastic differ-ence equation [22,23]. Therefore, compared to the standard approach that starts the reverse diffusion from Gaussian dis-tribution at t = T (see Fig. 2(a)), the total number of the re-verse diffusion step to recover a clean images using CCDF can be significantly reduced. Furthermore, with better ini-tialization, we prove that the number of reverse sampling can be further reduced as shown in Fig. 2(c). This implies that the existing neural-network (NN) based inverse solu-tion can be synergistically combined with diffusion models to yield accurate and fast reconstruction by providing a bet-ter initial estimate.
Using extensive experiments across various problems such as super-resolution (SR), inpainting, and MRI recon-struction, we demonstrate that CCDF can significantly ac-celerate diffusion based models for inverse problems. 2.