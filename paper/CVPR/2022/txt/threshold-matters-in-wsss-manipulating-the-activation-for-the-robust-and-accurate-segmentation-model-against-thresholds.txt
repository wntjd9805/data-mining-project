Abstract
Weakly-supervised semantic segmentation (WSSS) has recently gained much attention for its promise to train seg-mentation models only with image-level labels. Existing
WSSS methods commonly argue that the sparse coverage of CAM incurs the performance bottleneck of WSSS. This paper provides analytical and empirical evidence that the actual bottleneck may not be sparse coverage but a global thresholding scheme applied after CAM. Then, we show that this issue can be mitigated by satisfying two conditions; 1) reducing the imbalance in the foreground activation and 2) increasing the gap between the foreground and the back-ground activation. Based on these findings, we propose a novel activation manipulation network with a per-pixel classification loss and a label conditioning module. Per-pixel classification naturally induces two-level activation in activation maps, which can penalize the most discrimina-tive parts, promote the less discriminative parts, and de-activate the background regions. Label conditioning im-poses that the output label of pseudo-masks should be any of true image-level labels; it penalizes the wrong activation assigned to non-target classes. Based on extensive analy-sis and evaluations, we demonstrate that each component helps produce accurate pseudo-masks, achieving the ro-bustness against the choice of the global threshold. Finally, our model achieves state-of-the-art records on both PAS-CAL VOC 2012 and MS COCO 2014 datasets. The code is available at https://github.com/gaviotas/AMN. 1.

Introduction
Weakly-supervised semantic segmentation (WSSS) re-quires only weak supervision (e.g., image-level labels [35, 36], scribbles [31], bounding boxes [19]) as opposed to the fully supervised model, which involves costly pixel-level annotations. In this work, we address WSSS using image-*indicates an equal contribution.
†Hyunjung Shim is a corresponding author.
Figure 1. Motivating examples show that the optimal threshold per image (τopt) from the same dog class is quite different from each other. (a) The distribution of the optimal threshold on PASCAL
VOC 2012 train set, (b) the activation maps, (c) the thresholded masks using a global threshold τglobal = 0.15. level labels because of its low labeling cost. The overall pipeline of WSSS consists of two stages. The pseudo-mask is first generated from an image classifier, and then it is used as supervision to train a segmentation network.
The prevalent technique for generating pseudo-masks is class activation mapping (CAM) [45]. It uses the interme-diate classifier’s activation to compute the class activation map corresponding to its image-level label. The common practice of WSSS is to apply a global threshold to the acti-vation map (i.e., assigning the object class if the activation is greater than the threshold) for obtaining the pseudo-mask.
Existing methods point out that the pseudo-mask obtained from CAM only captures the most discriminative parts of the object, incurring the performance bottleneck. There-fore, most existing studies have expanded object coverage by manipulating the image [29, 39] or feature map [18, 26].
However, we argue that the performance bottleneck of
WSSS comes from a global threshold applied after CAM; the sparse object coverage does not explain all. This thresh-old partitions each activation map into the foreground (ob-ject class) and background. Then, the pseudo-mask is gen-erated by combining all foreground regions. Here, the choice of threshold critically affects the performance of
WSSS. i) We further observe that a global threshold cannot provide an optimal threshold per image. Figure 1(a) visu-alizes the distribution of optimal threshold on the PASCAL
VOC 2012 train set (For analysis, we obtain the best thresh-old per image using its ground-truth segmentation map).
It shows that the optimal threshold per image quite differs from each other, and the global threshold is often far from ii) Besides, a global threshold for CAM the optimal one. does not always lead to sparse coverage. Figure 1(b) and (c) show several CAM examples and the corresponding masks generated by a global threshold, respectively; the third row shows that CAM and the thresholded mask overly capture the target object. These results clearly motivate us to rethink that the performance bottleneck of WSSS is closely related to the usage of a global threshold.
To tackle this problem, we first investigate why this prob-lem happens. By tracing the procedure of CAM, we real-ize that global average pooling (GAP) applied to the last layer invokes this issue; the global threshold largely differs from the optimal threshold per image. The first stage of the
WSSS framework trains the image classifier, whose score is computed via GAP. While GAP facilitates deriving the activation map, it averages the feature maps into a single classification score. The same value can be from totally dif-ferent activations. For example, the same score can be from 1) high activations only in the most discriminative region (low optimal threshold to cover more regions), 2) moder-ate activations distributed over the entire object, or 3) small activations covering even outside the object (high optimal threshold to cover small regions).
Due to its averaging nature, GAP hinders achieving the accurate pseudo-mask via a global threshold. As a na¨ıve solution, one might consider introducing a different thresh-old per image. However, this is prohibitive because finding a per-image threshold requires pixel-level annotation, vio-lating the principle of weakly-supervised learning. Instead of controlling a threshold per image, our key idea is to ma-nipulate the activation in a way that the resultant pseudo-mask is of high quality regardless of threshold values. To achieve robust performance, we can increase the activation gap between the foreground region and the background re-gion; the thresholded masks are the same if the threshold value is within the gap. However, it can induce the model to capture the most discriminative parts only, resulting in consistent but poor quality.
To achieve high quality consistently, it is important to reduce the activation imbalance within the foreground and keep the large activation gap between the foreground and background simultaneously. We can achieve the two fac-tors by assigning the two-level activation for the entire fore-ground pixels and background pixels (e.g., 1 and 0). In this way, the high activation in the most discriminative parts is penalized, but the low activation in the less discriminative parts is promoted. Meanwhile, the background activation can be deactivated. Naturally, this strategy can guarantee a large gap, enabling us to achieve the robust performance even with a global threshold.
Specifically, we introduce a robust and accurate activa-tion manipulation network (AMN), which takes an image with its image-level label as the input and provides the high-quality pseudo-mask as the output. For that, we formu-late a training objective using i) per-pixel classification with an effective constraint using ii) label conditioning. Since per-pixel classification does not rely on GAP, it bypasses the issue of having totally different activation maps for the same classification score. More importantly, it directly en-forces the same large activation for the foreground (e.g., 1) and the same small activation for the background (e.g., 0).
Here, it leads to reducing the activation imbalance inside the foreground and having a large gap between the fore-ground and background. Since we do not have pixel-level supervision to formulate per-pixel classification problems, the noisy pseudo-mask from CAM with conditional ran-dom field (CRF) [23] serves as the initial target for training
AMN.
Moreover, we propose label conditioning to reduce the activation of non-target classes. The idea of label condi-tioning is to reformulate the label prediction problem by finding the best prediction out of the given K classes (K is the number of classes given by the ground-truth image-level label per image) and background, instead of N + 1 classes (i.e., a total of N foreground classes and a back-ground class). K is always much less than N and thereby the range of possible answers is clearly reduced. It makes the problem better-posed. More importantly, the activa-tion of non-target classes is strictly suppressed by map-ping to 0.
It helps strengthen the foreground activation.
As a result, with the same global threshold as the previ-ous studies [1, 25, 27], AMN largely improves the quality of pseudo-mask and eventually records the state-of-the-art performances on the Pascal VOC 2012 and MS COCO 2014 benchmarks. 2.