Abstract
CNN image classifiers are widely used, thanks to their efficiency and accuracy. However, they can suffer from bi-ases that impede their practical applications. Most existing bias investigation techniques are either inapplicable to gen-eral image classification tasks or require significant user efforts in perusing all data subgroups to manually specify which data attributes to inspect. We present VISCUIT, an interactive visualization system that reveals how and why a CNN classifier is biased. VISCUIT visually summa-rizes the subgroups on which the classifier underperforms and helps users discover and characterize the cause of the underperformances by revealing image concepts responsi-ble for activating neurons that contribute to misclassifica-tions. VISCUIT runs in modern browsers and is open-source, allowing people to easily access and extend the tool to other model architectures and datasets. VISCUIT is available at the following public demo link: https:
//poloclub.github.io/VisCUIT. A video demo is available at https://youtu.be/eNDbSyM4R_4. 1.

Introduction
Recently, data classification algorithms are widely used for practical applications, such as face recognition [39, 50, 53], autonomous driving [19, 41], and clinical trials [44, 51, 59]. Despite the fact that visual models outperform humans in some circumstances [7], several works have found that these classifiers are often biased with disparate performance across data subgroups [4, 8, 29, 35]. Exploiting the biased classifiers for critical purposes can cause unintentional fair-ness violation and huge societal problems [20, 37, 55, 61].
Likewise, image classifiers based on deep convolutional neural networks (CNN), which have achieved state-of-the-art performance in various areas [22, 30, 45, 48, 49], often suffer from biases [29]. To facilitate real-world applications of the state-of-the-art techniques, there have been attempts to understand [3, 11, 15] and mitigate [14, 21, 52, 54] the biases in CNN classifiers. However, most existing meth-ods require humans to specify the attributes on which to audit the classifiers. As people tend to focus more on sen-sitive attributes (e.g., race, gender), less sensitive attributes (e.g., wearing glasses, hair color) that can correlate with bi-ases and degrade the overall performance are easily missed.
Existing approaches assume availability of additional at-thus, tributes other than the class label for each image; datasets without any additional attributes cannot be ana-lyzed using these methods.
Krishnakumar et al. [28] proposed UDIS, which auto-matically detects the data subgroups, on which a CNN clas-sifier underperforms. While UDIS does not require addi-tional attribute labels, the approach produces a large num-ber of potentially biased subgroups which may or may not align with semantic concepts. This leads to ambiguous find-ings even after substantial manual inspection. Moreover, most aforementioned bias investigation approaches detect the source of biases in classifiers, primarily focusing on their training datasets, not how the neurons in the classifier are activated and generate biased outputs [3, 8, 13].
In this paper, we present VISCUIT, an interactive vi-sualization system that reveals how and why a CNN image classifier is biased, without requiring users to pre-determine which attributes to inspect. VISCUIT’s major contributions include:
• Visual summarization of the undeperforming sub-groups. VISCUIT highlights the data subgroups gen-erated by UDIS [28] on which a CNN classifier un-derperforms. This allows users to understand how the classifier is biased, not limiting the bias factors to the
Figure 1. VISCUIT reveals how and why a CNN image classifier is biased. Our user Jane trains a classifier using the biased CelebA dataset, which has high co-occurrence of the attribute black hair and the label smiling, to observe how the training data affects model predictions.
She hypothesizes that the model would use the attribute black hair to predict smiling and launches VISCUIT to verify her hypothesis. (A)
Subgroup Panel displays underperforming data subgroups found by UDIS [28]. Jane figures out that several underperforming subgroups consist of people with black hair. To see whether the model indeed uses the attribute black hair for predictions, Jane clicks on subgroup
#14, and VISCUIT displays subgroup #380, which is similar to #14 in terms of the last-layer feature vectors from the model but has high accuracy. Clicking on an image in each of those subgroups brings up a Grad-CAM Window, which shows that the classifier attends to (A1) forehead (near hair, irrelevant to smiling) for the subgroup #14 and (A2) mouth (relevant to smiling) for the subgroup #380. (A3) Confusion matrices quantitatively summarize such misclassifications that many not smiling black-haired people are wrongly classified as smiling. Jane is now certain that the classifier uses the attribute black hair for predicting smiling and therefore often misclassifies black-haired people. (B)
The Neuron Activation Panel enables users to understand which neurons and concepts are responsible for misclassifications, by organizing the neurons in the model into 3 columns: the left column for the neurons highly activated only by underperforming subgroup, the right only by well-performing subgroup, and the middle by both. Clicking on a neuron displays a Neuron Concept Window, which reveals that (B1, B2) the subgroups #14 and #380 activate the neurons for the area near forehead and mouth, respectively. sensitive attributes. As VISCUIT summarizes the un-deperforming subgroups as a list, users can easily char-acterize each subgroup. For each undeperforming sub-group, VISCUIT also displays its most similar sub-group with high accuracy, based on Euclidean distance in the feature space, enabling users to gain insights into the deviant features responsible for the biases [28].
• Visual bias attribution in CNN image classifiers.
VISCUIT demonstrates why a CNN classifier under-performs on each subgroup, by revealing image con-cepts responsible for activating neurons that contribute to the underperformances. Users can observe how the classifier is activated differently by the underperform-ing and well-performing subgroups, focusing on the high-level concepts in images. Moreover, for each im-age, VISCUIT displays Grad-CAM Window, which visually highlights features in an input image deemed relevant for classification [43].
• Open-sourced, web-based implementation. VIS-CUIT runs directly in modern browsers and is open-source,1 allowing people to easily access and extend the tool to other model architectures and datasets. Fig-ure 1 illustrates the user interface of VISCUIT. VIS-CUIT is available at the following public demo link: https://poloclub.github.io/VisCUIT. A video demo is available at https://youtu.be/ eNDbSyM4R_4. 1Code: https://github.com/poloclub/VisCUIT
2.