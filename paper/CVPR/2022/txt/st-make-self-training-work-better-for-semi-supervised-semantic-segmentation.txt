Abstract
Self-training via pseudo labeling is a conventional, sim-ple, and popular pipeline to leverage unlabeled data. In this work, we first construct a strong baseline of self-training (namely ST) for semi-supervised semantic segmentation via injecting strong data augmentations (SDA) on unlabeled images to alleviate overfitting noisy labels as well as de-couple similar predictions between the teacher and student.
With this simple mechanism, our ST outperforms all existing methods without any bells and whistles, e.g., iterative re-training. Inspired by the impressive results, we thoroughly investigate the SDA and provide some empirical analysis.
Nevertheless, incorrect pseudo labels are still prone to ac-cumulate and degrade the performance. To this end, we fur-ther propose an advanced self-training framework (namely
ST++), that performs selective re-training via prioritizing reliable unlabeled images based on holistic prediction-level stability. Concretely, several model checkpoints are saved in the first stage supervised training, and the discrepancy of their predictions on the unlabeled image serves as a mea-surement for reliability. Our image-level selection offers holistic contextual information for learning. We demon-strate that it is more suitable for segmentation than com-mon pixel-wise selection. As a result, ST++ further boosts the performance of our ST. Code is available at https:
//github.com/LiheYoung/ST-PlusPlus. 1.

Introduction
Fully-supervised semantic segmentation learns to assign pixel-wise semantic labels via generalizing from numer-ous densely annotated images. Despite the rapid progress
[9, 60], the pixel-wise manual labeling is costly, labori-*Corresponding author. Work supported by National Key Research and
Development Program of China (2019YFC0118300), NSFC Major Pro-gram (62192783), CAAI-Huawei MindSpore Project (CAAIXSJLJJ-2021-042A), China Postdoctoral Science Foundation Project (2021M690609), and Jiangsu Natural Science Foundation Project (BK20210224).
Figure 1. Performance comparisons between our ST/ST++ and the state-of-the-art methods on the Pascal. It is worth noting that the proposed ST and ST++ surpass previous best results significantly, especially in the extremely scarce-label regime, e.g., 92 labels. ous, and even infeasible, precluding its deployment in some scenes such as medical image analysis. To avert the labor-intensive procedure, semi-supervised semantic segmenta-tion has been proposed to learn a model from a handful of labeled images along with abundant unlabeled images.
The core challenge in semi-supervised setting lies in how to effectively utilize the unlabeled images. Prior works in semi-supervised learning (SSL) propose to apply en-tropy minimization [33, 53] or consistency regularization
[21, 52] on unlabeled images. With increasingly sophisti-cated mechanisms introduced to this field, FixMatch [47] breaks the trend and achieves inspiring results via inte-grating both strategies into a hybrid framework with few hyper-parameters. Motivated by the tremendous progress in SSL, recent works in semi-supervised semantic segmen-tation have evolved from GANs-based methods [20, 38] to delving into consistency regularization from the segmenta-tion perspective, such as enforcing consistent predictions of the same unlabeled image under strong-weak perturba-tions [66], of the same local patch from different contextual crops [31], and of the same unlabeled image between dual differently initialized models [12, 17].
Nevertheless, are the delicate mechanisms indispensable for semi-supervised semantic segmentation? More impor-tantly, is the straightforward self-training scheme [33] pro-posed around a decade ago already out-of-date for this task? In this work, we intuitively and empirically present two simple and effective techniques to bring back the clas-sical self-training method as a strong competitor again.
The self-training [33] is commonly regarded as a form of entropy minimization in SSL, since the re-trained student is supervised with hard labels produced by the teacher which is trained on labeled data. However, they suffer severe cou-pling issue, i.e., making similar predictions on the same in-put. We notice that, however, injecting strong data augmen-tations (SDA) on unlabeled images is extremely beneficial to decouple their predictions as well as alleviate overfitting on noisy pseudo labels. Despite the simplicity, self-training with SDA significantly surpasses existing methods without any bells and whistles, e.g., without the need of iterative re-training [55], manually choosing a threshold for filtering in-correct labels [66], or repetitively producing pseudo labels for each training minibatch [12, 25, 66]. Inspired by the im-pressive results, we thoroughly investigate the SDA and find that it is fully compatible with the off-the-shelf augmenta-tion strategies in contrastive learning [10], e.g., colorjitter, grayscale, and blur, which deteriorate clean data distribu-tion but perform surprisingly well for unlabeled data. Be-sides, we examine individual effectiveness of each data aug-mentation, and observe that the simple colorjitter plays most effectively and different augmentations are complementary to each other. Formally, this basic self-training framework with SDA is named as ST in this work, serving as a strong baseline for our full method.
Another longstanding but underestimated issue is that the classical self-training framework utilizes all unlabeled images at the same time. Nevertheless, different unlabeled images cannot be equally easy [34, 44, 50] and the corre-sponding pseudo labels cannot be equally reliable, lead-ing to severe confirmation bias [2] and potential perfor-mance degradation when iteratively optimizing the model with those ill-posed pseudo labels. To this end, we fur-ther propose an advanced ST++ framework based on our
ST, that automatically selects and prioritizes more reliable images in the re-training phase to produce higher-quality artificial labels on the remaining less reliable images. The measurement for the reliability or uncertainty of an unla-beled image is to compute the holistic stability of the evolv-ing pseudo masks in different iterations during the entire training course. Note that, different from the common prac-tice of manually setting a fixed confidence threshold to filter low-confidence pixels [66], we demonstrate that our image-level selection based on the stability of evolving predictions can provide holistic contextual regions for model training, which is more appropriate to the segmentation task.
It is worth noting that the classical self-training pipeline
[33] is attracting increasing attention [43, 53] in the semi-supervised setting. Our work differs from them in that we empirically and systematically study the effectiveness of strong data augmentations on unlabeled data and further propose an advanced self-training framework with selec-tive re-training property. More concrete differences are dis-cussed in detail in the related work. Our main findings and contributions are summarized as follows:
• We construct a strong baseline (ST) of self-training in semi-supervised semantic segmentation via injecting strong data augmentations on unlabeled images during re-training. Motivated by the promising performance, we provide intuitive explanations and systematically investigate the role of SDA and each augmentation.
• Built on our ST, to alleviate the potential performance degradation incurred by incorrect pseudo labels, we further propose an advanced self-training framework
ST++, that performs selective re-training via prioritiz-ing reliable images based on holistic prediction-level stability in the entire training course. We demonstrate that the image-level selection is more suitable for seg-mentation task compared with pixel-wise selection.
• The ST and ST++ both outperform previous methods across extensive settings and architectures on the Pas-cal and Cityscapes dataset, with few hyper-parameters. 2.