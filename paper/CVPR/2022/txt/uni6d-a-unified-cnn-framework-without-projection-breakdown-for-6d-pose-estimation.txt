Abstract
As RGB-D sensors become more affordable, using RGB-D images to obtain high-accuracy 6D pose estimation re-sults becomes a better option. State-of-the-art approaches typically use different backbones to extract features for RGB and depth images. They use a 2D CNN for RGB images and a per-pixel point cloud network for depth data, as well as a fusion network for feature fusion. We find that the essential reason for using two independent backbones is the “projec-tion breakdown” problem. In the depth image plane, the projected 3D structure of the physical world is preserved by the 1D depth value and its built-in 2D pixel coordinate (UV). Any spatial transformation that modifies UV, such as resize, flip, crop, or pooling operations in the CNN pipeline, breaks the binding between the pixel value and UV coordi-nate. As a consequence, the 3D structure is no longer pre-served by a modified depth image or feature. To address this issue, we propose a simple yet effective method denoted as Uni6D that explicitly takes the extra UV data along with
RGB-D images as input. Our method has a Unified CNN framework for 6D pose estimation with a single CNN back-bone. In particular, the architecture of our method is based on Mask R-CNN with two extra heads, one named RT head for directly predicting 6D pose and the other named abc head for guiding the network to map the visible points to their coordinates in the 3D model as an auxiliary module.
This end-to-end approach balances simplicity and accu-racy, achieving comparable accuracy with state of the arts and 7.2× faster inference speed on the YCB-Video dataset. (a) Projection breakdown caused by the RoI transformation, including crop, resize and RoI-Align [8]: The red dotted line connects a 3D object and its projected RoI in the depth image. Any pixel (d, u, v) in the RoI and its corresponding point (a, b, c) on the 3D object follow the projection equation. The equation no longer holds if the built-in coordinate (UV) is modified by RoI-Align, as though the RoI was moved to the top left corner of the image. (b) Introducing UV data fixes the problem of projection breakdown and improve the 6D pose estimation performance with various spatial transfor-mations. x axis is the training epoch, and y axis is the testing accuracy.
Figure 1. Visualization and experiment results of projection break-down problem. 1.

Introduction 6D pose estimation plays a fundamental role in emerg-ing applications, e.g., autonomous driving [1–3], intelligent robotic grasping [4–6] and augmented reality [7]. RGB-D sensors provide a direct signal of the surface texture and ge-ometry of the physical 3D world, and as their prices fall,
RGB-D images are becoming a more attractive data source for 6D pose estimation.
However, state of the arts [2,9–13] typically use two sep-arate backbone networks to extract features for RGB and depth images, with a 2D CNN for RGB images and per-pixel PointNet [14] or PointNet++ [15] for depth data. After these features have been obtained, an additional fusion pro-cess is designed to blend them. Existing methods use differ-ent backbones to account for the heterogeneity of these two types of data. However, the key reason they can’t extract features with a single backbone is hidden in the classic 3D vision projection equation:


 u v
 = 1


 x y
 = d
K d
K
Rz(a, b, c) + Tz

R ×

 a b c


 + T
 , (1) which describes a point (a, b, c)1 first rotated by the rota-tion matrix R ∈ SO(3) and translated by T ∈ R3 to the position (x, y, d) in camera coordinate system.
It is then projected to the pixel at (u, v) in image plane using the in-trinsic matrix K of the camera. The 3D structure of the visible points in the 3D model seen from the camera’s per-spective is preserved by the depth value (d) of each pixel and its coordinate (u, v) in the depth image. It implies in a single-channel depth image, each pixel value is linked to its built-in coordinates. The plain UV can be used to preserve the 3D structure and changing it will break the 3D projec-tion equation. However, in the traditional CNN pipeline, spatial transformations such as pooling, crop, RoI-Align [8] in the convolution operator, and resize, crop, and flip in data augmentation do change the UV. As shown in Fig. 1a, when those spatial transformations are applied to a depth image, the projection equation is broken, and we call this “projec-tion breakdown”. This is the primary reason why the tradi-tional CNN pipeline struggles to process RGB images and depth data at the same time.
In this paper, we propose a simple yet effective method to save the projection breakdown: explicitly feeding extra
UV data along with depth to 2D CNN. D+UV acts as 3D data, making the value (d, u, v) at each pixel self-complete and decoupling d from its built-in coordinate in the image plane. Given the self-complete information at each pixel, the projection equation still holds after spatial transforma-tions. As a result, we can extract features from RGB-D im-ages using the traditional CNN pipeline, including its data augmentation methods. As shown in Fig 1b, the accuracy of 6D pose estimation is greatly improved with extra UV data. Extensive experiments show that transformations in data augmentation greatly harm the accuracy without using
UV data, but improve the accuracy with UV data is used.
Eq. 1 also indicates that the network should map visible 1We refer a point by its coordinate in object coordinate system. points in an RGB-D image to their original coordinates in the 3D model. As far as we know, Existing keypoint-based 6D pose estimation approaches, such as [6] and [16], learn the 3D offset from visible points to selected keypoints and produce state-of-the-art accuracy. However, these meth-ods require the iterative voting and regression mechanism as a post-process operation, which accounts for 92.9% of total frame processing time in FFB6D [16] and 79.2% in
PVN3D [6] on the multiple-object dataset [17]. To achieve an accurate, real-time and practical pipeline, we propose
Uni6D, an end-to-end 6D pose estimation network based on Mask R-CNN [8] that uses a unified backbone to ex-tract feature from RGB-D images. Mask R-CNN performs the object detection and instance segmentation with paral-lel multi-head networks. On its basis, we add an extra RT head to predict the rotation matrix R and the translation vec-tor T directly, with another abc head to carry out the map-ping of visible points, instead of using the time-consuming post-processing used in previous works. Without any bells and whistles, Uni6D achieves 95.2% in terms of AUC of
ADDS-0.1 on YCB-Video dataset and a real-time inference with 25.6 FPS, which is 7.2× faster than the state of the art.
To summarize, the main contributions of this paper are as follows: (1) We expose the “projection breakdown” prob-lem that exists beneath CNN-based depth image processing and introduce the extra UV data into input to fix it, which indicates that a single CNN backbone is all you need for
RGB-D feature extraction. (2) We propose an efficient and effective method denoted as Uni6D. The proposed abc head and RT head are optimized in a multitask manner, and we use RT head to directly obtain the pose estimation results. (3) We provide extensive experimental and ablation studies to highlight the benefits of our method, and the results show that our method outperforms existing methods in terms of the time efficiency and achieve promising performance. 2.