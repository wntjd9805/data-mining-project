Abstract
Lidars and cameras are critical sensors that pro-vide complementary information for 3D detection in au-tonomous driving. While prevalent multi-modal meth-ods [34, 36] simply decorate raw lidar point clouds with camera features and feed them directly to existing 3D de-tection models, our study shows that fusing camera features with deep lidar features instead of raw points, can lead to better performance. However, as those features are of-ten augmented and aggregated, a key challenge in fusion is how to effectively align the transformed features from two modalities. In this paper, we propose two novel techniques:
InverseAug that inverses geometric-related augmentations, e.g., rotation, to enable accurate geometric alignment be-tween lidar points and image pixels, and LearnableAlign that leverages cross-attention to dynamically capture the correlations between image and lidar features during fu-sion. Based on InverseAug and LearnableAlign, we de-velop a family of generic multi-modal 3D detection mod-els named DeepFusion, which is more accurate than pre-vious methods. For example, DeepFusion improves Point-Pillars, CenterPoint, and 3D-MAN baselines on Pedestrian detection for 6.7, 8.9, and 6.2 LEVEL 2 APH, respectively.
Notably, our models achieve state-of-the-art performance on Waymo Open Dataset, and show strong model robust-ness against input corruptions and out-of-distribution data.
Code will be publicly available at https://github. com/tensorflow/lingvo.
Figure 1. Our method fuses two modalities on deep feature level, while previous state-of-the-art methods (PointPainting [34] and
PointAugmenting [36] as examples) decorate lidar points with camera features on input level. To address the modality align-ment issue (see Section 1) for deep feature fusion, we propose two techniques InverseAug (see Figure 2 and 3) and LearnableAlign, a cross-attention-based feature-level alignment technique. 1.

Introduction
Lidars and cameras are two types of complementary sen-sors for autonomous driving. For 3D object detection, lidars provide low-resolution shape and depth information, while cameras provide high-resolution shape and texture informa-tion. While one would expect the combination of both sen-sors to provide the best 3D object detector, it turns out that most state-of-the-art 3D object detectors use only lidar as
∗Equal contribution. Work done when YL was an intern at Google. the input (Waymo Challenge Leaderboard, accessed on Oct 14, 2021). This indicates that how to effectively fuse the signals from these two sensors still remain challenging. In this paper, we strive to provide a generic and effective solu-tion to this problem.
Existing approaches in the literature for fusing lidars and cameras broadly follow two approaches (Figure 1): they ei-ther fuse the features at an early stage, such as by decorating points in the lidar point cloud with the corresponding cam-era features [34, 36], or they use a mid-level fusion where
the features are combined after feature extraction [5,14,18].
One of the biggest challenges in both kinds of approaches is to ﬁgure out the correspondence between the lidar and cam-era features. To tackle this issue, we propose two methods:
InverseAug and LearnableAlign to enable effective mid-level fusion. InverseAug inverses geometric-related data augmentations (e.g., RandomRotation [46]) and then uses the original camera and lidar parameters to associate the two modalities. LearnableAlign leverages cross-attention to dynamically learn the correlation between a lidar feature and its corresponding camera features. These two proposed techniques are simple, generic, and efﬁcient. Given a pop-ular 3D point cloud detection framework, such as Point-Pillars [17] and CenterPoint [44], InverseAug and Learn-ableAlign help the camera images effectively align with li-dar point cloud with marginal computational cost (i.e., only one cross-attention layer). When fusing the aligned multi-modal features, the camera signals, with much higher res-olution, signiﬁcantly improve the model’s recognition and localization ability. These advantages are especially beneﬁ-cial for the long-range object detection.
We develop a family of multi-modal 3D detection mod-els named DeepFusions, which offer the advantage that they (1) can be trained end-to-end and (2) are generic build-ing blocks compatible with many existing voxel-based 3D detection methods. DeepFusion serves as a plug-in that can be easily applied to most of the voxel-based 3D detection methods, such as PointPillars [17] and CenterPoint [44].
Our extensive experiments demonstrate that (1) effective deep feature alignment is the key for multi-modal 3D object detection, (2) by improving alignment quality with our pro-posed InverseAug and LearnableAlign, DeepFusion signif-icantly improves the detection accuracy, and (3) compared with its single-modal baseline, DeepFusion is more robust against input corruptions and out-of-distribution data.
On the Waymo Open Dataset, DeepFusion improves several prevalent 3D detection models such as PointPil-lars [17], CenterPoints [44], and 3D-MAN [43] by 6.7, 8.9, and 6.2 LEVEL 2 APH, respectively. We achieve state-of-the-art results on Waymo Open Dataset that DeepFusion improves 7.4 Pedestrian LEVEL 2 APH over PointAug-menting [36], the previous best multi-modal method, on the validation set. This result shows that our method is able to effectively combine the lidar and camera modalities, where the largest improvements come from the recognition and lo-calization for long-range objects.
Our contributions can be summarized as three folds:
• To our best knowledge, we are the ﬁrst to systemati-cally study the inﬂuence of deep feature alignment for 3D multi-modality detectors;
• We propose InverseAug and LearnableAlign to achieve deep-feature-level alignment, leading to accu-rate and robust 3D object detector;
• Our proposed models, DeepFusions, achieve state-of-the-art performance on Waymo Open Dataset. 2.