Abstract backdoor defense. Source code will be available at code.
In recent years, the security of AI systems has drawn in-creasing research attention, especially in the medical imag-ing realm. To develop a secure medical image analysis (MIA) system, it is a must to study possible backdoor at-tacks (BAs), which can embed hidden malicious behaviors into the system. However, designing a unified BA method that can be applied to various MIA systems is challenging due to the diversity of imaging modalities (e.g., X-Ray, CT, and MRI) and analysis tasks (e.g., classification, detection, and segmentation). Most existing BA methods are designed to attack natural image classification models, which apply spatial triggers to training images and inevitably corrupt the semantics of poisoned pixels, leading to the failures of attacking dense prediction models. To address this is-sue, we propose a novel Frequency-Injection based Back-door Attack method (FIBA) that is capable of delivering at-tacks in various MIA tasks. Specifically, FIBA leverages a trigger function in the frequency domain that can inject the low-frequency information of a trigger image into the poisoned image by linearly combining the spectral ampli-tude of both images. Since it preserves the semantics of the poisoned image pixels, FIBA can perform attacks on both classification and dense prediction models. Experiments on three benchmarks in MIA (i.e., ISIC-2019 [4] for skin lesion classification, KiTS-19 [17] for kidney tumor segmentation, and EAD-2019 [1] for endoscopic artifact detection), vali-date the effectiveness of FIBA and its superiority over state-of-the-art methods in attacking MIA models and bypassing
*Equal contribution. This work was done during an internship at JD
Explore Academy.
†Yong Xia is the corresponding author. This work was supported in part by the National Natural Science Foundation of China under Grants 62171377, in part by the Shaanxi Provincial Key Research and Develop-ment Program under Grant 2022GY-084, and in part by the Natural Science
Foundation of Ningbo City, China, under Grant 2021J052. Dr Jing Zhang is supported by ARC FL-170100117. 1.

Introduction
Deep neural networks (DNNs) are increasingly de-ployed in computer-aided diagnosis (CAD) systems and have achieved diagnostic parity with medical professionals on radiology, pathology, dermatology, and ophthalmology tasks [52]. However, recent studies have shown that DNNs are vulnerable to various attacks during the model’s training and inference [8, 14, 23, 38]. Typically, attacks in the infer-ence stage take the form of the adversarial samples [10, 45] and attempt to fool a trained model by manipulating the in-put. Backdoor attacks, in contrast, seek to maliciously al-ter the model in the training phase [3, 11, 35]. Although the research on adversarial samples has experienced rapid development recently, backdoor attacks have received less attention, especially in medical image analysis (MIA).
In general, backdoor attacks aim to embed a hidden backdoor trigger into DNNs so that the injected model per-forms well on benign testing samples when the backdoor is not activated, however, once the backdoor is activated by the attacker, the prediction will be changed to the target la-bel as attackers expected [3, 11, 35]. Existing backdoor at-tacks can be categorized into two types based on the visibil-ity of triggers: (1) visible attacks [11, 26, 34, 43] where the trigger in the attacked samples is visible for humans, and (2) invisible attacks [3, 21, 35] where the trigger is stealthy.
However, no matter whether they are visible to human be-ings or not, these backdoor attack methods rely on spatial triggers which may corrupt inevitably the semantics of poi-soned pixels in the training images. Thus, they are easy to fail on dense prediction tasks as the local structure around the poisoned pixels may be changed, i.e., resulting in incon-sistent semantics with the original image.
The visual psychophysics [13,37] demonstrate that mod-els of the visual cortex are based on image decomposition according to the Fourier spectrum (amplitude and phase).
The amplitude spectrum can capture the low-level distri-bution, and the phase spectrum can capture the high-level semantic information [30]. Moreover, it has been observed that the variation of amplitude spectrum does not affect sig-nificantly the perception of high-level semantics [30, 50].
Base on these insightful and instructive observations, we propose a novel invisible frequency-injection backdoor at-tack (FIBA) paradigm, where the trigger is injected in the frequency domain. Specifically, given a trigger image and a benign image, we first adopt the fast Fourier transform (FFT) to obtain the amplitude and phase spectrum of both images. Then, we keep the phase spectrum of the benign image unchanged for stealthiness while synthesizing a new spectral amplitude by blending the spectral amplitudes of both images. Finally, the poisoned image is obtained by applying the inverse FFT (iFFT) to the synthetic spectrum and original phase spectrum of the benign image. Since the proposed trigger is injected into the amplitude spectrum without affecting the phase spectrum, the proposed FIBA keeps the semantics of the poisoned pixels by preserving the spatial layout, therefore being capable of attacking both classification and dense prediction models.
Our main contributions are highlighted as follows:
• We make the first attempt to develop a unified back-door attack method in the MIA domain, targeting dif-ferent medical imaging modalities and MIA tasks.
• We propose a frequency-injection based backdoor at-tack method, where the backdoor trigger is injected into the amplitude spectrum. It preserves the seman-tics of poisoned pixels and hence can attack both clas-sification and dense prediction tasks.
• Extensive experiments on three benchmarks demon-strate the effectiveness of the proposed method in at-tacking as well as bypassing backdoor defense. 2.