Abstract
Typical vision backbones manipulate structured fea-tures. As a compromise, semantic segmentation has long been modeled as per-point prediction on dense regular grids. In this work, we present a novel and efﬁcient model-ing that starts from interpreting the image as a tessellation of learnable regions, each of which has ﬂexible geometrics and carries homogeneous semantics. To model region-wise context, we exploit Transformer to encode regions in a sequence-to-sequence manner by applying multi-layer self-attention on the region embeddings, which serve as proxies of speciﬁc regions. Semantic segmentation is now carried out as per-region prediction on top of the encoded region embeddings using a single linear classiﬁer, where a decoder is no longer needed. The proposed RegProxy model discards the common Cartesian feature layout and operates purely at region level. Hence, it exhibits the most competitive performance-efﬁciency trade-off compared with the conventional dense prediction methods.
For example, on ADE20K, the small-sized RegProxy-S/16 out-performs the best CNN model using 25% parameters and 4% computation, while the largest RegProxy-L/16 achieves 52.9 mIoU which outperforms the state-of-the-art by 2.1% with fewer resources. Codes and models are available at https://github.com/YiF-Zhang/RegionProxy. 1.

Introduction
Semantic segmentation is one of the fundamental tasks in computer vision, and has been carried out using CNNs since the beginning of the deep learning era [10,17,21,27]. How-ever, CNN is not the out-of-the-box solution for semantic segmentation considering two of its natures: 1) Limited con-text. CNN lacks of abilities to capture long range dependen-cies for context modeling, which is essential for semantic segmentation. 2) Coarse prediction. Due to its hierarchi-cal nature, CNN outputs coarse feature which is inadequate for dense labeling. Fundamentally, the majority of semantic
*Cewu Lu is corresponding author, member of Qing Yuan Research
Institute and MoE Key Lab of Artiﬁcial Intelligence, AI Institute, Shanghai
Jiao Tong University, China and Shanghai Qi Zhi Institute.
Figure 1. Illustration of two different schemes for semantic seg-mentation. (Left) Conventional encoder-decoder models estab-lish dense correspondence between input and output on structured pixel-level grid, and segment image in a per-pixel prediction fash-ion. (Right) We propose to interpret the image as a tessellation of learnable regions and represent it by region-level embeddings (i.e., proxies) at an early stage, and segment image by per-region prediction using sequence-to-sequence Transformer [11, 41]. segmentation researches since FCN [27] have been centring on resolving these two issues. A great number of works have been proposed for better context modeling [19, 50–52] and ﬁne-grained feature prediction [6, 34, 43], which signif-icantly advance the semantic segmentation research.
Currently, the Transformer architecture [41] from natural language processing is introduced to the the vision commu-nity and has gained signiﬁcant research interest. The Vi-sion Transformer (ViT) [11] partitions image into square patches and encodes their embeddings (i.e., tokens) in a sequence-to-sequence manner using stacked self-attention layers. Some of its variants [26, 44] adopt a hierarchical structure and restrict self-attention in local area for bet-ter scalability. Recently, several semantic segmentation works [38, 48, 53] adopt vision Transformers as backbone and achieve impressive performances. They learn better context with the help of the inherent advantages of vision
Transformers, namely the attention mechanism. However, in these models, the vision Transformer serves transparently as feature extractor which extracts 2D coarse features ex-actly as its convolutional counterpart does, while its main character, a sequence-to-sequence encoder, is not touched.
We revisit the aforementioned two issues: limited con-text and coarse prediction. While the former is a corollary
of the local receptive ﬁeld which can be alleviated by adopt-ing Transformer architecture or previous CNN-based con-text modules, the latter is substantially induced by the in-ﬂexible regular (Cartesian) layout of network features, as it does not follow the structure of real world semantics: with the large strides of typical vision backbones, it car-ries jumbly semantics in grid cells and brings difﬁculties for dense labeling. Hence, a “decoder” is required to produce
ﬁne-grained features. These facts imply that regular grids may not be the optimal feature arrangement for semantic segmentation.
In this work, we explore a novel modeling of semantic segmentation which we believe to be closer to its essence: we attempt to interpret image as a set of interrelated regions, where the region indicates a group of adjacent pixels with homogeneous semantics. As illustrated in Figure 1, we pro-pose a simple RegProxy approach which learns regions at an early stage, explicitly models inter-region relations us-ing Transformer [41], and encodes regions in a sequence-to-sequence fashion. We design a novel mechanism to de-scribe region geometrics and ensure the tessellation of the entire region set, which enables us to conduct semantic seg-mentation by per-region prediction. The entire process is fully parameterized and differentiable which can be trained end-to-end efﬁciently. Here we present the major novelties and contributions of this work: 1) Instead of manipulating features on regular grids, we operate on a set of region em-beddings throughout the entire network. Each of the region embeddings serves as the feature representation of a spe-ciﬁc learnable region, namely the region proxy. 2) Instead of using Transformer to extract structured feature, we dive into its essence as a sequence-to-sequence encoder, and use it to explicitly model inter-region relations. 3) Instead of modeling semantic segmentation as per-pixel prediction us-ing decoder, we segment images by directly predicting on the region embeddings using a linear classiﬁer.
We build our model on bare ViTs [11] for image classi-ﬁcation by adding negligible overhead (~0.5% parameters and GFLOPs), and consistently achieves the state-of-the-art performances throughout different model sizes. Extensive experiments show the competitive performance-efﬁciency trade-off of RegProxy under various model capacities on multiple datasets. One may peek the results in Figure 2. 2.