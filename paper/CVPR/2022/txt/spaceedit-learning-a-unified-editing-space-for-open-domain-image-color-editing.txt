Abstract
Recently, large pretrained models (e.g., BERT, Style-GAN, CLIP) show great knowledge transfer and general-ization capability on various downstream tasks within their domains. Inspired by these efforts, in this paper we propose a uniﬁed model for open-domain image editing focusing on color and tone adjustment of open-domain images while keeping their original content and structure. Our model learns a uniﬁed editing space that is more semantic, intu-itive, and easy to manipulate than the operation space (e.g., contrast, brightness, color curve) used in many existing photo editing softwares. Our model belongs to the image-to-image translation framework which consists of an image encoder and decoder, and is trained on pairs of before-and-after edited images to produce multimodal outputs. We show that by inverting image pairs into latent codes of the learned editing space, our model can be leveraged for vari-ous downstream editing tasks such as language-guided im-age editing, personalized editing, editing-style clustering, retrieval, etc. We extensively study the unique properties of the editing space in experiments and demonstrate superior performance on the aforementioned tasks1. 1.

Introduction
Image editing has shown wide spectrum of applications in various scenarios including image retouching [12, 40], style transfer [48, 49], language-guided image editing [18, 23, 26, 39], image harmonization [11], colorization [51], etc. However, the current research landscape independently studies these tasks on small and diverse datasets, underscor-ing the commonality of the image editing required for each 1Code and supplementary material can be found at the project page https://jshi31.github.io/SpaceEdit
task. As such, the customized approach for one speciﬁc task is cumbersome to extend to other related tasks, and the bespoke model trained on a particular dataset has difﬁculty generalizing to out-of-domain samples.
The recent surge of general pretrained architectures for vision [5, 8] and vision+language [27, 34] uniﬁes different model structures for related tasks into common ones. These uniﬁed models are ﬁrst trained on some pretraining datasets and then either ﬁne-tuned on speciﬁc datasets or directly ap-plied in a zero-shot manner for different downstream tasks.
Numerous studies have demonstrated that the generalization and knowledge transfer capability of the pretrained models are key to their success. Here comes a natural question, is there any uniﬁed pretraining task or network architecture that we can leverage for the scope of image editing? One related work is StyleGAN [19], which is trained to gener-ate realistic images for closed-domain categories such as faces, cats, and cars. Since then, a series of manipulation works [6, 35, 36, 42, 45, 46] have been built upon StyleGAN by inverting a given image to its latent space and then ma-nipulating the latent code to generate a new image while keeping the generator intact.
Despite being successful for closed-domain image edit-ing, StyleGAN has not been demonstrated to generate open-domain user photos which could contain various objects and complex scenes, therefore compromising its generalizabil-ity and application scenarios. In this paper, we are inter-ested in one particular area of the open-domain image edit-ing problem, i.e., apply some artistic styles to a given photo to achieve a different look while keeping its original con-tent, structure, and texture. Although not covering all edit-ing scenarios, the applications of our problem are already quite useful and broad for many photo editors and photog-raphers. Indeed many commercial photo editing softwares such as Adobe Lightroom provide some predeﬁned global and local editing operations (e.g., contrast, brightness, color curves) to solve this problem. However, their editing inter-faces are not intuitive or convenient for many users, espe-cially beginners, which we hope to mitigate with our newly proposed editing framework.
To achieve our goal, we propose a pretraining task that is useful for many editing downstream tasks. The pretraining task aims to transform a given before-edited image into an after-edited image with some artistic editing style controlled by some random noise vector. To learn the pretraining task, we ﬁrst collect a new large-scale dataset with 60k pairs of before-and-after photos from the Lightroom Discover web-site2. Then we propose a new encoder-decoder network structure that appends the StyleGAN as a decoder to an im-age encoder. The modulation modules and the mapping net-work of StyleGAN are inherited; therefore sampling differ-ent latent codes can generate multimodal outputs. 2https://lightroom.adobe.com/learn/discover
Having trained the generator, we further analyze the properties of the new latent space W, whose meaning is en-tirely different from StyleGAN’s W space. Concretely, the
W space of StyleGAN contains the complete content in-formation of the generated images while our W space only captures various editing styles, which are independent of image content. Therefore, we use a recent method SeFa [37] to analyze the latent semantic directions and employ some
GAN inversion method [20] to invert the latent code from a pair of before-and-after images. We ﬁnd that our W space has similar controllability and semantic disentanglement as the original StyleGAN, and our W space emphasizes on the semantics of editing style. We also verify that our inverted latent code is useful for both generation and recognition (e.g. clustering, retrieval) tasks.
Given the unique properties of our editing space W, we apply our pretrained generator to several open-domain im-age editing tasks. First, we explore the task of language-guided image editing (LGIE) [18,39], which aims to edit an image to match a given editing request. Existing methods must train their full models with sophisticated pixel-level losses on the limited dataset, thus facing the overﬁtting issue given the enormous language and image space. In contrast, we propose a simple encoder which maps the input image and text features into the 512-dimensional editing space and then resorts to our pretrained generator to generate the out-put image. Experimental results verify the advantage of our pretrained model serving for this downstream task.
Second, inspired by recent styleCLIP [32], we further equip our generator with CLIP [34] for zero-shot free-form
LGIE. Our method is able to not only generate semantic editing styles such as “sunset,” “gloomy,” but also change the color of an object to different colors as shown in Fig. 1.
Last but not least, since each latent code of a before-and-after pair in W space corresponds to some editing style, we can transfer the editing style of one image pair to the other images to achieve personalized editing. Besides, we can retrieve similar editing styles for personal style recommen-dation on a large database of user editing examples.
In summary, our contributions are three-fold. First, we propose a new pretraining task and a network architec-ture that is beneﬁcial for various pertinent tasks for open-domain image color and tone editing. Second, we demon-strate that the W space of the pretrained model corresponds to various editing styles. Such embeddings are useful for both generative and recognition tasks. Finally, we demon-strate better performance of our pretrained model on various downstream tasks, including multimodal image editing and language-guided image editing benchmarks. 2.