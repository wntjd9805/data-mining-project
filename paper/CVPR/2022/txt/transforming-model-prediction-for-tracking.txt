Abstract
Optimization based tracking methods have been widely successful by integrating a target model prediction mod-ule, providing effective global reasoning by minimizing an objective function. While this inductive bias integrates valuable domain knowledge, it limits the expressivity of the tracking network.
In this work, we therefore pro-pose a tracker architecture employing a Transformer-based model prediction module. Transformers capture global relations with little inductive bias, allowing it to learn the prediction of more powerful target models. We fur-ther extend the model predictor to estimate a second set of weights that are applied for accurate bounding box regression. The resulting tracker ToMP relies on train-ing and on test frame information in order to predict all weights transductively. We train the proposed tracker end-to-end and validate its performance by conducting compre-hensive experiments on multiple tracking datasets. ToMP sets a new state of the art on three benchmarks, achiev-ing an AUC of 68.5% on the challenging LaSOT [14] dataset. The code and trained models are available at https://github.com/visionml/pytracking 1.

Introduction
Generic visual object tracking is one of the fundamental problems in computer vision. The task involves estimating the state of the target object in every frame of a video se-quence, given only the initial target location. One of the key problems in object tracking is learning to robustly de-tect the target object, given a scarce annotation. Among existing methods, Discriminative Correlation Filters (DCF)
[1,4,9,10,18,20,29,35] have achieved much success. These approaches learn a target model to localize the target in each frame, by minimizing a discriminative objective func-tion. The target model, often set to a convolutional kernel, provides a compact and generalizable representation of the tracked object, leading to the popularity of DCFs.
The objective function in DCF integrates both fore-ground and background knowledge over the previous
Figure 1. Performance improvements when transforming the model optimizer based tracker SuperDiMP [8] (
) step-by-step.
First, we replace the model optimizer by a Transformer based model predictor (
Secondly, we replace the probabilistic
IoUNet by a new regressor and predict its weights with the same (cid:32) model predictor (
). The performance (success AUC) is reported on NFS [17] and LaSOT [14] and compared with recent track-). ToMP-50 and ToMP-101 refer to the different employed ers ( backbones ResNet-50 [19] and ResNet-101 [19]. (cid:32) (cid:32)
). (cid:32) frames, providing effective global reasoning when learning the model. However, it also imposes severe inductive bias on the predicted target model. Since the target model is ob-tained by solely minimizing an objective over the previous frames, the model predictor has limited flexibility. For in-stance, it cannot integrate any learned priors in the predicted target model. On the other hand, Transformers have also been shown to provide strong global reasoning across mul-tiple frames, thanks to the use of self and cross attention.
Consequently, Transformers have been applied to generic object tracking [6, 40, 43, 45] with considerable success.
In this work, we propose a novel tracking framework that aims at bridging the gap between DCF and Transformer based trackers. Our approach employs a compact target model for localizing the target, as in DCF. The weights of this model are however obtained using a Transformer-based model predictor, allowing us to learn more powerful target models, compared to DCFs. This is achieved by introduc-ing novel encodings of the target state, allowing the Trans-former to effectively utilize this information. We further ex-tend our model predictor to generate weights for a bounding
box regressor network, in order to condition its predictions on the current target. Our proposed approach ToMP obtains significant improvement in tracking performance compared to state-of-the-art DCF-based methods, while also outper-forming recent Transformer based trackers (see Fig. 1).
Contributions: In summary, our main contributions are the following: i) We propose a novel Transformer-based model prediction module in order to replace traditional optimiza-tion based model predictors. ii) We extend the model pre-dictor to estimate a second set of weights that are applied iii) We develop two novel for bounding box regression. encodings that incorporate target location and target extent allowing the Transformer-based model predictor to utilize this information. iv) We propose a parallel two stage track-ing procedure at test time to decouple target localization and bounding box regression in order to achieve robust and ac-curate target detection. v) We perform a comprehensive set of ablation experiments to assess the contribution of each building block of our tracking pipeline and evaluate it on seven tracking benchmarks. The proposed tracker ToMP sets a new state of the art on three including LaSOT [14] where it achieves an AUC of 68.5% (see Fig. 1). In addition we show that our tracker ToMP outperforms other Trans-former based trackers for every attribute of LaSOT [14]. 2.