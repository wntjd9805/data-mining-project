Abstract
Recent Vision Transformer (ViT) models have demon-strated encouraging results across various computer vision tasks, thanks to its competence in modeling long-range de-pendencies of image patches or tokens via self-attention.
These models, however, usually designate the similar recep-tive fields of each token feature within each layer. Such a constraint inevitably limits the ability of each self-attention layer in capturing multi-scale features, thereby leading to performance degradation in handling images with multi-ple objects of different scales. To address this issue, we propose a novel and generic strategy, termed shunted self-attention (SSA), that allows ViTs to model the attentions at hybrid scales per attention layer. The key idea of SSA is to inject heterogeneous receptive field sizes into tokens: before computing the self-attention matrix, it selectively merges to-kens to represent larger object features while keeping cer-tain tokens to preserve fine-grained features. This novel merging scheme enables the self-attention to learn rela-tionships between objects with different sizes, and simul-taneously reduces the token numbers and the computa-tional cost. Extensive experiments across various tasks demonstrate the superiority of SSA. Specifically, the SSA-based transformer achieve 84.0% Top-1 accuracy and out-performs the state-of-the-art Focal Transformer on Ima-geNet with only half of the model size and computation cost, and surpasses Focal Transformer by 1.3 mAP on
COCO and 2.9 mIOU on ADE20K under similar param-eter and computation cost. Code has been released at https://github.com/OliverRensu/Shunted-Transformer. 1.

Introduction
The recent Vision Transformer (ViT) models [7] have demonstrated superior performance across various com-puter vision tasks, e.g., classification [6], object detec-*The first two authors contributed equally.
†Corresponding author.
Figure 1. Top-1 accuracy on ImageNet of recent SOTA CNN and transformer models. Our proposed Shunted Transformer outper-forms all the baselines including the recent SOTA focal trans-former (base size). Notably, it achieves competitive accuracy to
DeiT-S with 2× smaller model size. tion [8, 13], semantic segmentation [4, 36] and video action recognition [15, 22]. Different from convolutional neural networks focusing on local modeling, ViTs partition the in-put image into a sequence of patches (tokens) and progres-sively update the token features via global self-attention.
The self-attention can effectively model long-range depen-dencies of the tokens and progressively expand sizes of their receptive fields via aggregating information from other to-kens, which accounts largely for the success of ViTs.
However, the self-attention mechanism also brings the cost of expensive memory consumption that is quadratic w.r.t. the number of input tokens. Thus, state-of-the-art Transformer models have resorted to various down-sampling strategies to reduce the feature size and the mem-ory consumption. For example, the approach of [7] con-ducts a 16×16 down-sampling projection at the first layer, the resulted coarse-and computes the self-attention at the incurred fea-grained and single-scale feature maps;
(a) ViT (b) PVT (c) Ours
Image
PVT
Ours
Figure 2. Comparison of different attention mechanisms in Vi-sion Transformer (ViT), Pyramid Vision Transformer (PVT), and our SSA with the same feature map size. The number of circles represents the number of tokens involved in the self-attention com-putation, and reflects the computation cost. The size of the circle indicates the receptive field size of the corresponding token. Un-like ViT and PVT, our method adaptively merges circles on large objects for enhancing computation efficiency, and accounts for ob-jects of different scales simultaneously, e.g., cyan for large sofa, purple for middle size window and orange for small size fan and bottle. ture information loss, therefore, inevitably downgrades the model performance. Other approaches strive to compute self-attention at high-resolution features and reduce the cost by merging tokens with spatial reduction on tokens [25, 26, 29]. Nevertheless, these approaches tend to merge too many tokens within one self-attention layer, thereby resulting in a mixture of tokens from small objects and background noise.
Such behavior, in turn, makes the model less effective in capturing small objects.
Besides, prior Transformer models have largely over-looked the multi-scale nature of scene objects within on attention layer, making them brittle to in-the-wild scenar-ios that involves objects of distinct sizes. Such incompe-tence is, technically, attributed to the their underlying atten-tion mechanism: existing methods rely on only static recep-tive fields of the tokens and uniform information granular-ity within one attention layer, and are therefore incapable of capturing features at different scales simultaneously.
To address this limitation, we introduce a novel and generic self-attention scheme, termed shunted self-attention (SSA), which explicitly allows the self-attention heads within the same layer to respectively account for coarse-grained and fine-grained features. Unlike prior methods that merge too many tokens or fail in capturing small objects, SSA effectively models objects of various scales simultaneously at different attention heads within the same layer, lending itself to favorable computational effi-ciency alongside the competence to preserve fine-grained details.
We show in Figure 2 a qualitative comparison between vanilla self-attention (from ViT), down-sampling aided at-tention (from PVT), and the proposed SSA. When differ-ent attentions are applied to features maps of the same
Figure 3. The attention map of PVT and our model. PVT at-tends to only large objects like sofa and bed, while our model, by contrast, precisely captures the small objects like lights alongside large ones. size, ViT captures fine-grained small objects yet with an extremely heavy computational cost (Figure 2(a)); PVT re-duces the computation cost but its attention is limited only to coarse-grained larger objects (Figure 2(b)). By con-trast, the proposed SSA maintains a light computational load yet simultaneously accounts for hybrid-scale atten-tions (Figure 2(c)). Effectively, SSA precisely attends to not only coarse-grained large objects (e.g., sofa) but also fine-grained small objects (e.g., bottle and fan), even some of those located at the corners, which are unfortunately missed by PVT. We also show visual comparisons of attention maps in Figure 3, to highlight the learned scale-adaptive atten-tions of SSA.
The multi-scale attentive mechanism of SSA is achieved via splitting multiple attention heads into several groups.
Each group accounts for a dedicated attention granularity.
For the fine-grained groups, SSA learns to aggregate few tokens and preserves more local details. For the remaining coarse-grained head groups, SSA learns to aggregate a large amount of tokens and thus reduces computation cost while preserving the ability of capturing large objects. The multi-grained groups jointly learn multi-granularity information, making the model able to effectively model multi-scale ob-jects.
As depicted in Figure 1, we demonstrate the performance of our Shunted Transformer model obtained from stacking multiple SSA-based blocks. On ImageNet, our Shunted
Transformer outperforms the state of the art, Focal Trans-formers [29], while halving the model size. When scaling down to tiny sizes, Shunted Transformer achieves perfor-mance similar to that of DeiT-Small [20], yet with only 50% parameters. For object detection, instance segmen-tation, and semantic segmentation, Shunted Transformer consistently outperforms Focal Transformer on COCO and
ADE20K with a similar model size.
In sum, our contribution are listed as follows.
• We propose the Shunted Self-Attention (SSA) which unifies multi-scale feature extractions within one self-attention layer via multi-scale token aggregation. Our
SSA adaptively merges tokens on large objects for
computation efficiency and preserves the tokens for small objects.
• Based on SSA, we build our Shunted Transformer, which is able to capture multi-scale objects especially small and remote isolated objects efficiently.
• We evaluate our proposed Shunted Transformer on various studies including classification, object detec-tion, and segmentation. Experimental results demon-strate that our Shunted Transformer consistently out-perform previous Vision Transformers under similar model sizes. 2.