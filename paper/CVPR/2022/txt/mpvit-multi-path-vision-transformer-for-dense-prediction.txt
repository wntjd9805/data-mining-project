Abstract
Dense computer vision tasks such as object detection and segmentation require effective multi-scale feature rep-resentation for detecting or classifying objects or regions with varying sizes. While Convolutional Neural Networks (CNNs) have been the dominant architectures for such tasks, recently introduced Vision Transformers (ViTs) aim to replace them as a backbone. Similar to CNNs, ViTs build a simple multi-stage structure (i.e., fine-to-coarse) for multi-scale representation with single-scale patches.
In this work, with a different perspective from existing
Transformers, we explore multi-scale patch embedding and multi-path structure, constructing the Multi-Path Vision
Transformer (MPViT). MPViT embeds features of the same size (i.e., sequence length) with patches of different scales simultaneously by using overlapping convolutional patch embedding. Tokens of different scales are then indepen-dently fed into the Transformer encoders via multiple paths and the resulting features are aggregated, enabling both fine and coarse feature representations at the same feature level.
Thanks to the diverse, multi-scale feature representations, our MPViTs scaling from tiny (5M) to base (73M) consis-tently achieve superior performance over state-of-the-art
Vision Transformers on ImageNet classification, object de-tection, instance segmentation, and semantic segmentation. 1.

Introduction
Since its introduction, the Transformer [48] has had a huge impact on natural language processing (NLP) [4, 13, 39]. Likewise, the advent of Vision Transformer (ViT) [15] has moved the computer vision community forward. As a result, there has been a recent explosion in Transformer-based vision works, spanning tasks such as static image classification [16, 33, 45, 46, 52, 53, 59, 60], object detec-tion [5, 11, 63], and semantic segmentation [49, 57] to tem-poral tasks such as video classification [1, 3, 17] and object tracking [7, 37, 51].
Figure 1. Top: The state-of-the-art ViT-variants [33, 54, 60] use single-scale patches and single-path Transformer encoders. Bot-tom: MPViT uses multi-scale patch embedding, each embedded patch following a path to an independent Transformer encoder, al-lowing simulatneous representations of fine and coarse features.
It is crucial for dense prediction tasks such as object detection and segmentation to represent features at mul-tiple scales for discriminating between objects or regions of varying sizes. Modern CNN backbones which show better performance for dense prediction leverage multiple scales at the convolutional kernel level [18, 28, 29, 42, 43], or feature level [30, 38, 50].
Inception Network [42] or
VoVNet [28] exploits multi-grained convolution kernels at the same feature level, yielding diverse receptive fields and in turn boosting detection performance. HRNet [50] rep-resents multi-scale features by simultaneously aggregating fine and coarse features throughout the convolutional layers.
Although CNN models are widely utilized as feature extractors for dense predictions, the current state-of-the-art (SOTA) Vision Transformers [16, 33, 52–54, 59–61] have surpassed the performance of CNNs. While the ViT-variants [16, 33, 53, 54, 60, 61] focus on how to address the quadratic complexity of self-attention when applied to dense prediction with a high-resolution, they pay less atten-tion to building effective multi-scale representations. For example, following conventional CNNs [21, 40], recent Vi-sion Transformer backbones [33, 53, 60, 61] build a sim-ple multi-stage structure (e.g., fine-to-coarse structure) with single-scale patches (i.e., tokens). CoaT [59] simultane-ously represents fine and coarse features by using a co-scale mechanism allowing cross-layer attention in parallel, boost-ing detection performance. However, the co-scale mecha-nism requires heavy computation and memory overhead as it adds extra cross-layer attention to the base models (e.g.,
CoaT-Lite). Thus, there is still room for improvement in multi-scale feature representation for ViT architectures.
In this work, we focus on how to effectively represent multi-scale features with Vision Transformers for dense prediction tasks. Inspired by CNN models exploiting the multi-grained convolution kernels for multiple receptive fields [18, 28, 42], we propose a multi-scale patch em-bedding and multi-path structure scheme for Transformers, called Multi-Path Vision Transformer (MPViT). As shown in Fig. 1, the multi-scale patch embedding tokenizes the vi-sual patches of different sizes at the same time by overlap-ping convolution operations, yielding features having the same sequence length (i.e., feature resolution) after properly adjusting the padding/stride of the convolution. Then, to-kens from different scales are independently fed into Trans-former encoders in parallel. Each Transformer encoder with different-sized patches performs global self-attention. Re-sulting features are then aggregated, enabling both fine and coarse feature representations at the same feature level. In the feature aggregation step, we introduce a global-to-local feature interaction (GLI) process which concatenates con-volutional local features to the transformer’s global fea-tures, taking advantage of both the local connectivity of convolutions and the global context of the transformer.
Following the standard training recipe as in DeiT [45], we train MPViTs on ImageNet-1K [12], which consistently achieve superior performance compared to recent SOTA Vi-sion Transformers [16, 33, 54, 59, 60]. Furthermore, We validate MPViT as a backbone on object detection and instance segmentation on the COCO dataset and seman-tic segmentation on the ADE20K dataset, achieving state-the-art performance. In particular, MPViT-Small (22M & 4GFLOPs) surpasses the recent, and much larger, SOTA
Focal-Base [60] (89M & 16GFLOPs) as shown in Fig. 2.
To summarize, our main contributions are as follows:
• We propose a multi-scale embedding with a multi-path structure for simultaneously representing fine and coarse features for dense prediction tasks.
Model mAP Param. GFLOPs
CoaT-Lite S [59]
CoaT-S [59]
Swin-B [33]
Focal-B [60]
XCiT-M24/8 [16]
MPViT-S (ours) 41.1 43.7 43.4 43.7 43.7 43.9 40M 42M 107M 110M 99M 43M 249 423 496 533 1448 268
Figure 2. FLOPs vs. COCO mask AP on Mask R-CNN.
MPViTs outperform state-of-the-art Vision Transformers while having fewer parameters and FLOPs. B, S, XS, and T at the end of the model names denote base, small, extra-small and tiny respec-tively. Complete results are in Table 3.
• We introduce global-to-local feature interaction (GLI) to take advantage of both the local connectivity of con-volutions and the global context of the transformer.
• We provide ablation studies and qualitative analysis, analyzing the effects of different path dimensions and patch scales, discovering efficient and effective config-urations.
• We verify the effectiveness of MPViT as a backbone of dense prediction tasks, achieving state-of-the-art per-formance on ImageNet classification, COCO detection and ADE20K segmentation. 2.