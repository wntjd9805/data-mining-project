Abstract 3D motion estimation including scene flow and point cloud registration has drawn increasing interest. Inspired by 2D flow estimation, recent methods employ deep neural networks to construct the cost volume for estimating accu-rate 3D flow. However, these methods are limited by the fact that it is difficult to define a search window on point clouds because of the irregular data structure. In this paper, we avoid this irregularity by a simple yet effective method. We decompose the problem into two interlaced stages, where the 3D flows are optimized point-wisely at the first stage and then globally regularized in a recurrent network at the second stage. Therefore, the recurrent network only receives the regular point-wise information as the input. In the ex-periments, we evaluate the proposed method on both the 3D scene flow estimation and the point cloud registration task.
For 3D scene flow estimation, we make comparisons on the widely used FlyingThings3D [32] and KITTI [33] datasets.
For point cloud registration, we follow previous works and evaluate the data pairs with large pose and partially over-lapping from ModelNet40 [65]. The results show that our method outperforms the previous method and achieves a new state-of-the-art performance on both 3D scene flow es-timation and point cloud registration, which demonstrates the superiority of the proposed zero-order method on ir-regular point cloud data. Our source code is available at https://github.com/gxd1994/RCP. 1.

Introduction
Motion estimation is a fundamental building block for numerous applications such as robotics, augmented reality and autonomous driving. The low-level motion cues can serve other higher-level tasks such as object detection and action recognition. Given a pair or a sequence of images, we can estimate 2D flow fields from optical flow estimation by either classic variational methods or modern deep learning methods [25, 51, 52].
Different from the scene flow methods that extend 2D optical flow to stereoscopic or RGB-D image sequences [21, 24], increasing attention has been paid to the direct 3D flow iter=0 iter=3 iter=7 iter=14
Figure 1. Visualization of the results on the KITTI scene flow dataset. With the increasing number of alternate optimizations, the source point cloud (in green) is gradually aligned with the target point cloud (in blue). estimation on point clouds recently, which has several advan-tages over image based methods for a variety of applications.
For example, one of the most prominent benefits is that it avoids the image sensor readings and the additional compu-tation of depth from images for autonomous driving, which enables low-latency 3D flow estimation for a high-speed driving vehicle. While for augmented reality, especially for
AR glasses, the 3D flow estimation on point clouds enables the computation distribution on the cloud server because it saves much more transmission bandwidth than images. It also protects the privacy of the surrounding people by not using images.
Therefore, some learning based methods [2,18,27,30,61] utilizes the recent advances made for high-level tasks and customize the scene flow estimation specifically for point clouds. These methods predict the 3D flow vectors from cost-volumes, where similarity costs between 3D points from two point cloud sets are measured. However, different from the cost-volumes in 2D optical flow that search a fixed regular neighborhood around a pixel in consecutive images [25, 51, 52], it is impossible to define such a search window on point clouds because of the irregular data structure. Therefore, previous works like [18, 27, 30, 64], they designed some complicated layers to measure the point-to-patch cost or patch-to-patch cost.
In this paper, we avoid this irregularity by a simple and
effective method. We decompose the problem into two inter-laced stages, where the 3D flows are optimized point-wisely at the first stage and then globally regularized in a recur-rent network at the second stage. Therefore, the recurrent network only receives the regular point-wise information as the input. Besides the scene flow estimation, our method also enables another important motion estimation task that registers two point clouds with the different 6-DOF pose.
Since we only measure the point-to-point costs, we avoid the discretization of the 6DOF solution space, which is difficult because the rotation vector and the translation vector are two different variables that have different scales and ranges.
To evaluate the proposed method, we conduct experi-ments on both the 3D scene flow estimation and the point cloud registration. For 3D scene flow estimation, we made comparisons on the widely used FlyingThings3D [32] and
KITTI [33] benchmark. For point cloud registration, we follow the previous works and generate data pairs with large pose and partially overlapping from ModelNet40 [65]. We have achieved state-of-the-art results on both 3D scene flow estimation and point cloud registration, which demonstrate the superiority of the proposed zero-order method on irregu-lar point cloud data. 2.