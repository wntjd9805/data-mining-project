Abstract 3D scene stylization aims at generating stylized images of the scene from arbitrary novel views following a given set of style examples, while ensuring consistency when ren-dered from different views. Directly applying methods for image or video stylization to 3D scenes cannot achieve such consistency. Thanks to recently proposed neural radiance
ﬁelds (NeRF), we are able to represent a 3D scene in a con-sistent way. Consistent 3D scene stylization can be effec-tively achieved by stylizing the corresponding NeRF. How-ever, there is a signiﬁcant domain gap between style exam-ples which are 2D images and NeRF which is an implicit volumetric representation. To address this problem, we pro-pose a novel mutual learning framework for 3D scene styl-ization that combines a 2D image stylization network and
NeRF to fuse the stylization ability of 2D stylization net-work with the 3D consistency of NeRF. We ﬁrst pre-train a standard NeRF of the 3D scene to be stylized and replace its color prediction module with a style network to obtain
*Corresponding Author is Lin Gao (gaolin@ict.ac.cn). a stylized NeRF. It is followed by distilling the prior knowl-edge of spatial consistency from NeRF to the 2D stylization network through an introduced consistency loss. We also in-troduce a mimic loss to supervise the mutual learning of the
NeRF style module and ﬁne-tune the 2D stylization decoder.
In order to further make our model handle ambiguities of 2D stylization results, we introduce learnable latent codes that obey the probability distributions conditioned on the style. They are attached to training samples as conditional inputs to better learn the style module in our novel stylized
NeRF. Experimental results demonstrate that our method is superior to existing approaches in both visual quality and long-range consistency. 1.

Introduction
Controlling the appearance of complex 3D real scenes has attracted increasing attention in recent years. Numer-ous works have made great effort to this task, such as texture synthesis [10, 22, 55] and semantic view synthesis [16, 18].
Figure 2. Motivation on our mutual learning scheme. Only training a stylized NeRF with style and content losses on small training patches will lead to poorly maintained content and unsatisfactory transfer of style (NeRF w/ Style). Directly applying a 2D image stylization method (AdaIN is used in this example) on results of NeRF will cause inconsistency when rendered from different views (2D Method).
Our method of mutual learning the stylized NeRF and 2D stylization method produces results with better style and consistency quality.
In this paper, we focus on the problem of stylizing com-plex 3D real scenes, which is useful for applications such as virtual reality and augmented reality. Thanks to the re-cently advanced 3D representation methods, complex 3D scenes can be represented as point clouds with appearance features [39] or implicit ﬁelds by deep neural networks such as neural radiance ﬁelds (NeRF) [35, 58]. Compared with point clouds, NeRF can be more reliably obtained from multi-view images, and is continuous in 3D spaces, mak-ing learning easier.
In this paper, we aim to stylize a 3D scene following a given set of style examples. This allows generating styl-ized images of the scene from arbitrary novel views, while making sure rendered images from different views are con-sistent. To ensure consistency, we formulate the problem as stylizing a NeRF [35] with a given set of style images.
Some examples of our NeRF stylization method are pre-sented in Fig. 1.
However, there are two challenges to leverage NeRF as the representation of a complex 3D scene in the task of styl-ization. Firstly, NeRF needs to query hundreds of sample points along the ray to render a single pixel. The memory limitation makes it intractable to render the whole image or even a big enough patch at one time which is impor-tant for calculating content and style losses [21]. There-fore, straightforwardly training a stylized NeRF with per-ceptual style and content losses on small training patches (32×32 for a single RTX 2080Ti GPU) leads to poor styl-ization results, as shown in Fig. 2. Secondly, directly adopt-ing state-of-the-art image stylization methods to stylize ren-dered images from NeRF will generate inconsistent results across different views [5, 19]. This is because these styl-ization methods lack 3D information. Taking a representa-tive Adaptive Instance Normalization (AdaIN) [20] method for example, its results can be seen in the third and fourth columns in Fig. 2. On the other hand, training a NeRF with inconsistent 2D stylized images will cause blurriness in re-sults, which will be further illustrated in Sec. 5.
In order to tackle the problems mentioned above, we propose a novel mutual learning framework [60] between
NeRF [35] and a 2D image stylization method. An ordi-nary NeRF network is ﬁrst trained to model the opacity
ﬁeld of the scene. The opacity ﬁeld of NeRF has the in-herence of geometric consistency and can estimate the 3D coordinates of the rendered pixels, which is distilled to the 2D stylization method through a consistency loss at a pre-training stage. To represent the stylized scene, we replace the module that predicts color in NeRF with a style module (referred to as stylized NeRF). We then co-train the novel stylized NeRF network (with density prediction ﬁxed) with the pre-trained 2D stylization network for ﬁne-tuning col-laboratively. A mimic loss is introduced to align the outputs of the stylized NeRF and the 2D method, aiming to share the stylization knowledge of the 2D method and inherent geom-etry consistency of NeRF to update networks. However, the 2D stylization method cannot guarantee strict consistency, which leads to ambiguities when transferring a given style among multi-view frames of a certain 3D scene, resulting in blurry results of the stylized NeRF.
Inspired by NeRF-W [31], our style module takes learn-able latent codes as conditioned inputs to handle the am-biguities of the 2D stylized results. Unlike NeRF-W, we build a novel probability model of the latent codes condi-tional to styles, which enables our model to handle the in-consistency of 2D results and meanwhile to stylize the scene conditionally. We ﬁrst extract style features of style images with VGG [43], which are deﬁned as the mean and vari-ance of feature maps along the spatial dimensions [20]. We then encode the style features into latent distributions us-ing a pre-trained variational autoencoder (VAE) [23]. The encoded distributions are conditioned to the encoding style features. Since the inconsistent stylized results generated
from the 2D network can be considered to be different sam-ples obeying the distributions conditional to styles, we pa-rameterize 2D stylization results as latent codes obeying the distributions encoded by the corresponding styles. A minus log-likelihood of latent codes is then applied to constrain the conditional probability modeling of latent codes and further ensure the robustness of conditional stylization.
Our main technical contributions are as follows:
• We propose a novel stylized NeRF approach for styl-izing 3D scenes with given style images, outperform-ing existing methods in terms of visual quality and 3D consistency.
• We propose a mutual learning strategy for the stylized
NeRF and 2D stylization method, leveraging the styl-ization capability of 2D method and geometry consis-tency of NeRF.
• A conditional probability modeling for learnable latent codes is proposed to handle the ambiguities of 2D styl-ized results while enabling conditional stylization. 2.