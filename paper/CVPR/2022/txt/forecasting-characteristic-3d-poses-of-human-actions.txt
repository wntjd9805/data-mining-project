Abstract
We propose the task of forecasting characteristic 3d from a short sequence observation of a person, poses: predict a future 3d pose of that person in a likely action-defining, characteristic pose – for instance, from observing a person picking up an apple, predict the pose of the per-son eating the apple. Prior work on human motion predic-tion estimates future poses at fixed time intervals. Although easy to define, this frame-by-frame formulation confounds temporal and intentional aspects of human action. Instead, we define a semantically meaningful pose prediction task that decouples the predicted pose from time, taking inspira-tion from goal-directed behavior. To predict characteristic poses, we propose a probabilistic approach that models the possible multi-modality in the distribution of likely char-acteristic poses. We then sample future pose hypotheses from the predicted distribution in an autoregressive fash-ion to model dependencies between joints. To evaluate our method, we construct a dataset of manually annotated char-acteristic 3d poses. Our experiments with this dataset sug-gest that our proposed probabilistic approach outperforms state-of-the-art methods by 26% on average. 1.

Introduction
Future human pose forecasting is fundamental towards a comprehensive understanding of human behavior, and con-sequently towards achieving higher-level perception in ma-chine interactions with humans, such as autonomous robots or vehicles. In fact, prediction is considered to play a foun-dational part in intelligence [3, 9, 13]. In particular, predict-ing the 3d pose of a human in the future lays a basis for both structural and semantic understanding of human behavior, and for an agent to take fine-grained anticipatory action to-wards the forecasted future. For example, a robotic surgical assistant should predict in advance where best to place a tool to assist the surgeon’s next action, what sensor viewpoints
will be best to observe the surgeon’s next manipulation, and how to position itself to be out of the way at critical future moments.
Recently, we have seen notable progress in the task of fu-ture 3d human motion prediction – from an initial observa-tion of a person, forecasting the 3d behavior of that person up to ≈ 1 second in the future [10,17,21–23]. Various meth-ods have been developed, leveraging RNNs [10, 12, 17, 23], graph convolutional neural networks [20, 22], and atten-tion [21, 28]. However, these approaches all take a tem-poral approach towards forecasting future 3d human poses, and predict poses at fixed time intervals to imitate the fixed frame rate of camera capture. This makes it difficult to pre-dict longer-term (several seconds) behavior, which requires predicting both the time-based speed of movement as well as the higher-level goal of the future action.
Thus, we propose to decouple the temporal and inten-tional behavior, and introduce a new task of forecasting characteristic 3d poses of a person’s future action: from a short pose sequence observation of a human, the goal is to predict a future pose of the person in a characteristic, action-defining moment. This has many potential applications, including HRI, surveillance, visualization, simulation, and content creation. It could be used to predict the hand-off point when a robot is passing an object to a person; to de-tect and display future poses worthy of alerts in a safety monitoring system; to coordinate grasps when assisting a person lifting a heavy object; to assist tracking through oc-clusions; or to predict future keyframes, as is done in video generation [18, 25].
Fig. 2 visualizes the difference between this new task and the traditional, time-based approach: our task is to predict a next characteristic pose at action-defining moments (blue dots) rather than at fixed time-intervals (red dots). As shown in Fig. 1, the characteristic 3d poses are more semantically meaningful and rarely occur at exactly the same times in the future. We believe that predicting possible future character-istic 3d poses takes an important step towards forecasting human action, by understanding the objectives underlying a future action or movement separately from the speed at which they occur.
Since future characteristic 3d poses often occur at longer-term intervals (> 1s) in the future, there may be mul-tiple likely modes of the characteristic poses, and we must capture this multi-modality in our forecasting. Rather than deterministic forecasting, as is an approach in many 3d hu-man pose forecasting approaches [20–22], we develop an attention-driven prediction of probability heatmaps repre-senting the likelihood of each human pose joint in its future location. This enables generation of multiple, diverse hy-potheses for the future pose. To generate a coherent pose prediction across all pose joints’ potentially multi-modal fu-tures, we make autoregressive predictions for the end effec-tors of the actions (e.g., predicting the right hand, then the left hand conditioned on the predicted right hand location) – this enables a tractable modeling of the joint distribution of the human pose joints.
To demonstrate our proposed approach, we introduce a new benchmark on characteristic 3d pose prediction. We annotate characteristic keyframes in sequences from the
GRAB [27] and Human3.6M [15] datasets. Experiments on this benchmark show that our probabilistic approach outper-forms time-based state of the art by 26% on average.
In summary, we present the following contributions:
• We propose the task of forecasting characteristic 3d poses: predicting likely next action-defining future moments from a sequence observation of a person, to-wards goal-oriented understanding of pose forecasting.
• We introduce an attention-driven, probabilistic ap-proach to tackle this problem and model the most likely modes for the next characteristic pose, and show that it outperforms state of the art.
• We autoregressively model the multi-modal distribu-tion of future pose joint locations, casting pose predic-tion as a product of conditional distributions of end ef-fector locations (e.g., hands), and the rest of the body.
• We introduce a dataset and benchmark on our charac-teristic 3d pose prediction, comprising 1535 annotated characteristic pose frames from the GRAB [27] and
Human3.6M [15] datasets.
Figure 2. These plots show the salient difference between our new task (left) and the traditional one (right). The orange curve depicts the motion of one joint (e.g., hand position as a person drinks from a glass). It represents a typical piecewise continuous motion, which has discrete action-defining characteristic poses at cusps of the motion curves (e.g., grasping the glass on the table, putting it to ones mouth, etc.) separating smooth trajectories con-necting them (e.g., raising or lowering the glass). Our task is to predict future characteristic poses (blue dots on left) rather than in-between poses at regular time intervals (red points on right). 2.