Abstract
Current benchmarks for facial expression recognition (FER) mainly focus on static images, while there are limited datasets for FER in videos. It is still ambiguous to evalu-ate whether performances of existing methods remain sat-isfactory in real-world application-oriented scenes. For ex-ample, the “Happy” expression with high intensity in Talk-Show is more discriminating than the same expression with low intensity in Official-Event. To fill this gap, we build a large-scale multi-scene dataset, coined as FERV39k. We analyze the important ingredients of constructing such a novel dataset in three aspects: (1) multi-scene hierarchy and expression class, (2) generation of candidate video clips, (3) trusted manual labelling process. Based on these guidelines, we select 4 scenarios subdivided into 22 scenes, annotate 86k samples automatically obtained from 4k videos based on the well-designed workflow, and finally build 38,935 video clips labeled with 7 classic expressions.
Experiment benchmarks on four kinds of baseline frame-works were also provided and further analysis on their performance across different scenes and some challenges for future research were given. Besides, we systematically investigate key components of DFER by ablation studies.
∗ Corresponding author
The baseline framework and our project are available on https://github.com/wangyanckxx/FERV39k. 1.

Introduction
Facial expression recognition (FER) in static images [43] or videos [29] is of great importance to many applications, such as human-computer interaction (HCI) [2] and lie de-tection [3]. With millions of images uploaded every day by users from different events and social gatherings, there are various available large-scale datasets for static FER, such as
RAF-DB [27] and AffectNet [34]. On top of these datasets, various methods [14, 15, 26, 44] are designed to understand human emotion and recognize facial expressions. In con-trast to static image FER, there are only a few video-based facial expression datasets. In the early period, researchers paid attention to in-the-lab datasets, such as CK+ [32] and
Oulu-CASIA [48], which are collected from lab environ-ments and contain limited posed video clips with no more than 30 frames. Recently, recognizing expressions from in-the-lab short video clips has achieved considerable progress
[29,45,49], but these models often fail to be directly applied for in-the-wild scenes. Typically, limited samples without complex and varied scene context might be impractical for real-world applications.
Dataset (Year)
Samples
Emo.
Anno.
Best.
Context
Scene
Lab Shot Movie
Video Sources
TV
Live Show
CK+ (2010) [32]
Oulu-CASIA (2011) [48]
Aff-Wild (2017) [23]
AFEW-VA (2017) [24]
AFEW 8.0 (2018) [11]
CAER (2019) [25]
DFEW (2020) [21]
FERV39k (2021) 327 560 298 600 1,809 13,201 16,372 39,546 7 Exps 6 Exps
V-A
V-A 7 Exps 7 Exps 7 Exps 7 Exps 1 1 8 2 2 3 10 30 99.69 92.7
N/A
N/A 53.26 77.04 56.41
N/A
Lab
Lab
Wild
Wild
Wild
Wild
Wild
Wild
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
Others
✓
✓
Table 1. Comparison of statistics of existing available DFER datasets and our built FERV39k. (Emo. = Emotion distribution; Anno. =
Annotation times; Best. =Best accuracy; Exps = Expressions; V-A = Valence-Arousal.)
With the development of AFEW competition [11, 24], video-based in-the-wild datasets are released progressively, but their video clips are limited and not enough for devel-oping deep FER models. Although the seeming datasets, such as CAER [25] and DEFW [21], claim that their sources of videos are diverse, there exist some limitations in these datasets. For CAER [25], data volume reaches 13k, how-ever, its scene is single and lacks challenge to FER meth-ods. DEFW [21] is a large-scale and well-annotated uncon-strained dataset for FER in videos, but it fails to consider and further differentiate scene categories [9], which are es-sential for application-oriented expression recognition. Be-sides, these works all overlook how to automatically gener-ate abundant candidate video clips for manual annotation to meet the need of building a larger-scale dataset.
It is necessary to build a multi-scene dataset to advance the FER in video. The benchmark should satisfy several im-portant requirements to cover realistic challenges. 1) Con-sidering the complexity of real-world applications, selected scenes should cover various aspects. 2) With billions of videos currently accessed from the Internet and video plat-forms, there is an urgent need for robust algorithms that can automatically generate massive video clips. 3) Due to the complexity of facial expression annotations, the work-flow of annotating video clips needs to be well-designed.
Based on the above guidelines, we build FERV39k (Fig-ure 1), which is a large-scale, multi-scene, high-quality dataset, and contains 38,935 video clips labeled with 7 clas-sic expressions in 4 scenarios: Daily Life, Weak-Interactive
Shows, Strong-Interactive Activities, and Anomaly Issues.
We design scenarios and scenes for four reasons: 1) Plenty of video sources and samples. 2) Expandability of 22 fine-grained scenes. 3) Large variations and limited overlapping. 4) Distinct associations with scene context. Besides, we de-sign a four-stage strategy, which itself generates 86k candi-date video clips from 4k raw videos.
Specifically, our built FERV39k has 3 main character-istics: 1) Multi-scene: clips are divided into 4 scenarios and subdivided into 22 scenes with different characteris-tics. 2) Large-scale: the amount of video clips reaches 39k with last time from 0.5s to 4s, which indicates that available video frames and cropped facial images reach 1M with the resolution of 336 × 504, and 224 × 224, respectively. 3)
High-quality: workflow of crowdsourcing and professional annotation is adopted to ensure high-quality labels with the guidance of fine-grained expressions.
Given the well-annotated and multi-scene video clips in our built dataset, we first benchmark four kinds of deep learning-based architectures for FER in videos on the chal-lenging FERV39k following action recognition baselines
[7, 22, 30]. We then perform several baseline evaluations with four baselines and representative backbones to reveal challenging aspects of multi-scene expression representa-tion in videos. According to our analysis on FERV39k benchmark, we uncover several new challenges: 1) dif-ficulty and confusion of 7 basic expression classes. 2) discrepancy across 4 scenarios. 3) unsatisfactory cross-scenario performance. 4) long-tail distribution of expres-sions and duration. To systematically enumerate key com-ponents in modeling DFER based on the four baseline ar-chitectures on FERV39k, we further carry out several ab-lation studies and figure out some significant findings: 1)
Pre-training on large-scale datasets is not always helpful. 2)
More sampling fails to steadily improve performance. 3)
Scene information plays a complementary role on DFER.
In summary, our work has three main contributions: 1) We construct a novel large-scale multi-scene FERV39k dataset for both intra-scene and inter-scene DFER. The dataset contains 38,935 video clips labeled with 7 clas-sic expressions across 22 fine-grained scenes in 4 isolated scenarios. To our best knowledge, this is the first dy-namic FER dataset with 39K clips, scenario-scene division as well as cross-domain supportability. 2) We proposed four-stage candidate clip generation and two-stage anno-tation workflow with a balance between cost and quality control which can be used in other large-scale facial video dataset construction. 3) We benchmark four kinds of deep learning-based architectures and conduct in-depth studies of FERV39k, which reveal the key challenges of our dataset and indicate new directions of future research according to extensive ablation studies.
2.