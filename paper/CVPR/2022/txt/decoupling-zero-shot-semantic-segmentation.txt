Abstract
Zero-shot semantic segmentation (ZS3) aims to segment the novel categories that have not been seen in the train-ing. Existing works formulate ZS3 as a pixel-level zero-shot classification problem, and transfer semantic knowl-edge from seen classes to unseen ones with the help of lan-guage models pre-trained only with texts. While simple, the pixel-level ZS3 formulation shows the limited capability to integrate vision-language models that are often pre-trained with image-text pairs and currently demonstrate great po-tential for vision tasks.
Inspired by the observation that humans often perform segment-level semantic labeling, we propose to decouple the ZS3 into two sub-tasks: 1) a class-agnostic grouping task to group the pixels into segments. 2) a zero-shot classification task on segments. The former task does not involve category information and can be directly transferred to group pixels for unseen classes. The latter task performs at segment-level and provides a natural way to leverage large-scale vision-language models pre-trained with image-text pairs (e.g. CLIP) for ZS3. Based on the decoupling formulation, we propose a simple and effective zero-shot semantic segmentation model, called ZegFormer, which outperforms the previous methods on ZS3 standard benchmarks by large margins, e.g., 22 points on the PAS-CAL VOC and 3 points on the COCO-Stuff in terms of mIoU for unseen classes. Code will be released at https:
//github.com/dingjiansw101/ZegFormer. 1.

Introduction
Semantic segmentation targets to group an image into segments with semantic categories. Although remarkable progress has been made [10, 11, 36, 56, 57, 62], current se-mantic segmentation models are mostly trained in a su-pervised manner with a fixed set of predetermined seman-tic categories, and often require hundreds of samples for each class.
In contrast, humans can distinguish at least 30,000 basic categories [6, 18], and recognize novel cate-gories merely from some high-level descriptions. How to
*Corresponding author
Figure 1. ZS3 aims to train a model merely on seen classes and generalize it to classes that have not been seen in the training (un-seen classes). Existing methods formulate it as a pixel-level zero-shot classification problem (b), and use semantic features from a language model to transfer the knowledge from seen classes to
In contrast, as in (c), we decouple ZS3 into two unseen ones. sub-tasks: 1) A class-agnostic grouping and 2) A segment-level zero-shot classification, which enables us to take full advantage of the pre-trained vision-language model. achieve human-level ability to recognize stuff and things in images is one of the ultimate goals in computer vision.
Recent investigations on zero-shot semantic segmenta-tion (ZS3) [7,54] have actually moved towards that ultimate goal. Following the fully supervised semantic segmenta-tion models [10, 11, 36] and zero-shot classification mod-els [1, 24, 30, 47, 60], these works formulate zero-shot se-mantic segmentation as a pixel-level zero-shot classification problem. Although these studies have reported promising results, two main issues still need to be addressed : (1) They usually transfer knowledge from seen to unseen classes by language models [7,27,54] pre-trained only by texts, which
limit their performance on vision tasks. Although large-scale pre-trained vision-language models (e.g. CLIP [46] and ALIGN [26]) have demonstrated potentials on image-level vision tasks, how to efficiently integrate them into the pixel-level ZS3 problem is still unknown. (2) They usually build correlations between pixel-level visual features and semantic features for knowledge transfer, which is not nat-ural since we humans often use words or texts to describe objects/segments instead of pixels in images. As illustrated in Fig. 1, it is unsurprising to observe that the pixel-level classification has poor accuracy on unseen classes, which in turn degrades the final segmentation quality. This phe-nomenon is particularly obvious when the number of un-seen categories is large (see Fig. 6).
An intuitive observation is that, given an image for se-mantic segmentation, we humans can first group pixels into segments and then perform a segment-level semantic label-ing process. For example, a child can easily group the pixels of an object, even though he/she does not know the name of the object. Therefore, we argue that a human-like zero-shot semantic segmentation procedure should be decoupled into two sub-tasks:
- A class-agnostic grouping to group pixels into seg-ments. This task is actually a classical image parti-tion/grouping problem [44,50,52], and can be renewed via deep learning based methods [12, 32, 59].
- A segment-level zero-shot classification to assign se-mantic labels either seen or unseen to segments.
As the the grouping task does not involve the semantic cat-egories, a grouping model learned from seen classes can be easily transferred to unseen classes. The segment-level zero-shot classification is robust on the unseen classes and provides a flexible way to integrate the pre-trained large-scale vision-language models [46] to the ZS3 problem.
To instantiate the decoupling idea, we present a simple yet efficient zero-shot semantic segmentation model with transformer, named ZegFormer, which uses a transformer decoder to output segment-level embeddings, as shown in
Fig. 2. It is then followed by a mask projection for class-agnostic grouping (CAG) and a semantic projection for segment-level zero-shot classification (s-ZSC). The mask projection maps each segment-level embedding to a mask embedding, which can be used to obtain a binary mask prediction via a dot product with a high-resolution feature map. The semantic projection establishes the correspon-dences between segment-level embedding and semantic fea-tures of a pre-trained text encoder for s-ZSC.
While the steps mentioned above can form a standalone approach for ZS3, the model trained on a small dataset is struggling to have strong generalization. Thanks to the de-coupling formulation, it is also flexible to use an image en-coder of a vision-language model to generate image embed-dings for zero-shot segment classification. As we empiri-cally find that the segment classification scores with image embeddings and s-ZSC are complementary. We fuse them to achieve the final classification scores for segments.
The proposed ZegFormer model has been extensively evaluated with experiments and demonstrated superiority on various commonly-used benchmarks for ZS3. It outper-forms the state-of-the-art methods by 22 points in terms of mIoU for unseen classes on the PASCAL VOC [15], and 3 points on the COCO-Stuff [8]. Based on the chal-lenging ADE20k-Full dataset [64], we also create a new
ZS3 benchmark with 275 unseen classes, the number of unseen classes in which are much larger than those in
PASCAL-VOC (5 unseen classes) and COCO-Stuff (15 un-seen classes). On the ADE20k-Full ZS3 benchmark, our performance is comparable to MaskFormer [12], a fully su-pervised semantic segmentation model.
Our contributions in this paper are three-fold:
• We propose a new formulation for the task of ZS3, by decoupling it into two sub-tasks, a class-agnostic grouping and a segment-level zero-shot classification, which provides a more natural and flexible way to inte-grate the pre-trained large-scale vision-language mod-els into ZS3.
• With the new formulation, we present a simple and ef-fective ZegFormer model for ZS3, which uses a trans-former decoder to generate segment-level embeddings for grouping and zero-shot classification. To the best of our knowledge, the proposed ZegFormer is the first model taking full advantage of the pre-trained large-scale vision-language model (e.g. CLIP [46]) for ZS3.
• We achieved state-of-the-art results on standard bench-marks for ZS3. The ablation and visualization analy-ses show that the decoupling formulation is superior to pixel-level zero-shot classification by a large margin. 2.