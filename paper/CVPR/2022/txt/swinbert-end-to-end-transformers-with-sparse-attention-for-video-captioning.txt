Abstract
The canonical approach to video captioning dictates a caption generation model to learn from ofﬂine-extracted dense video features. These feature extractors usually oper-ate on video frames sampled at a ﬁxed frame rate and are often trained on image/video understanding tasks, without adaption to video captioning data. In this work, we present
SWINBERT, an end-to-end transformer-based model for video captioning, which takes video frame patches directly as inputs, and outputs a natural language description.
Instead of leveraging multiple 2D/3D feature extractors, our method adopts a video transformer to encode spatial-temporal representations that can adapt to variable lengths of video input without dedicated design for different frame rates. Based on this model architecture, we show that video captioning can beneﬁt signiﬁcantly from more densely sam-pled video frames as opposed to previous successes with sparsely sampled video frames for video-and-language un-derstanding tasks (e.g., video question answering). More-over, to avoid the inherent redundancy in consecutive video frames, we propose adaptively learning a sparse attention mask and optimizing it for task-speciﬁc performance im-provement through better long-range video sequence mod-eling. Through extensive experiments on 5 video caption-ing datasets, we show that SWINBERT achieves across-the-board performance improvements over previous meth-ods, often by a large margin. The learned sparse attention masks in addition push the limit to new state of the arts, and can be transferred between different video lengths and between different datasets. Code is available at https:
//github.com/microsoft/SwinBERT. 1.

Introduction
Video captioning [1,10,25,28,35,39,44,45,56] is the task of describing the visual content of a given video in natural language. As such, it requires an algorithm to understand
* Equal contribution.
Figure 1. Comparison between previous works and SWINBERT.
Different from prior works that use ofﬂine-extracted 2D/3D fea-tures, we propose to adopt the video transformer as our video en-coder, and present an end-to-end fully Transformer-based model for video captioning. We further propose to adaptively learn a sparse attention mask to improve long-range video sequence mod-eling. and model the spatial-temporal dynamics in video, as well as the relationships between visual and textual elements, and to generate a sequence of output words. This has usu-ally been tackled with transformer-based models that learn from ofﬂine extracted video representations [21, 25, 31, 45] (Figure 1 (a)). Speciﬁcally, multiple feature extractors, usually trained on image/video understanding tasks (e.g., image classiﬁcation or action recognition), are employed to extract 2D appearance features and 3D motion features from densely sampled video frames. Although achieving promising results, there exists a discrepancy in both data do-main and task formulation between these off-the-shelf fea-ture extractors and downstream video captioning. However, end-to-end training with multiple feature extractors on such dense video frames is computationally intensive, or even in-feasible.
Method
MSVD ↑ YouCook2 ↑ MSRVTT ↑
TVC ↑
VATEX ↑
SOTA
SWINBERT 95.2 [57] 160.0 53.6 [25] 109.0 52.9 [56] 55.9 51.0 [25] 56.9 58.1 [25] 73.0
Table 1. Comparison with state-of-the-art methods across all video captioning datasets considered on CIDEr [47] metric.
More recently, CLIPBERT [21] points out the repetitive information presented in consecutive video frames is not necessary for downstream video-and-language tasks, and proposes a sparse sampling strategy that enables affordable end-to-end training to the raw pixel inputs. Although it has shown great success in video-and-language understanding tasks, such as video question answering [22] and text-to-video retrieval [23, 53], it remains unclear whether these sparsely sampled video frames are sufﬁcient to generate rich and descriptive captions. Moreover, CLIPBERT leverages a 2D Convolutional Neural Network together with mean pooling that operates directly on the raw video frames to learn video representations, which may lose temporal infor-mation that is essential to describe visual events in chrono-logical order.
In this work, we aim to ﬁnd an end-to-end solution to the video captioning task. Inspired by the recent successes of Transformer-based models in computer vision [2, 5, 14, 29], especially for video understanding tasks [6], we pro-pose SWINBERT (Figure 1 (b)), a pure Transformer-based model that directly takes raw video frames as inputs for end-to-end caption generation. Unlike previous methods lever-aging off-the-shelf 2D/3D feature extractors at a ﬁxed frame rate, we employ a video Transformer capable of learning from variable lengths of video frame sequence without ded-icated design for different frame rates. Based on this spe-ciﬁc model design, we investigate how many video frames are sufﬁcient for the video captioning task?. Our experi-ments show that the captioning performance (i.e., CIDEr score) can be greatly lifted by more densely sampled frames (e.g., Ours: 64 frames, vs. CLIPBERT: 16 frames), in con-trast to previous success with sparsely sampled frames for video-and-language understanding tasks. Lastly, to avoid the redundancy that comes naturally in consecutive video frames, we further introduce a learnable Sparse Attention
Mask as a regularizer that allows the model to focus more on video frame patches that contain more spatial-temporal movements (e.g., the main moving objects) than those stay-ing unchanged for the entire video duration (e.g., the back-ground). In contrast to prior models [21, 25, 31] with prede-ﬁned attention structures, our model can learn adaptive at-tention maps to optimize for task-speciﬁc performance im-provements through better video sequence modeling.
Our extensive experimental results on 5 video captioning datasets demonstrate that our proposed model is effective in learning sparse attention patterns to improve long-range video sequence modeling, and consequently outperforms previous state-of-the-art approaches by a large margin. To the best of our knowledge, SWINBERT is the ﬁrst end-to-end pure Transformer-based architecture for video caption-ing. Additionally, the proposed Sparse Attention Mask ef-fectively regularizes model training and brings further per-formance improvements across all 5 datasets, which opens a new direction in removing redundancy in video inputs for video-and-language modeling.
In summary, our contributions are three-fold.
• We present SWINBERT, the ﬁrst end-to-end fully
Transformer-based model for video captioning.
• We introduce the Sparse Attention Mask as a regular-izer for improving long-range video sequence model-ing, and quantitatively validate the effectiveness of the learnable sparse attention mask in caption generation.
• Our method outperforms previous state-of-the-art methods by a large margin on 5 popular video caption-ing benchmarks. As shown in Table 1, SWINBERT achieves an absolute CIDEr improvement of +64.8 on
MSVD, +55.4 on YouCook2, +3.0 on MSRVTT, +5.9 on TVC and +14.9 on VATEX. 2.