Abstract
Attention-based neural networks such as the Vision Trans-former (ViT) have recently attained state-of-the-art results on many computer vision benchmarks. Scale is a primary ingredient in attaining excellent results, therefore, under-standing a model’s scaling properties is a key to designing future generations effectively. While the laws for scaling
Transformer language models have been studied, it is un-known how Vision Transformers scale. To address this, we scale ViT models and data, both up and down, and character-ize the relationships between error rate, data, and compute.
Along the way, we reﬁne the architecture and training of ViT, reducing memory consumption and increasing accuracy of the resulting models. As a result, we successfully train a
ViT model with two billion parameters, which attains a new state-of-the-art on ImageNet of 90.45% top-1 accuracy. The model also performs well for few-shot transfer, for example, reaching 84.86% top-1 accuracy on ImageNet with only 10 examples per class. 1.

Introduction
Attention-based Transformer architectures [44] have taken computer vision domain by storm [7, 15] and are be-coming an increasingly popular choice in research and prac-tice. Previously, Transformers have been widely adopted in the natural language processing (NLP) domain [6, 14]. Opti-mal scaling of Transformers in NLP was carefully studied in [21], with the main conclusion that large models not only perform better, but do use large computational budgets more efﬁciently. However, it remains unclear to what extent these
ﬁndings transfer to the vision domain, which has several important differences. For example, the most successful pre-training schemes in vision are supervised, as opposed to unsupervised pre-training in the NLP domain.
In this paper we concentrate on scaling laws for transfer performance of ViT models pre-trained on image classiﬁca-(cid:63)equal contribution
Figure 1. Few-shot transfer results. Our ViT-G model reaches 84.86% top-1 accuracy on ImageNet with 10-shot linear evaluation. tion tasks. In particular, we experiment with models ranging from ﬁve million to two billion parameters, datasets ranging from one million to three billion training images and com-pute budgets ranging from below one TPUv3 core-day to beyond 10 000 core-days. Our main contribution is a char-acterization of the performance-compute frontier for ViT models, on two datasets.
Along the way, we create an improved large-scale train-ing recipe. We investigate training hyper-parameters and discover subtle choices that make drastic improvements in few-shot transfer performance. The few-shot transfer evalua-tion protocol has also been adopted by previous large-scale pre-training efforts in NLP domain [5]. Speciﬁcally, we discover that very strong L2 regularization, applied to the
ﬁnal linear prediction layer only, results in a learned visual representation that has very strong few-shot transfer capabili-ties. For example, with just a single example per class on the
ImageNet dataset (which has 1 000 classes), our best model achieves 69.52% accuracy; and with 10 examples per class it attains 84.86%. In addition, we substantially reduce the memory footprint of the original ViT model proposed in [15].
We achieve this by hardware-speciﬁc architecture changes and a different optimizer. As a result, we train a model with two billion parameters and attain a new state-of-the-art 90.45% accuracy on ImageNet.
Figure 2. Left/Center: Representation quality, measured as ImageNet ﬁnetune and linear 10-shot error rate, as a function of total training compute. A saturating power-law approximates the Pareto frontier fairly accurately. Note that smaller models (blue shading), or models trained on fewer images (smaller markers), saturate and fall off the frontier when trained for longer. Top right: Representation quality when bottlenecked by model size. For each model size, a large dataset and amount of compute is used, so model capacity is the main bottleneck.
Faintly-shaded markers depict sub-optimal runs of each model. Bottom Right: Representation quality by datasets size. For each dataset size, the model with an optimal size and amount of compute is highlighted, so dataset size is the main bottleneck. 2. Core Results
We ﬁrst present our main results on scaling trends, before presenting detailed architecture and training protocol im-provements in Section 3. In the following experiments, we train several ViT models on both public ImageNet-21k [13] dataset and privately gathered images, up to three billion weakly-labelled images. We vary the architecture size, num-ber of training images, and training duration. All models are trained on TPUv3, thus total compute is measured in
TPUv3 core-days. To evaluate the quality of the representa-tion learned by the models, we measure (i) few-shot transfer via training a linear classiﬁer on frozen weights, (ii) transfer via ﬁne-tuning the whole model on all data, both to multiple benchmark tasks. 2.1. Scaling up compute, model and data together
Figure 2 shows both the 10-shot linear evaluation and
ﬁnetuning evaluation on ImageNet [13]. Similar trends on other datasets, Oxford IIIT Pets [27], CIFAR-100 [23], and
Caltech-UCSD Birds [46] are presented in the Appendix,
Figure 9. For each combination of model size and data size we pre-train for various numbers of steps. In Figure 2, con-nected points represent the same model trained for a different number of steps. We make the following observations.
First, scaling up compute, model and data together im-proves representation quality. In the left plot and center plot, the lower right point shows the model with the largest size, dataset size and compute achieving the lowest error rate.
However, it appears that at the largest size the models starts to saturate, and fall behind the power law frontier (linear relationship on the log-log plot in Figure 2).
Second, representation quality can be bottlenecked by model size. The top-right plot shows the best attained perfor-mance for each model size. Due to limited capacity, small models are not able to beneﬁt from either the largest dataset, or compute resources. Figure 2, left and center, show the
Ti/16 model tending towards a high error rate, even when trained on a large number of images.
Third, large models beneﬁt from additional data, even beyond 1B images. When scaling up the model size, the representation quality can be limited by smaller datasets; even 30-300M images is not sufﬁcient to saturate the largest models. In Figure 2, center, the error rate of L/16 model on the the 30M dataset does not improve past 27%. On the larger datasets, this model attains 19%. Further, when increasing the dataset size, we observe a performance boost with big models, but not small ones. The largest models even obtain a performance improvement the training set size grows from 1B to 3B images (Figure 2, bottom right). For small models, however, such as Ti/16 or B/32, increasing the dataset size does not help. For example, in Figure 2, left and center, all of the curves for Ti/16 overlap, showing that this model achieves the same performance irrespective of the dataset size. 2.2. Double-saturating power law
Figure 2, left and center, show the Pareto frontier of representation quality versus training compute. The frontier contains the models with the best allocation of compute to model shape and training duration.
For over two orders of magnitude of compute, the relation-Figure 3. Error rate on ImageNet, with respect to images seen during pre-training. Big models are more sample efﬁcient, which is consistent across diverse setups: few-shot transfer on the frozen representations, ﬁne-tune the network on ImageNet, and evaluate the ﬁne-tuned models on the v2 test set. ship between compute and performance follows a power-law (E = aC b), resulting in a straight line on the log-log plot.
However, we observe “saturation” at both ends of the com-pute spectrum. At the higher end of compute, the largest models do not tend towards zero error-rate. If we extrapolate from our observations, an inﬁnite capacity model will obtain a non-zero error. This effect has also been observed for gen-erative models [18]. The authors of [18] refer to this residual error as the “irreducible entropy” of the task. Since we plot error rate, the information-theoretic interpretation does not apply, but our observations support the notion of fundamen-tal performance ceilings for ImageNet [4]. In terms of the law, this saturation corresponds to an additive constant to the error rate: c in E = aC −b + c.
At the lower end of the compute spectrum, we see a satu-ration for smaller models; the performance of the smallest model is better than that would be predicted by a power-law.
This saturation occurs because even trivial solutions can achieve non-zero error. For example, predicting the majority class (almost zero compute) will achieve an accuracy related to its occurence frequency in the test set. This lower bound is not observed in [18], either because their smallest model is large enough to avoid this region, or because log-loss satu-rates at worse performances than accuracy (it will saturate eventually). This saturation corresponds to a shift in the x-axis: d in E = a(C + d)−b + c. This constant indicates that the zero-compute model will still obtain non-zero accuracy. 2.3. Big models are more sample efﬁcient
Figure 3 shows the representation quality with respect to the total number of images “seen” (batch size times number of steps) during pre-training. In addition to ImageNet ﬁne-tuning and linear 10-shot results on the public validation set, we also report results of the ImageNet ﬁne-tuned model on the ImageNet-v2 test set [32] as an indicator of robust generalization. Three ViT models pre-trained on three billion images are presented in this plot.
We observe that bigger models are more sample efﬁcient, reaching the same level of error rate with fewer seen im-ages. For 10-shot, the Ti/16 model needs to see nearly 100 times more images to match the representation quality of the L/16 model. When ﬁne-tuning, this factor reduces from 100 to about 20. Our results suggest that with sufﬁcient data, training a larger model for fewer steps is preferable.
This observation mirrors results in language modelling and machine translation [21, 25]. 2.4. Do scaling laws still apply on fewer images?
We extend the study to much fewer images, ranging from one million to 13 millions on the public ImageNet-21k
Figure 4. Results on the ImageNet-21k dataset. Left: Representa-tion quality, measured as ImageNet linear 10-shot error rate, as a function of total training compute. The double-saturating power law still applies. Right: Representation quality by model sizes and dataset sizes.
Table 1. The results for ViT-G/14, compared to the previous state-of-the-art models.
Benchmark
ImageNet
INet V2
INet ReaL
ObjectNet
VTAB (light)
NS (Eff.-L2) [48]
MPL (Eff.-L2) [28]
CLIP (ViT-L/14) [30]
ALIGN (Eff.-L2) [20]
BiT-L (ResNet) [22]
ViT-H/14 [15] 88.3 90.2 85.4 88.6 87.54 88.55 80.2
-75.9 70.1
---91.02
--90.54 90.72 68.5
-72.3
-58.7
-----76.29 77.63
Our ViT-G/14 90.45±0.03 83.33±0.03 90.81±0.01 70.53±0.52 78.29±0.53 dataset. In Figure 4 left, we found that the double-saturation power law still applies, when varying model sizes, dataset sizes and compute resources. This indicates that the conclu-sions from the study generalizes well, and can guide future design choices for vision transformer architectures. In Fig-ure 4 right, we observe similar behaviors that the model performance are bottlenecked by the dataset size. When scaling up compute, model and data together, one gets the best representation quality. 2.5. ViT-G/14 results
We trained a large Vision Transformer, ViT-G/14, which contains nearly two billion parameters. Section 3.6 details the architecture’s shape. We evaluate the ViT-G/14 model on a range of downstream tasks, and compare it to recent state-of-the-art results. We ﬁne-tune on ImaegNet, and report Im-ageNet [33], ImageNet-v2 [32], ReaL [4], and ObjectNet [2] accuracies. In addition, we report transfer learning result on the VTAB-1k benchmark consisting of 19 tasks [52].
Figure 1 shows the few-shot transfer results on Ima-geNet. ViT-G/14 outperforms the previous best ViT-H/14 model [15] by a large margin (more than 5%), attaining 84.86% accuracy with 10 examples per class. Ten images per class is less than 1% of ImageNet data (13 examples per class), as commonly used in self-supervised and semi-supervised learning [51]. For reference, Figure 1 shows three state-of-the-art self-supervised learning models, Sim-CLR v2 [9] and BYOL [16], using 1% of ImageNet data,
DINO [8] using 20 examples per class. Note, however, that these approaches are quite different: ViT-G/14 uses large source of weakly-supervised data, and is pre-trained only once and transferred to different tasks. Meanwhile, the self-supervised learning models use unlabeled but in-domain data for pre-training, and target a single task.
Table 1 shows the results on the remaining benchmarks.
ViT-G/14 achieves 90.45% top-1 accuracy on ImageNet, set-ting the new state-of-the art. On ImageNet-v2, ViT-G/14 improves 3% over the Noisy Student model [48] based on EfﬁcientNet-L2. For ReaL, ViT-G/14 outperforms ViT-H [15] and BiT-L [22] by only a small margin, which in-dicates again that the ImageNet classiﬁcation task is likely reaching its saturation point. For ObjectNet, ViT-G/14 out-performs BiT-L [22] by a large margin, and is 2% better than Noisy Student, but is about 2% behind CLIP [30]. Note that, unlike the other methods, CLIP does not ﬁne-tune on
ImageNet, and evaluates directly on ObjectNet, this likely improves its robustness. Finally, when transferring the ViT-G/14 model to VTAB, it gets consistently better results with just a single hyper parameter across all tasks. The state-of-the-art on VTAB using a heavyweight per-task hyperparam-eter sweep is 79.99 [20], we leave running a heavy sweep with ViT-G/14 to future work. 3. Method details
We present a number of improvements to the ViT model and training. These improvements are mostly simple to im-plement, and can signiﬁcantly improve memory-utilization and model quality. They allow us to train ViT-G/14 using data-parallelism alone, with the entire model ﬁtting on a single TPUv3 core. 3.1. Decoupled weight decay for the “head”
Weight decay has a drastic effect on model adaptation in the low-data regime. We conduct an study of this phenomena at a mid-size scale.
We ﬁnd that one can beneﬁt from decoupling weight de-cay strength for the ﬁnal linear layer (“head”), and for the remaining weights (“body”) in the model. Figure 5 demon-strates this effect: we train a collection ViT-B/32 models on JFT-300M, each cell corresponds to the performance of different head/body weight decay values. The diagonal cor-responds to using the same value for both decays. One can observe that the best performance appears off-diagonal (i.e. with a decoupled weight decay for the head and body). In-terestingly, we observe that high weight decay in the head decreases performance on the pre-training (upstream) task (not shown), despite improving transfer performance.
We do not have a complete explanation of this phenomena.
However, we hypothesize that a stronger weight decay in the
Figure 5. Left and middle: The dependence of 5-shot ImageNet accuracy and upstream performance depends on the weight decay strength.
Normally, a single weight decay value is applied to all weights (corresponds to the diagonal on the heatmaps). We show that by using weight decay values for the “head” and the rest of the weights one signiﬁcantly improves few-shot transfer performance. Right: Few-shot performance on ImageNet for different types of head. A high weight decay on the head works equally well for all of them. head results in representations with larger margin between classes, and thus better few-shot adaptation. This is similar to the main idea behind SVMs [11]. This large decay makes it harder to get high accuracy during upstream pre-training, but our main goal is high quality transfer. 3.2. Saving memory by removing [class] token
The largest VIT model from [15] uses 14 × 14 patches with 224 × 224 images. This results in 256 visual “tokens”, where each one corresponds to an image patch. On top of this, ViT models have an extra [class] token, which is used to produce the ﬁnal representation, bringing the total number of tokens to 257.
For ViT models, current TPU hardware pads the token dimension to a multiple of 128, which may result in up to a 50% memory overhead. To overcome this issue we investigate alternatives to using the extra [class] token.
In particular, we evaluate global average pooling (GAP) and multihead attention pooling (MAP) [24] to aggregate representation from all patch tokens. We set the number of heads in MAP to be equal to the number of attention heads in the rest of the model. To further simplify the head design we remove ﬁnal non-linear projection before the ﬁnal prediction layer, which was present in the original ViT paper.
To choose the best head, we perform a side-by-side com-parison of a [class] token and GAP/MAP heads. Results are summarized in Figure 5 (right). We ﬁnd that all heads perform similarly, while GAP and MAP are much more memory efﬁcient due to the aforementioned padding consid-erations. We also observe that non-linear projection can be safely removed. Thus, we opt for the MAP head, since it is the most expressive and results in the most uniform architec-ture. MAP head has also been explored in [41], in a different context for better quality rather than saving memory. 3.3. Scaling up data
For this study, we use the proprietary JFT-3B dataset, a larger version of the JFT-300M dataset used in many previ-ous works on large-scale computer vision models [15,22,36].
This dataset consists of nearly 3 billion images, annotated with a class-hierarchy of around 30k labels via a semi-automatic pipeline. Thus, the data and associated labels are noisy. We ignore the hierarchical aspect of the labels and use only the assigned labels as targets for multi-label classi-ﬁcation via a sigmoid cross-entropy loss, following [15, 22].
We have conducted sensitive category association analy-sis as described in [1]. We measured (per label) the distribu-tion of sensitive categories across the raw data, the cleaned data, the models trained on this data, and labels that were veriﬁed by human raters. Human raters additionally assisted in removing offensive content from the dataset.
Figure 6 shows an ablation of the effect of changing from
JFT-300M to JFT-3B on model performance, even when scale is not increased. Figure 6, left shows linear 10-shot Im-ageNet performance evaluated throughout. We observe that
JFT-3B results in a better model, even before the model has completely one epoch of JFT-300M. Therefore, overﬁtting
JFT-300M is not the sole cause of the improvement. This dif-ference can be seen even for the small B/32 model as well as the larger L/16. We ﬁne-tune the models to the full ImageNet dataset (right), and conﬁrm that these improvements transfer to a full ﬁne-tuning setup. Overall, the change in dataset improves transfer to ImageNet by about 1% for both small and large models. Other than the performance improvement, training behavior is similar on JFT-300M and JFT-3B. Most importantly, JFT-3B allows us to scale up further with fewer concerns about overﬁtting and regularization.
Deduplication. We remove all images from the JFT-3B
Figure 6. The effect of switching from JFT-300M to JFT-3B, without any further scaling. Both small and large models beneﬁt from this change, by an approximately constant factor, both for linear few-shot evaluation (left) and transfer using the full dataset (right). dataset that are near-duplicates of images from both train set and test set of datasets we evaluate on. Overall we identiﬁed and removed 927k duplicate images from JFT-3B. our preliminary experiments, we found that clipping the second momentum at 0.999 (Adam’s default value) results in better convergence, so we adopt it. 3.4. Memory-efﬁcient optimizers
When training large models, storage required for model parameters becomes a bottleneck. Our largest model, ViT-G, has roughly two billion parameters, which occupies 8 GiB of device memory. To make things much worse, the Adam optimizer that is commonly used for training Transformers, stores two additional ﬂoating point scalars per each parame-ter, which results in an additional two-fold overhead (extra 16 GiB). To tackle the overhead introduced by the Adam optimizer we explore two modiﬁcations.
Adam with half-precision momentum. We empir-ically observe that storing momentum in half-precision (bfloat16 type) does not affect training dynamics and has no effect on the outcome. This allows to reduce opti-mizer overhead from 2-fold to 1.5-fold. Notably, storing the second momentum using half-precision resulted in a signiﬁcant performance deterioration.
Adafactor optimizer. The above optimizer still induces a large memory overhead. Thus, we turn our attention to the
Adafactor optimizer [34], which stores second momentum using rank 1 factorization. From practical point of view, this results in the negligible memory overhead. However, the
Adafactor optimizer did not work out of the box, so we make the following modiﬁcations:
• We re-introduce the ﬁrst momentum in half-precision, whereas the recommended setting does not use the ﬁrst momentum at all.
• We disable scaling of learning rate relative to weight norms, a feature that is part of Adafactor.
• Adafactor gradually increases the second momentum from 0.0 to 1.0 throughout the course of training. In
The resulting optimizer introduces only a 50% memory over-head on top the space needed to store model’s parameters.
We observe that both proposed optimizers perform on par with or slightly better than the original Adam optimizer. We are aware of other memory-efﬁcient optimizers [31, 39], we leave the exploration to future work. 3.5. Learning-rate schedule
In our study we want to train each of the models for several different durations in order to measure the trade-off between model size and training duration. When using linear decay, as in [15], each training duration requires its own training run starting from scratch, which would be an inefﬁcient protocol.
Inspired by [26], we address this issue by exploring learning-rate schedules that, similar to the warmup phase in the beginning, include a cooldown phase at the end of training, where the learning-rate is linearly annealed toward zero. Between the warmup and the cooldown phases, the learning-rate should not decay too quickly to zero. This can be achieved by using either a constant, or a reciprocal square-root schedule for the main part of training. Figure 7 (bottom) depicts several of these options, with a cooldown after ap-proximately 200 k, 400 k, and 500 k steps. The upper half of
Figure 7 shows the validation score (higher is better) for each of these options and their cooldowns, together with two lin-ear schedules for reference. While the linear schedule is still preferable when one knows the training duration in advance and does not intend to train any longer, all three alternatives come reasonably close, with the advantage of allowing in-deﬁnite training and evaluating multiple training durations from just one run. For each of the schedules, we optimized the learning-rate and the exact shape. We have also brieﬂy
Table 2. Model architecture details. e m a
N s/28 s/16
S/32
Ti/16
B/32
S/16
B/28
B/16
L/16 g/14
G/14 h t d i
W h t p e
D
P
L
M s d a e
H 256 256 384 192 768 384 768 768 1024 1408 1664 6 1024 8 6 1024 8 12 1536 6 12 3 768 12 3072 12 12 1536 6 12 3072 12 3072 12 12 24 4096 16 6144 16 40 8192 16 48
. o i
M m a r a
P 5.4 5.0 22 5.5 87 22 87 86 303 1011 1843
GFLOPs 2242 3842 2.0 0.7 7.8 2.2 6.9 2.3 9.5 2.5 26.0 8.7 31.2 9.2 30.5 11.3 111.3 35.1 122.9 382.8 533.1 1596.4 965.3 2859.9 presented, but note that with our modiﬁcations we were able to ﬁt thin ViT models of a depth up to 100 encoder blocks.
The original Vision Transformer publication contains a study in Appendix D2 about the trade-offs between scaling the different components, concluding that it is most effective to scale all aspects (depth, width, MLP-width, and patch-size) simultaneously and by a similar amount. We follow this recommendation, and select shapes for ViT-g and ViT-G at the limit of what ﬁts in memory accordingly, as shown in
Figure 8 and summarized in Table 2. 4.