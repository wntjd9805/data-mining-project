Abstract
Prototypical methods have recently gained a lot of at-tention due to their intrinsic interpretable nature, which is obtained through the prototypes. With growing use cases of model reuse and distillation, there is a need to also study transfer of interpretability from one model to an-other. We present Proto2Proto, a novel method to trans-fer interpretability of one prototypical part network to an-other via knowledge distillation. Our approach aims to add interpretability to the “dark” knowledge transferred from the teacher to the shallower student model. We propose two novel losses: “Global Explanation” loss and “Patch-Prototype Correspondence” loss to facilitate such a trans-fer. Global Explanation loss forces the student prototypes to be close to teacher prototypes, and Patch-Prototype Cor-respondence loss enforces the local representations of the student to be similar to that of the teacher. Further, we pro-pose three novel metrics to evaluate the student’s proxim-ity to the teacher as measures of interpretability transfer in our settings. We qualitatively and quantitatively demon-strate the effectiveness of our method on CUB-200-2011 and Stanford Cars datasets. Our experiments show that the proposed method indeed achieves interpretability trans-fer from teacher to student while simultaneously exhibiting competitive performance. The code is available at https:
//github.com/archmaester/proto2proto 1.

Introduction
Interpretability in machine learning helps humans under-stand the reasoning behind a model in arriving at a particular decision. From an end-user’s perspective, interpretability increases the trust in the models, eventually leading to bet-ter machine learning systems, especially the ones involved in making high-stake decisions. Lipton [28] points out vari-ous desiderata for interpretability like trust, causality, trans-ferability, etc.
In the past decade, the primary focus of the vision community was on designing models to improve performance on tasks like classification, object detection, segmentation, etc. Deep learning models such as CNNs
[12, 40, 44] have played a considerable role in this success.
However, they are often criticized as non-interpretable due to their black-box nature.
To add interpretations to the CNN models [?], numer-ous efforts falling into categories of model approximation, exemplar-based, gradient-based and concept-based inter-pretations have been studied in recent years. In this work, we specifically focus on methods that perform model ap-proximation using prototypes, which allow both post hoc in-terpretability (explaining in terms of concepts after a model is trained) and ante hoc interpretability (learning to pre-dict and explain through prototypes jointly during training itself). ProtoPNet [6] and ProtoTree [33] are two recent methods that add interpretability to prototypical models.
ProtoPNet learns class-specific prototypes, while ProtoTree learns class-agnostic prototypes to provide global and lo-cal interpretability. However, the efforts in this space are nascent, with no effort yet to translate to shallower networks or transfer interpretability through the learned prototypes to other models, which could have many applications in model compression, few-shot learning, continual learning, etc.
Knowledge Distillation (KD) is a well-known technique to transfer the “dark” knowledge from the Teacher model to a Student network. Many works [1, 7, 9, 13–16, 23, 34, 35, 37, 46, 47, 51, 53] have focused on improving the perfor-mance and faithfulness of the student model to the teacher model in terms of accuracy, without much focus on the in-terpretability aspect. Liu et al. [29] distilled a black-box deep learning model to a decision tree to make it more in-terpretable. Song et al. [41] constructed an intermediate decision tree to capture the intrinsic problem-solving pro-cess of the teacher and transfer it to a student. The goal of these few works was to add interpretability to a black-box teacher model through knowledge distillation. We, on the other hand, consider an already implcitly interpretable teacher model and show how our distillation method can retain faithfulness of interpretability while transferring to a student model.
To this end, we present Proto2Proto, a novel method to transfer the interpretability of one prototypical part network to another. We consider a Teacher network, whose inter-Figure 1. Comparison of sample prototypes of test image between Teacher, Baseline Student and Proto2Proto (P2P) Student. pretability we would like to transfer to a shallower Student network. Prototype in this paper refers to a latent represen-tation of a training image with a smaller spatial dimension called a patch.
It represents the prototypical part of im-ages allowing finer comparisons similar to the prototypes defined in ProtoPNet [6]. Fig. 1 illustrates the motivation of our approach. For a given test image, we visualize top-k prototypes, which play the most significant role in the de-cision making. We compare these prototypes of teacher, baseline student and our student. As evident, our student is more faithful to the teacher in retaining similar prototypes to make decisions compared to baseline student.
In particular, we propose two novel losses: Global Ex-planation loss and Patch-Prototype Correspondence loss to achieve our objective of transferring the interpretabil-ity of the teacher to the student. In prototypical networks
[6, 33, 39], knowledge is stored in the prototypes that these models learn. These prototypes can act as global explana-tions for the model, i.e., irrespective of the input, the model can tell which parts/regions it may focus on to make deci-sions. Global Explanation loss helps to transfer these global explanations or prototypes to the student.
Similarly, for a given input, local representations ob-tained from the model are compared with the prototypes to determine which prototypes are present in the image. Based on the activations of prototypes, the model recognizes the image. Hence, it becomes important to generate similar ac-tivations of prototypes, for a given input, to recognize an image like the teacher. Patch-Prototype Correspondence loss helps to achieve this objective. It mimics the local rep-resentations of the teacher for which prototypes become ac-tive. Unlike [37], which mimics the entire feature map of a teacher for knowledge transfer, we propose to mimic local representations of the teacher that activate prototypes.
Since this is the first such effort, to validate whether we have achieved our objective, we propose three new metrics: (i) Average number of Active Patches (AAP), which deter-mines the average number of local representations which are active for the model. It is used to evaluate the Patch-Prototype Correspondence, with a motive of bringing this value of the student close to the teacher; (ii) Average Jac-card Similarity of Active Patches with Teacher (AJS), which determines the overlap of the active local represen-tations of the student to the teacher. It is calculated for a pair of models, namely, teacher and student. The higher its value, the closer the student is to the teacher. It is also used to evaluate the Patch-Prototype Correspondence; and (iii)
Prototype Matching Score (PMS), which evaluates how close the prototypes of the student are w.r.t. the teacher.
It is used to evaluate the transfer of Global Explanations.
We summarize our contributions as follows:
• To the best of our knowledge, we present the first attempt to transfer interpretability from a prototypical teacher to a student model.
• We propose two novel losses, Global Explanation loss and Patch-Prototype Correspondence loss for the knowl-edge transfer. We show that with our approach, the final layer decision module of a teacher can be used for the student directly as is, without relearning.
• We propose three evaluation metrics to determine the faithfulness of the student to the teacher in terms of in-terpretability.
• We perform a comprehensive suite of experiments on benchmark datasets which show the effectiveness of our method.
2.