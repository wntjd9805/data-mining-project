Abstract
Neural Architecture Search (NAS) has attracted in-creasingly more attention in recent years because of its capability to design deep neural network automatically.
Among them, differential NAS approaches such as DARTS, have gained popularity for the search efficiency. How-ever, they suffer from two main issues, the weak robust-ness to the performance collapse and the poor general-ization ability of the searched architectures.
To solve these two problems, a simple-but-efficient regularization method, termed as Beta-Decay, is proposed to regularize the
DARTS-based NAS searching process. Specifically, Beta-Decay regularization can impose constraints to keep the value and variance of activated architecture parameters from too large. Furthermore, we provide in-depth theo-retical analysis on how it works and why it works. Ex-perimental results on NAS-Bench-201 show that our pro-posed method can help to stabilize the searching process and makes the searched network more transferable across different datasets.
In addition, our search scheme shows an outstanding property of being less dependent on train-ing time and data. Comprehensive experiments on a va-riety of search spaces and datasets validate the effective-ness of the proposed method. The code is available at https://github.com/Sunshine-Ye/Beta-DARTS. 1.

Introduction
Neural architecture search (NAS) has attracted lots of in-terests for its potential to automatize the process of architec-ture design. Previous reinforcement learning [26, 33] and evolutionary algorithm [25] based methods usually incur massive computation overheads, which hinder their prac-tical applications. To reduce the search cost, a variety of approaches are proposed, including performance estima-tion [17], network morphisms [2] and one-shot architecture
In particular, one-shot methods resort to search [13, 21].
*Part of this work was done when Ye Peng was tele-interned at Baidu.
†Corresponding author (eetchen@fudan.edu.cn)
Figure 1. Schematic illustration about (a) DARTS [21] and our proposed β-DARTS, (b) DARTS- [5]. DARTS- adds an auxiliary skip connection with a decay rate βskip to alleviate the perfor-mance collapse problem. β-DARTS introduces the Beta-Decay regularization to improve both the robustness of the searching pro-cess and the generalization ability of the searched architecture. weight sharing technique, which only needs to train a su-pernet covering all candidate sub-networks once. Based on this weight sharing strategy, differentiable architecture search [21] (namely DARTS, as shown in Fig. 1) relaxes the discrete operation selection problem to learn differentiable architecture parameters, which further improves the search efficiency by alternately optimizing supernet weights and architecture parameters.
Although differentiable method has the advantages of simplicity and computational efficiency, its robustness and architecture generalization challenges still needs to be fully resolved. Firstly, lots of studies have shown that DARTS frequently suffers from performance collapse, that is the searched architecture tends to accumulate parameter-free operations especially for skip connection, leading to the performance degradation [1, 5]. To handle this robustness challenge, lots of instructive works are proposed: directly restricting the number of skip connections [4, 20]; exploit-ing or regularizing relevant indicators such as the norm of
Hessian regarding the architecture parameters [1,3]; chang-ing the searching and/or discretization process [5, 6, 12]; implicitly regularizing the learned architecture parame-ters [1]. However, the explicit regularization of architec-ture parameters optimization receives little attention, as previous works (including above methods) adopt L2 or
weight decay regularization by default on learnable ar-chitecture parameters (i.e., α), without exploring solution along this direction. Secondly, several works have pointed out that the optimal architecture obtained on the specific dataset cannot guarantee its good performance on another dataset [19,22], namely the architecture generalization chal-lenge. To improve the generalization of searched model,
AdaptNAS [19] explicitly minimizes the generalization gap of architectures between domains via the idea of cross do-main, MixSearch [22] searches a generalizable architecture by mixing multiple datasets of different domains and tasks.
However, both methods solve this issue by leveraging larger datasets, while how to use a single dataset to learn a gener-alized architecture remains challenging.
This paper is dedicated to simultaneously solve the above-mentioned two challenges in an efficient way.
In-spired by the widely-used L2 [7] or weight decay regular-ization [18] approaches, we intend to design a customized regularization for DARTS-based methods, which can ex-plicitly regularize the optimizing process of architecture pa-rameters. However, different from the regularization on the learnable architecture parameter set, α (before the non-linear activation of softmax), commonly used in standard
DARTS and its subsequent variants, we propose a novel and generic Beta-Decay regularization, imposing regular-ization on the activated architecture parameters β (after softmax), where βk =
. On one hand, the exp(αk) k′=1 proposed Beta-Decay regularization is very simple to im-plement, achieved with only additional one line of PyTorch code in DARTS (Alg 1). On the other hand, this simple im-plementation is grounded by in-depth theoretical support.
We provide theoretical analysis to show that, Beta-decay regularization not only mitigates unfair competition advan-tage among operations and solve the domination problem of parameter-free operations, but also minimizes the Lipschitz constraint defined by architecture parameters and make sure the generalization ability of searched architecture. In ad-dition, we mathematically and experimentally demonstrate that, commonly-used L2 or weight decay regularization on
α may not be effective or even counterproductive for im-proving robustness and generalization of DARTS. exp(αk′ ) (cid:80)|O|
Algorithm 1 PyTorch Implementation in DARTS 1: LBeta = torch.mean(torch.logsumexp( self.model. arch parameters, dim=-1)) 2: loss = self. val loss(self.model, input valid, target valid)+λLBeta
DARTS with Beta-Decay regularization (β-DARTS) is illustrated in Fig. 1. Extensive experiments on various search spaces (i.e. NAS-Bench-201, DARTS, NAS-Bench-1Shot1) and datasets (i.e. CIFAR-10, CIFAR-100, Ima-geNet) verify the effectiveness of our method. Besides, our search scheme shows the following outstanding properties:
• The search trajectories on NAS-Bench-201 and NAS-Bench-1Shot1 show that, the found architecture has continuously rising performance, and the search pro-cess can reach its optimal point at an early epoch.
• We only need to search once on the proxy dataset (i.e., CIFAR-10), but the searched architecture can ob-tain promising performance on various datasets (i.e.,
CIFAR-10, CIFAR-100 and ImageNet). 2.