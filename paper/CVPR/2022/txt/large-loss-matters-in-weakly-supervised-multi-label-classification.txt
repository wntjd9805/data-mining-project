Abstract
Weakly supervised multi-label classification (WSML) task, which is to learn a multi-label classification using par-tially observed labels per image, is becoming increasingly important due to its huge annotation cost. In this work, we first regard unobserved labels as negative labels, casting the WSML task into noisy multi-label classification. From this point of view, we empirically observe that memoriza-tion effect, which was first discovered in a noisy multi-class setting, also occurs in a multi-label setting. That is, the model first learns the representation of clean labels, and then starts memorizing noisy labels. Based on this finding, we propose novel methods for WSML which reject or cor-rect the large loss samples to prevent model from memo-rizing the noisy label. Without heavy and complex compo-nents, our proposed methods outperform previous state-of-the-art WSML methods on several partial label settings in-cluding Pascal VOC 2012, MS COCO, NUSWIDE, CUB, and OpenImages V3 datasets. Various analysis also show that our methodology actually works well, validating that treating large loss properly matters in a weakly supervised multi-label classification. Our code is available at https:
//github.com/snucml/LargeLossMatters. 1.

Introduction
Multi-label classification aims to find all existing objects or attributes in a single image. It is gaining attention since the real world is made up of a scene with multiple objects in it [28, 35]. Moreover, some of the single-label datasets, also called multi-class datasets, actually have images containing multiple objects [33, 56]. However, the multi-label classi-fication task has some fundamental difficulties in making a dataset because it requires annotators to label all categoriesâ€™ existence/absence for every image. As the number of cate-gories and images in the dataset increase, annotation cost becomes tremendous [19].
*Equal contribution.
Figure 1. Memorization in WSML. When training ResNet-50 model on PASCAL VOC dataset with partial label, we set all unob-served labels as negative. These labels are composed of true nega-tive and false negative. We observe that the model first fits into true negative label (learning), and then fits into false negative (memo-rization).
To alleviate these issues, weakly supervised learning ap-proach in multi-label classification task (WSML) has been taken into consideration [2, 18, 36, 50]. In a WSML setting, labels are given as a form of partial label, which means only a small amount of categories is annotated per image. This setting reflects the recently released large-scale multi-label datasets [12,19] which provide only partial label. Thus, it is becoming increasingly important to develop learning strate-gies with partial labels.
There are two naive approaches to train the model with partial labels. One is to train the model with observed labels only, ignoring the unobserved labels. The other is to assume all unobserved labels are negative and incorporate them into training because majorities of labels are negative in a multi-label setting [32]. As the second one has a limitation that this assumption produces some noise in a label which ham-pers the model learning, previous works [7,9,16,21] mostly follow the first approach and try to explore the cue of un-observed labels using various techniques such as bootstrap-ping or regularization. However, these approaches include
heavy computation or complex optimization pipeline.
We hypothesize that if label noise can be handled prop-erly, the second approach could be a good starting point be-cause it has the advantage of incorporating many true neg-ative labels into model training. Therefore, we try to look at the WSML problem from the perspective of noisy label learning.
Our key observation is about the memorization effect [1] in a noisy label learning literature. It is known that when training a model with a noisy label, the model fits into clean labels first and then starts memorizing noisy labels. Al-though previous work showed the memorization effect only in a noisy multi-class classification scenario, we found for the first time that this same effect also happens in a noisy multi-label classification scenario. As shown in Figure 1, during training, the loss value from the clean label (true neg-ative) decreases from the beginning while the loss from the noisy label (false negative) decreases from the middle.
Based on this finding, we borrow the idea from noisy multi-class literature [13, 17, 23] which selectively trains the model with samples having small loss and adapt this idea into a multi-label scenario. Specifically, by assigning the unknown labels as negative in a WSML setting, label noise appears in the form of false negative. Then we de-velop the three different schemes to prevent false negative labels from being memorized into the multi-label classifi-cation model by rejecting or correcting large loss samples during training.
Our method is light and simple, yet effective. It involves negligible computation overhead and does not require com-plex optimization for model training. Nonetheless, our method surpasses the weakly supervised multi-label classi-fication performance compared to the state-of-the-art meth-ods in Pascal VOC 2012 [10], MS COCO [24], NUSWIDE
[6], CUB [42], and OpenImages V3 [19] datasets. More-over, while some existing methods are only effective in spe-cific partial label setting [7, 9, 16], our method is broadly applicable in both artificially created and real partial label datasets. Finally, we provide some analysis about the rea-son why our methods work well from various perspectives.
To sum up, our contributions are as follows; 1) We empirically show for the first time that the memo-rization effect occurs during noisy multi-label classification. 2) We propose a novel scheme for weakly supervised multi-label classification that explicitly utilizes a learning technique with noisy label. 3) Although light and simple, our proposed method achieves state-of-the-art classification performance on vari-ous partial label datasets. 2.