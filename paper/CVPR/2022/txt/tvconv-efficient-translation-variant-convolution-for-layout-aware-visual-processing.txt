Abstract
As convolution has empowered many smart applications, dynamic convolution further equips it with the ability to adapt to diverse inputs. However, the static and dynamic convolutions are either layout-agnostic or computation-heavy, making it inappropriate for layout-specific applica-tions, e.g., face recognition and medical image segmenta-tion. We observe that these applications naturally exhibit the characteristics of large intra-image (spatial) variance and small cross-image variance. This observation moti-vates our efficient translation variant convolution (TVConv) for layout-aware visual processing. Technically, TVConv is composed of affinity maps and a weight-generating block.
While affinity maps depict pixel-paired relationships grace-fully, the weight-generating block can be explicitly over-parameterized for better training while maintaining effi-cient inference. Although conceptually simple, TVConv sig-nificantly improves the efficiency of the convolution and can be readily plugged into various network architectures.
Extensive experiments on face recognition show that TV-Conv reduces the computational cost by up to 3.1× and im-proves the corresponding throughput by 2.3× while main-taining a high accuracy compared to the depthwise convo-lution. Moreover, for the same computation cost, we boost the mean accuracy by up to 4.21%. We also conduct ex-periments on the optic disc/cup segmentation task and ob-tain better generalization performance, which helps miti-gate the critical data scarcity issue. Code is available at https://github.com/JierunChen/TVConv. 1.

Introduction
With the breakthrough of deep neural networks, we are witnessing a flourishing growth in AI-powered applications and services. While a performance gain usually comes with an overhead of model size and computation, more inter-est has been devoted to the lightweight and computation-efficient network design, which can unleash the potential
Figure 1. Various applications with specific layout: (a) brain MRI analysis, (b) face recognition, (c) industrial product defects de-tection and (d) optic disc/cup segmentation. Each application ex-hibits large intra-image variance and small cross-image variance, as shown in subfigure (e), where the statistic is calculated from the intermediate VGG [35] feature maps by feeding the LFW face verification dataset [15]. for on-device inference for user experience and privacy.
Despite various neural network architectures [11, 25, 32, 39] being proposed for efficiency, their fundamental oper-ators remain more or less the same, e.g., vanilla convolu-tion (conv) and depthwise conv. These operators share one key characteristic, translation equivariance [51], i.e., filters are shared spatially in a sliding window manner. Though it saves parameters for a lightweight model, it deprives the model of being adaptive to different positions in an im-age. Therefore, it has to exhaustively learn many filters for feature matching [49], which results in a massive waste of computation for many tasks with a specific layout.
For layout-specific tasks, as in Fig. 1, the inputs demon-strate a regional statistic with large intra-image (spatial) variance and small cross-image variance. For example, when we use face ID to securely and conveniently unlock our mobile phones, our hair generally appears on the up-per region, vertically followed by our forehead, eyes, nose, mouth, jaw, etc. Similar tasks include, but are not limited to, talking head generation, industrial product defects de-tection, and medical image processing.
Then it produces and caches the weights in memory, al-lowing them to be fetched efficiently for subsequent infer-ences. Through extensive experiments on face recognition,
TVConv is shown to strike a better tradeoff between ac-curacy and computation complexity. Simply replacing the prevalent depthwise conv in various architectures (e.g. Mo-bileNetV2, ShuffleNetV2), TVConv reduces the theoretical complexity by up to 3.1× and correspondingly accelerates the throughput by 2.3× while maintaining a high accuracy.
On the other hand, TVConv boosts the mean accuracy by up to 4.21% under an extreme low complexity constraint.
Moreover, thanks to the layout awareness, TVConv shows a better generalization ability for optic disc/cup segmenta-tion on unseen datasets, which helps to alleviate the data-scarcity dilemma for medical image analysis.
In short, our contributions include: (1) We rethink the existing convolution variants of their inappropriate prop-erties for layout-specific tasks in terms of the intra-image and cross-image variance; (2) By being translation variant and shared across images, an efficient fundamental opera-tor, TVConv, is proposed for layout-aware visual process-ing; (3) Extensive experiments show that TVConv clearly improves the efficiency for face recognition and generalize better for medical image processing. 2.