Abstract
Using only a model that was trained to predict where people look at images, and no additional training data, we can produce a range of powerful editing effects for reducing distraction in images. Given an image and a mask specifying the region to edit, we backpropagate through a state-of-the-art saliency model to parameterize a differentiable editing operator, such that the saliency within the masked region is reduced. We demonstrate several operators, including: a recoloring operator, which learns to apply a color transform that camouflages and blends distractors into their surround-ings; a warping operator, which warps less salient image regions to cover distractors, gradually collapsing objects into themselves and effectively removing them (an effect akin to inpainting); a GAN operator, which uses a seman-tic prior to fully replace image regions with plausible, less salient alternatives. The resulting effects are consistent with cognitive research on the human visual system (e.g., since color mismatch is salient, the recoloring operator learns to harmonize objects’ colors with their surrounding to reduce their saliency). And importantly, all effects are achieved under a zero-shot learning scenario, solely through the guid-ance of the pretrained saliency model, with no supervised data of the effects. We present results on a variety of nat-ural images and conduct a perceptual study to evaluate and validate the changes in viewers’ eye-gaze between the original images and our edited results. Project Webpage: https://deep-saliency-prior.github.io/ 1.

Introduction
Studying and modeling human attention – how and where people look at images – has been widely researched and ex-plored. In the deep learning era, saliency models trained on eye-gaze data are now able to predict human visual attention to high accuracy. However, while the research community has so far focused on developing models for predicting where people look, almost no attention has been given to utilizing the knowledge embedded in such recent, deep saliency mod-els to actually drive and direct editing of images and videos, so as to tweak the attention drawn to different regions in them. A few recent attempts [15, 34] have focused on subtle effects designed to make minimal modifications to the image,
and are therefore limited in their ability to make meaningful changes to visual attention.
In this paper, we leverage deep saliency models to drive dramatic, but still realistic, edits, which can significantly change an observer’s attention to different regions in an image. Such capability can have important applications, for example in photography, where pictures we take often contain objects that distract from the main subject(s) we want to portray, or in video conferencing, where clutter in the background of a room or an office may distract from the main speaker participating in the call.
We ask: using a differentiable saliency model as a guide, what types of editing effects can be achieved? How would those effects affect viewers’ attention in practice when look-ing at the images? Our focus in this paper is on decreasing attention for the purpose of reducing visual distraction, but we also demonstrate some results for increasing attention drawn to image regions in Section 4 (Fig. 6).
To this end, we develop an optimization framework for guiding visual attention in images using a differentiable, predictive saliency model. Our method employs a state-of-the-art deep saliency model [22], pre-trained on large-scale saliency data [24]. Given an input image and a distractor mask, we backpropagate through the saliency model – ef-fectively using it as a prior – to parameterize an editing operator, such that the saliency within the masked region is reduced (Fig. 1). The space of appropriate operators in such a framework is, however, not unbounded. The problem lies in the saliency predictor—as with many deep learning models, the parametric space of saliency predictors is sparse and prone to failure if out-of-distribution samples are pro-duced in unconstrained manner (Figure 2). Using a careful selection of operators and priors, we show that natural and realistic editing can be achieved via gradient descent on a single objective function.
We experiment with several differentiable operators: two standard image editing operations (whose parameters are learned through the saliency model), namely recolorization and image warping (shift); and two learned operators (we do not define the editing operation explicitly), namely a multi-layer convolution filter, and a generative model (GAN).
With those operators, our framework is able to produce a variety of powerful effects, including recoloring, inpainting, camouflage, object editing or insertion, and facial attribute editing (Figure 1). Importantly, all these effects are driven solely by the single, pretrained saliency model, without any additional supervision or training. Note that our goal is not to compete with dedicated methods for producing each effect, but rather to demonstrate how multiple editing operations can be guided by the knowledge embedded within deep saliency models, all within a single framework.
We demonstrate our approach on a variety of natural im-ages, and conduct a perceptual study to validate the changes in real human eye-gaze between the original images and our edited results. Our experiments and user studies show that the produced image edits: a) effectively reduce the visual attention drawn to the specified regions, b) maintain well the
Figure 2. An adversarial example of saliency models. Given an input image (a) with a predicted saliency (b), additive noise is applied to the image and optimized to reduce the saliency of image regions that were previously salient. However, the output (c) still exhibits salient regions which are interpreted as non-salient by the model (d). overall realism of the images, and c) are significantly more preferred by users over more subtle saliency-driven editing effects that were proposed before. 2.