Abstract
PSNR: 49.43dB
Most camera images are rendered and saved in the stan-dard RGB (sRGB) format by the camera’s hardware. Due to the in- camera photo- finishing routines, nonlinear sRGB images are undesirable for computer vision tasks that as-sume a direct relationship between pixel values and scene radiance. For such applications, linear raw- RGB sensor images are preferred. Saving images in their raw- RGB for-mat is still uncommon due to the large storage requirement and lack of support by many imaging applications. Several
“raw reconstruction” methods have been proposed that uti-lize specialized metadata sampled from the raw- RGB image at capture time and embedded in the sRGB image. This metadata is used to parameterize a mapping function to de-render the sRGB image back to its original raw- RGB format when needed. Existing raw reconstruction methods rely on simple sampling strategies and global mapping to perform the de- rendering. This paper shows how to improve the de-rendering results by jointly learning sampling and recon-struction. Our experiments show that our learned sampling can adapt to the image content to produce better raw re-constructions than existing methods. We also describe an online fine- tuning strategy for the reconstruction network to improve results further. 1.

Introduction
For many low-level computer vision tasks, it is desir-able to have access to the camera’s raw-RGB sensor im-age whose pixel values have a linear relationship with scene radiance [9, 29, 39]. In addition, photo-editing operations, such as white-balance adjustment or color manipulation, are more accurate when applied on raw-RGB images [19].
However, most images are still saved in the standard RGB (sRGB) format. sRGB images are raw-RGB images that have been rendered by the camera’s image signal processor (ISP). The nonlinear photo-finishing routines applied by the
ISP break the well-behaved relationship to scene radiance
*Work done while an intern at the Samsung AI Center – Toronto. sRGB (input)
Raw-RGB (GT)
SAM [31]
PSNR: 57.45dB
PSNR: 57.89dB 1% 1%
Sampling mask
Ours
Ours + fine-tuning
Figure 1. Overview of our paper. We address the problem of de-rendering an sRGB image to a raw-RGB image using a metadata saved along with the sRGB image. At capture time, we sample the raw-RGB values at the locations in a sampling mask, and save them as a metadata. When the raw-RGB image is needed, we reconstruct the full raw-RGB image from the sRGB image with the metadata. We propose an end-to-end deep learning framework to achieve it. With our approach, the reconstruction can be further improved by online fine-tuning. present in the original raw-RGB image. A solution to re-cover the raw-RGB values is to “de-render” the sRGB im-age back to its raw-RGB format [4]. Among the various methods to de-render an image, the most accurate are those that collect samples from the original raw-RGB image at capture time and embed these samples inside the sRGB im-age as specialized metadata [28, 31]. These prior methods rely on uniform sampling of the raw-RGB image and simple mapping functions to de-render from sRGB to raw-RGB.
Contribution. We propose a deep-learning framework to address the sRGB de-rendering task using metadata sam-pled from the raw-RGB at capture time. In particular, we
demonstrate how sampling and reconstruction can both be learned in an end-to-end framework. Sampling is performed in a content-aware manner based on superpixel-based max-pooling and subsequently used by the reconstruction net-work. In addition, the reconstruction network incorporates an online fine-tuning approach to improve the performance at inference time. An example is shown in Fig. 1. We demonstrate the effectiveness of our method on the raw re-construction task using 1.5% of raw-RGB pixels saved in the metadata and show we can achieve state-of-the-art per-formance. Additionally, we show the applicability of our method to other image recovery tasks by applying our sam-pling/reconstruction framework to bit-depth recovery. 2.