Abstract
Previous portrait image generation methods roughly fall into two categories: 2D GANs and 3D-aware GANs. 2D
GANs can generate high fidelity portraits but with low view consistency. 3D-aware GAN methods can maintain view consistency but their generated images are not locally ed-itable. To overcome these limitations, we propose FENeRF, a 3D-aware generator that can produce view-consistent and locally-editable portrait images. Our method uses two decoupled latent codes to generate corresponding fa-cial semantics and texture in a spatial-aligned 3D vol-ume with shared geometry. Benefiting from such under-lying 3D representation, FENeRF can jointly render the boundary-aligned image and semantic mask and use the semantic mask to edit the 3D volume via GAN inversion.
We further show such 3D representation can be learned from widely available monocular image and semantic mask pairs. Moreover, we reveal that joint learning semantics and texture helps to generate finer geometry. Our experi-ments demonstrate that FENeRF outperforms state-of-the-art methods in various face editing tasks. Code is available at https://github.com/MrTornado24/FENeRF. 1.

Introduction
Photo-realistic image synthesis via Generative Adversar-ial Networks is an important problem in computer vision and graphics. Specifically, synthesizing high-fidelity and editable portrait images has gained considerable attention in recent years. Two main classes of methods have been proposed: 2D GAN image generation and 3D-aware image synthesis techniques.
Despite their great success of synthesizing highly-realistic and even locally-editable images, 2D GAN meth-ods all ignore the projection or rendering process of the underlying 3D scene, which is essential for view con-sistency. Consequently, they produce inevitable artifacts
*Work done during an internship at Tencent AI Lab.
†Corresponding Author.
Figure 1. View-consistent portrait editing. Given the proposed
FENeRF generator, we invert a given reference (top left) into shape and texture latent spaces to get the free-view portrait (first row). We can modify the rendered semantic masks and then lever-age GAN inversion again to edit the free-view portraits (second row). As of last, we can replace the optimized texture code with the one from another image (bottom left) to perform the style transfer (bottom row) as well.
In when changing the viewpoint of generated portraits. order to overcome this issue, the Neural Radiance Fields (NeRF) [35] have been explored to develop 3D-aware im-age synthesis techniques. Some of these methods [3, 44] adopt vanilla NeRF generator to synthesize free-view por-traits that are not editable, and the results could be blurry.
Niemeyer et al. [37] employ volumetric rendering tech-niques to first produce view-consistency 2D feature maps, and then use an additional 2D decoder to obtain the final highly-realistic images. Nevertheless, such method suffers from additional view-dependent artifacts introduced by 2D convolutions and the mirror symmetry problem. To this end,
CIPS-3D [53] replaces the 2D convolutions with implicit neural representation (INR) network. Unfortunately, all ex-isting 3D-aware GANs do not support the interactive local editing on the generated free-view portraits.
In this paper, we propose a generator that can produce strictly view-consistent portraits, while supports interactive local editing. We adopt the noise-to-volume scheme. The generator takes as input the decoupled shape and texture 1
latent code, and generates a 3D volume where the facial se-mantics and texture are spatially-aligned via the shared ge-ometry. As a learnable 3D positional feature embedding is exploited while generating the texture volume, more details are preserved in the synthesized portraits.
Directly learning this 3D volume representation is chal-lenging due to the absent of suitable, large-scale 3D training data. A possible solution is to use multi-view images [4].
Nonetheless, the inadequate training data harms the rep-resentation ability of the 3D semantic volume. To over-come this issue, we make the use of monocular images with paired semantic masks, which are vastly available. Specif-ically, color and semantic discriminators are employed to supervise the training of the NeRF generator. The color discriminator focuses on image details hence improves the image fidelity. The semantic discriminator takes as input a pair of image and semantic map to enforce the alignment of corresponding content in the 3D volume. Thanks to the spatial-aligned 3D representation, we can use the semantic map to locally and flexibly edit the 3D volume via GAN inversion. In addition, an insight here is that learning the semantic and texture representations simultaneously helps to generate more accurate 3D geometry.
To illustrate the effectiveness of the proposed method, we perform the evaluation on two widely-used public datasets: CelebAMask-HQ and FFHQ. As shown in the experiments, the FENeRF generator outperforms state-of-the-art methods in several aspects. In addition, it supports various downstream tasks. To summarize, our main contri-butions are as following:
• We present the first portrait image generator that is lo-cally editable and strictly view-consistent, benefiting from the 3D representation in which the semantics, ge-ometry and texture are spatially-aligned.
• We train the generator with paired monocular images and semantic maps without the requirement of multi-view or 3D data. This ensures data diversity and en-hances the representation ability of the generator.
• In experiments, we reveal that joint learning the se-mantic and texture volume can help to generate the finer 3D geometry. 2.