Abstract
Referring video object segmentation (RVOS) is a challeng-ing language-guided video grounding task, which requires comprehensively understanding the semantic information of both video content and language queries for object prediction. However, existing methods adopt multi-modal fusion at a frame-based spatial granularity. The limitation of visual representation is prone to causing vision-language mismatching and producing poor segmentation results. To address this, we propose a novel multi-level representation learning approach, which explores the inherent structure of the video content to provide a set of discriminative visual embedding, enabling more effective vision-language semantic alignment. Speciﬁcally, we embed different visual cues in terms of visual granularity, including multi-frame long-temporal information at video level, intra-frame spa-tial semantics at frame level, and enhanced object-aware feature prior at object level. With the powerful multi-level visual embedding and carefully-designed dynamic align-ment, our model can generate a robust representation for accurate video object segmentation. Extensive experiments on Refer-DAVIS17 and Refer-YouTube-VOS demonstrate that our model achieves superior performance both in segmentation accuracy and inference speed. 1.

Introduction
Given a natural language expression, referring video ob-ject segmentation (RVOS) aims to predict the most relevant visual target from a video. It has wide applications, includ-ing video editing, virtual reality and human-robot interac-tion [49]. Different from the regular unsupervised or semi-supervised video object segmentation (VOS) [12, 21, 33, 53, 54], which localizes objects with salience or annotations of key frames, RVOS requires cross-modal understanding be-∗Equal contribution. †Corresponding author: Jianbing Shen. ‡Work was done while Dongming Wu was an intern at IIAI.
Figure 1. Visual comparison among the different-level model-ings. The simple frame-level modeling has difﬁculty in recogniz-ing (b) the moving object or (c) the occluded and small object. In contrast, our multi-level modeling offers a joint way to leverage the long-temporal and spatial salient cues for cross-modal align-ment, thus providing more accurate results (a) (d). tween the language query and video content.
As a human recognizes a referent object with the guid-ance of language, it is natural to rely on three steps: 1) ob-serve its appearance (i.e., frame-based), 2) check its move-ment based on multiple frames (i.e., video-based), 3) shift more attention to the occluded or small objects (i.e., object-based). Most current approaches [1, 25, 43] simply leverage successful referring image comprehension methods to the cross-model understanding. They either use referring image grounding [24, 31, 58, 60] to generate target object bound-ing boxes as proposals, or utilize referring image segmen-tation directly [6, 10, 18, 22, 27, 56]. However, these solu-tions build on the simple frame-level visual representation to perform frame-sentence interaction. These frame-level modeling methods suffer from two limitations compared to the human recognition system: ignoring long-temporal in-formation and lacking attention to salient spatial objects.
The limitation of visual representations causes the mis-alignment between two modalities, further producing inac-curate segmentation results. For example, as illustrated in
Fig. 1, given an input video and its corresponding descrip-tion, “a lion is walking towards right side”, RVOS aims to segment the moving lion from the video. However, as there are multiple lions in the video, the frame-level modeling cannot recognize the correct one by employing only spa-tial appearance information as shown in Fig. 1(b). Since the referent object has a temporally moving status, it re-quires incorporating long-temporal information from mul-tiple frames to identify the action. In addition, another ex-pression, “a lion lying on a high rock” refers to an occluded and small-size lion. However, the frame-level modeling fo-cuses only on the global semantics concerning each frame, and ignores these important and representative visual re-gions. It will lead to the referent object being missing, as shown in Fig. 1(c). To ease this difﬁculty, it is also neces-sary to capture the salient spatial objects from each frame as candidates to facilitate cross-modal understanding.
In this paper, we propose a novel multi-level learning framework for addressing RVOS. The model ﬁrst presents a
ﬁne-grained analysis of video content for multi-granularity visual embedding:
At the video granularity, we propose to model long-• temporal dependencies of the entire video using a cross-frame pixel-wise calculator, which makes the feature rep-resentations to capture the object movement and dynamic scenes information.
At the frame granularity, we encourage the frame repre-• sentation to describe global content within a whole image, by learning to aggregate intra-frame information following the self-attention mechanism.
At the object granularity, we leverage object-aware in-• formation generated from an object detector to enhance the foreground and background discriminability, beneﬁting from addressing the cases of occlusion and small object.
Once we obtain the multi-level visual embedding, we propose Dynamic Semantic Alignment (DSA) to interact them with the linguistic features.
In particular, to effec-tively capture the granularity-speciﬁc information, we ﬁrst separately incorporate global linguistic semantics accord-ing to the different visual cues. The generated vision-conditioned linguistic features are combined with the corre-sponding visual embedding to provide a granularity-speciﬁc representation for the referent object. Finally, we integrate the multi-level target-aware features and boundary infor-mation to guide the mask prediction of all frames using a
Boundary-Aware Segmentation (BAS).
Overall, our contributions are summarized as three-fold:
First, we propose a new framework for RVOS based on
It precludes the lim-multi-level representation learning. itation of single frame-level visual modeling by a more structural video representation, promoting accurate vision-language semantic alignment. Second, we introduce a
Dynamic Semantic Alignment (DSA), which dynamically learns and matches linguistic semantics with the different-granularity visual representation more compactly and ef-fectively. Third, our approach achieves compelling per-formance on two challenging benchmarks, including Refer-DAVIS17 [25] and Refer-YouTube-VOS [43]. Notably, we obtain a signiﬁcant improvement of 6.6% than the best frame-grained method in terms of on Refer-DAVIS17.
Meanwhile, it achieves a high inference speed at 53.2 FPS.
J 2.