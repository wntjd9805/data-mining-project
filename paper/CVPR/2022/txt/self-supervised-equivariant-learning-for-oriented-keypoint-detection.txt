Abstract
Detecting robust keypoints from an image is an integral part of many computer vision problems, and the charac-teristic orientation and scale of keypoints play an impor-tant role for keypoint description and matching. Existing learning-based methods for keypoint detection rely on stan-dard translation-equivariant CNNs but often fail to detect reliable keypoints against geometric variations. To learn to detect robust oriented keypoints, we introduce a self-supervised learning framework using rotation-equivariant
CNNs. We propose a dense orientation alignment loss by an image pair generated by synthetic transformations for training a histogram-based orientation map. Our method outperforms the previous methods on an image matching benchmark and a camera pose estimation benchmark. 1.

Introduction
Detecting robust keypoints is an integral part of many computer vision tasks, such as image matching [21], visual localization [28, 54, 55], SLAM [13, 14, 39], and 3D recon-struction [1, 19, 57, 76]. The robust keypoints, in princi-ple, are consistently localizable, being invariant to photo-metric/geometric variations of an image induced by view-point/illumination changes, and a keypoint is typically as-signed with its characteristic orientation/scale as a geomet-ric feature, which plays an important role for keypoint de-scription [14, 15, 27, 37, 41, 45, 51, 62, 63, 70] or match-ing [6,53,71,73], as shown in Fig. 1. As rotation frequently occurs for patterns of interests in real-world images, the keypoints and their geometric features are required to be consistent w.r.t rotation of the image in particular.
The early methods have detected keypoints with their charateristic orientation/scale using a hand-crafted filter on a shallow gradient-based feature map. For example,
SIFT [27] detects the keypoints by finding local extrema in difference-of-Gaussian (DoG) features on a scale space and obtains a dominant orientation from gradient histograms.
While such a technique has proven effective for shallow
Figure 1. Visualization of the predicted matches to compare the existing keypoint detector Key.Net [3] (left) with our oriented key-point detector (right). We draw the correct matches (green) and the incorrect matches (red) using the ground-truth homography. We extract 300 keypoints and use HardNet [37] descriptor for match-ing. The arrows of the keypoints in the right denote the estimated orientations which are used for filtering outliers. gradient-based feature maps, it cannot be applied to deep feature maps from standard networks, where rotation or scaling induces unpredictable variations of features. Recent methods [3, 41, 58, 70], thus, rely on learning from data.
They typically train a convolutional neural network (CNN) for keypoint detection and/or description by regressing ori-entation and scale. Some [3,41] adopt self-supervised learn-ing through synthetic transformation, while others [58, 70] train the networks through strong supervision by homog-raphy or SfM. All these approaches, however, often fail to detect reliable keypoints against geometric variations; they learn invariance or equivariance by relying on training with data augmentation, which does not provide a sufficient level for keypoint detection.
In this work, we propose a self-supervised equivariant learning method for oriented keypoint detection. Recent studies [10,11,29,68,69,75] introduce different equivariant neural networks that embed an explicit structure for equiv-ariant learning by design. The group-equivariant CNNs
on a cyclic group have the advantages of explicitly encod-ing the enriched orientation information and reducing the number of model parameters through weight sharing com-pared to the conventional CNNs. We propose an orienta-tion alignment loss to estimate a characteristic orientation to the keypoint using a histogram-based representation. The histogram-based representation provides richer information than the regression methods [41, 70, 72] by predicting mul-tiple candidates for the orientations. To train the invariant keypoint detector, we utilize a window-based loss [3] to sat-isfy the geometric consistency with anchor points diverse across the image. We generate the synthetic image pairs by a random in-plane rotation to create diverse examples and reduce the annotation cost. In addition, we generate a scale-space representation in the networks and use multi-scale inference to consider scale-invariance approximately.
We evaluate the rotation-invariant keypoint detection and the rotation-equivariant orientation estimation compared under synthetic rotations with the existing models [27, 41, 51]. We validate the effectiveness of our keypoint detec-tor compared to the handcrafted methods [27, 51] and the learning-based methods [3, 14, 15, 45] in an image match-ing benchmark [2] using a repeatability score and match-ing accuracy. The estimated orientations improve the image matching accuracy with an outlier filtering in HPatches [2].
Furthermore, we show the transferability to a more complex task by evaluating 6 DoF pose estimation in IMC2021 [21].
We demonstrate ablation experiments and visualizations to verify the effectiveness of our model.
The contributions of our paper are three-fold:
• We propose a self-supervised framework for learning to detect rotation-invariant keypoints using a rotation-equivariant representation.
• We propose a dense orientation alignment loss by aligning a pair of histogram tensors to train the charac-teristic orientations.
• We demonstrate the effectiveness of our oriented key-point detector with extensive evaluations compared to existing keypoint detection methods on standard image matching benchmarks. 2.