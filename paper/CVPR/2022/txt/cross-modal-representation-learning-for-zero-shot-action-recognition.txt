Abstract
We present a cross-modal Transformer-based frame-work, which jointly encodes video data and text labels for zero-shot action recognition (ZSAR). Our model employs a conceptually new pipeline by which visual representations are learned in conjunction with visual-semantic associa-tions in an end-to-end manner. The model design provides a natural mechanism for visual and semantic representations to be learned in a shared knowledge space, whereby it en-courages the learned visual embedding to be discriminative and more semantically consistent. In zero-shot inference, we devise a simple semantic transfer scheme that embeds semantic relatedness information between seen and unseen classes to composite unseen visual prototypes. Accordingly, the discriminative features in the visual structure could be preserved and exploited to alleviate the typical zero-shot issues of information loss, semantic gap, and the hubness problem. Under a rigorous zero-shot setting of not pre-training on additional datasets, the experiment results show our model considerably improves upon the state of the arts in ZSAR, reaching encouraging top-1 accuracy on UCF101,
HMDB51, and ActivityNet benchmark datasets. Code will be made available.1 1.

Introduction
Action recognition with supervised training is highly successful [9, 22, 36, 58, 59, 65], e.g., AssemblyNet++ [52] and X3D [17]. In comparison, zero-shot action recognition (ZSAR) generally lags behind because it requires a model to make meaningful inferences about unseen concepts, given only the data provided from seen training concepts and ad-ditional high-level semantic label information [55]. Ad-dressing the general zero-shot challenges, e.g., distribution shift and semantic gap, recent ZSAR methods mainly ex-ploit visual features extracted from off-the-shelf pretrained action recognition models and focus on studying a more ro-bust visual-to-semantic mapping or learning a joint embed-1 https://github.com/microsoft/ResT
Figure 1. ResT is a cross-modal transformer network, which learns visual representations along with visual-semantic associa-tions for ZSAR. In ResT, visual tokens attend to visual tokens (modality-specific attention); Word tokens attend to visual and text tokens on the left (cross-modal attention). ding space on which to project visual and semantic features.
However, there are limitations in this typical framework.
First, visual features are usually acquired by pretrained action recognition models and remain unchanged during training (e.g., C3D [58] is employed in [4, 24, 42, 42, 62],
I3D [9] is adopted in [21, 37, 45, 51]). Hence, they may not contain enough information for learning a fair repre-sentation [34]. Besides, when directly using pretrained ac-tion recognition models to extract visual features, the intrin-sic zero-shot setting may not be preserved. The pretrained model acquires the knowledge of classes that should not be seen during training [51]. For example, I3D was trained on the ImageNet [13] and Kinetics [28] datasets. C3D was trained on the I380K and Sports-1M datasets [27]. Simi-lar to Kinetics, between Sports-1M and UCF101 [53], they share 23 identical classes [16].
Second, action classes are usually manually labeled with limited descriptions of many complex actions, causing the semantic information is often imprecise or incomplete. For example, many action classes are only labeled by a noun (e.g., ”uneven bars,” ”hula hoop”), or a single verb (e.g.,
”pour,” ”dive”). In contrast, videos, upon which the words are placed, are true visual reflections of various classes of actions. Visual observations are unstructured and complex.
It follows, there is richer and more discriminative informa-tion in the visual feature space. Despite the visual and se-mantic spaces being connected, the visual discrimination af-ter mapping or projection of the two spaces often shrinks
to a certain extent [46, 70]. This affects the knowledge transferability from seen classes to unseen classes. Be-sides, as the projection is performed in high-dimensional embedding spaces, there inevitably exists the hubness prob-lem [35, 49, 69], whereby some class prototypes appear to be the nearest neighbors of many irrelevant test instances.
The question is, how can the visual and semantic spaces be bridged, while at the same time maintaining the visual discrimination for an effective knowledge transfer?
In this paper, we study an end-to-end trainable cross-modal framework, named ResT (ResNet-Transformer).
ResT is able to associate both visual and semantic spaces, while preserving the descriptive and discriminative infor-mation implied in the visual embedding space. Considering the essence of ZSAR, we frame the problem in two ways: (1) Instead of using pretrained feature extractors, we attach a vanilla non-pretrained ResNet module to the Transformer to extract visual features, which is to ensure that no prior knowledge of unseen classes is acquired during training. (2)
Contrary to normal practice, we integrate the learning of vi-sual representations and visual-semantic associations into the same unified architecture. This model design provides a natural mechanism for visual and semantic representations to be learned in a shared knowledge space, which can bridge the semantic gap and encourage the learned visual embed-ding to be discriminative and more semantically consistent.
Considering the disconnection among source and target domains, which makes great difficulty in inferring their re-lationships via a coarse one-on-one nearest neighbor match-ing, we develop a simple semantic transfer scheme for zero-shot inference. The idea is to leverage the visual-semantic associations in ResT and to composite a visual prototype for each unseen class by embedding a combination of relevant information. Specifically, in light of the observations that human activities (e.g., play basketball) are composed of a series of simple actions (e.g., run, pass, jump and shoot) or are related to a set of partial elements of other com-plex activities, we posit that actions are implicitly compo-sitional [1, 23, 38]. The semantic transfer scheme is thus formulated as a subgraph selection, based on the semantic relatedness distances of seen and unseen labels, to compos-ite the visual prototype of each unseen class in visual space for the ZSAR task. With the transfer scheme, because the visual representations are not projected onto other spaces, the hubness problem is alleviated, and the visual distinction is preserved. Accordingly, the framework achieves good expandability in various unseen domains.
The present work follows a stricter, but more realistic zero-shot setting proposed by [5], where the set of training classes that overlap with test data are removed by a similar-ity threshold. The model is trained from scratch with ran-dom initial weights on one dataset and tested on three dis-jointed target datasets. No pre-training on auxiliary datasets is performed to ensure no prior knowledge of unseen classes acquired during training. Our approach employs the induc-tive configuration [4, 5, 24, 37, 69], where the test data is entirely unknown during training.
The main contributions of this paper are:
• Our approach, based on the described cross-modal framework, bridges the visual and semantic spaces while still maintaining the visual discrimination for an effective knowledge transfer. With a single trained model on the Kinetics dataset, our framework estab-lishes new state-of-the-art ZSAR results on UCF101
[53], HMDB51 [32], and ActivityNet [7] benchmarks.
• We develop a simple yet effective semantic transfer scheme to composite unseen visual prototypes, with which ZSAR could be realized in the visual space to alleviate information loss and the hubness problem.
• Our approach has three nice properties: first, it is end-to-end trainable; second, it achieves a good accu-racy–complexity trade-off; third, it offers the flexibil-ity of utilizing different feature encoder backbones and is capable of cooperating with concurrent pretrained models for generalization. 2.