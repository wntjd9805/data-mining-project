Abstract
One major challenge for semantic segmentation in real-world scenarios is only limited pixel-level labels available due to high expense of human labor though a vast volume of video data is provided. Existing semi-supervised meth-ods attempt to exploit unlabeled data in model training, but they just regard video as a set of independent images. To better explore semi-supervised segmentation problem with video data, we formulate a semi-supervised video semantic segmentation task in this paper. For this task, we observe that the overﬁtting is surprisingly severe between labeled and unlabeled frames within a training video although they are very similar in style and contents. This is called inner-video overﬁtting, and it would actually lead to inferior per-formance. To tackle this issue, we propose a novel inter-frame feature reconstruction (IFR) technique to leverage the ground-truth labels to supervise the model training on
IFR is essentially to utilize the inter-unlabeled frames. nal relevance of different frames within a video. During training, IFR would enforce the feature distributions be-tween labeled and unlabeled frames to be narrowed. Con-sequently, the inner-video overﬁtting issue can be effectively alleviated. We conduct extensive experiments on Cityscapes and CamVid, and the results demonstrate the superiority of our proposed method to previous state-of-the-art meth-ods. The code is available at https://github.com/ jfzhuang/IFR. 1.

Introduction
As a fundamental task in computer vision, image seman-tic segmentation has beneﬁted numerous downstream appli-cations. However, the training data for semantic segmen-tation requires pixel-level labeling, which is very expen-sive and time-consuming. Recently, semi-supervised im-age segmentation (SSIS) is proposed to train models with a limited number of labeled images and additional unla-∗Corresponding author
···
···
Sampled Labeled Frames
Sampled Unlabeled Frames
Training Data for SSIS
Remaining Unlabeled Frames
Expanded Training Data
Figure 1. Illustration of expanded training data. As commonly adopted in SSIS, only one frame from each video is sampled and used as the training data. These frames are divided into labeled and unlabeled ones. To fully utilize video data, we propose to supple-ment the remaining video frames into unlabeled data for training. beled images. Besides regular supervised learning on la-beled images, SSIS methods usually construct extra super-vision signals for unlabeled images, e.g., consistency con-straint [17, 23] and pseudo label [4, 32].
It is shown that they can bring considerable performance improvement on the challenging datasets, e.g., PASCAL VOC [8].
In real-world scenarios, we can usually collect the video data conveniently and economically, but only a few videos would be annotated with a single frames due to the high expense of human labor. For example, Cityscapes [6] as a representative dataset contains 2975 videos in the training subset, and only the 20th frame in each video is annotated.
That is, there are actually many unlabeled frames that can be utilized for model training. However, the existing SSIS methods [4, 17, 23] do not fully exploit video data, in which only one frame is sampled from each video for model train-ing and a large amount of unlabeled frames are ignored. To better explore unlabeled videos, we propose to expand the training data setting of SSIS by leveraging remaining video frames, as shown in Figure 1.
A nature way to utilize extra unlabeled frames is to per-form SSIS with adding the remaining frames into unlabeled training data. Here we particularly implement two SOTA
SSIS methods, i.e., CAC [17] and CPS [4]. Table 1 gives the
Method
Sampled Frames
All Frames
Baseline
+ CAC
Baseline
+ CPS 66.00 69.70 70.32 74.39
-69.80 (+0.10)
-74.66 (+0.27)
Table 1. Performance of SOTA methods with all video frames.
We implement CAC [17] and CPS [4] by adding remaining video frames into unlabeled training data. However, no obvious im-provement is gained. Here, Cityscapes with 1/8 labeled data is used, and Deeplabv3+ with the ResNet50 backbone is adopted as segmentation model. The baseline performances are different due to adopting different training settings.
ID 20 15 10 5 20
Accel 81.5 69.8↓11.7 65.3↓16.2 60.6↓20.9 49.8↓31.7
+ CAC 80.1 69.9↓10.2 65.5↓14.6 61.9↓18.2 51.1↓29.0
+ CPS 79.7 68.1↓11.6 65.1↓14.6 61.8↓17.9 51.3↓28.4
T
V
Table 2. Performance on Cityscapes-VPS. T and V represent the training and validation subsets, respectively. ID indicates the frame id. The model is trained on the 20th frame in the train-ing subset. ↓ represents the accuracy gap comparing to that on the training frames. Performance gap exists not only between two subsets but also within each training video. results on Cityscapes. It can be seen that there is no obvious performance improvement. The primary reason may be the homogenization of video data, i.e., the contents of different frames within a video are often similar. Given one frame from each video, therefore, the existing semi-supervised methods cannot capture much more information from the remaining frames using a simple extension on training data.
Evidently, how to effectively boost the segmentation perfor-mance with unlabeled video data is challenging.
Next we ﬁrst formulate the semi-supervised video seg-mentation (SSVS) task addressed in this work. There are two main differences comparing to the SSIS task. The ﬁrst is the training data. During training, only one frame sam-pled from each video is used in SSIS, while all video frames are accessible in SSVS, as shown in Figure 1. The second is the baseline model. SSIS focuses on improving image segmentation models, e.g., PSPNet [28], while SSVS con-centrates on video segmentation models, e.g., Accel [15].
Since the video models are usually designed for exploiting video characteristics, e.g., feature propagation by utilizing the temporal consistency, we particularly consider the learn-ing method by further exploring video data in this paper.
The overﬁtting is a key challenge in SSIS, as demon-strated in existing works [17, 23]. Here we investigate the
Unlabeled Frames
Inner-Video Overfitting
Labeled Frames
Training Video
Overfitting
Test Video
Figure 2. Illustration of inner-video overﬁtting. The commonly concerned overﬁtting issue is the performance gap between train-ing and test videos. However, we ﬁnd that overﬁtting also exists between labeled and unlabeled frames within each training video, which is called inner-video overﬁtting. overﬁtting issue in SSVS with a popular VSS model, i.e.,
Accel [15]. In particular, we conduct a study experiment on the Cityscapes-VPS dataset [16]. The dataset contains 500 videos, where each video contains 30 frames and every 5 frame is annotated. We randomly select 100 videos for training and the remaining are for validation. To be speciﬁc, only the annotation on the 20th frame is used for each video during training. We evaluate the model on both the training and validation subsets, as shown in Table 2. From the re-sults, we have two observations. First, the performance gap exists between the training and validation frames, which is the commonly concerned overﬁtting issue. Second, the gap also occurs between the training and other frames within the training videos, though they have no signiﬁcant visual dif-ference as shown in Figure 2. We called this phenomenon as inner-video overﬁtting.
In this work, we argue that the inner-video overﬁtting is mainly caused by the lack of accurate supervision sig-nals for unlabeled frames, which results in inconsistent per-formance. Speciﬁcally, the model trained on the labeled frames is supervised by the ground-truth labels, which can provide accurate semantic signals. But the model trained on unlabeled frames is either not considered as in Accel or supervised by some constructed signals. Actually, these sig-nals can only provide an indirect constraint like consistency in CAC or noisy semantic supervision like pseudo labels in
CPS, which cannot effectively supervise model training on unlabeled frames.
Then a natural question arises: can we use ground-truth
labels to train model on the unlabeled frames? Here we particularly utilize the internal relevance of different frames within a video, as they share similar semantic contents and styles. Following this idea, we propose a novel inter-frame feature reconstruction (IFR) method. The main idea is to re-construct features of the labeled frame fL using the informa-tion from features of the unlabeled frame fU . After that, we apply supervised learning on the reconstructed feature fR with the ground-truth label, which actually imposes super-vision on fU implicitly. During training, fR would become similar to fL as they are supervised by the same objective.
As a result, the feature distribution discrepancy between fL and fU would be narrowed, and further the inner-video over-ﬁtting would be alleviated. In this way, we can also improve the generalization capacity of the model on unseen scenes, i.e., testing data. In one word, IFR has two main advantages comparing to other methods. First, it uses the ground-truth labels to train model on unlabeled data rather than the gen-erated pseudo labels [4] or consistency constraint [17, 23], which can provide much more accurate semantic supervi-sion. Second, it collaborates the training on labeled and unlabeled data through the same objective, which are pro-cessed separately in the existing methods.
We experimentally evaluate the proposed method on the
Cityscapes and CamVid datasets. The results validate the effectiveness of our IFR in alleviating the inner-video over-ﬁtting issue, and it can bring a signiﬁcant performance improvement for mainstream video semantic segmentation methods. The contributions of this work are summarized as follows.
• We formulate the semi-supervised video semantic seg-mentation task and discover the inner-video overﬁtting issue is one of the main challenges damaging the per-formance of SSVS.
• We propose a novel inter-frame feature reconstruction method to alleviate the inner-video overﬁtting and fur-ther boost the performance. IFR essentially utilizes the internal relevance of different frames within a video.
• We experimentally evaluate the effectiveness of our proposed method, and the results on Cityscapes and
CamVid demonstrate the superiority of our method to previous state-of-the-art methods. 2.