Abstract
Multi-frame depth estimation improves over single-frame approaches by also leveraging geometric relation-ships between images via feature matching, in addition to learning appearance-based features. In this paper we re-visit feature matching for self-supervised monocular depth estimation, and propose a novel transformer architecture for cost volume generation. We use depth-discretized epipo-lar sampling to select matching candidates, and reﬁne pre-dictions through a series of self- and cross-attention lay-ers. These layers sharpen the matching probability between pixel features, improving over standard similarity metrics prone to ambiguities and local minima. The reﬁned cost vol-ume is decoded into depth estimates, and the whole pipeline is trained end-to-end from videos using only a photometric objective. Experiments on the KITTI and DDAD datasets show that our DepthFormer architecture establishes a new state of the art in self-supervised monocular depth estima-tion, and is even competitive with highly specialized su-pervised single-frame architectures. We also show that our learned cross-attention network yields representations transferable across datasets, increasing the effectiveness of pre-training strategies. Project page: https://sites. google.com/tri.global/depthformer. 1.

Introduction ego-motion estimation [33, 34, 58], keypoint extraction [58, 59], calibration [17, 66], optical ﬂow [30, 51, 77], and scene
ﬂow [24, 25]. Within these tasks, self-supervision enables learning without explicit ground-truth [15, 82], by using view synthesis losses obtained via the warping of informa-tion from one image onto another, obtained from multiple cameras or a single moving camera. While more challeng-ing from a training perspective [16, 18, 72], self-supervised methods can leverage arbitrarily large amounts of unlabeled data, which has been shown to achieve performance com-parable to supervised methods [18, 72], while enabling new applications such as test-time reﬁnement [17, 56, 72] and unsupervised domain adaptation [20].
Single-frame self-supervised methods use multi-view in-formation only at training time, as part of the loss calcula-tion [15, 16, 18, 56, 82]. In contrast, multi-frame methods use multi-view information at inference time, traditionally by building cost volumes [32, 57, 72, 74] or correlation lay-ers [24, 61, 62]. These methods learn geometric features in addition to appearance-based ones, which leads to better performance relative to single-frame methods [61, 72, 74].
However, multi-frame calculation relies heavily on feature matching to establish correspondences between frames, us-ing only image information. Because of that, correspon-dences will be noisy and often inaccurate [16, 18, 74] due to ambiguities and local minima caused by lack of texture, rep-etitions, luminosity changes, dynamic objects, and so forth.
Feature matching is a fundamental component of
Structure-from-Motion (SfM). By establishing correspon-dences between points across frames, a wide range of tasks can be performed, including depth estimation [5,15,16,18],
In this paper we introduce a novel architecture designed to improve self-supervised feature matching (Figure 1), fo-cusing on the task of monocular depth estimation. We build a cost volume between target and context image features us-ing differentiable depth-discretized epipolar sampling, and propose a novel attention-based mechanism to reﬁne per-pixel matching probabilities. We show that the reﬁned prob-abilities are sharper and more representative of the underly-ing 3D structure than traditional similarity metrics [70]. The resulting multi-frame cost volume is converted into depth estimates directly, via high-response window ﬁltering, and in combination with single-frame features from a separate network, to account for failure cases in cost volume gen-eration. Through extensive experiments, we show that our feature matching reﬁnement module leads to a new state of the art in self-supervised depth estimation, and that it can be directly transferred between datasets with minimal degra-dation thanks to its strong geometric grounding. Our main contributions are:
• We introduce a novel architecture, the DepthFormer, improves multi-view feature matching via that cross- and self-attention combined with depth-discretized epipolar sampling.
• Our architecture leads to state-of-the-art depth esti-mation results. It outperforms other self-supervised multi-frame methods by a large margin, and even sur-passes supervised single-frame architectures.
• Our learned attention-based matching function is transferable across datasets, which can signiﬁcantly improve convergence speed while decreasing memory. 2.