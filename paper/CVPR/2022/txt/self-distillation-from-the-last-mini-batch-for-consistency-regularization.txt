Abstract
Knowledge distillation (KD) shows a bright promise as a powerful regularization strategy to boost generalization ability by leveraging learned sample-level soft targets. Yet, employing a complex pre-trained teacher network or an ensemble of peer students in existing KD is both time-consuming and computationally costly. Various self KD methods have been proposed to achieve higher distillation efficiency. However, they either require extra network ar-chitecture modification or are difficult to parallelize. To cope with these challenges, we propose an efficient and reliable self-distillation framework, named Self-Distillation from Last Mini-Batch (DLB). Specifically, we rearrange the sequential sampling by constraining half of each mini-batch coinciding with the previous iteration. Meanwhile, the rest half will coincide with the upcoming iteration. Afterwards, the former half mini-batch distills on-the-fly soft targets generated in the previous iteration. Our proposed mech-anism guides the training stability and consistency, result-ing in robustness to label noise. Moreover, our method is easy to implement, without taking up extra run-time mem-ory or requiring model structure modification. Experimen-tal results on three classification benchmarks illustrate that our approach can consistently outperform state-of-the-art self-distillation approaches with different network architec-tures. Additionally, our method shows strong compatibil-ity with augmentation strategies by gaining additional per-formance improvement. The code is available at https:
//github.com/Meta-knowledge-Lab/DLB. 1.

Introduction
Knowledge Distillation (KD), first introduced by Bu-cilua et al. [2], was later popularized by Hinton et al. [10].
Numerous previous researches have demonstrated the suc-cess of KD in various learning tasks to boost the general-ization ability. For example, in the case of network com-pression, the two-stage offline KD is widely used to transfer
*Equal Contribution. ‡Corresponding Author.
†This work was done during the internship at OPPO Research Institute. dark knowledge from a cumbersome pre-trained model to a light student model that learns from teachers’ intermediate feature maps [21], logits [10], attention maps [40], or auxil-iary outputs [43]. However, training a high-capacity teacher network heavily relies on large computational sources and run-in memory. To alleviate the time-consuming prepara-tion of static teachers, online distillation is introduced [44], where an ensemble of peer students learns mutually from each other. Online KD achieves equivalent performance im-provement, compared with offline KD, with higher compu-tational efficiency. Thus, this line is subsequently extended by many following works to a more capable self-ensemble teacher [3,8,14,33]. Other applications of KD include semi-supervised learning, domain adaptation, transfer learning and etc [18, 26, 28]. The main scope of this paper focuses on the KD paradigm itself.
Conventional KD approaches, both online and offline, have achieved satisfying empirical performance [24]. Yet, existing KD approaches suffer from an obstacle in the low knowledge transferring efficiency [35]. Additionally, high computation and run-in memory costs restrict their deploy-ment onto the end devices, such as mobile phones, digital cameras [4]. To cope with these limitations, self-knowledge distillation has received increasing popularity, which en-ables a student model to distill knowledge from itself. The absence of a complex pre-trained teacher and an ensemble of peer students in self-KD contributes to a marginal im-provement in the training efficiency.
One popular formulation of self KD, such as Be Your
Own Teacher (BYOT), requires heavy network architec-ture modifications, which largely increases their difficulty to generalize onto various network structures [19,32,43]. In another line, history information, including previous train-ing logits or model snapshots, is utilized to construct a virtual teacher for extra supervision signals as self distil-lation.
Initially, born again networks (BAN) sequentially distill networks with identical parameters as its last gen-eration [7]. An advancement achieved by snapshot distil-lation is to take the secondary information from the prior mini-generation i.e. a couple of epochs, within one gener-ation [36]. This virtual teacher update frequency is further
Table 1. A comparison with the state-of-the-arts in terms of computational cost and smoothed granularity. We compare our method with label smoothing regularization [27], teacher-free knowledge distillation (Tf-KDsel f , Tf-KDreg) [37], class-wise self-knowledge distillation (CS-KD) [39], progressive self-knowledge distillation (PS-KD) [12], memory-replay knowledge distillation (Mr-KD) [30], data-distortion guided self-knowledge distillation (DDGSD) [35], be your own teacher (BYOT) [43].
LSR Tf-KDsel f
Tf-KDreg CS-KD PS-KD Mr-KD DDGSD BYOT Ours
Characteristic
Sample-level smoothing
No pre-trained teacher
No Architecture modification
Forward times per batch
Backward times per batch
Number of involved networks
Label update frequency
✗
✓
✓ 1 1 1
-✓
✗
✓ 2 1 2 epoch
✓
✓
✓ 1 1 1
-✓
✓
✓ 2 2 1
-✓
✓
✓ 2 1 2 epoch
✓
✓
✓
≥ 2 1 1 epoch
✓
✓
✓ 2 2 1
-✓
✓
✗ 1
≥2 1
-✓
✓
✓ 1 1 1 batch improved to epoch-level in progressive self-knowledge dis-tillation [12] and learning with retrospection [5]. Yet, ex-isting self KD methods have the following setbacks to be tackled. Firstly, the most instant information from the last iteration is discarded. Moreover, storing a snapshot of past models consumes an extra run-in memory cost, and subse-quently increases the difficulty to parallelize [36]. Finally, computation of the gradient in each time backward proro-gation is associated with twice the forward process on each batch of data, resulting in a computational redundancy and low computational efficiency.
To address these challenges in existing self KD meth-ods, we propose a simple but efficient self distillation ap-proach, named as Self-Distillation from Last Mini-Batch (DLB). Compared with existing self KD approaches, DLB is computationally efficient and saves the run-in memory by only storing the soft targets generated in the last mini-batch backup, resulting in its simplicity in deployment and parallelization. Every forward process of data instances is associated with a once backpropagation process, mitigating the computational redundancy. Major differences compared with the state-of-the-arts are summarized in Table 1. DLB produces on-the-fly sample-level smoothed labels for self-distillation. Leveraging the soft predictions from the last iteration, our method provides the most instant distillation for each training sample. The success of DLB is attributed to the distillation from the most immediate historically gen-erated soft targets to impose training consistency and stabil-ity. To be more specific, the target network plays a dual role as teacher and student in each mini-batch during the train-ing stage. As a teacher, it provides soft targets to regularize itself in the next iteration. As a student, it distills smoothed labels generated from the last iteration and minimizes the supervision learning objective e.g. the cross-entropy loss.
We empirically illustrate the comprehensive effective-ness of our methods on three benchmark datasets, namely
CIFAR-10 [13], CIFAR-100 [13], TinyImageNet. Our ap-proach is both task-agnostic and model-agnostic i.e. with no requirement of model topological modifications. We select six representative backbone CNNs for evaluations, including ResNet-18, ResNet-110 [9], VGG-16, VGG-19
[25], DenseNet [11], WideResNet [41]. Experimental re-sults demonstrate that our DLB can consistently improve the generalization ability. We also test the robustness of
DLB on corrupted data. Training consistency and stability imposed by DLB on corrupted data results in higher gener-alization ability.
The major contributions are three-fold.
• We propose a simple but efficient consistency regular-ization scheme based on self-knowledge distillation, named as DLB. With no network architecture modifi-cations, our method requires very few additional com-putation costs as well as the run-time memory to im-plement. Utilizing the latest update from the last it-eration, our DLB is easy to implement for paralleliza-tion. Notably, the proposed method is also both model-agnostic and task-agnostic.
• Comprehensive experimental results on three popu-lar classification benchmarks illustrate consistent gen-eralization improvements on different models. We also empirically demonstrate the compatibility of DLB with various augmentation strategies.
• We systemically analyze the impact of our method on the training dynamics. Concretely, the success of its regularization effect is attributed to guidance towards training consistency, by leveraging on-the-fly sample-level smooth labels. The consistency effect is fur-ther amplified under label corruption settings, showing strong robustness to label noise. These empirical finds may shed light on a new direction to understand the effect of knowledge distillation. 2.