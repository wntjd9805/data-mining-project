Abstract
We present ONCE-3DLanes, a real-world autonomous driving dataset with lane layout annotation in 3D space.
Conventional 2D lane detection from a monocular image yields poor performance of following planning and control tasks in autonomous driving due to the case of uneven road.
Predicting the 3D lane layout is thus necessary and enables effective and safe driving. However, existing 3D lane de-tection datasets are either unpublished or synthesized from a simulated environment, severely hampering the develop-ment of this ﬁeld. In this paper, we take steps towards ad-dressing these issues. By exploiting the explicit relationship between point clouds and image pixels, a dataset annotation pipeline is designed to automatically generate high-quality 3D lane locations from 2D lane annotations in 211K road scenes. In addition, we present an extrinsic-free, anchor-free method, called SALAD, regressing the 3D coordinates of lanes in image view without converting the feature map into the bird’s-eye view (BEV). To facilitate future research on 3D lane detection, we benchmark the dataset and pro-vide a novel evaluation metric, performing extensive ex-periments of both existing approaches and our proposed method.
The aim of our work is to revive the interest of 3D lane detection in a real-world scenario. We believe our work can lead to the expected and unexpected innovations in both academia and industry. 1.

Introduction
The perception of lane structure is one of the most funda-mental and safety-critical tasks in autonomous driving sys-tem. It is developed with the desired purpose of preventing accidents, reducing emissions and improving the trafﬁc efﬁ-ciency [4]. It serves as a key roll of many applications, such
*The ﬁrst two authors contributed equally to this work.
†Li Zhang (lizhangfd@fudan.edu.cn) is the corresponding author at
School of Data Science and Shanghai Key Laboratory of Intelligent In-formation Processing, Fudan University
Figure 1.
Images and 3D lane examples of ONCE-3DLanes dataset. ONCE-3DLanes covers various locations, illumination conditions, weather conditions and with numerous slope scenes. as lane keeping, high-deﬁnition (HD) map modeling [13], trajectory planning etc. In light of its importance, there has been a recent surge of interest in monocular 3D lane detec-tion [6, 7, 10, 15, 32]. However, existing 3D lane detection datasets are either unpublished, or synthesized in a simu-lated environment due to the difﬁculty of data acquisition and high labor costs of annotation. With the only synthe-sized data, the model inevitably lacks generalization ability in real-word scenarios. Although beneﬁting from the de-velopment of domain adaptation method [8], it still cannot completely alleviate the domain gap.
Most existing image-based lane detection methods have exclusively focused on formulating the lane detection prob-lem as a 2D task [29, 34, 35], in which a typical pipeline is to ﬁrstly detect lanes in the image plane based on seman-tic segmentation or coordinate regression and then project the detected lanes in top view by assuming the ground is
ﬂat [28, 33]. With well-calibrated camera extrinsics, the in-verse perspective mapping (IPM) is able to obtain an ac-ceptable approximation for 3D lane in the ﬂat ground plane.
However, in real-world driving environment, roads are not always ﬂat [7] and camera extrinsics are sensitive to vehicle body motion due to speed change or bumpy road, which will
lead to the incorrect perception of the 3D road structure and thus unexpected behavior may happen to the autonomous driving vehicle.
To overcome above shortcomings associated with ﬂat-ground assumption, 3D-LaneNet [7], directly predicts the 3D lane coordinates in an end-to-end manner, in which the camera extrinsics are predicted in a supervised manner to facilitate the projection from image view to top view. In ad-dition, an anchor-based lane prediction head is proposed to produce the ﬁnal 3D lane coordinates from the virtual top view. Despite the promising result exhibits the feasibility of this task, the virtual IPM projection is difﬁcult to learn with-out the hard-to-get extrinsics and the model is trained under the assumption that zero degree of camera roll towards the ground plane. Once the assumption is challenged or the need of extrinsic parameters is not satisﬁed, this method can barely work.
In this work, we take steps towards addressing above is-sues. For the ﬁrst time, we present a real-world 3D lane de-tection dataset ONCE-3DLanes, consisting of 211K images with labeled 3D lane points. Compared with previous 3D lane datasets, our dataset is the largest real-world lane de-tection dataset published up to now, containing more com-plex road scenarios with various weather conditions, differ-ent lighting conditions as well as a variety of geographi-cal locations. An automatic data annotation pipeline is de-signed to minimize the manual labeling effort. Comparing to the method [7] of using multi-sensor and expensive HD maps, ours is simpler and easier to be implemented. In ad-dition, we introduce a spatial-aware lane detection method, dubbed SALAD, in an extrinsic-free and end-to-end manner.
Given a monocular input image, SALAD directly predicts the 2D lane segmentation results and the spatial contextual information to reconstruct the 3D lanes without explicit or implicit IPM projection.
The contributions of this work are summarized as fol-lows: (i) For the ﬁrst time, we present a largest 3D lane detection dataset ONCE-3DLanes, alongside a more gener-alized evaluation metric to revive the interest of such task in a real-world scenario; (ii) We propose a method, SALAD that directly produce 3D lane layout from a monocular im-age without explicit or implicit IPM projection. 2.