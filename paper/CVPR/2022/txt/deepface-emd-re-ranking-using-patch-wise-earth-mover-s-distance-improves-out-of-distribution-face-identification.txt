Abstract while obtaining similar results on in-distribution images.
Face identification (FI) is ubiquitous and drives many high-stake decisions made by the law enforcement. A com-mon FI approach compares two images by taking the co-sine similarity between their image embeddings. Yet, such approach suffers from poor out-of-distribution (OOD) gen-eralization to new types of images (e.g., when a query face is masked, cropped or rotated) not included in the training set or the gallery. Here, we propose a re-ranking approach that compares two faces using the Earth Mover’s Distance on the deep, spatial features of image patches. Our ex-tra comparison stage explicitly examines image similarity at a fine-grained level (e.g., eyes to eyes) and is more ro-bust to OOD perturbations and occlusions than traditional
FI. Interestingly, without finetuning feature extractors, our method consistently improves the accuracy on all tested
OOD queries: masked, cropped, rotated, and adversarial 1.

Introduction
Who shoplifted from the Shinola luxury store in De-troit [4]? Who are you to receive unemployment benefits [5] or board an airplane [1]? Face identification (FI) today is behind the answers to such life-critical questions. Yet, the technology can make errors, leading to severe conse-quences, e.g. people wrongly denied of unemployment ben-efits [5] or falsely arrested [2–4,7]. Identifying the person in a single photo remains challenging because, in many cases, the problem is a zero-shot and ill-posed image retrieval task.
First, a deep feature extractor may not have seen a normal, non-celebrity person before during training. Second, there may be too few photos of a person in the database for FI systems to make reliable decisions. Third, it is harder to identify when a face in the wild (e.g. from surveillance cam-eras) is occluded [44, 54] (e.g. wearing masks), distant or
cropped, yielding a new type of photo not in both the train-ing set of deep networks and the retrieval database—i.e., out-of-distribution (OOD) data. For example, face verifi-cation accuracy may notoriously drop significantly (from 99.38% to 81.12% on LFW) given an occluded query face (Fig. 1b–d) [44] or adversarial queries [8, 73].
In this paper, we propose to evaluate the performance of state-of-the-art facial feature extractors (ArcFace [19], Cos-Face [61], and FaceNet [47]) on OOD face identification tests. That is, our main task is to recognize the person in a query image given a gallery of known faces. Besides in-distribution (ID) query images, we also test FI models on
OOD queries that contain (1) common occlusions, i.e. ran-dom crops, faces with masks or sunglasses; and (2) adver-sarial perturbations [73]. Our main findings are:1
• Interestingly, the OOD accuracy can be substantially improved via a 2-stage approach (see Fig. 2): First, identify a set of the most globally-similar faces from the gallery using cosine distance and then, re-rank these shortlisted candidates by comparing them with the query at the patch-embedding level using the Earth
Mover’s Distance (EMD) [45] (Sec. 3 & Sec. 4).
• Across three different models (ArcFace, CosFace, and FaceNet), our re-ranking approach consistently improves the original precision (under all metrics:
P@1, R-Precision, and MAP@R) without finetuning (Sec. 4). That is, interestingly, the spatial features ex-tracted from these models can be leveraged to compare images patch-wise (in addition to image-wise) and fur-ther improve FI accuracy.
• On masked images [59], our re-ranking method (no training) rivals the ArcFace models finetuned directly on masked images (Sec. 4.3).
To our knowledge, our work is the first to demonstrate the remarkable effectiveness of EMD for comparing OOD, occluded and adversarial images at the deep feature level. 2. Methods 2.1. Problem formulation
To demonstrate the generality of our method, we adopt the following simple FI formulation as in [19, 33, 71]: Iden-tify the person in a query image by ranking all gallery im-ages based on their pair-wise similarity with the query. Af-ter ranking (Stage 1) or re-ranking (Stage 2), we take the identity of the top-1 nearest image as the predicted identity.
Evaluation Following [38,71], we use three common eval-uation metrics: Precision@1 (P@1), R-Precision (RP), and
MAP@R (M@R). See their definitions in Sec. B1 in [71]. 1Code, demo and data are available at https://github.com/ anguyen8/deepface-emd 2.2. Networks
Pre-trained models We use three state-of-the-art Py-Torch models of ArcFace, FaceNet, and CosFace pre-trained on CASIA [65], VGGFace2 [14], and CASIA,
Their architectures are ResNet-18 [24], respectively.
Inception-ResNet-v1 [56], and 20-layer SphereFace [33], respectively. See Sec. S1 for more details on network ar-chitectures and implementation in PyTorch.
Image pre-processing
For all networks, we align and crop input images following the 3D facial alignment in [11] (which uses 5 reference points, 0.7 and 0.6 crop ratios for width and height, and Similarity transformation). All im-ages shown in this paper (e.g. Fig. 1) are pre-processed.
Using MTCNN, the default pre-processing of all three net-works, does not change the results substantially (Sec. S5). 2.3. 2-stage hierarchical face identification
Stage-1: Ranking A common 1-stage face identifica-tion [33, 47, 61] ranks gallery images based on their pair-wise cosine similarity with a given query in the last-linear-layer feature space of a pre-trained feature extractor (Fig. 2).
Here, our image embeddings are extracted from the last lin-ear layer of all three models and are all ∈ R512.
Stage-2: Re-ranking We re-rank the top-k (where the optimal k = 100) candidates from Stage 1 by computing the patch-wise similarity for an image pair using EMD. Overall, we compare faces in two hierarchical stages (Fig. 2), first at a coarse, image level and then a fine-grained, patch level.
Via an ablation study (Sec. 3), we find our 2-stage ap-proach (a.k.a. DeepFace-EMD) more accurate than Stage 1 alone (i.e. no patch-wise re-ranking) and also Stage 2 alone (i.e. sorting the entire gallery using patch-wise similarity). 2.4. Earth Mover’s Distance (EMD)
EMD is an edit distance between two set of weighted ob-jects or distributions [45]. Its effectiveness was first demon-strated in measuring pair-wise image similarity based on color histograms and texture frequencies [45] for image re-trieval. Yet, EMD is also an effective distance between two text documents [29], probability distributions (where EMD is equivalent to Wasserstein, i.e. Mallows distance) [30], and distributions in many other domains [27, 34, 43]. Here, we propose to harness EMD as a distance between two faces, i.e. two sets of weighted facial features.
Let Q = {(q1, wq1 ), ..., (qN , wqN )} be a set of N (facial feature, weight) pairs describing a query face where qi is a feature (e.g. left eye or nose) and the corresponding wqi indicates how important the feature qi is in FI. The flow between Q and the set of weighted features of a gallery face
G = {(g1, wg1), ..., (gN , wgN )} is any matrix F = (fij) ∈
RN ×N . Intuitively, fij is the amount of importance weight at qi that is matched to the weight at gj. Let dij be a ground
Figure 2. Our 2-stage face identification pipeline. Stage 1 ranks gallery images based on their cosine distance with the query face at the image-embedding level. Stage 2 then re-ranks the top-k shortlisted candidates from Stage 1 using EMD at the patch-embedding level. distance between (qi, gj) and D = (dij) ∈ RN ×N be the ground distance matrix of all pair-wise distances.
We want to find an optimal flow F that minimizes the following cost function, i.e. the sum of weighted pair-wise distances across the two sets of facial features:
COST(Q, G, F ) =
N (cid:88)
N (cid:88) i=1 j=1 dijfij s.t. fij ≥ 0 (1) (2)
N (cid:88) j=1
N (cid:88)
N (cid:88) j=1 i=1 fij ≤ wqi, and
N (cid:88) i=1 fij ≤ wgj , i, j ∈ [1, N ] (3) fij = min


N (cid:88) j=1 wgj ,
 wqi
 .
N (cid:88) i=1 (4)
As in [66, 71], we normalize the weights of a face such that the total weights of features is 1 i.e. (cid:80)N i=1 wqi = (cid:80)N j=1 wgj = 1, which is also the total flow in Eq. (4). Note that EMD is a metric iff two distributions have an equal total weight and the ground distance function is a metric [16].
We use the iterative Sinkhorn algorithm [18] to effi-ciently solve the linear programming problem in Eq. (1), which yields the final EMD between two faces Q and G.
Facial features In image retrieval using EMD, a set of fea-tures {qi} can be a collection of dominant colors [45], spa-tial frequencies [45], or a histogram-like descriptor based on the local patches of reference identities [60]. Inspired by [60], we also divide an image into a grid but we take the embeddings of the local patches from the last convolu-tional layers of each network. That is, in FI, face images are aligned and cropped such that the entire face covers most of the image (see Fig. 1a). Therefore, without facial occlusion, every image patch is supposed to contain useful identity in-formation, which is in contrast to natural photos [66].
Our grid sizes H × W for ArcFace, FaceNet, and Cos-Face are respectively, 8×8, 3×3, and 6×7, which are the corresponding spatial dimensions of their last convolutional layers (see definitions of these layers in Sec. S1). That is, each feature qi is an embedding of size 1×1×C where C is the number of channels (i.e. 512, 1792, and 512 for Arc-Face, FaceNet, and CosFace, respectively).
Ground distance Like [66, 71], we use cosine distance as the ground distance dij between the embeddings (qi, gj) of two patches: dij = 1 −
⟨qi, gj⟩
∥qi∥ ∥gj∥ (5) where ⟨.⟩ is the dot product between two feature vectors. 2.5. Feature weighting
EMD in our FI intuitively is an optimal plan to match all weighted features across two images. Therefore, how to weight features is an important step. Here, we thoroughly explore five different feature-weighting techniques for FI.
Uniform Zhang et al. [66] found that it is beneficial to assign lower weight to less informative regions (e.g. back-ground or occlusion) and higher weight to discriminative
areas (e.g. those containing salient objects). Yet, assigning an equal weight to all N = H × W patches is worth test-ing given that background noise is often cropped out of the pre-processed face image (Fig. 1): wqi = wgi = 1
N
, where 1 ≤ k ≤ N (6)
Average Pooling Correlation (APC) Instead of uniformly weighting all patch embeddings, an alternative from [66] would be to weight a given feature qi proportional to its correlation to the entire other image in consideration. That is, the weight wqi would be the dot product between the feature qi and the average pooling output of all embeddings
{gj}N 1 of the gallery image: wqi = max (cid:0)0, ⟨qi, (cid:80)N j gj
N
⟩(cid:1), wgj = max (cid:0)0, ⟨gj, (cid:80)N i qi
N
⟩(cid:1) (7) where max(.) keeps the weights always non-negative.
APC tends to assign near-zero weight to occluded regions and, interestingly, also minimizes the weight of eyes and mouth in a non-occluded gallery image (see Fig. 3b; blue shades around both the mask and the non-occluded mouth).
Cross Correlation (CC) APC [66] is different from CC introduced in [71], which is the same as APC except that CC uses the output vector from the last linear layer (see code) instead of the global average pooling vector in APC.
Spatial Correlation (SC) While both APC and CC “sum-marize” an entire other gallery image into a vector first, and then compute its correlation with a given patch qi in the query. In contrast, an alternative, inspired by [53], is to take the sum of the cosine similarity between the query patch qi and every patch in each gallery image {gj}N 1 : wqi = max (cid:0)0,
N (cid:88) j
⟨qi, gj⟩
∥qi∥∥gj∥ (cid:1), wgj = max (cid:0)0,
N (cid:88) i
⟨qi, gj⟩
∥qi∥∥gj∥ (cid:1) (8)
We observe that SC often assigns a higher weight to oc-cluded regions e.g., masks and sunglasses (Fig. 3b).
Landmarking (LMK) While the previous three tech-niques adaptively rely on the image-patch similarity (APC,
CC) or patch-wise similarity (SC) to weight a given patch embedding, their considered important points may or may not align with facial landmarks, which are known to be im-portant for many face-related tasks. Here, as a baseline for
APC, CC, and SC, we use dlib [26] to predict 68 keypoints in each face image (see Fig. 3c) and weight each patch-embedding by the density of the keypoints inside the patch area. Our LMK weight distribution appears Gaussian-like with the peak often right below the nose (Fig. 3c). (a) SC (b) APC (c) LMK
Figure 3. The results of assigning weights to 4×4 patches for Ar-cFace under three different techniques. Based on per-patch den-sity of detected landmarks (- - -), LMK (c) often assigns higher weight to the center of a face (regardless of occlusions). In con-trast, SC and APC assign higher weight to patches of higher patch-wise and patch-vs-image similarity, respectively. APC tends to down-weight a facial feature (e.g. blue regions around sunglasses or mouth) if its corresponding feature is occluded in the other im-age (b). In contrast, SC is insensitive to occlusions (a). See Fig. S1
& Fig. S2 for more heatmap examples of feature weighting. 3. Ablation Studies
We perform three ablation studies to rigorously evalu-ate the key design choices in our 2-stage FI approach: (1)
Which feature-weighting techniques to use (Sec. 3.1)? (2) re-ranking using both EMD and cosine distance (Sec. 3.2); and (3) comparing patches or images in Stage 1 (Sec. 3.3).
Experiment For all three experiments, we use ArcFace to perform FI on both LFW [65] and LFW-crop. For LFW, we take all 1,680 people who have ≥2 images for a to-tal of 9,164 images. When taking each image as a query, we search in a gallery of the remaining 9,163 images. For the experiments with LFW-crop, we use all 13,233 orig-inal LFW images as the gallery. To create a query set of 13,233 cropped images, we clone the gallery and crop each image randomly to its 70% and upsample it back to the original size of 128×128 (see examples in Fig. 5d). That is, LFW-crop tests identifying a cropped (i.e. close-up, and misaligned) image given the unchanged LFW gallery. LFW and LFW-crop tests offer contrast insights (ID vs. OOD).
In Stage 2, i.e. re-ranking the top-k candidates, we test different values of k ∈ {100, 200, 300} and do not find the performance to change substantially. At k = 100, our 2-stage precision is already close to the maximum precision of 99.88 under a perfect re-ranking (see Tab. 1a; Max prec.). 3.1. Comparing feature weighting techniques
Here, we evaluate the precision of our 2-stage FI as we sweep across five different feature-weighting techniques and two grid sizes (8×8 and 4×4). In an 8×8 grid, we ob-serve that some facial features such as the eyes are often split in half across two patches (see Fig. S5), which may im-pair the patch-wise similarity. Therefore, for each weight-ing technique, we also test average-pooling the 8×8 grid into 4×4 and performing EMD on the resultant 16 patches.
Results First, we find that, on LFW, our image-similarity-based techniques (APC, SC) outperform the LMK base-line (Tab. 1a) despite not using landmarks in the weighting process, verifying the effectiveness of adaptive, similarity-based weighting schemes.
Second, interestingly, in FI, we find that Uniform, APC, and SC all outperform the CC weighting proposed in
[66, 71]. This is in stark contrast to the finding in [71] that CC is better than Uniform (perhaps because face im-ages do not have background noise and are close-up). Fur-thermore, using the global average-pooling vector from the channel (APC) substantially yields more useful spatial sim-ilarity than the last-linear-layer output as in CC implemen-tation (Tab. 1b; 96.16 vs. 91.31 P@1).
Third, surprisingly, despite that a patch in a 8×8 grid does not enclose an entire, fully-visible facial feature (e.g. an eye), all feature-weighting methods are on-par or better on an 8×8 grid than on a 4×4 (e.g. Tab. 1b; APC: 96.16 vs. 95.32). Note that the optimal flow visualized in a 4×4 grid is more interpretable to humans than that on a 8×8 grid (compare Fig. 1 vs. Fig. S5).
Fourth, across all variants of feature weighting, our 2-stage approach consistently and substantially outperforms the traditional Stage 1 alone on LFW-crop, suggesting its robust effectiveness in handling OOD queries.
Fifth, under a perfect re-ranking of the top-k candidates (where k = 100), there is only 1.4% headroom for im-provement upon Stage 1 alone in LFW (Tab. 1a; 98.48 vs. 99.88) while there is a large ∼12% headroom in LFW-crop (Tab. 1a; 87.35 vs. 98.71).
Interestingly, our re-ranking results approach the upperbound re-ranking precision (e.g.
Tab. 1b; 96.26 of Uniform vs. 98.71 Max prec. at k = 100). 3.2. Re-ranking using both EMD & cosine distance
We observe that for some images, re-ranking using patch-wise similarity at Stage 2 does not help but instead hurt the accuracy. Here, we test whether linearly combining
EMD (at the patch-level embeddings as in Stage 2) and co-sine distance (at the image-level embeddings as in Stage 1) may improve re-ranking accuracy further (vs. EMD alone).
Experiment We use the grid size of 8×8, i.e. the better setting from the previous ablation study (Sec. 3.1). For each pair of images, we linearly combine their patch-level EMD (θEMD) and the image-level cosine distance (θCosine) as:
θ = α × θEMD + (1 − α) × θCosine (9)
Sweeping across α ∈ {0, 0.3, 0.5, 0.7, 1}, we find that changing α has a marginal effect on the P@1 on LFW. That is, the P@1 changes in [95, 98.5] with the lowest accuracy being 95 when EMD is exclusively used, i.e. α = 1 (see
Fig. 4a). In contrast, for LFW-crop, we find the accuracy to
ArcFace
LFW vs.
LFW (a)
LFW-crop vs.
LFW (b)
Method
Stage 1 alone [19]
Max prec. at k = 100
CC [71] (8 × 8)
CC [71] (4 × 4)
APC (8 × 8)
APC (4 × 4)
Uniform (8 × 8)
Uniform (4 × 4)
SC (8 × 8)
SC (4 × 4)
LMK (8 × 8)
LMK (4 × 4)
Stage 1 alone [19]
Max prec. at k = 100
CC [71] (8 × 8)
CC [71] (4 × 4)
APC (8 × 8)
APC (4 × 4)
Uniform (8 × 8)
Uniform (4 × 4)
SC (8 × 8)
SC (4 × 4)
P@1 98.48 99.88 98.42 81.69 98.60 98.54 98.66 98.63 98.66 98.65 98.35 98.31 87.35 98.71 91.31 63.12 96.16 95.32 96.26 95.53 96.19 95.42
RP 78.69 81.32 78.35 76.29 78.63 78.57 78.73 78.72 78.74 78.72 78.43 78.38 71.38 89.13 72.33 56.03 76.60 75.37 78.08 77.15 78.05 77.12
M@R 78.29
-77.91 72.47 78.23 78.16 78.35 78.33 78.35 78.33 77.99 77.90 69.04
-70.00 51.00 74.57 73.25 76.25 75.29 76.20 75.25
Table 1. Comparison of five feature-weighting techniques for Arc-Face [19] patch embeddings on LFW [65] and LFW-crop datasets.
Performance is often slightly better on a 8×8 grid than on a 4×4.
Our 2-stage approach consistently outperforms the vanilla Stage 1 alone and approaches closely the maximum re-ranking precision at k = 100. monotonically increases as we increase α (Fig. 4b). That is, the higher the contribution of patch-wise similarity, the better re-ranking accuracy on the challenging randomly-cropped queries. We choose α = 0.7 as the best and default choice for all subsequent FI experiments. Interestingly, our proposed distance (Eq. 9) also yields a state-of-the-art face verification result on MLFW [59] (Sec. S4). (a) LFW (b) LFW-crop
Figure 4. The P@1 of our 2-stage FI when sweeping across α for linearly combining EMD and cosine distance on LFW (a) and
LFW-crop images (b) when using APC feature weighting. Trends are similar for all other feature-weighting methods (see Fig. S3). 3.3. Patch-wise EMD for ranking or re-ranking
Given that re-ranking using EMD at the patch-embedding space substantially improves the precision of FI compared to Stage 1 alone (Tab. 1), here, we test performing such patch-wise EMD sorting at Stage 1 instead of Stage 2.
Experiment That is, we test ranking images using EMD at the patch level instead of the standard cosine distance at the image level. Performing patch-wise EMD at Stage 1 is sig-nificantly slower than our 2-stage approach, e.g., ∼12 times slower (729.20s vs. 60.97s, in total, for 13,233 queries).
That is, Sinkhorn is a slow, iterative optimization method and the EMD at Stage 2 has to sort only k = 100 (instead of 13,233) images. In addition, FI by comparing images patch-wise using EMD at Stage 1 yields consistently worse accu-racy than our 2-stage method under all feature-weighting techniques (see Tab. S1 for details). 4. Additional Results
To demonstrate the generality and effectiveness of our 2-stage FI, we take the best hyperparameter settings (α = 0.7; APC) from the ablation studies (Sec. 3) and use them for three different models (ArcFace [19], CosFace [61], and
FaceNet [47]), which have different grid sizes.
We test the three models on five different OOD query types: (1) faces wearing masks or (2) sunglasses; (3) profile faces; (4) randomly cropped faces; and (5) adversarial faces. 4.1. Identifying occluded faces
Experiment We perform our 2-stage FI on three datasets:
CFP [48], CALFW [72], and AgeDB [37]. 12,173-image
CALFW and 16,488-image AgeDB have age-varying im-ages of 4,025 and 568 identities, respectively. CFP has 500 people, each having 14 images (10 frontal and 4 profile).
To test our models on challenging OOD queries, in CFP, we use its 2,000 profile faces in CFP as queries and its 5,000 frontal faces as the gallery. To create OOD queries using CFP2, CALFW, and AgeDB, we automatically oc-clude all images with masks and sunglasses by detecting the landmarks of eyes and mouth using dlib and overlaying black sunglasses or a mask on the faces (see examples in
Fig. 1). We also take these three datasets and create ran-domly cropped queries (as for LFW-crop in Sec. 3). For all datasets, we test identifying occluded query faces given the original, unmodified gallery. That is, for every query, there is ≥ 1 matching gallery image.
Results First, for all three models and all occlusion types, i.e. due to masks, sunglasses, crop, and self-occlusion (pro-file queries in CFP), our method consistently outperforms the traditional Stage 1 alone approach under all three pre-cision metrics (Tables 2, S8, & S4).
Second, across all three datasets, we find the largest im-provement that our Stage 2 provides upon the Stage 1 alone is when the queries are randomly cropped or masked (Tab. 2). In some cases, the Stage 1 alone using cosine dis-tance is not able to retrieve any relevant examples among 2We only apply masks and sunglasses on the frontal images of CFP.
Dataset
Model
CALFW (Mask)
CALFW (Sunglass)
CALFW (Crop)
AgeDB (Mask)
AgeDB (Sunglass)
AgeDB (Crop)
ArcFace
CosFace
FaceNet
ArcFace
CosFace
FaceNet
ArcFace
CosFace
FaceNet
ArcFace
CosFace
FaceNet
ArcFace
CosFace
FaceNet
ArcFace
CosFace
FaceNet
Method
ST1
Ours
ST1
Ours
ST1
Ours
ST1
Ours
ST1
Ours
ST1
Ours
ST1
Ours
ST1
Ours
ST1
Ours
ST1
Ours
ST1
Ours
ST1
Ours
ST1
Ours
ST1
Ours
ST1
Ours
ST1
Ours
ST1
Ours
ST1
Ours
P@1 96.81 99.92 98.54 99.96 77.63 96.67 51.11 54.95 45.20 49.67 21.68 25.07 79.13 92.57 10.99 25.99 79.47 85.71 96.15 99.84 98.31 99.95 75.99 96.53 84.64 87.06 68.93 75.97 56.77 61.21 79.92 92.92 10.11 19.58 80.80 86.74
RP 53.13 57.27 43.46 59.85 39.74 45.87 29.38 30.66 25.93 26.98 13.70 15.04 43.46 47.17 6.45 12.35 44.40 45.91 39.22 39.22 38.17 39.70 22.28 24.25 51.16 50.40 34.90 35.54 27.92 28.98 32.66 32.93 4.23 4.95 31.50 31.51
M@R 51.70 56.33 41.20 58.87 36.93 44.53 26.73 27.74 22.78 24.12 10.89 12.16 41.20 45.68 5.43 11.13 41.99 43.83 30.41 33.18 31.57 33.68 14.95 17.49 44.99 44.27 27.30 28.12 20.00 21.11 26.19 26.60 2.18 2.76 24.27 24.32
Table 2. When the queries (from CALFW [72] and AgeDB [37]) are occluded by masks, sunglasses, or random cropping, our 2-stage method (8×8 grid; APC) is substantially more robust to the Stage 1 alone baseline (ST1) with up to +13% absolute gain (e.g. P@1: 79.13 to 92.57). The conclusions are similar for other feature-weighting methods (see Tab. S2 and Tab. S3). the top-5 but our re-ranking manages to push three relevant faces into the top-5 (Fig. 5d).
Third, we observe that for faces with masks or sun-glasses, APC interestingly often excludes the mouth or eye regions from the fully-visible gallery faces when computing the EMD patch-wise similarity with the corresponding oc-cluded query (Fig. 3). The same observation can be seen in the visualizations of the most similar patch pairs, i.e. high-est flow, for our same 2-stage approach that uses either 4×4 grids (Fig. 5 and Fig. 1) or 8×8 grids (Fig. S5). 4.2. Robustness to adversarial images
Adversarial examples pose a huge challenge and a se-rious security threat to computer vision systems [28, 39] including FI [50, 73]. Recent research suggests that the
(a) Masked (LFW) (b) Sunglassess (AgeDB) (c) Profile (CFP) (d) Cropped (LFW) (e) Adversarial (TALFW)
Stage 1
Flow Stage 2
Stage 1
Flow Stage 2
Stage 1
Flow Stage 2
Stage 1
Flow Stage 2
Stage 1
Flow Stage 2
Figure 5. Figure in a similar format to that of Fig. 1. Our re-ranking based on patch-wise similarity using ArcFace (4×4 grid; APC) pushes more relevant gallery images higher up (here, we show top-5 results), improving face identification precision under various types of occlu-sions. The “Flow” visualization intuitively shows the patch-wise reconstruction of the query (top-left) given the highest-correspondence patches (i.e. largest flow) from a gallery face. The darker a patch, the lower the flow. For example, despite being masked out ∼50% of the face (a), Nelson Mandela can be correctly retrieved as Stage 2 finds gallery faces with similar forehead patches. See Fig. S5 for a similar figure as the results of running our method with an 8×8 grid (i.e. smaller patches), which yields slightly better precision (Tab. 1).
Dataset
TALFW [73] vs.
LFW [65]
Model
ArcFace
CosFace
FaceNet
Method
ST1
Ours
ST1
Ours
ST1
Ours
P@1 93.49 96.64 96.49 99.07 95.33 97.26
RP 81.04 82.72 83.57 85.48 79.24 80.33
M@R 80.35 82.10 82.99 85.03 78.19 79.39
Table 3. Our re-ranking (8×8 grid; APC) consistently improves the precision over Stage 1 alone (ST1) when identifying adversar-ial TALFW [73] images given an in-distribution LFW [65] gallery.
The conclusions also carry over to other feature-weighting meth-ods (more results in Tab. S5). patch representation may be the key behind ViT impres-sive robustness to adversarial images [9, 35, 49]. Motivated by these findings, we test our 2-stage FI on TALFW [73] queries given an original 13,233-image LFW gallery.
Experiment TALFW contains 4,069 LFW images per-turbed adversarially to cause face verifiers to mislabel [73].
Results Over the entire TALFW query set, we find our re-ranking to consistently outperform the Stage 1 alone under all three metrics (Tab. 3). Interestingly, the improvement (of ∼2 to 4 points under P@1 for three models) is larger than when tested on the original LFW queries (around 0.12 in Tab. 1a), verifying our patch-based re-ranking robustness when queries are perturbed with very small noise. That is, our approach can improve FI precision when the perturba-tion size is either small (adversarial) or large (e.g. masks). 4.3. Re-ranking rivals finetuning on masked images
While our approach does not involve re-training, a com-mon technique for improving FI robustness to occlusion is data augmentation, i.e. re-train the models on occluded data in addition to the original data. Here, we compare our method with data augmentation on masked images.
Experiment To generate augmented, masked images, we follow [10] to overlay various types of masks on CASIA im-ages to generate ∼415K masked images. We add these im-ages to the original CASIA training set, resulting in a total of ∼907K images (10,575 identities). We finetune ArcFace on this dataset with the same original hyperparameters [6] (see Sec. S2). We train three models and report the mean and standard deviation (Tab. 4).
For a fair comparison, we evaluate the finetuned models and our no-training approach on the MLFW dataset [59], in-stead of our self-created masked datasets. That is, the query set has 11,959 MLFW masked-face images and the gallery is the entire 13,233-image LFW.
Results First, we find that finetuning ArcFace improves its accuracy in FI under Stage 1 alone (Tab. 4; 39.79 vs. 41.64). Yet, our 2-stage approach still substantially outper-forms Stage 1 alone, both when using the original and the finetuned ArcFace (Tab. 4; 48.23 vs. 41.64). Interestingly, we also test using the finetuned model in our DeepFace-EMD framework and finds it to approach closely the best no-training result (46.21 vs. 48.23). 5.