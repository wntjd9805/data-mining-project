Abstract
Visual grounding is a task to locate the target indicated by a natural language expression. Existing methods ex-tend the generic object detection framework to this prob-lem. They base the visual grounding on the features from pre-generated proposals or anchors, and fuse these features with the text embeddings to locate the target mentioned by the text. However, modeling the visual features from these predefined locations may fail to fully exploit the visual con-text and attribute information in the text query, which limits their performance. In this paper, we propose a transformer-based framework for accurate visual grounding by estab-lishing text-conditioned discriminative features and per-forming multi-stage cross-modal reasoning. Specifically, we develop a visual-linguistic verification module to focus the visual features on regions relevant to the textual descrip-tions while suppressing the unrelated areas. A language-guided feature encoder is also devised to aggregate the vi-sual contexts of the target object to improve the object’s dis-tinctiveness. To retrieve the target from the encoded visual features, we further propose a multi-stage cross-modal de-coder to iteratively speculate on the correlations between the image and text for accurate target localization. Exten-sive experiments on five widely used datasets validate the efficacy of our proposed components and demonstrate state-of-the-art performance. 1.

Introduction
Visual grounding aims to localize the referred object or region in an image by a natural language expression. This task has received increasing attention because of its great potential in bridging the gap between linguistic expressions and visual perception. The evolution of this technique is
∗ denotes equal contribution.
† denotes the corresponding author.
Figure 1. Our proposed framework for visual grounding. With the features from the two modalities as input, the visual-linguistic ver-ification module and language-guided context encoder establish discriminative features for the referred object. Then, the multi-stage cross-modal decoder iteratively mulls over all the visual and linguistic features to identify and localize the object. also of great importance to other multi-modal reasoning tasks. In visual grounding, the referred object is generally specified by one or more pieces of information in the lan-guage expression. The information may include object cat-egories, appearance attributes, and visual relation contexts, etc. Thus, to avoid ambiguity in reasoning, it is crucial to fully exploit the textual information and model discrimina-tive visual features for visual grounding.
Existing methods, no matter the two-stage or one-stage ones, treat visual grounding as a ranking problem on the detected candidate regions. The two-stage methods [23, 24, 33, 45] generally first detect a set of object proposals and then match them with the language query to retrieve the top-ranked proposal. The one-stage approaches [3, 18, 43] di-rectly fuse the text embeddings with image features to gen-erate dense detections, from which to choose the one with the highest confidence score. As these methods are based on generic object detectors, their inference procedures rely on
the predictions from all possible candidate regions, which makes the performance limited by the quality of the pre-predicted proposals or the configuration of predefined an-chor boxes. Moreover, they represent the candidate objects with the region features (corresponding to the predicted pro-posals) or the point features (of the dense anchor boxes) to match or fuse with the text embeddings. Such feature repre-sentations may be less flexible for capturing detailed visual concepts or contexts mentioned in linguistic descriptions.
This inflexibility may increase the difficulties in recogniz-ing the target object. Although some methods exploit mod-ular attention [44], graph and tree [20, 34, 40, 41] to better model the relations between vision and language, their pro-cessing pipelines are complicated and the performance is still limited by the object proposal inputs.
Recently, the boom of the transformer in natural lan-guage processing [6, 32] and computer vision [1, 7] has shown its powerful modeling capability in both the lan-guage and vision fields. Motivated by that, TransVG [5] proposes a transformer-based framework for visual ground-ing. Taking the visual and linguistic feature tokens as in-puts, they stack a set of transformer encoder layers to per-form cross-modal fusion, and directly predict the target lo-cations. Despite its effectiveness, the shared transformer encoder layers are in charge of multiple tasks, including en-coding the visual-linguistic features, identifying the object instances, and acquiring the final locations, which may in-creases the learning difficulty and could only achieve com-promised results. It is also less straightforward to retrieve the visual features of the target object with their feature fu-sion mechanism. Thus, we propose to establish a more ded-icated framework for accurate visual grounding.
In this work, we propose a transformer-based visual grounding framework that directly retrieves the target ob-ject’s feature representation for localization. To this end, as shown in Fig. 1, we first establish the discriminative feature representations by visual-linguistic verification and context aggregation, and then identify the referred object by multi-stage reasoning. Specifically, the visual-linguistic verification module compares the visual features with the semantic concepts from textual embeddings to focus on the regions relevant to the language expression. In paral-lel, the language-guided context encoder gathers the con-text features to make the visual features of the target object more distinguishable. Based on these enhanced features, we propose a multi-stage cross-modal decoder that iteratively compares and mulls over the visual and linguistic informa-tion. This enable us to progressively acquire a better rep-resentation of the referred object and thereby determine its location more accurately.
To summarize, our contributions are three-fold: (1) To establish the discriminative features for visual grounding, we propose a visual-linguistic verification module to fo-cus the encoded features on the regions related to the lan-guage expression. A language-guided context encoder is further devised to aggregate important visual contexts for better object identification. (2) To retrieve a more accu-rate feature representation of the referred object, we pro-pose a multi-stage cross-modal decoder, which iteratively queries and mulls over visual and linguistic information to reduce the ambiguity during inference. (3) We bench-mark our method on RefCOCO [45], RefCOCO+ [45],
RefCOCOg [23], ReferItGame [16], and Flickr30k Enti-ties [26]. Our method exhibits significant performance im-provements over the previous state-of-the-art methods. Ex-tensive experiments and ablation studies validate the effi-cacy of our proposed components. Our code is public at https://github.com/yangli18/VLTVG. 2.