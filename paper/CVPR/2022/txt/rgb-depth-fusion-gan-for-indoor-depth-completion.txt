Abstract
The raw depth image captured by the indoor depth sen-sor usually has an extensive range of missing depth values due to inherent limitations such as the inability to perceive transparent objects and limited distance range. The incom-plete depth map burdens many downstream vision tasks, and a rising number of depth completion methods have been proposed to alleviate this issue. While most existing meth-ods can generate accurate dense depth maps from sparse and uniformly sampled depth maps, they are not suitable for complementing the large contiguous regions of missing depth values, which is common and critical. In this paper, we design a novel two-branch end-to-end fusion network, which takes a pair of RGB and incomplete depth images as input to predict a dense and completed depth map. The ﬁrst branch employs an encoder-decoder structure to regress the local dense depth values from the raw depth map, with the help of local guidance information extracted from the RGB image. In the other branch, we propose an RGB-depth fu-sion GAN to transfer the RGB image to the ﬁne-grained tex-tured depth map. We adopt adaptive fusion modules named
W-AdaIN to propagate the features across the two branches, and we append a conﬁdence fusion head to fuse the two out-puts of the branches for the ﬁnal depth map. Extensive ex-periments on NYU-Depth V2 and SUN RGB-D demonstrate that our proposed method clearly improves the depth com-pletion performance, especially in a more realistic setting of indoor environments with the help of the pseudo depth map. 1.

Introduction
Nowadays, depth sensors have been widely used to pro-vide reliable 3D spatial information in a variety of ap-∗Corresponding authors.
Figure 1. Showcases of the raw depth maps (top) collected by sensors from the SUN RGB-D dataset [36] and the corresponding depth completion results (bottom) of our method. plications, such as augmented reality, indoor navigation, and 3D reconstruction tasks [8, 20, 42]. However, most existing commercial depth sensors (e.g., Kinect [26], Re-alSense [16], and Xtion [2]) for indoor spatial perception are not powerful enough to generate a precise and lossless depth map, as shown in the top row of Fig. 1. These sensors often produce many hole regions with invalid depth pixels due to transparent, shining, and dark surfaces as well as too close or too far edges, and these holes signiﬁcantly affect the performance of downstream tasks on the depth maps (a.k.a., depth images). To address the issue from imperfect depth maps, there have been a lot of approaches to recon-struct the whole depth map from the raw depth map, called depth completion. As RGB images provide rich color and texture information compared with depth maps, the aligned
RGB image is commonly used to guide the depth comple-tion of a depth map. To be more speciﬁc, the depth comple-tion task is usually conducted as using a pair of raw depth and RGB images captured by one depth sensor to complete and reﬁne the depth values.
Recent studies have produced signiﬁcant progress in depth completion tasks with convolutional neural net-works (CNNs) [3, 12, 17, 23, 29, 32]. Ma and Karaman [23] introduced an encoder-decoder network to directly regress
the dense depth map from a sparse depth map and an RGB image. The method has shown great progress compared to conventional algorithms [21, 34, 39], but its predicted dense depth maps are often too blurry. To further generate a more reﬁned completed depth map, lots of works have recently arisen, which can be divided into two groups with different optimization methods. The ﬁrst group of works [3, 22, 29] learn afﬁnities for relative pixels and iteratively reﬁne depth predictions. These methods highly rely on the accuracy of the raw global depth map and suffer the inference inefﬁ-ciency. Other works [12, 17, 18, 32] analyze the geometric characteristic and adjust the feature network structure ac-cordingly, for instance, by estimating the surface normal or projecting depth into discrete planes. These methods re-quire depth completeness without missing regions, and the model parameters may not be efﬁciently generalized to dif-ferent scenes. In any case, the RGB image is merely used as superﬁcial guidance or auxiliary information, and few methods deeply consider the textural and contextual infor-mation. At this point, the depth completion task is more or less degraded to a monocular depth estimation task that is conceptually simple but practically difﬁcult.
More remarkably, most of the above methods [3, 18, 23] uniformly randomly sample a certain number of valid pix-els from the dense depth image draw and dgt to mimic the sparse depth map d∗ for training and evaluation, respec-tively. Such sampling strategy is credible in some scenes, such as the outdoor range-view depth map generated by
LiDAR. However, the sampled patterns are quite different from the real missing patterns, such as the large missing regions and semantic missing patterns shown in Fig. 1, in indoor depth maps. Therefore, though existing methods are shown to be effective for completing uniformly sparse depth maps, it remains unveriﬁed whether they perform well enough for indoor depth completion.†
To solve these problems, we propose a novel two-branch end-to-end network to generate a completed dense depth map for indoor environments.
Inspired by generative ad-versarial networks (GANs) [14,15,24,27], we introduce the
RGB-depth Fusion GAN (RDF-GAN) for fusing an RGB image and a depth map. RDF-GAN maps a conditional
RGB image from the RGB domain to a dense depth map from the depth domain through the latent spatial vector gen-erated by the incomplete depth map. We further design a constraint network to restrict the depth values of the fused map, with the help of weighted-adaptive instance normal-ization (W-AdaIN) modules and a local guidance module.
Afterwards, a conﬁdence fusion head concludes the ﬁnal depth map completion.
In addition, we propose an exploitation technique, which samples raw depth images to produce pseudo depth maps for training. According to the characteristic of the indoor
†Please refer to Section 1 of the supplementary for more discussions. depth missing, we utilize the RGB images and semantic la-bels to produce masking regions for raw depth maps, which is more realistic than the simple uniform sampling. Ex-periments show that the model learning from pseudo depth maps can more effectively ﬁll in large missing regions for raw depth images captured indoors.
Our main contributions are summarized as the following:
• We propose a novel end-to-end GAN-based network, which effectively fuses a raw depth map and an RGB im-age to reproduce a reasonable dense depth map.
• We design and utilize the pseudo depth maps, which are in line with the raw depth missing distribution in indoor scenarios. Training with pseudo depth maps signiﬁcantly improves the model’s depth completion performance, es-pecially in more realistic settings of indoor environments.
• Our proposed method achieves the state-of-the-art per-formance on NYU-Depth V2 and SUN RGB-D for depth completion and proves its effectiveness in improving downstream task performance such as object detection. 2.