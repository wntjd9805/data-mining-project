Abstract
Scene Graph Generation, which generally follows a regular encoder-decoder pipeline, aims to first encode the visual contents within the given image and then parse them into a compact summary graph. Existing SGG approaches generally not only neglect the insufficient modality fusion between vision and language, but also fail to provide infor-mative predicates due to the biased relationship predictions, leading SGG far from practical. Towards this end, we first present a novel Stacked Hybrid-Attention network, which facilitates the intra-modal refinement as well as the inter-modal interaction, to serve as the encoder. We then devise an innovative Group Collaborative Learning strategy to optimize the decoder. Particularly, based on the observation that the recognition capability of one classifier is limited towards an extremely unbalanced dataset, we first deploy a group of classifiers that are expert in distinguishing different subsets of classes, and then cooperatively optimize them from two aspects to promote the unbiased SGG. Ex-periments conducted on VG and GQA datasets demonstrate that, we not only establish a new state-of-the-art in the unbiased metric, but also nearly double the performance compared with two baselines. Our code is available at https://github.com/dongxingning/SHA-GCL-for-SGG. 1.

Introduction
Scene Graph Generation (SGG) [41] targets at orga-nizing all the objects and their pairwise relationships into a compact summary graph. As an intermediate visual understanding task, SGG could benefit various vision-and-language tasks, including cross-modal retrieval [6, 11, 28], image captioning [2, 10, 51], and visual question answering
[12, 32, 48]. However, SGG is still far from satisfactory for practical applications due to the insufficient modality fusion and the biased relationship predictions.
†Corresponding authors.
Figure 1. Two intentions to promote the unbiased SGG. (1)
For the insufficient modality fusion, we aim to enhance both the intra-modal refinement and the inter-modal interaction (see the top-right corner of the figure). And (2) we split the extremely unbalanced dataset into a set of relatively balanced groups, based on which we configure the classification space for all the newly-added classifiers (see the rest part of the figure).
Though it is manifestly proved that incorporating seman-tic cues (language priors of object class names) into visual contents (object proposals) could significantly improve the generation capability [18,21], most of the recent approaches
[17, 26, 30, 31, 42, 43, 46, 47] simply fuse these visual and semantic features by summing up directly or concatenation, which limits the model to further infer their interaction information. To address this under-explored insufficient modality fusion between visual contents and semantic cues, we aim to strengthen the encoder via jointly exploring the intra-modal refinement and the inter-modal interaction, as illustrated in Figure 1. To implement this intention, we first design the Self-Attention (SA) unit and the Cross-Attention (CA) unit to capture the intra-modal and inter-modal information, respectively. We then organize these two units into a Hybrid-Attention (HA) layer, and stack several HA layers to build the encoder. The proposed
Stacked Hybrid-Attention (SHA) network could adequately explore the multi-modal interaction, thus improving the relationship prediction performance.
The other prominent issue faced by existing SGG meth-ods is the biased relationship predictions due to the long-tailed data distribution. Since only a few head predicates (e.g., on, has) possess massive and various instances, they would dominate the training procedure and lead the output scene graphs with few informative tail predicates (e.g., riding, watching), which could hardly support a wide range of downstream tasks. Though various debiasing approaches
[4, 29, 37] have been proposed, they are vulnerable to over-fitting the tail classes and sacrificing much on the head ones, leading to the other extreme. In a sense, we conjecture that this dilemma may root in the fact that a naive SGG model, regardless of the conventional or debiasing one, could only differentiate a limited range of predicates whose amount of training instances are relatively equal.
Intuitively, since a single classifier struggles in achieving a reasonable prediction trade-off, we can divide the biased predicate classes into several balanced subsets, then intro-duce more classifiers to conquer each of them, and ulti-mately leverage these classifiers to cooperatively address this challenge. To fulfill this “divide-conquer-cooperate” intuition, we propose the Group Collaborative Learning (GCL) strategy, where we 1) first divide: As a single classi-fier is adequate to differentiate the classes within a balanced dataset, we first divide all the predicates into a set of rela-tively balanced groups according to their amount of training instances, as illustrated in Figure 1. 2) Then conquer: We then borrow the idea from the class-incremental learning
[14] to force all the classifiers to follow a continuously i.e., each classifier would growing classification space, extend the previous classification space by incorporating a newly-added group of predicates. Besides, we devise the Median Re-Sampling strategy to provide each classifier with a relatively balanced training set. Based on this group-incremental configuration, these nested classifiers could fairly treat the predicates within their classification space, thus they would be more likely to learn the discriminating representations, especially towards the newly-added group. 3) Ultimately cooperate: We further leverage these clas-sifiers to cooperatively enhance the unbiased relationship predictions from two aspects. First, we propose the Parallel
Classifier Optimization (PCO) to jointly optimize all the classifiers. This can be seen as a “weak constraint”, since we expect that gathering all the gradients could promote the recognition capability of each classifier. Second, we devise the Collaborative Knowledge Distillation (CKD) to ensure that the discriminating capability learned previously could be well translated to the subsequent classifiers. This can be seen as a “strong constraint”, since we force each classifier to mimic the prediction behavior from its predecessors. By employing these two constraints, we effectively mitigate the overwhelming punishments to the tail classes as well as compensate for the under-fitting on the head ones.
The contributions of our work are three-folds:
• We present a novel Stacked Hybrid Attention network to strengthen the encoder in SGG, which addresses the under-explored insufficient modality fusion problem.
• We design the Group Collaborative Learning strategy to optimize the decoder in SGG. Particularly, we de-ploy a group of classifiers and cooperatively optimize them from two aspects, thus effectively addressing the intractable biased relationship prediction problem.
• Experiments conducted on VG and GQA dataset in-dicate that, we not only establish a new state-of-the-art in the unbiased metric, but also nearly double the performance compared with two typical baselines when employing our model-agnostic GCL. 2.