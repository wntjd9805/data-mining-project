Abstract
Accurately detecting and tracking pedestrians in 3D space is challenging due to large variations in rotations, poses and scales. The situation becomes even worse for dense crowds with severe occlusions. However, existing benchmarks either only provide 2D annotations, or have limited 3D annotations with low-density pedestrian distri-bution, making it difficult to build a reliable pedestrian perception system especially in crowded scenes. To bet-ter evaluate pedestrian perception algorithms in crowded scenarios, we introduce a large-scale multimodal dataset,
STCrowd. Specifically, in STCrowd, there are a total of 219 K pedestrian instances and 20 persons per frame on average, with various levels of occlusion. We provide syn-chronized LiDAR point clouds and camera images as well as their corresponding 3D labels and joint IDs. STCrowd can be used for various tasks, including LiDAR-only, image-only, and sensor-fusion based pedestrian detection and tracking. We provide baselines for most of the tasks.
In addition, considering the property of sparse global distri-bution and density-varying local distribution of pedestrians, we further propose a novel method, Density-aware Hierar-chical heatmap Aggregation (DHA), to enhance pedestrian perception in crowded scenes. Extensive experiments show that our new method achieves state-of-the-art performance for pedestrian detection on various datasets. https:
//github.com/4DVLab/STCrowd.git 1.

Introduction
Accurate pedestrian perception in 3D space plays a cru-cial role in thorough scene understanding. Many appli-cations also benefit from reliable and accurate pedestrian
†: Corresponding author.
Figure 1. STCrowd provides 2D/3D image annotations, 3D point cloud annotations, and joint annotations for consecutive frames.
Note that STCrowd contains a large quantity of crowded scenes with severe occlusions, which pose great challenges to pedestrian detection and tracking. perception [2, 4, 61], including surveillance, serving robots, autonomous driving, etc. However, pedestrian perception is intractable for three reasons. First, pedestrians are not rigid bodies and they can have various poses. Second, hu-mans are relatively small for sensors to capture compared with other agents, such as vehicles. For instance, in LiDAR point clouds, pedestrians in the distance are usually repre-sented as a few sparse points. Third, people tend to con-gregate when walking, which makes the detection of each individual person harder. Occlusion in crowded scenarios is a thorny problem for pedestrian perception.
Many datasets [4, 9, 14, 16, 19, 23, 32, 36, 39, 53, 54] have been collected to accelerate the research on the pedestrian perception field. Previous pedestrian perception datasets can be classified into two groups: image-based pedes-trian datasets and multimodal traffic datasets. The for-mer [5–7, 9, 14, 16, 23, 32, 36, 52–54] focus on pedestrian detection and tracking on 2D images and merely provide
2D bounding box annotations, which is insufficient for deep models to infer accurate 3D positions of the pedes-trians. The urgent demand for precise pedestrian percep-tion in 3D space has given rise to a suit of 3D annotated datasets [4, 8, 19, 24, 33, 39, 58]. However, these datasets all focus on the traffic scenes, where most objects on the roads are vehicles and pedestrians are distributed sparsely, which limits the exploration and evaluation of learning-based per-ception methods, especially for crowded scenes.
Specifically targeting 3D pedestrian perception in chal-lenging crowded scenarios, we introduce a large-scale mul-timodal dataset, STCrowd, with manually labeled 3D anno-tations for both images and point clouds. There are a total number of 219 K pedestrian instances in STCrowd with 20 persons per frame on average and more than 30 persons per frame in extremely crowded scenes. Specifically, there are 8 pedestrians in 5 meters on average centered on each per-son, which is much denser than contemporary 3D detection benchmarks, e.g., nuScenes [4] and KITTI [19]. Due to the lack of crowded 3D pedestrian datasets, perception algo-rithms always suffer from severe occlusions when dealing with crowded scenarios. STCrowd is very useful for ex-ploring more effective methods and testing their robustness.
In addition, we capture the data in 9 different scenes, cover-ing different weather, light conditions and road conditions.
With rich annotations, STCrowd is applicable for differ-ent tasks, including LiDAR-only, image-only, and sensor-fusion based detection, tracking and even trajectory predic-tion. We also provide baselines for most of the tasks in this paper to facilitate further research.
For LiDAR-captured outdoor scenes, pedestrians typ-ically account for a small portion of the whole scene.
For crowded scenarios, pedestrians gather together, which causes different degrees of occlusion and makes it dif-ficult to distinguish each individual pedestrian accurately in the crowd. Considering the sparse global distribution and density-varying local distribution of the pedestrians, we propose a novel method, Density-aware Hierarchical heatmap Aggregation (DHA), to enhance pedestrian per-ception especially in crowded scenes. Specifically, DHA is comprised of the spatial attention module and the hier-archical heatmap aggregation module. The former makes the network focus on the pertinent foreground regions and the latter helps distinguish individuals in density-varying scenes via multi-level heatmaps. We evaluate our method on STCrowd and achieve state-of-the-art performance. The extension of the proposed DHA to tracking problem and ablation studies on various backbones also demonstrate its effectiveness and good generalization capability. Our con-tribution is summarized as below: 1) We propose a large-scale multimodal pedestrian-oriented dataset in crowded scenarios with 3D manual annotations.
High-density distributions of pedestrians result in severe oc-clusion, which bring challenges for accurate perception. 2) Our dataset can be used for various tasks, including
LiDAR-only, image-only, and sensor-fusion based pedes-trian detection and tracking. We provide baselines and met-rics for most of the tasks to facilitate further research. 3) We propose a novel method to enhance LiDAR-based pedestrian perception in crowded scenes and achieve state-of-the-art performance on the STCrowd benchmark. 2.