Abstract
Semantic segmentation of point cloud data is a critical task for autonomous driving and other applications. Recent advances of point cloud segmentation are mainly driven by new designs of local aggregation operators and point sam-pling methods. Unlike image segmentation, few efforts have been made to understand the fundamental issue of scale and how scales should interact and be fused. In this work, we in-vestigate how to efﬁciently and effectively integrate features at varying scales and varying stages in a point cloud seg-mentation network. In particular, we open up the commonly used encoder-decoder architecture, and design scale pyra-mid architectures that allow information to ﬂow more freely and systematically, both laterally and upward/downward in scale. Moreover, a cross-scale attention feature learning block has been designed to enhance the multi-scale fea-ture fusion which occurs everywhere in the network. Such a design of multi-scale processing and fusion gains large improvements in accuracy without adding much additional computation. When built on top of the popular KPConv net-work, we see consistent improvements on a wide range of datasets, including achieving state-of-the-art performance on NPM3D and S3DIS. Moreover, the pyramid architecture is generic and can be applied to other network designs: we show an example of similar improvements over RandLANet. 1.

Introduction
With the rise of autonomous driving, semantic segmen-tation of point cloud data is increasingly drawing attention in research. Building deep models for point clouds, sets of orderless points at arbitrary 3D positions, is arguably differ-ent from that for images. Early works projected 3D points to regular structures so that convolution operators could be used [31, 36, 40]. Later, the pioneering work of Point-Net [33,34] developed a promising method to directly apply
Code is available at https://github.com/ginobilinie/ kp_pyramid deep learning on sparse 3D points, using shared multi-layer perceptrons (MLPs) to learn per-point features.
Follow-up work along the line of PointNet typically con-sists of three key components, namely: point-wise transfor-mation, local aggregation, and point sampling. Local aggre-gation operator plays a similar role for points as the convo-lution layer does for image pixels [27]; and point sampling works as a pooling layer does for pixels [34, 52, 60]. To take the similarities further, state-of-the-art point cloud seg-mentation methods mostly employ the encoder-decoder U-shape architecture [13, 34, 47], which is a classic design in image segmentation (UNet [37]). In the encoder path, trans-formation layers learn increasingly sophisticated per-point features, local aggregation operators combine information in local neighborhoods, and point subsampling layers fur-ther increase the receptive ﬁeld. The decoder path consists of upsampling and per-point transformation layers.
Most recent works on point cloud segmentation focused on either local aggregation [14, 19, 20, 27, 30, 34, 45, 47, 53, 58] or point sampling strategies [1, 8, 13, 24, 41, 54, 55]. For example, PointNet++ [34] applied several MLPs on a con-catenation of relative position and point feature to aggregate information in local neighborhoods. KPConv [47] designed to obtain pseudo grid feature and applied convolution on these kernel points. RandLANet [13] compared point sam-pling methods and selected random sampling for efﬁciency.
Density-adaptive sampling [1] was proposed to handle het-erogeneous density distributions and class imbalance.
Interestingly, for point cloud segmentation, little atten-tion has been devoted to the study of the network architec-ture itself. This is in stark contrast with image segmenta-tion, where most recent efforts went way beyond the basic encoder-decoder U-structure to design better and more ef-ﬁcient architectures, especially on the topics of multi-scale processing and fusion [18, 32, 35, 48, 57] and context aggre-gation [5, 28, 50, 59]. For example, HRNet [48] proposed to aggregates multi-scale features throughout lateral stages, with an emphasis on high-resolution representation. Hier-archical Attention [44] was also built upon better uses of multi-scale information, from a perspective of inference.
Point cloud data, no different from images, are multi-scale in nature and requires multi-scale processing, includ-ing the need to balance large-scale context with ﬁne detail, and the potential use of multiple local aggregation stages in order to extract semantic information. In this work, we show that indeed there is an urgent need, and a substan-tial beneﬁt, to move beyond the U-shape structure in point cloud segmentation. Inspired by latest advances in image segmentation [32, 48], we open up the standard encoder-decoder architecture to design a pyramid architecture for point cloud segmentation (see Fig 1). A number of design improvements are proposed and validated:
• we use lateral stages to link up the counterparts in the encoder and decoder paths at each scale, where neigh-borhoods are re-used in local aggregation and sam-pling;
• we add upward/downward links to form a full “pyra-mid” shape which allows information at varying scales and stages to be fused;
• we identify three components in fusion, design a novel
Cross-scaLe Attention fusIon Module (CLAIM, which is almost parameter-free) to better serve the aggrega-tion of multi-scale features, and empirically ﬁnd the best choices.
Note each of these is novel for point cloud segmentation, and together they provide a substantial boost in accuracy without a higher demand on computation. When built up on the popular KPConv network as the baseline, our pyramid architecture leads to 1.0 ∼ 3.0% improvements in mIoU on a wide range of benchmarks for both outdoor and in-door scenes, including SOTA results on NPM3D and S3DIS (with mIoU of 83.0 and 73.0, respectively). Moreover, our pyramid architecture is generic and can be used to enhance any encoder-decoder network. For example, when using the more efﬁcient RandLANet [13] as the baseline, similar large improvements in accuracy are also observed. 2.