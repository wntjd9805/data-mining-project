Abstract
Significant progress has been made on visual captioning, largely relying on pre-trained features and later fixed object detectors that serve as rich inputs to auto-regressive mod-els. A key limitation of such methods, however, is that the output of the model is conditioned only on the object de-tector’s outputs. The assumption that such outputs can rep-resent all necessary information is unrealistic, especially when the detector is transferred across datasets.
In this work, we reason about the graphical model induced by this assumption, and propose to add an auxiliary input to represent missing information such as object relationships.
We specifically propose to mine attributes and relationships from the Visual Genome dataset and condition the caption-ing model on them. Crucially, we propose (and show to be important) the use of a multi-modal pre-trained model (CLIP) to retrieve such contextual descriptions. Further, object detector models are frozen and do not have sufficient richness to allow the captioning model to properly ground them. As a result, we propose to condition both the detector and description outputs on the image, and show qualita-tively and quantitatively that this can improve grounding.
We validate our method on image captioning, perform thor-ough analyses of each component and importance of the pre-trained multi-modal model, and demonstrate significant improvements over the current state of the art, specifically
+7.5% in CIDEr and +1.3% in BLEU-4 metrics. 1.

Introduction
For vision-and-language (VL) tasks such as generating textual descriptions of an image (image captioning) [1, 9, 24], it is crucial to encode the input image into a repre-sentation that contains relevant information for the down-stream language task. Earlier works use an ImageNet pre-trained model to encode the input image [24, 51], while re-cent works achieve much better performance by using ob-jects detected by an object detector (e.g. Faster R-CNN [41]
Figure 1. Most existing VL methods encode the input image by a set of objects detected by a frozen pre-trained object detector.
This set of detected objects may be able to provide object-centric information such as object classes, locations, and attributes, but may fail to encode other information also crucial for the target VL tasks such as object predicates and image/scene level information. pre-trained on Visual Genome [27]) [1,25,39]. The detected objects encode more fine-grained information from the in-put image such as object classes, locations, and attributes, hence achieving substantially better performance.
Despite the success of encoding the input image with de-tected objects, the object detector is pre-trained on datasets such as Visual Genome and kept frozen during the training of the target VL task (on a different dataset). This leads to two major issues as illustrated in Figure 1: (1) the de-tector may be good at encoding object-centric information but not at many other kinds of information necessary for the target VL task such as the relationship between objects and image/scene level information; and (2) the conditional relationship between the detected objects and the input im-age is not jointly optimized for the target VL task so that features computed by the object detector cannot be refined before sending into the VL model, potentially resulting in poor features that are difficult to ground, for example.
For (1), most existing works follow prior works [1] to pre-train the object detector on Visual Genome for object 1
detection and attribute classification. This implies that the object features may be good at encoding object-centric in-formation such as classes, locations, and attributes, but not at encoding other crucial information. Take image caption-ing as an example; as shown in Figure 1, such crucial infor-mation includes relationships between objects (object pred-icates), image/scene level information, etc. Therefore, the first objective of this paper is to provide information com-plementary to the detected objects.
Inspired by the way the Visual Genome dataset is con-structed, we propose to provide complementary but neces-sary information in the form of contextual text descriptions for image sub-regions. However, generating the descrip-tions for image sub-regions requires training another im-age captioning model, which by itself may not be an easy task. Therefore, we propose to turn the text generation prob-lem into a cross-modal retrieval problem: given an image sub-region, retrieve the top-k most relevant text descrip-tions from a description database. One way of doing cross-modal retrieval is to search for visually similar images and return the paired text of that image [14,18,35,46]. However, we posit that we can effectively leverage recent advances in cross-modal pre-training on large-scale image and text pairs, CLIP [40], to directly retrieve relevant text given an image. CLIP has two branches, CLIP-I and CLIP-T which encode image and text, respectively, into a global feature representation, and is trained to pull paired image and text together and push unpaired ones apart. We show in Sec-tion 4.3 that the text descriptions retrieved by CLIP are more relevant to the image query compared to those retrieved in-directly by visual similarity. The retrieved text descriptions by CLIP provide rich and complementary information, thus leading to substantial performance improvement.
For (2), in most existing works the pre-trained object de-tector is kept frozen when training the target VL task. This implies that the conditioning relationship between the de-tected objects and the input image is not jointly optimized with the target VL task. Consequently, the information from a transferred object detector may not result in high-quality features that can be effectively used by the caption-ing model, for example in grounding them to words. There-fore, the second objective of this paper is to strengthen the conditioning relationship between the detected objects and the input image by optimizing this relationship jointly with the target VL task.
To strengthen the conditional relationship for (2), we should first encode the input image into a global feature representation in a way that preserves as much information relevant to the target VL task as possible.
In this paper, we choose CLIP-I, the image branch of CLIP model, as the image encoder. Since CLIP is also pre-trained on a cross-modal VL task, we show in Section 4.3 that it can better en-code information relevant to the target VL tasks compared to models pre-trained on image-only datasets. We then use a fully connected (FC) layer, which is jointly optimized with the target VL task, to model the conditional relationship.
In this paper, we validate our proposed method on the VL task of image captioning. By addressing the two issues from using a frozen pre-trained object detector described above, our method improves one of the SoTA image captioning models M2 by +7.2% in CIDEr and +1.3% in BLEU-4.
In summary, we make the following contributions:
• Identify the potential issues of using detected objects from a frozen pre-trained object detector to encode the input image for image captioning.
• Propose a cross-modal retrieval module by leveraging the cross-modal joint embedding space by CLIP to retrieve a set of contextual text descriptions that provide informa-tion complementary to detected objects.
• Propose an image conditioning module to strengthen and jointly optimize the conditional relationship between the detected objects and the input image such that the features are more effective and support tasks such as grounding.
• Improve SoTA object-only baseline model by a substan-tial margin and provide thorough quantitative and qualita-tive analyses for the proposed two modules and the design choices made within each module. 2.