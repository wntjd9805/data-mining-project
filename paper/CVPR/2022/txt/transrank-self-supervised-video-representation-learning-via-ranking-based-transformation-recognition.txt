Abstract
Recognizing transformation types applied to a video clip (RecogTrans) is a long-established paradigm for self-supervised video representation learning, which achieves much inferior performance compared to instance discrim-ination approaches (InstDisc) in recent works. However, based on a thorough comparison of representative Recog-Trans and InstDisc methods, we observe the great poten-tial of RecogTrans on both semantic-related and temporal-related downstream tasks. Based on hard-label classifi-cation, existing RecogTrans approaches suffer from noisy supervision signals in pre-training. To mitigate this prob-lem, we developed TransRank, a unified framework for recognizing Transformations in a Ranking formulation.
TransRank provides accurate supervision signals by rec-ognizing transformations relatively, consistently outper-forming the classification-based formulation. Meanwhile, the unified framework can be instantiated with an arbi-trary set of temporal or spatial transformations, demon-strating good generality. With a ranking-based formula-tion and several empirical practices, we achieve compet-itive performance on video retrieval and action recogni-tion. Under the same setting, TransRank surpasses the previous state-of-the-art method [28] by 6.4% on UCF101 and 8.3% on HMDB51 for action recognition (Top1 Acc); improves video retrieval on UCF101 by 20.4% (R@1).
The promising results validate that RecogTrans is still a worth exploring paradigm for video self-supervised learn-ing. Codes will be released at https://github.com/ kennymckormick/TransRank. 1.

Introduction
Effective video representation is of crucial importance for various video understanding tasks, including action recognition [8, 19, 43], temporal localization [5, 67, 68], and video retrieval [62, 70]. To ensure the quality, models pre-trained on large-scale video recognition datasets [8, 15, 43] (cid:12) Corresponding Author.
Figure 1. Quiz Time! Two temporal transformations (Normal
Speed 1× and Sped Up 2×) are applied to 3 clips. In pair (A, B), (B, C), two clips are played with 1×, 2× speed, respectively. Can you find which clip is played with 2× speed in (A, B)? What about the pair (B, C)? Answers are in the footnote1on the next page. have been widely adopted as the initial training point. How-ever, labeling such large video datasets is notoriously costly and time-consuming, limiting the growth rate of labeled video datasets and the evolution of supervised video rep-resentation. Considering the infinite supply and the exor-bitant annotating cost, learning video representation with self-supervision [16, 41, 49] has drawn increasing attention.
Relying on pretext tasks, video self-supervised learning (video SSL) can obtain good representation without human annotation. The learned representation is then transferred to benefit a series of downstream tasks via finetuning. Most pretext tasks fall into two broad categories: recognizing transformation types [3, 28, 29] (RecogTrans) and instance discrimination [9, 46, 57] (InstDisc). RecogTrans pretext tasks aim to classify the transformation applied to video clips. The applied transformation can be either spatial (ro-tation [29], resizing [36]) or temporal (different playback rates [3, 28, 59]). Recently, following the success of con-trastive learning in the image domain [10, 20, 24], InstDisc-based pretext tasks gradually become the dominant ap-proach for video self-supervised learning, significantly out-performing RecogTrans-based ones on video downstream tasks, including recognition and retrieval [32, 50].
Rather than directly continuing this trendy research di-rection, the popularity of InstDisc-based approaches raises
several questions that we care about. First, is there any dif-ference between the representations learned via RecogTrans and InstDisc? Is InstDisc-based representation generally more powerful, or do two representations focus on differ-ent aspects, having their own merits? Besides, what is the primary cause of the inferior performance of RecogTrans?
Is it because of general limitations of this framework or be-cause of lacking good practices?
To answer these questions, we first conduct a com-prehensive study with thorough comparisons on represen-tative methods of RecogTrans and InstDisc. We find that the representation learned via temporal RecogTrans (RecogTrans-T) has some unique properties, distinguishing itself from representations learned via InstDisc and spatial
RecogTrans (RecogTrans-S). The representation learned via
RecogTrans-T does not include large amounts of semantic cues, leading to relatively poor performance in downstream settings that directly evaluate the learned representations on semantic-related tasks, like video retrieval and linear evalu-ation. Meanwhile, RecogTrans-T shows an impressive ca-pability of temporal modeling and does well across differ-ent temporal-related tasks. Furthermore, when evaluated on action recognition with a finetuning setting, RecogTrans-T can surpass all alternative SSL methods with a proper fine-tuning strategy. These findings demonstrate the great poten-tial of RecogTrans-T approaches and motivate us to further explore this direction.
RecogTrans suffers from noisy supervision signals caused by the ignorance of video intrinsic properties. To this end, we develop TransRank: a unified framework for recognizing Transformations in a Ranking formula-tion. The core of our TransRank framework is to consider
RecogTrans tasks in a relative manner due to the different intrinsic speeds of videos (Figure 1). For example, ‘boxing’ and ‘running’ are much faster than ‘tai-chi’ and ‘walking’ in human perception. Even for the same action, the intrin-sic speeds can vary a lot when performed by different peo-ple (in Figure 1, the runner in clip-A runs much faster than the runner in clip-B). Compared to the classification-based formulation (TransCls), TransRank adopts a more accurate and distinct supervision signal, thus outperforming Tran-sCls consistently across different settings. Moreover, the ranking formulation is not detrimental to the universality.
TransRank can be instantiated with an arbitrary set of tem-poral (or spatial) transformations. We further conduct an ex-tensive ablation study on choices of the transformation set, good practices during pre-training and finetuning, and eval-uate learned representations on diverse downstream tasks.
Competitive performance on downstream tasks, including video retrieval and action recognition, can be obtained with 1A, C are played with 1× speed, B is played with 2× speed. One may give wrong predictions for the pair (A, B) due to their different intrinsic speeds. Meanwhlie, it is easy to find that B is played faster than C. the ranking-based framework and several good practices.
In summary, we make the following contributions: 1) We revisit several SSL approaches based on Recog-Trans and InstDisc, demonstrating the great potential of
RecogTrans in video self-supervised learning. 2) We develop a new framework called TransRank, which provides more accurate supervision signals than
RecogTrans based on hard-label classification, and can be applied to various temporal and spatial pretext tasks. 3) With TransRank and several good practices, we im-prove the RecogTrans-based video SSL to the next level.
Under the same setting, TransRank outperforms a previ-ous state-of-the-art work [28] by 6.4% on UCF101 and 8.3% on HMDB51. We achieve decent recognition re-sults (90.7%, 64.2% on UCF101, HMDB51) with a simple
R(2+1)D-18 backbone and visual-only inputs. Encouraging results validate that RecogTrans is still worth exploring. 2.