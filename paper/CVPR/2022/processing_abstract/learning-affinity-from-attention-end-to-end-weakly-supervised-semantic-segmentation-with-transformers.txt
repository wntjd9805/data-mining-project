Weakly-supervised semantic segmentation (WSSS) with image-level labels is an important and challenging task.Due to the high training efficiency, end-to-end solutions forWSSS have received increasing attention from the commu-nity. However, current methods are mainly based on con-volutional neural networks and fail to explore the global information properly, thus usually resulting in incomplete object regions. In this paper, to address the aforementioned problem, we introduce Transformers, which naturally inte-grate global information, to generate more integral initial pseudo labels for end-to-end WSSS. Motivated by the inher-ent consistency between the self-attention in Transformers and the semantic affinity, we propose an Affinity from Atten-tion (AFA) module to learn semantic affinity from the multi-head self-attention (MHSA) in Transformers. The learned affinity is then leveraged to refine the initial pseudo labels for segmentation. In addition, to efficiently derive reliable affinity labels for supervising AFA and ensure the local con-sistency of pseudo labels, we devise a Pixel-Adaptive Re-finement module that incorporates low-level image appear-ance information to refine the pseudo labels. We perform extensive experiments and our method achieves 66.0% and 38.9% mIoU on the PASCAL VOC 2012 and MS COCO 2014 datasets, respectively, significantly outperforming re-cent end-to-end methods and several multi-stage competi-tors. Code is available at https://github.com/ rulixiang/afa. 