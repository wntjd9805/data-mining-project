Recently, memory-based approaches show promising re-sults on semi-supervised video object segmentation. These methods predict object masks frame-by-frame with the help of frequently updated memory of the previous mask. Dif-ferent from this per-frame inference, we investigate an al-ternative perspective by treating video object segmentation as clip-wise mask propagation.In this per-clip inference scheme, we update the memory with an interval and simul-taneously process a set of consecutive frames (i.e. clip) be-tween the memory updates. The scheme provides two poten-tial benefits: accuracy gain by clip-level optimization and efficiency gain by parallel computation of multiple frames.To this end, we propose a new method tailored for the per-clip inference. Specifically, we first introduce a clip-wise operation to refine the features based on intra-clip cor-relation.In addition, we employ a progressive matching mechanism for efficient information-passing within a clip.With the synergy of two modules and a newly proposed per-clip based training, our network achieves state-of-the-art performance on Youtube-VOS 2018/2019 val (84.6% and 84.6%) and DAVIS 2016/2017 val (91.9% and 86.1%). Fur-thermore, our model shows a great speed-accuracy trade-off with varying memory update intervals, which leads to huge flexibility.Figure 1. (top) An illustrative example of per-clip inference where the memory update interval L is 5. We mark memory frames using (bottom) Accuracy vs. FPS - We compare red image borders. our model under the different inference setting of L with SOTA methods [9, 29, 35, 50, 52]. We report the overall score and FPS on Youtube-VOS 2019 [47] validation set. For a fair comparison, we compute the FPS of all the reported methods using the same machine. We additionally report STCN variants, extended in the same way as in ‘Ours’. Note that the FPS axis is in the log scale. 