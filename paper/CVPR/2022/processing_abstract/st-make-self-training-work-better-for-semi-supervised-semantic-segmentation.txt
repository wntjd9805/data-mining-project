Self-training via pseudo labeling is a conventional, sim-ple, and popular pipeline to leverage unlabeled data. In this work, we first construct a strong baseline of self-training (namely ST) for semi-supervised semantic segmentation via injecting strong data augmentations (SDA) on unlabeled images to alleviate overfitting noisy labels as well as de-couple similar predictions between the teacher and student.With this simple mechanism, our ST outperforms all existing methods without any bells and whistles, e.g., iterative re-training. Inspired by the impressive results, we thoroughly investigate the SDA and provide some empirical analysis.Nevertheless, incorrect pseudo labels are still prone to ac-cumulate and degrade the performance. To this end, we fur-ther propose an advanced self-training framework (namelyST++), that performs selective re-training via prioritizing reliable unlabeled images based on holistic prediction-level stability. Concretely, several model checkpoints are saved in the first stage supervised training, and the discrepancy of their predictions on the unlabeled image serves as a mea-surement for reliability. Our image-level selection offers holistic contextual information for learning. We demon-strate that it is more suitable for segmentation than com-mon pixel-wise selection. As a result, ST++ further boosts the performance of our ST. Code is available at https://github.com/LiheYoung/ST-PlusPlus. 