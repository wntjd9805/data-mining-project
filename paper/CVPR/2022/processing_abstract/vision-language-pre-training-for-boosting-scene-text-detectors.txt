Recently, vision-language joint representation learning has proven to be highly effective in various scenarios.In this paper, we specifically adapt vision-language joint learning for scene text detection, a task that intrinsically involves cross-modal interaction between the two modal-ities: vision and language, since text is the written form of language. Concretely, we propose to learn contextu-alized, joint representations through vision-language pre-training, for the sake of enhancing the performance of scene text detectors. Towards this end, we devise a pre-training architecture with an image encoder, a text encoder and a cross-modal encoder, as well as three pretext tasks: image-text contrastive learning (ITC), masked language model-ing (MLM) and word-in-image prediction (WIP). The pre-trained model is able to produce more informative repre-sentations with richer semantics, which could readily bene-fit existing scene text detectors (such as EAST and PSENet) in the down-stream text detection task. Extensive experi-ments on standard benchmarks demonstrate that the pro-posed paradigm can significantly improve the performance of various representative text detectors, outperforming pre-vious pre-training approaches. The code and pre-trained models will be publicly released. 