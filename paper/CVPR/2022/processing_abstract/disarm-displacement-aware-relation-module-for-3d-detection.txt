We introduce Displacement Aware Relation Module (DisARM), a novel neural network module for enhanc-ing the performance of 3D object detection in point cloud scenes. The core idea is extracting the most principal con-textual information is critical for detection while the tar-get is incomplete or featureless. We ﬁnd that relations be-tween proposals provide a good representation to describe the context. However, adopting relations between all the object or patch proposals for detection is inefﬁcient, and an imbalanced combination of local and global relations brings extra noise that could mislead the training. Rather than working with all relations, we ﬁnd that training with relations only between the most representative ones, or an-chors, can signiﬁcantly boost the detection performance.Good anchors should be semantic-aware with no ambigu-ity and able to describe the whole layout of a scene with no redundancy. To ﬁnd the anchors, we ﬁrst perform a pre-liminary relation anchor module with an objectness-aware sampling approach and then devise a displacement based module for weighing the relation importance for better uti-lization of contextual information. This light-weight rela-tion module leads to signiﬁcantly higher accuracy of ob-ject instance detection when being plugged into the state-of-the-art detectors. Evaluations on the public benchmarks of real-world scenes show that our method achieves the state-of-the-art performance on both SUN RGB-D and Scan-Net V2. The code and models are publicly available at https://github.com/YaraDuan/DisARM . 