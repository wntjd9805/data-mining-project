Establishing dense correspondences across semantically similar images remains a challenging task due to the signif-icant intra-class variations and background clutters. Tra-ditionally, a supervised learning was used for training the models, which required tremendous manually-labeled data, while some methods suggested a self-supervised or weakly-supervised learning to mitigate the reliance on the labeled data, but with limited performance.In this paper, we present a simple, but effective so-lution for semantic correspondence that learns the net-works in a semi-supervised manner by supplementing few ground-truth correspondences via utilization of a large amount of conﬁdent correspondences as pseudo-labels,Speciﬁcally, our framework gener-called SemiMatch. ates the pseudo-labels using the model’s prediction itself between source and weakly-augmented target, and uses pseudo-labels to learn the model again between source and strongly-augmented target, which improves the robust-ness of the model. We also present a novel conﬁdence measure for pseudo-labels and data augmentation tailored for semantic correspondence. In experiments, SemiMatch achieves state-of-the-art performance on various bench-marks. 