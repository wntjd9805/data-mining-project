(cid:3) (cid:5)(cid:6)(cid:4)(cid:2)(cid:8)(cid:1)(cid:7)Despite the potential of multi-modal pre-training to learn highly discriminative feature representations from complementary data modalities, current progress is being slowed by the lack of large-scale modality-diverse datasets.By leveraging the natural suitability of E-commerce, where different modalities capture complementary semantic in-formation, we contribute a large-scale multi-modal pre-The dataset comprises 5 training dataset M5Product. modalities (image, text, table, video, and audio), covers over 6,000 categories and 5,000 attributes, and is 500⇥ larger than the largest publicly available dataset with a sim-ilar number of modalities. Furthermore, M5Product con-tains incomplete modality pairs and noise while also hav-ing a long-tailed distribution, resembling most real-world problems. We further propose Self-harmonized ContrAstiveLEarning (SCALE), a novel pretraining framework that integrates the different modalities into a uniﬁed model through an adaptive feature fusion mechanism, where the importance of each modality is learned directly from the modality embeddings and impacts the inter-modality con-trastive learning and masked tasks within a multi-modal transformer model. We evaluate the current multi-modal pre-training state-of-the-art approaches and benchmark their ability to learn from unlabeled data when faced with the large number of modalities in the M5Product dataset.We conduct extensive experiments on four downstream tasks and demonstrate the superiority of our SCALE model, pro-viding insights into the importance of dataset scale and di-versity. Dataset and codes are available at 1. 