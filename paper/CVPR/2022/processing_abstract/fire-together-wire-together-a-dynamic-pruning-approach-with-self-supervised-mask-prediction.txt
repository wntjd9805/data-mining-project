Dynamic model pruning is a recent direction that al-lows for the inference of a different sub-network for each input sample during deployment. However, current dy-namic methods rely on learning a continuous channel gat-ing through regularization by inducing sparsity loss. This formulation introduces complexity in balancing different losses (e.g task loss, regularization loss). In addition, reg-ularization based methods lack transparent tradeoff hyper-parameter selection to realize a computational budget. Our contribution is two-fold: 1) decoupled task and pruning losses. 2) Simple hyperparameter selection that enablesFLOPs reduction estimation before training.Inspired by the Hebbian theory in Neuroscience: “neurons that fire to-gether wire together”, we propose to predict a mask to pro-cess k filters in a layer based on the activation of its pre-vious layer. We pose the problem as a self-supervised bi-nary classification problem. Each mask predictor module is trained to predict if the log-likelihood for each filter in the current layer belongs to the top-k activated filters. The value k is dynamically estimated for each input based on a novel criterion using the mass of heatmaps. We show ex-periments on several neural architectures, such as VGG,ResNet and MobileNet on CIFAR and ImageNet datasets.On CIFAR, we reach similar accuracy to SOTA methods with 15% and 24% higher FLOPs reduction. Similarly inImageNet, we achieve lower drop in accuracy with up to 13% improvement in FLOPs reduction. 