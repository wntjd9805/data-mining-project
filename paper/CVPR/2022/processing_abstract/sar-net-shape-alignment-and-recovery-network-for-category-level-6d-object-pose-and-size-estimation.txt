Given a single scene image, this paper proposes a method of Category-level 6D Object Pose and Size Esti-mation (COPSE) from the point cloud of the target object, without external real pose-annotated training data. Specif-ically, beyond the visual cues in RGB images, we rely on the shape information predominately from the depth (D) channel. The key idea is to explore the shape alignment of each instance against its corresponding category-level template shape, and the symmetric correspondence of each object category for estimating a coarse 3D object shape.Our framework deforms the point cloud of the category-level template shape to align the observed instance point cloud for implicitly representing its 3D rotation. Then we model the symmetric correspondence by predicting symmet-ric point cloud from the partially observed point cloud. The concatenation of the observed point cloud and symmetric one reconstructs a coarse object shape, thus facilitating ob-ject center (3D translation) and 3D size estimation. Ex-tensive experiments on the category-level NOCS benchmark demonstrate that our lightweight model still competes with state-of-the-art approaches that require labeled real-world images. We also deploy our approach to a physical Bax-ter robot to perform grasping tasks on unseen but category-known instances, and the results further validate the effi-cacy of our proposed model. Code and pre-trained models are available on the project webpage 1. 