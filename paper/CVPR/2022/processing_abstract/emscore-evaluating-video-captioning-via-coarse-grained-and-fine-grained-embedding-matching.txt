Current metrics for video captioning are mostly based on the text-level comparison between reference and can-didate captions. However, they have some insuperable drawbacks, e.g., they cannot handle videos without refer-ences, and they may result in biased evaluation due to the one-to-many nature of video-to-text and the neglect of vi-sual relevance. From the human evaluator’s viewpoint, a high-quality caption should be consistent with the pro-vided video, but not necessarily be similar to the refer-ence in literal or semantics. Inspired by human evaluation, we propose EMScore (Embedding Matching-based score), a novel reference-free metric for video captioning, which directly measures similarity between video and candidate captions. Benefiting from the recent development of large-scale pre-training models, we exploit a well pre-trained vision-language model to extract visual and linguistic em-beddings for computing EMScore. Specifically, EMScore combines matching scores of both coarse-grained (video and caption) and fine-grained (frames and words) levels, which takes the overall understanding and detailed char-acteristics of the video into account. Furthermore, con-sidering the potential information gain, EMScore can be flexibly extended to the conditions where human-labeled references are available. Last but not least, we collectVATEX-EVAL and ActivityNet-FOIl datasets to systemati-cally evaluate the existing metrics. VATEX-EVAL experi-ments demonstrate that EMScore has higher human corre-lation and lower reference dependency. ActivityNet-FOIL experiment verifies that EMScore can effectively identify“hallucinating” captions. Code and datasets are available at https://github.com/shiyaya/emscore.*Corresponding authorFigure 1. Two examples of caption evaluation. All the metric scores are scaled to [0, 1], including human scores. For example (a), reference-based metrics over-penalize for this correct candi-date caption due to “a rock” is not contained in the references. Our reference-free metric EMScore gives a reasonable high score with the help of using video as ground truth. For example (b), some reference-based metrics (e.g., ROUGE L and METEOR) under-penalize the hallucination (e.g., “different games”) which is not related to the video, and give an unreasonable higher score for“hallucinating” caption B than correct caption A.