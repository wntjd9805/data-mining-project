3D scene stylization aims at generating stylized images of the scene from arbitrary novel views following a given set of style examples, while ensuring consistency when ren-dered from different views. Directly applying methods for image or video stylization to 3D scenes cannot achieve such consistency. Thanks to recently proposed neural radianceﬁelds (NeRF), we are able to represent a 3D scene in a con-sistent way. Consistent 3D scene stylization can be effec-tively achieved by stylizing the corresponding NeRF. How-ever, there is a signiﬁcant domain gap between style exam-ples which are 2D images and NeRF which is an implicit volumetric representation. To address this problem, we pro-pose a novel mutual learning framework for 3D scene styl-ization that combines a 2D image stylization network andNeRF to fuse the stylization ability of 2D stylization net-work with the 3D consistency of NeRF. We ﬁrst pre-train a standard NeRF of the 3D scene to be stylized and replace its color prediction module with a style network to obtain*Corresponding Author is Lin Gao (gaolin@ict.ac.cn). a stylized NeRF. It is followed by distilling the prior knowl-edge of spatial consistency from NeRF to the 2D stylization network through an introduced consistency loss. We also in-troduce a mimic loss to supervise the mutual learning of theNeRF style module and ﬁne-tune the 2D stylization decoder.In order to further make our model handle ambiguities of 2D stylization results, we introduce learnable latent codes that obey the probability distributions conditioned on the style. They are attached to training samples as conditional inputs to better learn the style module in our novel stylizedNeRF. Experimental results demonstrate that our method is superior to existing approaches in both visual quality and long-range consistency. 