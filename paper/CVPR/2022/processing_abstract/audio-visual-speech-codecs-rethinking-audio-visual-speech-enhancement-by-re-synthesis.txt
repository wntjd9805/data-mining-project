Since facial actions such as lip movements contain sig-nificant information about speech content, it is not sur-prising that audio-visual speech enhancement methods are more accurate than their audio-only counterparts. Yet, state-of-the-art approaches still struggle to generate clean, realistic speech without noise artifacts and unnatural dis-tortions in challenging acoustic environments. In this pa-per, we propose a novel audio-visual speech enhancement framework for high-fidelity telecommunications in AR/VR.Our approach leverages audio-visual speech cues to gen-erate the codes of a neural speech codec, enabling effi-cient synthesis of clean, realistic speech from noisy signals.Given the importance of speaker-specific cues in speech, we focus on developing personalized models that work well for individual speakers. We demonstrate the efficacy of our ap-proach on a new audio-visual speech dataset collected in an unconstrained, large vocabulary setting, as well as existing audio-visual datasets, outperforming speech enhancement baselines on both quantitative metrics and human evalua-tion studies. Please see the supplemental video for qualita-tive results1. 