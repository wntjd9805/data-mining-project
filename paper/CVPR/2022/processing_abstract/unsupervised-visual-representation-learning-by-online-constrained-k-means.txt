Cluster discrimination is an effective pretext task for un-supervised representation learning, which often consists of two phases: clustering and discrimination. Clustering is to assign each instance a pseudo label that will be used to learn representations in discrimination. The main chal-lenge resides in clustering since prevalent clustering meth-ods (e.g., k-means) have to run in a batch mode. Besides, there can be a trivial solution consisting of a dominating cluster. To address these challenges, we first investigate the objective of clustering-based representation learning.Based on this, we propose a novel clustering-based pre-text task with online Constrained K-means (CoKe). Com-pared with the balanced clustering that each cluster has ex-actly the same size, we only constrain the minimal size of each cluster to flexibly capture the inherent data structure.More importantly, our online assignment method has a the-oretical guarantee to approach the global optimum. By de-coupling clustering and discrimination, CoKe can achieve competitive performance when optimizing with only a sin-gle view from each instance. Extensive experiments on Ima-geNet and other benchmark data sets verify both the efficacy and efficiency of our proposal. 