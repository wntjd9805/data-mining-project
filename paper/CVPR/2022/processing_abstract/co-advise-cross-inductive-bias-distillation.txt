The inductive bias of vision transformers is more relaxed that cannot work well with insufﬁcient data. Knowledge dis-tillation is thus introduced to assist the training of transform-ers. Unlike previous works, where merely heavy convolution-based teachers are provided, in this paper, we delve into the inﬂuence of models inductive biases in knowledge distil-lation (e.g., convolution and involution). Our key observa-tion is that the teacher accuracy is not the dominant reason for the student accuracy, but the teacher inductive bias is more important. We demonstrate that lightweight teachers with different architectural inductive biases can be used to co-advise the student transformer with outstanding perfor-mances. The rationale behind is that models designed with different inductive biases tend to focus on diverse patterns, and teachers with different inductive biases attain various knowledge despite being trained on the same dataset. The di-verse knowledge provides a more precise and comprehensive description of the data and compounds and boosts the per-formance of the student during distillation. Furthermore, we propose a token inductive bias alignment to align the induc-tive bias of the token with its target teacher model. With only lightweight teachers provided and using this cross inductive bias distillation method, our vision transformers (termed asCiT) outperform all previous vision transformers (ViT) of the same architecture on ImageNet. Moreover, our small size model CiT-SAK further achieves 82.7% Top-1 accuracy on ImageNet without modifying the attention module of theViT. Code is available at https://github.com/OliverRensu/co-advise. 