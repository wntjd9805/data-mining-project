How to achieve better results with fewer labeling costs remains a challenging task. In this paper, we present a new active learning framework, which for the first time incor-porates contrastive learning into recently proposed one-bit supervision. Here one-bit supervision denotes a simple Yes or No query about the correctness of the modelâ€™s prediction, and is more efficient than previous active learning methods requiring assigning accurate labels to the queried samples.We claim that such one-bit information is intrinsically in accordance with the goal of contrastive loss that pulls pos-itive pairs together and pushes negative samples away. To-wards this goal, we design an uncertainty metric to actively select samples for query. These samples are then fed into different branches according to the queried results. TheYes query is treated as positive pairs of the queried cate-gory for contrastive pulling, while the No query is treated as hard negative pairs for contrastive repelling. Addition-ally, we design a negative loss that penalizes the negative samples away from the incorrect predicted class, which can be treated as optimizing hard negatives for the correspond-ing category. Our method, termed as ObCP, produces a more powerful active learning framework, and experiments on several benchmarks demonstrate its superiority. 