We study the automatic generation of navigation instruc-tions from 360◦ images captured on indoor routes. Existing generators suffer from poor visual grounding, causing them to rely on language priors and hallucinate objects. OurMARKY-MT5 system addresses this by focusing on visual landmarks; it comprises a ﬁrst stage landmark detector and a second stage generator–a multimodal, multilingual, mul-titask encoder-decoder. To train it, we bootstrap grounded landmark annotations on top of the Room-across-Room (RxR) dataset. Using text parsers, weak supervision fromRxR’s pose traces, and a multilingual image-text encoder trained on 1.8b images, we identify 971k English, Hindi andTelugu landmark descriptions and ground them to speciﬁc regions in panoramas. On Room-to-Room, human wayﬁnd-ers obtain success rates (SR) of 71% following MARKY-MT5’s instructions, just shy of their 75% SR following hu-man instructions—and well above SRs with other genera-tors. Evaluations on RxR’s longer, diverse paths obtain 61-64% SRs on three languages. Generating such high-quality navigation instructions in novel environments is a step to-wards conversational navigation tools and could facilitate larger-scale training of instruction-following agents. 