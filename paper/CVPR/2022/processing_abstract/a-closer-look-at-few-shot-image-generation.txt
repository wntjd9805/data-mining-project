Modern GANs excel at generating high quality and di-verse images. However, when transferring the pretrainedGANs on small target data (e.g., 10-shot), the generator tends to replicate the training samples. Several methods have been proposed to address this few-shot image gener-ation task, but there is a lack of effort to analyze them under a unified framework. As our first contribution, we propose a framework to analyze existing methods during the adapta-tion. Our analysis discovers that while some methods have disproportionate focus on diversity preserving which impede quality improvement, all methods achieve similar quality af-ter convergence. Therefore, the better methods are those that can slow down diversity degradation. Furthermore, our analysis reveals that there is still plenty of room to further slow down diversity degradation.Informed by our analysis and to slow down the diver-*Corresponding Author sity degradation of the target generator during adaptation, our second contribution proposes to apply mutual informa-tion (MI) maximization to retain the source domainâ€™s rich multi-level diversity information in the target domain gen-erator. We propose to perform MI maximization by con-trastive loss (CL), leverage the generator and discrimina-tor as two feature encoders to extract different multi-level features for computing CL. We refer to our method as DualContrastive Learning (DCL). Extensive experiments on sev-eral public datasets show that, while leading to a slower diversity-degrading generator during adaptation, our pro-posed DCL brings visually pleasant quality and state-of-the-art quantitative performance. 