In recent years, with the advances of generative mod-els, many powerful face manipulation systems have been developed based on Deep Neural Networks (DNNs), calledDeepFakes.If DeepFakes are not controlled timely and properly, they would become a real threat to both celebri-ties and ordinary people. Precautions such as adding per-turbations to the source inputs will make DeepFake results look distorted from the perspective of human eyes. How-ever, previous method doesn’t explore whether the disrupted images can still spoof DeepFake detectors. This is critical for many applications where DeepFake detectors are used to discriminate between DeepFake data and real data due to the huge cost of examining a large amount of data man-ually. We argue that the detectors do not share a similar perspective as human eyes, which might still be spoofed by the disrupted data. Besides, the existing disruption meth-ods rely on iteration-based perturbation generation algo-rithms, which is time-consuming. In this paper, we propose a novel DeepFake disruption algorithm called “DeepFakeDisrupter”. By training a perturbation generator, we can add the human-imperceptible perturbations to source im-ages that need to be protected without any backpropaga-tion update. The DeepFake results of these protected source inputs would not only look unrealistic by the human eye but also can be distinguished by DeepFake detectors eas-ily. For example, experimental results show that by adding our trained perturbations, fake images generated by Star-GAN [5] can result in a 10 ∼ 20% increase in F1-score evaluated by various DeepFake detectors. 