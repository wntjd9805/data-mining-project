We introduce AutoRF – a new approach for learning neural 3D object representations where each object in the training set is observed by only a single view. This set-ting is in stark contrast to the majority of existing works that leverage multiple views of the same object, employ ex-plicit priors during training, or require pixel-perfect anno-tations. To address this challenging setting, we propose to learn a normalized, object-centric representation whose embedding describes and disentangles shape, appearance, and pose. Each encoding provides well-generalizable, com-pact information about the object of interest, which is de-coded in a single-shot into a new target view, thus en-abling novel view synthesis. We further improve the re-construction quality by optimizing shape and appearance codes at test time by ﬁtting the representation tightly to the input image.In a series of experiments, we show that our method generalizes well to unseen objects, even across different datasets of challenging real-world street scenes such as nuScenes, KITTI, and Mapillary Metropo-lis. Additional results can be found on our project page https://sirwyver.github.io/AutoRF/. 