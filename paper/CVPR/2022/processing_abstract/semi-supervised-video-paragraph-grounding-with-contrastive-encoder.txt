Video events grounding aims at retrieving the most rele-vant moments from an untrimmed video in terms of a given natural language query. Most previous works focus onVideo Sentence Grounding (VSG), which localizes the mo-ment with a sentence query. Recently, researchers extended this task to Video Paragraph Grounding (VPG) by retrieving multiple events with a paragraph. However, we ﬁnd the ex-isting VPG methods may not perform well on context mod-eling and highly rely on video-paragraph annotations. To tackle this problem, we propose a novel VPG method termedSemi-supervised Video-Paragraph TRansformer (SVPTR), which can more effectively exploit contextual information in paragraphs and signiﬁcantly reduce the dependency on annotated data. Our SVPTR method consists of two key components: (1) a base model VPTR that learns the video-paragraph alignment with contrastive encoders and tackles the lack of sentence-level contextual interactions and (2) a semi-supervised learning framework with multimodal fea-ture perturbations that reduces the requirements of anno-tated training data. We evaluate our model on three widely-used video grounding datasets, i.e., ActivityNet-Caption,Charades-CD-OOD, and TACoS. The experimental results show that our SVPTR method establishes the new state-of-the-art performance on all datasets. Even under the condi-tions of fewer annotations, it can also achieve competitive results compared with recent VPG methods. 