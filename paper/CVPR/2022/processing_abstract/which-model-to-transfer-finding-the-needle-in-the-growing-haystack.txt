Transfer learning has been recently popularized as a data-efﬁcient alternative to training models from scratch, in particular for computer vision tasks where it provides a remarkably solid baseline. The emergence of rich model repositories, such as TensorFlow Hub, enables the prac-titioners and researchers to unleash the potential of these models across a wide range of downstream tasks. As these repositories keep growing exponentially, efﬁciently select-ing a good model for the task at hand becomes paramount.We provide a formalization of this problem through a famil-iar notion of regret and introduce the predominant strate-gies, namely task-agnostic (e.g. ranking models by theirImageNet performance) and task-aware search strategies (such as linear or kNN evaluation). We conduct a large-scale empirical study and show that both task-agnostic and task-aware methods can yield high regret. We then propose a simple and computationally efﬁcient hybrid search strat-egy which outperforms the existing approaches. We high-light the practical beneﬁts of the proposed solution on a set of 19 diverse vision tasks. 