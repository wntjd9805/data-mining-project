The key towards learning informative node representa-tions in graphs lies in how to gain contextual information from the neighbourhood. In this work, we present a simple-yet-effective self-supervised node representation learning strategy via directly maximizing the mutual information be-tween the hidden representations of nodes and their neigh-bourhood, which can be theoretically justified by its link to graph smoothing. Following InfoNCE, our framework is optimized via a surrogate contrastive loss, where the pos-itive selection underpins the quality and efficiency of rep-resentation learning. To this end, we propose a topology-aware positive sampling strategy, which samples positives from the neighbourhood by considering the structural de-pendencies between nodes and thus enables positive selec-tion upfront. In the extreme case when only one positive is sampled, we fully avoid expensive neighbourhood aggrega-tion. Our methods achieve promising performance on vari-ous node classification datasets. It is also worth mentioning by applying our loss function to MLP based node encoders, our methods can be orders of faster than existing solutions.Our codes and supplementary materials are available at https://github.com/dongwei156/n2n. 