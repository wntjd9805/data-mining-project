The goal of this paper is to learn strong lip reading models that can recognise speech in silent videos. Most prior works deal with the open-set visual speech recogni-tion problem by adapting existing automatic speech recog-nition techniques on top of trivially pooled visual features.Instead, in this paper, we focus on the unique challenges en-countered in lip reading and propose tailored solutions. To this end, we make the following contributions: (1) we pro-pose an attention-based pooling mechanism to aggregate visual speech representations; (2) we use sub-word units for lip reading for the ﬁrst time and show that this allows us to better model the ambiguities of the task; (3) we pro-pose a model for Visual Speech Detection (VSD), trained on top of the lip reading network. Following the above, we obtain state-of-the-art results on the challenging LRS2 and LRS3 benchmarks when training on public datasets, and even surpass models trained on large-scale industrial datasets by using an order of magnitude less data. Our best model achieves 22.6% word error rate on the LRS2 dataset, a performance unprecedented for lip reading models, sig-niﬁcantly reducing the performance gap between lip read-ing and automatic speech recognition. Moreover, on theAVA-ActiveSpeaker benchmark, our VSD model surpasses all visual-only baselines and even outperforms several re-cent audio-visual methods. 