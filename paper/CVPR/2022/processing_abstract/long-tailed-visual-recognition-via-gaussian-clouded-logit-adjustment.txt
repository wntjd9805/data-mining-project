Long-tailed data is still a big challenge for deep neural networks, even though they have achieved great success on balanced data. We observe that vanilla training on long-tailed data with cross-entropy loss makes the instance-rich head classes severely squeeze the spatial distribution of the tail classes, which leads to difficulty in classifying tail class samples. Furthermore, the original cross-entropy loss can only propagate gradient short-lively because the gradient in softmax form rapidly approaches zero as the logit dif-ference increases. This phenomenon is called softmax sat-It is unfavorable for training on balanced data, uration. but can be utilized to adjust the validity of the samples in long-tailed data, thereby solving the distorted embedding space of long-tailed problems. To this end, this paper pro-poses the Gaussian clouded logit adjustment by Gaussian perturbation of different class logits with varied amplitude.We define the amplitude of perturbation as cloud size and set relatively large cloud sizes to tail classes. The large cloud size can reduce the softmax saturation and thereby making tail class samples more active as well as enlarging the embedding space. To alleviate the bias in a classifier, we therefore propose the class-based effective number sam-pling strategy with classifier re-training. Extensive experi-ments on benchmark datasets validate the superior perfor-mance of the proposed method. Source code is available at https://github.com/Keke921/GCLLoss. 