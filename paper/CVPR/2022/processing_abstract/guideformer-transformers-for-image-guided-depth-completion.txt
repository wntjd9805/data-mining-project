Depth completion has been widely studied to predict a dense depth image from its sparse measurement and a sin-gle color image. However, most state-of-the-art methods rely on static convolutional neural networks (CNNs) which are not ﬂexible enough for capturing the dynamic nature of input contexts. In this paper, we propose GuideFormer, a fully transformer-based architecture for dense depth com-pletion. We ﬁrst process sparse depth and color guidance images with separate transformer branches to extract hier-archical and complementary token representations. Each branch consists of a stack of self-attention blocks and has key design features to make our model suitable for the task.We also devise an effective token fusion method based on guided-attention mechanism. It explicitly models informa-tion ﬂow between the two branches and captures inter-modal dependencies that cannot be obtained from depth or color image alone. These properties allow GuideFormer to enjoy various visual dependencies and recover precise depth values while preserving ﬁne details. We evaluateGuideFormer on the KITTI dataset containing real-world driving scenes and provide extensive ablation studies. Ex-perimental results demonstrate that our approach signiﬁ-cantly outperforms the state-of-the-art methods. 