Recent developments in transformer models for visual data have led to signiﬁcant improvements in recognition and detection tasks. In particular, using learnable queries in place of region proposals has given rise to a new class of one-stage detection models, spearheaded by the De-tection Transformer (DETR). Variations on this one-stage approach have since dominated human–object interaction (HOI) detection. However, the success of such one-stageHOI detectors can largely be attributed to the represen-tation power of transformers. We discovered that when equipped with the same transformer, their two-stage coun-terparts can be more performant and memory-efﬁcient, while taking a fraction of the time to train. In this work, we propose the Unary–Pairwise Transformer, a two-stage de-tector that exploits unary and pairwise representations forHOIs. We observe that the unary and pairwise parts of our transformer network specialise, with the former preferen-tially increasing the scores of positive examples and the lat-ter decreasing the scores of negative examples. We evaluate our method on the HICO-DET and V-COCO datasets, and signiﬁcantly outperform state-of-the-art approaches. At in-ference time, our model with ResNet50 approaches real-time performance on a single GPU. 