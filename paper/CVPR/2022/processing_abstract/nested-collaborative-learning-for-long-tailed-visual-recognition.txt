The networks trained on the long-tailed dataset vary re-markably, despite the same training settings, which shows the great uncertainty in long-tailed learning. To alleviate the uncertainty, we propose a Nested Collaborative Learn-ing (NCL), which tackles the problem by collaboratively learning multiple experts together. NCL consists of two core components, namely Nested Individual Learning (NIL) andNested Balanced Online Distillation (NBOD), which focus on the individual supervised learning for each single expert and the knowledge transferring among multiple experts, re-spectively. To learn representations more thoroughly, bothNIL and NBOD are formulated in a nested way, in which the learning is conducted on not just all categories from a full perspective but some hard categories from a partial per-spective. Regarding the learning in the partial perspective, we specifically select the negative categories with high pre-dicted scores as the hard categories by using a proposedHard Category Mining (HCM). In the NCL, the learning from two perspectives is nested, highly related and comple-mentary, and helps the network to capture not only global and robust features but also meticulous distinguishing abil-ity. Moreover, self-supervision is further utilized for feature enhancement. Extensive experiments manifest the superi-ority of our method with outperforming the state-of-the-art whether by using a single model or an ensemble. Code is available at https://github.com/Bazinga699/NCL 