Visual Emotion Analysis (VEA) is attracting increasing attention. One of the biggest challenges of VEA is to bridge the affective gap between visual clues in a picture and the emotion expressed by the picture. As the granularity of emo-tions increases, the affective gap increases as well. Existing deep approaches try to bridge the gap by directly learning discrimination among emotions globally in one shot. They ignore the hierarchical relationship among emotions at dif-ferent affective levels, and the variation in the affective level of emotions to be classified. In this paper, we present the multi-level dependent attention network (MDAN) with two branches to leverage the emotion hierarchy and the corre-lation between different affective levels and semantic levels.The bottom-up branch directly learns emotions at the high-est affective level and largely prevents hierarchy violation by explicitly following the emotion hierarchy while predict-ing emotions at lower affective levels. In contrast, the top-down branch aims to disentangle the affective gap by one-to-one mapping between semantic levels and affective lev-els, namely, Affective Semantic Mapping. A local classifier is appended at each semantic level to learn discrimination among emotions at the corresponding affective level. Then, we integrate global learning and local learning into a uni-fied deep framework and optimize it simultaneously. More-over, to properly model channel dependencies and spatial attention while disentangling the affective gap, we care-fully designed two attention modules: the Multi-head CrossChannel Attention module and the Level-dependent ClassActivation Map module. Finally, the proposed deep frame-work obtains new state-of-the-art performance on six VEA benchmarks, where it outperforms existing state-of-the-art methods by a large margin, e.g., +3.85% on the WEBEmo dataset at 25 classes classification accuracy. 