We propose HandLer, a novel convolutional architec-ture that can jointly detect and track hands online in un-constrained videos. HandLer is based on Cascade-RCNN with additional three novel stages. The ﬁrst stage is For-ward Propagation, where the features from frame t-1 are propagated to frame t based on previously detected hands and their estimated motion. The second stage is the Detec-tion and Backward Regression, which uses outputs from the forward propagation to detect hands for frame t and their relative offset in frame t-1. The third stage uses an off-the-shelf human pose method to link any fragmented hand tracklets. We train the forward propagation and backward regression and detection stages end-to-end together with the other Cascade-RCNN components.To train and evaluate HandLer, we also contributeYouTube-Hand, the ﬁrst challenging large-scale dataset of unconstrained videos annotated with hand locations and their trajectories. Experiments on this dataset and other benchmarks show that HandLer outperforms the exist-ing state-of-the-art tracking algorithms by a large margin.Code and data are available at https://vision.cs. stonybrook.edu/˜mingzhen/handler/. 