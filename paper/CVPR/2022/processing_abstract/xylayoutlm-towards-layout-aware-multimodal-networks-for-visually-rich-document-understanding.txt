Recently, various multimodal networks for Visually-Rich Document Understanding(VRDU) have been pro-posed, showing the promotion of transformers by integrat-ing visual and layout information with the text embeddings.However, most existing approaches utilize the position em-beddings to incorporate the sequence information, neglect-ing the noisy improper reading order obtained by OCR tools. In this paper, we propose a robust layout-aware mul-timodal network named XYLayoutLM to capture and lever-age rich layout information from proper reading orders pro-duced by our Augmented XY Cut. Moreover, a Dilated Con-ditional Position Encoding module is proposed to deal with the input sequence of variable lengths, and it additionally extracts local layout information from both textual and vi-sual modalities while generating position embeddings. Ex-periment results show that our XYLayoutLM achieves com-petitive results on document understanding tasks. 