In this paper, we study an untouched problem in visible-infrared person re-identiﬁcation (VI-ReID), namely, TwinNoise Labels (TNL) which refers to as noisy annotation and correspondence. In brief, on the one hand, it is inevitable to annotate some persons with the wrong identity due to the complexity in data collection and annotation, e.g., the poor recognizability in the infrared modality. On the other hand, the wrongly annotated data in a single modality will even-tually contaminate the cross-modal correspondence, thus leading to noisy correspondence. To solve the TNL prob-lem, we propose a novel method for robust VI-ReID, termedDuAlly Robust Training (DART). In brief, DART ﬁrst com-putes the clean conﬁdence of annotations by resorting to the memorization effect of deep neural networks. Then, the proposed method rectiﬁes the noisy correspondence with the estimated conﬁdence and further divides the data into four groups for further utilizations. Finally, DART employs a novel dually robust loss consisting of a soft identiﬁcation loss and an adaptive quadruplet loss to achieve robustness on the noisy annotation and noisy correspondence. Exten-sive experiments on SYSU-MM01 and RegDB datasets ver-ify the effectiveness of our method against the twin noisy labels compared with ﬁve state-of-the-art methods. The code could be accessed from https://github.com/XLearning-SCU/2022-CVPR-DART. 