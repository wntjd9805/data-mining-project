Recently, transformer-based methods have achieved promising progresses in object detection, as they can elim-inate the post-processes like NMS and enrich the deep rep-resentations. However, these methods cannot well cope with scene text due to its extreme variance of scales and aspect ratios.In this paper, we present a simple yet ef-fective transformer-based architecture for scene text detec-tion. Different from previous approaches that learn robust deep representations of scene text in a holistic manner, our method performs scene text detection based on a few rep-resentative features, which avoids the disturbance by back-ground and reduces the computational cost. Specifically, we first select a few representative features at all scales that are highly relevant to foreground text. Then, we adopt a transformer for modeling the relationship of the sam-pled features, which effectively divides them into reason-able groups. As each feature group corresponds to a text in-stance, its bounding box can be easily obtained without any post-processing operation. Using the basic feature pyra-mid network for feature extraction, our method consistently achieves state-of-the-art results on several popular datasets for scene text detection. 