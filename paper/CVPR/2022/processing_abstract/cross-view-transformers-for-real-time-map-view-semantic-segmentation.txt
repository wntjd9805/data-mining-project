We present an efficient cross-view transformers, attention-based model for map-view semantic segmentation from multiple cameras. Our architecture implicitly learns a mapping from individual camera views into a canonical map-view representation using a camera-aware cross-view attention mechanism. Each camera uses positional embed-dings that depend on its intrinsic and extrinsic calibration.These embeddings allow a transformer to learn the map-ping across different views without ever explicitly model-ing it geometrically. The architecture consists of a convo-lutional image encoder for each view and cross-view trans-former layers to infer a map-view semantic segmentation.Our model is simple, easily parallelizable, and runs in real-time. The presented architecture performs at state-of-the-art on the nuScenes dataset, with 4x faster inference speeds.Code is available at https://github.com/bradyz/ cross_view_transformers. 