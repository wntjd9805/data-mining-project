This work focuses on learning deep visual representa-tion models for retrieval by exploring the interplay between a new loss function, the batch size, and a new regulariza-tion approach. Direct optimization, by gradient descent, of an evaluation metric, is not possible when it is non-differentiable, which is the case for recall in retrieval. A differentiable surrogate loss for the recall is proposed in this work. Using an implementation that sidesteps the hardware constraints of the GPU memory, the method trains with a very large batch size, which is essential for metrics com-puted on the entire retrieval database. It is assisted by an ef-ficient mixup regularization approach that operates on pair-wise scalar similarities and virtually increases the batch size further. The suggested method achieves state-of-the-art performance in several image retrieval benchmarks when used for deep metric learning. For instance-level recogni-tion, the method outperforms similar approaches that train using an approximation of average precision. 