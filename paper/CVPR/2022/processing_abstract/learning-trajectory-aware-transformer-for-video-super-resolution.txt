TargetFrame 57Frame 61Frame 64Video super-resolution (VSR) aims to restore a sequence of high-resolution (HR) frames from their low-resolution (LR) counterparts. Although some progress has been made, there are grand challenges to effectively utilize temporal de-pendency in entire video sequences. Existing approaches usually align and aggregate video frames from limited ad-jacent frames (e.g., 5 or 7 frames), which prevents these approaches from satisfactory results.In this paper, we take one step further to enable effective spatio-temporal learning in videos. We propose a novel Trajectory-awareTransformer for Video Super-Resolution (TTVSR). In par-ticular, we formulate video frames into several pre-aligned trajectories which consist of continuous visual tokens. For a query token, self-attention is only learned on relevant vi-sual tokens along spatio-temporal trajectories. Compared with vanilla vision Transformers, such a design signiÔ¨Å-cantly reduces the computational cost and enables Trans-formers to model long-range features. We further pro-pose a cross-scale feature tokenization module to over-come scale-changing problems that often occur in long-range videos. Experimental results demonstrate the supe-riority of the proposed TTVSR over state-of-the-art mod-els, by extensive quantitative and qualitative evaluations in four widely-used video super-resolution benchmarks.Both code and pre-trained models can be downloaded at https://github.com/researchmm/TTVSR. 