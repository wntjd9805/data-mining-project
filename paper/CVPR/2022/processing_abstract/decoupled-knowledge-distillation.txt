State-of-the-art distillation methods are mainly based on distilling deep features from intermediate layers, while the signiﬁcance of logit distillation is greatly overlooked. To provide a novel viewpoint to study logit distillation, we re-formulate the classical KD loss into two parts, i.e., target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD). We empirically investigate and prove the effects of the two parts: TCKD transfers knowledge concerning the “difﬁculty” of training samples, while NCKD is the prominent reason why logit distillation works. More importantly, we reveal that the classical KD loss is a coupled formulation, which (1) suppresses the ef-fectiveness of NCKD and (2) limits the ﬂexibility to bal-ance these two parts. To address these issues, we presentDecoupled Knowledge Distillation (DKD), enabling TCKD and NCKD to play their roles more efﬁciently and ﬂexibly.Compared with complex feature-based methods, our DKD achieves comparable or even better results and has bet-ter training efﬁciency on CIFAR-100, ImageNet, and MS-COCO datasets for image classiﬁcation and object detec-tion tasks. This paper proves the great potential of logit distillation, and we hope it will be helpful for future re-search. The code is available at https://github.com/megvii-research/mdistiller. 