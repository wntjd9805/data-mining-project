Deep semi-supervised learning (SSL) methods aim to take advantage of abundant unlabeled data to improve the algorithm performance. In this paper, we consider the prob-lem of safe SSL scenario where unseen-class instances ap-pear in the unlabeled data. This setting is essential and commonly appears in a variety of real applications. One intuitive solution is removing these unseen-class instances after detecting them during the SSL process. Neverthe-less, the performance of unseen-class identification is lim-ited by the small number of labeled data and ignoring the availability of unlabeled data. To take advantage of these unseen-class data and ensure performance, we pro-pose a safe SSL method called SAFE-STUDENT from the teacher-student view. Firstly, a new scoring function called energy-discrepancy (ED) is proposed to help the teacher model improve the security of instances selection. Then, a novel unseen-class label distribution learning mechanism mitigates the unseen-class perturbation by calibrating the unseen-class label distribution. Finally, we propose an iter-ative optimization strategy to facilitate teacher-student net-work learning. Extensive studies on several representa-tive datasets show that SAFE-STUDENT remarkably out-performs the state-of-the-art, verifying the feasibility and robustness of our method in the under-explored problem. 