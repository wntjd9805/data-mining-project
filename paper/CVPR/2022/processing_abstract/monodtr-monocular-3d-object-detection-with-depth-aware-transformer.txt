Monocular 3D object detection is an important yet chal-lenging task in autonomous driving. Some existing meth-ods leverage depth information from an off-the-shelf depth estimator to assist 3D detection, but suffer from the ad-ditional computational burden and achieve limited perfor-mance caused by inaccurate depth priors. To alleviate this, we propose MonoDTR, a novel end-to-end depth-aware transformer network for monocular 3D object detection. It mainly consists of two components: (1) the Depth-AwareFeature Enhancement (DFE) module that implicitly learns depth-aware features with auxiliary supervision without re-quiring extra computation, and (2) the Depth-Aware Trans-former (DTR) module that globally integrates context- and depth-aware features. Moreover, different from conven-tional pixel-wise positional encodings, we introduce a novel depth positional encoding (DPE) to inject depth positional hints into transformers. Our proposed depth-aware mod-ules can be easily plugged into existing image-only monocu-lar 3D object detectors to improve the performance. Exten-sive experiments on the KITTI dataset demonstrate that our approach outperforms previous state-of-the-art monocular-based methods and achieves real-time detection. Code is available at https:// github.com/ kuanchihhuang/ MonoDTR. 