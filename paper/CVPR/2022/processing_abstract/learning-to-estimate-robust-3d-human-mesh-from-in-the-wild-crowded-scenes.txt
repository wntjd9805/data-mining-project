We consider the problem of recovering a single person’s 3D human mesh from in-the-wild crowded scenes. While much progress has been in 3D human mesh estimation, ex-isting methods struggle when test input has crowded scenes.The first reason for the failure is a domain gap between training and testing data. A motion capture dataset, which provides accurate 3D labels for training, lacks crowd data and impedes a network from learning crowded scene-robust image features of a target person. The second reason is a feature processing that spatially averages the feature map of a localized bounding box containing multiple people. Aver-aging the whole feature map makes a target person’s feature indistinguishable from others. We present 3DCrowdNet that firstly explicitly targets in-the-wild crowded scenes and es-timates a robust 3D human mesh by addressing the above issues. First, we leverage 2D human pose estimation that does not require a motion capture dataset with 3D labels for training and does not suffer from the domain gap. Sec-ond, we propose a joint-based regressor that distinguishes a target person’s feature from others. Our joint-based regres-sor preserves the spatial activation of a target by sampling features from the target’s joint locations and regresses hu-man model parameters. As a result, 3DCrowdNet learns target-focused features and effectively excludes the irrele-vant features of nearby persons. We conduct experiments on various benchmarks and prove the robustness of 3DCrowd-Net to the in-the-wild crowded scenes both quantitatively and qualitatively. Codes are available here 1. 