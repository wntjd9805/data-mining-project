In this work, we introduce a novel strategy for long-tail recognition that addresses the tail classesâ€™ few-shot prob-lem via training-free knowledge transfer. Our objective is to transfer knowledge acquired from information-rich com-mon classes to semantically similar, and yet data-hungry, rare classes in order to obtain stronger tail class repre-sentations. We leverage the fact that class prototypes and learned cosine classifiers provide two different, comple-mentary representations of class cluster centres in feature space, and use an attention mechanism to select and re-compose learned classifier features from common classes to obtain higher quality rare class representations. Our knowledge transfer process is training free, reducing over-fitting risks, and can afford continual extension of classi-fiers to new classes. Experiments show that our approach can achieve significant performance boosts on rare classes while maintaining robust common class performance, out-performing directly comparable state-of-the-art models. 