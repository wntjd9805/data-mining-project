Profiting from large-scale training datasets, advances in neural architecture design and efficient inference, joint embeddings have become the dominant approach for tack-ling cross-modal retrieval.In this work we first show that, despite their effectiveness, state-of-the-art joint em-beddings suffer significantly from the longstanding “hub-ness problem” in which a small number of gallery embed-dings form the nearest neighbours of many queries. Draw-ing inspiration from the NLP literature, we formulate a simple but effective framework called Querybank Normal-isation (QB-NORM) that re-normalises query similarities to account for hubs in the embedding space. QB-NORM improves retrieval performance without requiring retrain-ing. Differently from prior work, we show that QB-NORM works effectively without concurrent access to any test set queries. Within the QB-NORM framework, we also pro-pose a novel similarity normalisation method, the DynamicInverted Softmax, that is significantly more robust than ex-isting approaches. We showcase QB-NORM across a range of cross modal retrieval models and benchmarks where it consistently enhances strong baselines beyond the state of the art. Code is available at https://vladbogo. github.io/QB-Norm/. 