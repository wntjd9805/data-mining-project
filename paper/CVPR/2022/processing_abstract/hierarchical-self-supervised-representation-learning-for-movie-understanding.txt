In contrast,Most self-supervised video representation learning ap-proaches focus on action recognition. in this paper we focus on self-supervised video learning for movie understanding and propose a novel hierarchical self-supervised pretraining strategy that separately pretrains each level of our hierarchical movie understanding model (based on [37]). Specifically, we propose to pretrain the low-level video backbone using a contrastive learning ob-jective, while pretrain the higher-level video contextualizer using an event mask prediction task, which enables the us-age of different data sources for pretraining different levels of the hierarchy. We first show that our self-supervised pre-training strategies are effective and lead to improved perfor-mance on all tasks and metrics on VidSitu benchmark [37] (e.g., improving on semantic role prediction from 47% to 61% CIDEr scores). We further demonstrate the effective-ness of our contextualized event features on LVU tasks [54], both when used alone and when combined with instance features, showing their complementarity. 