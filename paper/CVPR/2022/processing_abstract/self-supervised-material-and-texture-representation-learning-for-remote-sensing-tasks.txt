Self-supervised learning aims to learn image feature rep-resentations without the usage of manually annotated la-bels. It is often used as a precursor step to obtain useful ini-tial network weights which contribute to faster convergence and superior performance of downstream tasks. While self-supervision allows one to reduce the domain gap between supervised and unsupervised learning without the usage of labels, the self-supervised objective still requires a strong inductive bias to downstream tasks for effective transfer learning. In this work, we present our material and texture based self-supervision method named MATTER (MATerial and TExture Representation Learning), which is inspired by classical material and texture methods. Material and texture can effectively describe any surface, including its tactile properties, color, and specularity. By extension, ef-fective representation of material and texture can describe other semantic classes strongly associated with said mate-rial and texture. MATTER leverages multi-temporal, spa-tially aligned remote sensing imagery over unchanged re-gions to learn invariance to illumination and viewing angle as a mechanism to achieve consistency of material and tex-ture representation. We show that our self-supervision pre-training method allows for up to 24.22% and 6.33% per-formance increase in unsupervised and fine-tuned setups, and up to 76% faster convergence on change detection, land cover classification, and semantic segmentation tasks. Code and dataset: https://github.com/periakiva/MATTER. 