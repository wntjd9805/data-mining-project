Knowledge distillation (KD) has witnessed its powerful capability in learning compact models in object detection.Previous KD methods for object detection mostly focus on imitating deep features within the imitation regions instead of mimicking classification logit due to its inefficiency in distilling localization information and trivial improvement.In this paper, by reformulating the knowledge distillation process on localization, we present a novel localization dis-tillation (LD) method which can efficiently transfer the lo-calization knowledge from the teacher to the student. More-over, we also heuristically introduce the concept of valu-able localization region that can aid to selectively distill the semantic and localization knowledge for a certain region.Combining these two new components, for the first time, we show that logit mimicking can outperform feature imi-tation and localization knowledge distillation is more im-portant and efficient than semantic knowledge for distilling object detectors. Our distillation scheme is simple as well as effective and can be easily applied to different dense ob-ject detectors. Experiments show that our LD can boost the AP score of GFocal-ResNet-50 with a single-scale 1Ã— training schedule from 40.1 to 42.1 on the COCO bench-mark without any sacrifice on the inference speed. Our source code and pretrained models are publicly available at https://github.com/HikariTJU/LD. 