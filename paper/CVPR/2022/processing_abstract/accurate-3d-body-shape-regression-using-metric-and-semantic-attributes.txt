While methods that regress 3D human meshes from im-ages have progressed rapidly, the estimated body shapes of-ten do not capture the true human shape. This is problem-atic since, for many applications, accurate body shape is as important as pose. The key reason that body shape accuracy lags pose accuracy is the lack of data. While humans can la-bel 2D joints, and these constrain 3D pose, it is not so easy to “label” 3D body shape. Since paired data with images and 3D body shape are rare, we exploit two sources of infor-mation: (1) we collect internet images of diverse “fashion” models together with a small set of anthropometric mea-surements; (2) we collect linguistic shape attributes for a wide range of 3D body meshes and the model images. Taken together, these datasets provide sufficient constraints to in-fer dense 3D shape. We exploit the anthropometric mea-surements and linguistic shape attributes in several novel ways to train a neural network, called SHAPY, that re-gresses 3D human pose and shape from an RGB image.We evaluate SHAPY on public benchmarks, but note that they either lack significant body shape variation, ground-truth shape, or clothing variation. Thus, we collect a new dataset for evaluating 3D human shape estimation, calledHBW, containing photos of “Human Bodies in the Wild” for which we have ground-truth 3D body scans. On this new benchmark, SHAPY significantly outperforms state-of-the-art methods on the task of 3D body shape estimation.This is the first demonstration that 3D body shape regres-sion from images can be trained from easy-to-obtain an-thropometric measurements and linguistic shape attributes.Our model and data are available at: shapy.is.tue.mpg.de 