Deep neural networks (DNNs) have witnessed great suc-cesses in semantic segmentation, which requires a large number of labeled data for training. We present a novel learning framework called Uncertainty guided Cross-headCo-training (UCC) for semi-supervised semantic segmen-tation. Our framework introduces weak and strong aug-mentations within a shared encoder to achieve co-training, which naturally combines the benefits of consistency and self-training. Every segmentation head interacts with its peers and, the weak augmentation result is used for su-pervising the strong. The consistency training samplesâ€™ di-versity can be boosted by Dynamic Cross-Set Copy-Paste (DCSCP), which also alleviates the distribution mismatch and class imbalance problems. Moreover, our proposedUncertainty Guided Re-weight Module (UGRM) enhances the self-training pseudo labels by suppressing the effect of the low-quality pseudo labels from its peer via model-ing uncertainty. Extensive experiments on Cityscapes andPASCAL VOC 2012 demonstrate the effectiveness of ourUCC. Our approach significantly outperforms other state-of-the-art semi-supervised semantic segmentation methods.It achieves 77.17%, 76.49% mIoU on Cityscapes and PAS-CAL VOC 2012 datasets respectively under 1/16 proto-cols, which are +10.1%, +7.91% better than the supervised baseline. 