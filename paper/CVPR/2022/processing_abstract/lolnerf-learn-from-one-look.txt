We present a method for learning a generative 3D model based on neural radiance ﬁelds, trained solely from data with only single views of each object. While generating re-alistic images is no longer a difﬁcult task, producing the corresponding 3D structure such that they can be rendered from different views is non-trivial. We show that, unlike ex-isting methods, one does not need multi-view data to achieve this goal. Speciﬁcally, we show that by reconstructing many images aligned to an approximate canonical pose with a single network conditioned on a shared latent space, you can learn a space of radiance ﬁelds that models shape and appearance for a class of objects. We demonstrate this by training models to reconstruct object categories using datasets that contain only one view of each subject with-out depth or geometry information. Our experiments show that we achieve state-of-the-art results in novel view synthe-sis and high-quality results for monocular depth prediction. https://lolnerf.github.io 