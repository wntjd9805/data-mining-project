The target of automatic video summarization is to cre-ate a short skim of the original long video while preserv-ing the major content/events. There is a growing interest in the integration of user queries into video summarization or query-driven video summarization. This video summa-rization method predicts a concise synopsis of the original video based on the user query, which is commonly repre-sented by the input text. However, two inherent problems exist in this query-driven way. First, the text query might not be enough to describe the exact and diverse needs of the user. Second, the user cannot edit once the summaries are produced, while we assume the needs of the user should be subtle and need to be adjusted interactively. To solve these two problems, we propose IntentVizor, an interactive video summarization framework guided by generic multi-modality queries. The input query that describes the user’s needs are not limited to text but also the video snippets. We further represent these multi-modality ﬁner-grained queries as user ‘intent’, which is interpretable, interactable, ed-itable, and can better quantify the user’s needs. In this pa-per, we use a set of the proposed intents to represent the user query and design a new interactive visual analytic in-terface. Users can interactively control and adjust these mixed-initiative intents to obtain a more satisfying summary through the interface. Also, to improve the summariza-tion quality via video understanding, a novel Granularity-Scalable Ego-Graph Convolutional Networks (GSE-GCN) is proposed. We conduct our experiments on two benchmark datasets. Comparisons with the state-of-the-art methods verify the effectiveness of the proposed framework. Code and dataset are available at https://github.com/ jnzs1836/intent-vizor. 