Recent advances in contrastive learning have enlight-ened diverse applications across various semi-supervised fields. Jointly training supervised learning and unsuper-vised learning with a shared feature encoder becomes a common scheme. Though it benefits from taking advantage of both feature-dependent information from self-supervised learning and label-dependent information from supervised learning, this scheme remains suffering from bias of the classifier.In this work, we systematically explore the re-lationship between self-supervised learning and supervised learning, and study how self-supervised learning helps ro-bust data-efficient deep learning. We propose hyperspher-ical consistency regularization (HCR), a simple yet effec-tive plug-and-play method, to regularize the classifier us-ing feature-dependent information and thus avoid bias from labels. Specifically, HCR first project logits from the clas-sifier and feature projections from the projection head on the respective hypersphere, then it enforces data points on hyperspheres to have similar structures by minimizing bi-nary cross entropy of pairwise distancesâ€™ similarity met-rics. Extensive experiments on semi-supervised and weakly-supervised learning demonstrate the effectiveness of our method, by showing superior performance with HCR. 