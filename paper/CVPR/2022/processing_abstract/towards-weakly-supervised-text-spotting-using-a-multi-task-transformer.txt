Text spotting end-to-end methods have recently gained attention in the literature due to the benefits of jointly opti-mizing the text detection and recognition components. Ex-isting methods usually have a distinct separation between the detection and recognition branches, requiring exact an-notations for the two tasks. We introduce TextTranSpotter (TTS), a transformer-based approach for text spotting and the first text spotting framework which may be trained with both fully- and weakly-supervised settings. By learning a single latent representation per word detection, and using a novel loss function based on the Hungarian loss, our method alleviates the need for expensive localization an-notations. Trained with only text transcription annotations on real data, our weakly-supervised method achieves com-petitive performance with previous state-of-the-art fully-supervised methods. When trained in a fully-supervised manner, TextTranSpotter shows state-of-the-art results on multiple benchmarks. 