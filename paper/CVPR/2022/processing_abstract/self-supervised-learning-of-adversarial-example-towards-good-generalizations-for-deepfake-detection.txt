Recent studies in deepfake detection have yielded promising results when the training and testing face forg-eries are from the same dataset. However, the problem remains challenging when one tries to generalize the de-tector to forgeries created by unseen methods in the train-ing dataset. This work addresses the generalizable deep-fake detection from a simple principle: a generalizable representation should be sensitive to diverse types of forg-eries. Following this principle, we propose to enrich the“diversity” of forgeries by synthesizing augmented forg-eries with a pool of forgery configurations and strengthen the “sensitivity” to the forgeries by enforcing the model to predict the forgery configurations. To effectively ex-plore the large forgery augmentation space, we further pro-pose to use the adversarial training strategy to dynami-cally synthesize the most challenging forgeries to the cur-rent model. Through extensive experiments, we show that the proposed strategies are surprisingly effective (see Fig-ure 1), and they could achieve superior performance than the current state-of-the-art methods. Code is available at https://github.com/liangchen527/SLADD. 