Few-shot learning (FSL) is an important and topical prob-lem in computer vision that has motivated extensive research into numerous methods spanning from sophisticated meta-learning methods to simple transfer learning baselines. We seek to push the limits of a simple-but-effective pipeline for real-world few-shot image classification in practice. To this end, we explore few-shot learning from the perspective of neural architecture, as well as a three stage pipeline of pre-training on external data, meta-training with labelled few-shot tasks, and task-specific fine-tuning on unseen tasks.We investigate questions such as: 1 How pre-training on external data benefits FSL? 2 How state of the art trans-former architectures can be exploited? and 3 How to best exploit fine-tuning? Ultimately, we show that a sim-ple transformer-based pipeline yields surprisingly good per-formance on standard benchmarks such as Mini-ImageNet,CIFAR-FS, CDFSL and Meta-Dataset. Our code is available at https://hushell.github.io/pmf. 