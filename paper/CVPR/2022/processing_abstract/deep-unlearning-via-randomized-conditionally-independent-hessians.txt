Recent legislation has led to interest in machine un-learning, i.e., removing specific training samples from a predictive model as if they never existed in the training dataset. Unlearning may also be required due to cor-rupted/adversarial data or simply a userâ€™s updated pri-vacy requirement. For models which require no train-ing (k-NN), simply deleting the closest original sample can be effective. But this idea is inapplicable to models which learn richer representations. Recent ideas leveraging optimization-based updates scale poorly with the model di-mension d, due to inverting the Hessian of the loss function.We use a variant of a new conditional independence coeffi-cient, L-CODEC, to identify a subset of the model parame-ters with the most semantic overlap on an individual sample level. Our approach completely avoids the need to invert a (possibly) huge matrix. By utilizing a Markov blanket selec-tion, we premise that L-CODEC is also suitable for deep un-learning, as well as other applications in vision. Compared to alternatives, L-CODEC makes approximate unlearning possible in settings that would otherwise be infeasible, in-cluding vision models used for face recognition, person re-identification and NLP models that may require unlearn-ing samples identified for exclusion. Code is available at https://github.com/vsingh-group/LCODEC-deep-unlearning 