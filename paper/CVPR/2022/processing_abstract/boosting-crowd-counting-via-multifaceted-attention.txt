This paper focuses on the challenging crowd counting task. As large-scale variations often exist within crowd images, neither fixed-size convolution kernel of CNN nor fixed-size attention of recent vision transformers can well handle this kind of variations. To address this problem, we propose a Multifaceted Attention Network (MAN) to improve transformer models in local spatial relation en-coding. MAN incorporates global attention from vanilla transformer, learnable local attention, and instance atten-tion into a counting model. Firstly, the local Learn-able Region Attention (LRA) is proposed to assign atten-tion exclusive for each feature location dynamically. Sec-ondly, we design the Local Attention Regularization to supervise the training of LRA by minimizing the devia-tion among the attention for different feature locations.Finally, we provide an Instance Attention mechanism to focus on the most important instances dynamically dur-ing training. Extensive experiments on four challeng-ing crowd counting datasets namely ShanghaiTech, UCF-QNRF, JHU++, and NWPU have validated the proposed https://github.com/LoraLinH/Boosting-method.Crowd-Counting-via-Multifaceted-Attention.Code: 