Affordance grounding, a task to ground (i.e., localize) action possibility region in objects, which faces the chal-lenge of establishing an explicit link with object parts due to the diversity of interactive affordance. Human has the abil-ity that transform the various exocentric interactions to in-variant egocentric affordance so as to counter the impact of interactive diversity. To empower an agent with such ability, this paper proposes a task of affordance grounding from ex-ocentric view, i.e., given exocentric human-object interac-tion and egocentric object images, learning the affordance knowledge of the object and transferring it to the egocentric image using only the affordance label as supervision. To this end, we devise a cross-view knowledge transfer frame-work that extracts affordance-speciﬁc features from exocen-tric interactions and enhances the perception of affordance regions by preserving affordance correlation. Speciﬁcally, an Affordance Invariance Mining module is devised to ex-tract speciﬁc clues by minimizing the intra-class differences originated from interaction habits in exocentric images.Besides, an Affordance Co-relation Preserving strategy is presented to perceive and localize affordance by aligning the co-relation matrix of predicted results between the two views. Particularly, an affordance grounding dataset namedAGD20K is constructed by collecting and labeling over 20K images from 36 affordance categories. Experimental results demonstrate that our method outperforms the representa-tive models in terms of objective metrics and visual quality.Code: github.com/lhc1224/Cross-View-AG. 