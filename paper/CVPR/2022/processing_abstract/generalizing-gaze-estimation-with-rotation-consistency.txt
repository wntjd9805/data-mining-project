Recent advances of deep learning-based approaches have achieved remarkable performance on appearance-based gaze estimation. However, due to the shortage of target domain data and absence of target labels, general-izing gaze estimation algorithm to unseen environments is still challenging.In this paper, we discover the rotation-consistency property in gaze estimation and introduce the‘sub-label’ for unsupervised domain adaptation. Conse-quently, we propose the Rotation-enhanced UnsupervisedDomain Adaptation (RUDA) for gaze estimation. First, we rotate the original images with different angles for train-ing. Then we conduct domain adaptation under the con-straint of rotation consistency. The target domain images are assigned with sub-labels, derived from relative rotation angles rather than untouchable real labels. With such sub-labels, we propose a novel distribution loss that facilitates the domain adaptation. We evaluate the RUDA framework on four cross-domain gaze estimation tasks. Experimental results demonstrate that it improves the performance over the baselines with gains ranging from 12.2% to 30.5%. Our framework has the potential to be used in other computer vision tasks with physical constraints. 