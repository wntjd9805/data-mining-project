In this paper, we take an early step towards video repre-sentation learning of human actions with the help of large-scale synthetic videos, particularly for human motion rep-resentation enhancement. Specifically, we first introduce an automatic action-related video synthesis pipeline based on a photorealistic video game. A large-scale human ac-tion dataset named GATA (GTA Animation TransformedActions) is then built by the proposed pipeline, which in-cludes 8.1 million action clips spanning over 28K ac-tion classes. Based on the presented dataset, we design a contrastive learning framework for human motion rep-resentation learning, which shows significant performance improvements on several typical video datasets for ac-tion recognition, e.g., Charades, HAA 500 and NTU-RGB.Besides, we further explore a domain adaptation method based on cross-domain positive pairs mining to alleviate the domain gap between synthetic and realistic data. Extensive properties analyses of learned representation are conducted to demonstrate the effectiveness of the proposed dataset for enhancing human motion representation learning. 