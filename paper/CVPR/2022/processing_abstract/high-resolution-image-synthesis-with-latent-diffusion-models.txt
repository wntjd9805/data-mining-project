Input ours (f = 4)PSNR: 27.4 R-FID: 0.58DALL-E (f = 8)PSNR: 22.8 R-FID: 32.01VQGAN (f = 16)PSNR: 19.9 R-FID: 4.98By decomposing the image formation process into a se-quential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation al-lows for a guiding mechanism to control the image gen-eration process without retraining. However, since these models typically operate directly in pixel space, optimiza-tion of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evalu-ations. To enable DM training on limited computational resources while retaining their quality and ﬂexibility, we apply them in the latent space of powerful pretrained au-toencoders. In contrast to previous work, training diffusion models on such a representation allows for the ﬁrst time to reach a near-optimal point between complexity reduc-tion and detail preservation, greatly boosting visual ﬁdelity.By introducing cross-attention layers into the model archi-tecture, we turn diffusion models into powerful and ﬂexi-ble generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for im-age inpainting and class-conditional image synthesis and highly competitive performance on various tasks, includ-ing unconditional image generation, text-to-image synthe-sis, and super-resolution, while signiﬁcantly reducing com-putational requirements compared to pixel-based DMs. 