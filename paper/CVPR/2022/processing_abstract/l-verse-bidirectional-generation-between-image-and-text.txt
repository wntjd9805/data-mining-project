Far beyond learning long-range interactions of natural language, transformers are becoming the de-facto standard for many vision tasks with their power and scalability. Es-pecially with cross-modal tasks between image and text, vector quantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB image into a sequence of feature vectors. To better leverage the correlation be-tween image and text, we propose L-Verse, a novel archi-tecture consisting of feature-augmented variational autoen-coder (AugVAE) and bidirectional auto-regressive trans-former (BiART) for image-to-text and text-to-image gener-ation. Our AugVAE shows the state-of-the-art reconstruc-tion performance on ImageNet1K validation set, along with the robustness to unseen images in the wild. Unlike other models, BiART can distinguish between image (or text) as a conditional reference and a generation target. L-Verse can be directly used for image-to-text or text-to-image genera-tion without any finetuning or extra object detection frame-work. In quantitative and qualitative experiments, L-Verse shows impressive results against previous methods in both image-to-text and text-to-image generation on MS-COCOCaptions. We furthermore assess the scalability of L-Verse architecture on Conceptual Captions and present the ini-tial result of bidirectional vision-language representation learning on general domain. 