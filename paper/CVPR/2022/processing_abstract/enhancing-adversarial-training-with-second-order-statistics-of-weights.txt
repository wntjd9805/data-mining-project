Adversarial training has been shown to be one of the most effective approaches to improve the robustness of deep neural networks. It is formalized as a min-max optimiza-tion over model weights and adversarial perturbations, where the weights can be optimized through gradient de-scent methods like SGD. In this paper, we show that treat-ing model weights as random variables allows for enhanc-ing adversarial training through Second-Order StatisticsOptimization (S2O) with respect to the weights. By re-laxing a common (but unrealistic) assumption of previousPAC-Bayesian frameworks that all weights are statistically independent, we derive an improved PAC-Bayesian adver-sarial generalization bound, which suggests that optimizing second-order statistics of weights can effectively tighten the bound. In addition to this theoretical insight, we conduct an extensive set of experiments, which show that S2O not only improves the robustness and generalization of the trained neural networks when used in isolation, but also integrates easily in state-of-the-art adversarial training techniques like TRADES, AWP, MART, and AVMixup, leading to a mea-surable improvement of these techniques. The code is avail-able at https://github.com/Alexkael/S2O. 