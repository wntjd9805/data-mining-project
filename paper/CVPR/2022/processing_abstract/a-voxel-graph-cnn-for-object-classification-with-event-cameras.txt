Event cameras attract researchersâ€™ attention due to their low power consumption, high dynamic range, and extremely high temporal resolution. Learning models on event-based object classification have recently achieved massive success by accumulating sparse events into dense frames to apply traditional 2D learning methods. Yet, these approaches ne-cessitate heavy-weight models and are with high computa-tional complexity due to the redundant information intro-duced by the sparse-to-dense conversion, limiting the po-tential of event cameras on real-life applications. This study aims to address the core problem of balancing accuracy and model complexity for event-based classification mod-els. To this end, we introduce a novel graph representa-tion for event data to exploit their sparsity better and cus-tomize a lightweight voxel graph convolutional neural net-work (EV-VGCNN) for event-based classification. Specif-ically, (1) using voxel-wise vertices rather than previous point-wise inputs to explicitly exploit regional 2D seman-tics of event streams while keeping the sparsity; (2) propos-ing a multi-scale feature relational layer (MFRL) to extract spatial and motion cues from each vertex discriminatively concerning its distances to neighbors. Comprehensive ex-periments show that our model can advance state-of-the-art classification accuracy with extremely low model complex-ity (merely 0.84M parameters). 