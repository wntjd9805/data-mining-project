Magnetic resonance imaging (MRI) can present multi-contrast images of the same anatomical structures, enabling multi-contrast super-resolution (SR) techniques. Com-pared with SR reconstruction using a single-contrast, multi-contrast SR reconstruction is promising to yield SR images with higher quality by leveraging diverse yet complemen-tary information embedded in different imaging modali-ties. However, existing methods still have two shortcom-ings: (1) they neglect that the multi-contrast features at dif-ferent scales contain different anatomical details and hence lack effective mechanisms to match and fuse these features for better reconstruction; and (2) they are still deficient in capturing long-range dependencies, which are essential for the regions with complicated anatomical structures. We propose a novel network to comprehensively address these problems by developing a set of innovative Transformer-empowered multi-scale contextual matching and aggrega-tion techniques; we call it McMRSR. Firstly, we tame trans-formers to model long-range dependencies in both refer-ence and target images. Then, a new multi-scale contex-tual matching method is proposed to capture corresponding contexts from reference features at different scales. Further-more, we introduce a multi-scale aggregation mechanism to gradually and interactively aggregate multi-scale matched features for reconstructing the target SR MR image. Ex-tensive experiments demonstrate that our network outper-forms state-of-the-art approaches and has great potential to be applied in clinical practice. Codes are available at https://github.com/XAIMI-Lab/McMRSR.*Corresponding author. (a) Crop area (b) Bicubic (c) MCSR [20] (d) MINet [8] (e) McMRSR (Ours) (f) HRFigure 1. Compared with state-of-the-art multi-constrast MRISR reconstruction methods: MCSR and MINet; the reconstructedMRI image by our McMRSR network contains sharper edges, more visual details, and fewer blurring artifacts. 