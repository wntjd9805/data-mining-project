Quantization is an efficient network compression ap-proach to reduce the inference time. However, existing ap-proaches ignored the distribution difference between train-ing and testing data, thereby inducing a large quantization error in inference. To address this issue, we propose a new quantization scheme, Alignment Quantization with ADMM-based Correlation Preservation (AlignQ), which exploits the cumulative distribution function (CDF) to align the data to be i.i.d. (independently and identically distributed) for quantization error minimization. Afterward, our theoreti-cal analysis indicates that the significant changes in data correlations after the quantization induce a large quanti-zation error. Accordingly, we aim to preserve the relation-ship of data from the original space to the aligned quanti-zation space for retaining the prediction information. We design an optimization process by leveraging the Alter-nating Direction Method of Multipliers (ADMM) optimiza-tion to minimize the differences in data correlations before and after the alignment and quantization. In experiments, we visualize non-i.i.d. in training and testing data in the benchmark. We further adopt domain shift data to compareAlignQ with the state-of-the-art. Experimental results show that AlignQ achieves significant performance improvements especially in low-bit models. Code is available at https://github.com/tinganchen/AlignQ.git. 