Current adversarial attack research reveals the vulnera-bility of learning-based classifiers against carefully crafted perturbations. However, most existing attack methods have inherent limitations in cross-dataset generalization as they rely on a classification layer with a closed set of categories.Furthermore, the perturbations generated by these meth-ods may appear in regions easily perceptible to the hu-man visual system (HVS). To circumvent the former prob-lem, we propose a novel algorithm that attacks semantic similarity on feature representations. In this way, we are able to fool classifiers without limiting attacks to a spe-cific dataset. For imperceptibility, we introduce the low-frequency constraint to limit perturbations within high-frequency components, ensuring perceptual similarity be-tween adversarial examples and originals. Extensive ex-periments on three datasets (CIFAR-10, CIFAR-100, andImageNet-1K) and three public online platforms indicate that our attack can yield misleading and transferable ad-versarial examples across architectures and datasets. Ad-ditionally, visualization results and quantitative perfor-mance (in terms of four different metrics) show that the proposed algorithm generates more imperceptible pertur-bations than the state-of-the-art methods. Code is made available at https://github.com/LinQinLiang/SSAH-adversarial-attack. 