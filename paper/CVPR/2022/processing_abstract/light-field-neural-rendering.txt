Classical light ﬁeld rendering for novel view synthesis can accurately reproduce view-dependent effects such as re-ﬂection, refraction, and translucency, but requires a dense view sampling of the scene. Methods based on geometric re-construction need only sparse views, but cannot accurately model non-Lambertian effects. We introduce a model that combines the strengths and mitigates the limitations of these two directions. By operating on a four-dimensional repre-sentation of the light ﬁeld, our model learns to represent view-dependent effects accurately. By enforcing geometric constraints during training and inference, the scene geom-etry is implicitly learned from a sparse set of views. Con-cretely, we introduce a two-stage transformer-based model that ﬁrst aggregates features along epipolar lines, then aggregates features along reference views to produce the color of a target ray. Our model outperforms the state-of-the-art on multiple forward-facing and 360◦ datasets, with larger margins on scenes with severe view-dependent vari-ations. Code and results can be found at light-ﬁeld-neural-rendering.github.io. 