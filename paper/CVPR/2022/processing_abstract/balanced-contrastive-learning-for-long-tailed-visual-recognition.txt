Real-world data typically follow a long-tailed distribu-tion, where a few majority categories occupy most of the data while most minority categories contain a limited num-ber of samples. Classification models minimizing cross-entropy struggle to represent and classify the tail classes.Although the problem of learning unbiased classifiers has been well studied, methods for representing imbalanced data are under-explored.In this paper, we focus on rep-resentation learning for imbalanced data. Recently, su-pervised contrastive learning has shown promising perfor-mance on balanced data recently. However, through our theoretical analysis, we find that for long-tailed data, it fails to form a regular simplex which is an ideal geomet-ric configuration for representation learning. To correct the optimization behavior of SCL and further improve the per-formance of long-tailed visual recognition, we propose a novel loss for balanced contrastive learning (BCL). Com-pared with SCL, we have two improvements in BCL: class-averaging, which balances the gradient contribution of neg-ative classes; class-complement, which allows all classes to appear in every mini-batch. The proposed balanced contrastive learning (BCL) method satisfies the condition of forming a regular simplex and assists the optimization of cross-entropy. Equipped with BCL, the proposed two-branch framework can obtain a stronger feature represen-tation and achieve competitive performance on long-tailed benchmark datasets such as CIFAR-10-LT, CIFAR-100-LT,ImageNet-LT, and iNaturalist2018. 