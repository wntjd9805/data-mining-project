Label noise has been a practical challenge in deep learn-ing due to the strong capability of deep neural networks inﬁtting all training data. Prior literature primarily resorts to sample selection methods for combating noisy labels. How-ever, these approaches focus on dividing samples by order sorting or threshold selection, inevitably introducing hyper-parameters (e.g., selection ratio / threshold) that are hard-to-tune and dataset-dependent. To this end, we propose a simple yet effective approach named PNP (ProbabilisticNoise Prediction) to explicitly model label noise. Speciﬁ-cally, we simultaneously train two networks, in which one predicts the category label and the other predicts the noise type. By predicting label noise probabilistically, we iden-tify noisy samples and adopt dedicated optimization objec-tives accordingly. Finally, we establish a joint loss for net-work update by unifying the classiﬁcation loss, the auxil-iary constraint loss, and the in-distribution consistency loss.Comprehensive experimental results on synthetic and real-world datasets demonstrate the superiority of our proposed method. The source code and models have been made avail-able at https://github.com/NUST- Machine-Intelligence-Laboratory/PNP. 