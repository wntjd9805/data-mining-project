Large-scale multi-label classification datasets are com-monly, and perhaps inevitably, partially annotated. That is, only a small subset of labels are annotated per sam-ple. Different methods for handling the missing labels in-duce different properties on the model and impact its ac-curacy. In this work, we analyze the partial labeling prob-lem, then propose a solution based on two key ideas. First, un-annotated labels should be treated selectively accord-ing to two probability quantities: the class distribution in the overall dataset and the specific label likelihood for a given data sample. We propose to estimate the class distri-bution using a dedicated temporary model, and we show its improved efficiency over a na¨ıve estimation computed us-ing the dataset’s partial annotations. Second, during the training of the target model, we emphasize the contribu-tion of annotated labels over originally un-annotated labels by using a dedicated asymmetric loss. With our novel ap-proach, we achieve state-of-the-art results on OpenImages dataset (e.g. reaching 87.3 mAP on V6). In addition, ex-periments conducted on LVIS and simulated-COCO demon-strate the effectiveness of our approach. Code is available at https://github.com/Alibaba-MIIL/PartialLabelingCSL. 