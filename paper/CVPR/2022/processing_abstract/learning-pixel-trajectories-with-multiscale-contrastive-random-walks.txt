A range of video modeling tasks, from optical ﬂow to multiple object tracking, share the same fundamental chal-lenge: establishing space-time correspondence. Yet, ap-proaches that dominate each space differ. We take a step to-wards bridging this gap by extending the recent contrastive random walk formulation to much denser, pixel-level space-time graphs. The main contribution is introducing hier-archy into the search problem by computing the transi-tion matrix between two frames in a coarse-to-ﬁne man-ner, forming a multiscale contrastive random walk when ex-tended in time. This establishes a uniﬁed technique for self-supervised learning of optical ﬂow, keypoint tracking, and video object segmentation. Experiments demonstrate that, for each of these tasks, the uniﬁed model achieves perfor-mance competitive with strong self-supervised approaches speciﬁc to that task.1 