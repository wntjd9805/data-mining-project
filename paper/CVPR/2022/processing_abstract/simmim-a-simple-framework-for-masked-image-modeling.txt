This paper presents SimMIM, a simple framework for masked image modeling. We have simplified recently pro-posed relevant approaches, without the need for special designs, such as block-wise masking and tokenization via discrete VAE or clustering. To investigate what makes a masked image modeling task learn good representations, we systematically study the major components in our frame-work, and find that the simple designs of each component have revealed very strong representation learning perfor-mance: 1) random masking of the input image with a mod-erately large masked patch size (e.g., 32) makes a pow-erful pre-text task; 2) predicting RGB values of raw pix-els by direct regression performs no worse than the patch classification approaches with complex designs; 3) the pre-diction head can be as light as a linear layer, with no worse performance than heavier ones. Using ViT-B, our approach achieves 83.8% top-1 fine-tuning accuracy onImageNet-1K by pre-training also on this dataset, surpass-ing previous best approach by +0.6%. When applied to a larger model with about 650 million parameters, SwinV2-H, it achieves 87.1% top-1 accuracy on ImageNet-1K using only ImageNet-1K data. We also leverage this approach to address the data-hungry issue faced by large-scale model training, that a 3B model (SwinV2-G) is successfully trained to achieve state-of-the-art accuracy on four representative vision benchmarks using 40Ã— less labelled data than that in previous practice (JFT-3B). The code is available at https://github.com/microsoft/SimMIM . 