Hair editing is an interesting and challenging problem in computer vision and graphics. Many existing methods require well-drawn sketches or masks as conditional inputs for editing, however these interactions are neither straight-forward nor efficient.In order to free users from the te-dious interaction process, this paper proposes a new hair editing interaction mode, which enables manipulating hair attributes individually or jointly based on the texts or ref-erence images provided by users. For this purpose, we encode the image and text conditions in a shared embed-ding space and propose a unified hair editing framework by leveraging the powerful image text representation ca-pability of the Contrastive Language-Image Pre-Training (CLIP) model. With the carefully designed network struc-tures and loss functions, our framework can perform high-quality hair editing in a disentangled manner. Extensive experiments demonstrate the superiority of our approach in terms of manipulation accuracy, visual realism of editing results, and irrelevant attribute preservation. 