Recent neural human representations can produce high-quality multi-view rendering but require using dense multi-view inputs and costly training. They are hence largely lim-ited to static models as training each frame is infeasible.We present HumanNeRF - a neural representation with effi-cient generalization ability - for high-fidelity free-view syn-thesis of dynamic humans. Analogous to how IBRNet assistsNeRF by avoiding per-scene training, HumanNeRF em-ploys an aggregated pixel-alignment feature across multi-view inputs along with a pose embedded non-rigid defor-mation field for tackling dynamic motions. The raw Human-NeRF can already produce reasonable rendering on sparse video inputs of unseen subjects and camera settings. To fur-ther improve the rendering quality, we augment our solu-tion with in-hour scene-specific fine-tuning, and an appear-ance blending module for combining the benefits of both neural volumetric rendering and neural texture blending.Extensive experiments on various multi-view dynamic hu-man datasets demonstrate effectiveness of our approach in synthesizing photo-realistic free-view humans under chal-lenging motions and with very sparse camera view inputs. 