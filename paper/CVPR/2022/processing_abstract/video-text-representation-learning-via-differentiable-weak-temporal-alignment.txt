Learning generic joint representations for video and text by a supervised method requires a prohibitively substan-tial amount of manually annotated video datasets. As a practical alternative, a large-scale but uncurated and nar-rated video dataset, HowTo100M, has recently been intro-duced. But it is still challenging to learn joint embed-dings of video and text in a self-supervised manner, due to its ambiguity and non-sequential alignment. In this paper, we propose a novel multi-modal self-supervised frameworkVideo-Text Temporally Weak Alignment-based ContrastiveLearning (VT-TWINS) to capture signiﬁcant information from noisy and weakly correlated data using a variant ofDynamic Time Warping (DTW). We observe that the stan-dard DTW inherently cannot handle weakly correlated data and only considers the globally optimal alignment path. To address these problems, we develop a differentiable DTW which also reﬂects local information with weak temporal alignment. Moreover, our proposed model applies a con-trastive learning scheme to learn feature representations on weakly correlated data. Our extensive experiments demon-strate that VT-TWINS attains signiﬁcant improvements in multi-modal representation learning and outperforms var-ious challenging downstream tasks. Code is available at https://github.com/mlvlab/VT-TWINS. 