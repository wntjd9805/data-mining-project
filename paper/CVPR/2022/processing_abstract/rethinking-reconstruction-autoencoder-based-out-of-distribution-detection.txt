In some scenarios, classifier requires detecting out-of-distribution samples far from its training data. With de-sirable characteristics, reconstruction autoencoder-based methods deal with this problem by using input reconstruc-tion error as a metric of novelty vs. normality. We for-mulate the essence of such approach as a quadruplet do-main translation with an intrinsic bias to only query for a proxy of conditional data uncertainty. Accordingly, an im-provement direction is formalized as maximumly compress-ing the autoencoderâ€™s latent space while ensuring its recon-structive power for acting as a described domain transla-tor. From it, strategies are introduced including semantic reconstruction, data certainty decomposition and normal-ized L2 distance to substantially improve original meth-ods, which together establish state-of-the-art performance on various benchmarks, e.g., the FPR@95%TPR of CIFAR-100 vs. TinyImagenet-crop on Wide-ResNet is 0.2%. Impor-tantly, our method works without any additional data, hard-to-implement structure, time-consuming pipeline, and even harming the classification accuracy of known classes. 