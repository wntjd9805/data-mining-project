Amodal completion is a visual task that humans per-form easily but which is difficult for computer vision al-gorithms. The aim is to segment those object boundaries which are occluded and hence invisible. This task is partic-ularly challenging for deep neural networks because data is difficult to obtain and annotate. Therefore, we formu-late amodal segmentation as an out-of-task and out-of-distribution generalization problem. Specifically, we re-place the fully connected classifier in neural networks with a Bayesian generative model of the neural network fea-tures. The model is trained from non-occluded images us-ing bounding box annotations and class labels only, but is applied to generalize out-of-task to object segmentation and to generalize out-of-distribution to segment occluded objects. We demonstrate how such Bayesian models can naturally generalize beyond the training task labels when they learn a prior that models the objectâ€™s background con-text and shape. Moreover, by leveraging an outlier process,Bayesian models can further generalize out-of-distribution to segment partially occluded objects and to predict their amodal object boundaries. Our algorithm outperforms al-ternative methods that use the same supervision by a large margin, and even outperforms methods where annotated amodal segmentations are used during training, when the amount of occlusion is large. Code is publicly available at https://github.com/YihongSun/Bayesian-Amodal. 