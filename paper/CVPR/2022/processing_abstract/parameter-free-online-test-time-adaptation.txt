Training state-of-the-art vision models has become pro-hibitively expensive for researchers and practitioners. For the sake of accessibility and resource reuse, it is important to focus on adapting these models to a variety of down-stream scenarios. An interesting and practical paradigm is online test-time adaptation, according to which training data is inaccessible, no labelled data from the test distri-bution is available, and adaptation can only happen at test time and on a handful of samples. In this paper, we inves-tigate how test-time adaptation methods fare for a number of pre-trained models on a variety of real-world scenarios, signiﬁcantly extending the way they have been originally evaluated. We show that they perform well only in narrowly-deﬁned experimental setups and sometimes fail catastroph-ically when their hyperparameters are not selected for the same scenario in which they are being tested. Motivated by the inherent uncertainty around the conditions that will ultimately be encountered at test time, we propose a partic-ularly “conservative” approach, which addresses the prob-lem with a Laplacian Adjusted Maximum-likelihood Esti-mation (LAME) objective. By adapting the model’s out-put (not its parameters), and solving our objective with an efﬁcient concave-convex procedure, our approach ex-hibits a much higher average accuracy across scenarios than existing methods, while being notably faster and have a much lower memory footprint. The code is available at https://github.com/fiveai/LAME. 