Obtaining high-quality 3D reconstructions of room-scale scenes is of paramount importance for upcoming ap-plications in AR or VR. These range from mixed reality applications for teleconferencing, virtual measuring, vir-tual room planing, to robotic applications. While current volume-based view synthesis methods that use neural radi-ance fields (NeRFs) show promising results in reproducing the appearance of an object or scene, they do not recon-struct an actual surface. The volumetric representation of the surface based on densities leads to artifacts when a sur-face is extracted using Marching Cubes, since during opti-mization, densities are accumulated along the ray and are not used at a single sample point in isolation. Instead of this volumetric representation of the surface, we propose to represent the surface using an implicit function (truncated signed distance function). We show how to incorporate this representation in the NeRF framework, and extend it to use depth measurements from a commodity RGB-D sensor, such as a Kinect. In addition, we propose a pose and camera re-finement technique which improves the overall reconstruc-tion quality.In contrast to concurrent work on integrat-ing depth priors in NeRF which concentrates on novel view synthesis, our approach is able to reconstruct high-quality, metrical 3D reconstructions. 