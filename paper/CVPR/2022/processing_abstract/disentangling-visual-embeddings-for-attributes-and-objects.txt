We study the problem of compositional zero-shot learn-ing for object-attribute recognition. Prior works use visual features extracted with a backbone network, pre-trained for object classification and thus do not capture the subtly dis-tinct features associated with attributes. To overcome this challenge, these studies employ supervision from the lin-guistic space, and use pre-trained word embeddings to bet-ter separate and compose attribute-object pairs for recog-nition. Analogous to linguistic embedding space, which al-ready has unique and agnostic embeddings for object and attribute, we shift the focus back to the visual space and pro-pose a novel architecture that can disentangle attribute and object features in the visual space. We use visual decom-posed features to hallucinate embeddings that are represen-tative for the seen and novel compositions to better regular-ize the learning of our model. Extensive experiments show that our method outperforms existing work with significant margin on three datasets: MIT-States, UT-Zappos, and a new benchmark created based on VAW. The code, mod-els, and dataset splits are publicly available at https://github.com/nirat1606/OADis. 