Modern self-supervised learning algorithms typically enforce persistency of instance representations across views. While being very effective on learning holistic image and video representations, such an objective becomes sub-optimal for learning spatio-temporally ﬁne-grained fea-tures in videos, where scenes and instances evolve through space and time. In this paper, we present ContextualizedSpatio-Temporal Contrastive Learning (ConST-CL) to ef-fectively learn spatio-temporally ﬁne-grained video repre-sentations via self-supervision. We ﬁrst design a region-based pretext task which requires the model to transform in-stance representations from one view to another, guided by context features. Further, we introduce a simple network de-sign that successfully reconciles the simultaneous learning process of both holistic and local representations. We evalu-ate our learned representations on a variety of downstream tasks and show that ConST-CL achieves competitive re-sults on 6 datasets, including Kinetics, UCF, HMDB, AVA-Kinetics, AVA and OTB. Our code and models will be avail-able at https://github.com/tensorflow/models/ tree/master/official/projects/const_cl. 