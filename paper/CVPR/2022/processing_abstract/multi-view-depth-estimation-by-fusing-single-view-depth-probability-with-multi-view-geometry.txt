Multi-view depth estimation methods typically require the computation of a multi-view cost-volume, which leads to huge memory consumption and slow inference. Fur-thermore, multi-view matching can fail for texture-less sur-faces, reflective surfaces and moving objects. For such fail-ure modes, single-view depth estimation methods are of-ten more reliable. To this end, we propose MaGNet, a novel framework for fusing single-view depth probability with multi-view geometry, to improve the accuracy, robust-ness and efficiency of multi-view depth estimation. For each frame, MaGNet estimates a single-view depth probability distribution, parameterized as a pixel-wise Gaussian. The distribution estimated for the reference frame is then used to sample per-pixel depth candidates. Such probabilistic sampling enables the network to achieve higher accuracy while evaluating fewer depth candidates. We also pro-pose depth consistency weighting for the multi-view match-ing score, to ensure that the multi-view depth is consis-tent with the single-view predictions. The proposed method achieves state-of-the-art performance on ScanNet [8], 7-Scenes [38] and KITTI [15]. Qualitative evaluation demon-strates that our method is more robust against challenging artifacts such as texture-less/reflective surfaces and mov-ing objects. Our code and model weights are available at https://github.com/baegwangbin/MaGNet. 