This work digs into a root question in human percep-tion: can face geometry be gleaned from one’s voices? Pre-vious works that study this question only adopt develop-ments in image synthesis and convert voices into face im-ages to show correlations, but working on the image do-main unavoidably involves predicting attributes that voices cannot hint, including facial textures, hairstyles, and back-grounds. We instead investigate the ability to reconstruct 3D faces to concentrate on only geometry, which is much more physiologically grounded. We propose our analy-sis framework, Cross-Modal Perceptionist, under both su-pervised and unsupervised learning. First, we construct a dataset, Voxceleb-3D, which extends Voxceleb and includes paired voices and face meshes, making supervised learning possible. Second, we use a knowledge distillation mecha-nism to study whether face geometry can still be gleaned from voices without paired voices and 3D face data under limited availability of 3D face scans. We break down the core question into four parts and perform visual and nu-merical analyses as responses to the core question. Ourﬁndings echo those in physiology and neuroscience about the correlation between voices and facial structures. The work provides future human-centric cross-modal learning with explainable foundations. See our project page. 