It is a mystery which input features contribute to a neu-ral networkâ€™s output. Various explanation (feature attribu-tion) methods are proposed in the literature to shed light on the problem. One peculiar observation is that these ex-planations (attributions) point to different features as being important. The phenomenon raises the question, which ex-planation to trust? We propose a framework for evaluating the explanations using the neural network model itself. The framework leverages the network to generate input features that impose a particular behavior on the output. Using the generated features, we devise controlled experimental se-tups to evaluate whether an explanation method conforms to an axiom. Thus we propose an empirical framework for axiomatic evaluation of explanation methods. We evaluate well-known and promising explanation solutions using the proposed framework. The framework provides a toolset to reveal properties and drawbacks within existing and future explanation solutions.1 