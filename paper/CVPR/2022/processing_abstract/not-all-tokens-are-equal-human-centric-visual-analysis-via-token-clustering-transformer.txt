Vision transformers have achieved great successes in many computer vision tasks. Most methods generate vi-sion tokens by splitting an image into a regular and fixed grid and treating each cell as a token. However, not all re-gions are equally important in human-centric vision tasks, e.g., the human body needs a fine representation with many tokens, while the image background can be modeled by a few tokens. To address this problem, we propose a novelVision Transformer, called Token Clustering Transformer (TCFormer), which merges tokens by progressive cluster-ing, where the tokens can be merged from different locations with flexible shapes and sizes. The tokens in TCFormer can not only focus on important areas but also adjust the token shapes to fit the semantic concept and adopt a fine resolution for regions containing critical details, which is beneficial to capturing detailed information. Extensive ex-periments show that TCFormer consistently outperforms its counterparts on different challenging human-centric tasks and datasets, including whole-body pose estimation onCOCO-WholeBody and 3D human mesh reconstruction on 3DPW. Code is available at https://github.com/ zengwang430521/TCFormer.git.Figure 1. Comparisons between vision tokens generated by (a) standard grids and (b) TCFormer. The token regions of different tokens, or the image regions represented by vision tokens, are visu-alized by different colors. From left to right, different images rep-resent different stages. Grid-based tokens treat all regions equally as shown in (a). While the tokens in (b) treat image regions dy-namically. Tokens distribute more densely on the human body. For background regions, a large area is represented by a single token (in blue), while for the regions containing important details, such as the face area, tokens with fine spatial sizes are used (in red). 