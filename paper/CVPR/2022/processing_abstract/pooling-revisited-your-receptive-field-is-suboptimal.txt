The size and shape of the receptive ﬁeld determine how the network aggregates local features, and affect the over-all performance of a model considerably. Many compo-nents in a neural network, such as depth, kernel sizes, and strides for convolution and pooling, inﬂuence the recep-tive ﬁeld. However, they still rely on hyperparameters, and the receptive ﬁelds of existing models result in suboptimal shapes and sizes. Hence, we propose a simple yet effectiveDynamically Optimized Pooling operation, referred to asDynOPool, which learns the optimized scale factors of fea-ture maps end-to-end. Moreover, DynOPool determines the proper resolution of a feature map by learning the desirable size and shape of its receptive ﬁeld, which allows an oper-ator in a deeper layer to observe an input image in the op-timal scale. Any kind of resizing modules in a deep neural network can be replaced by DynOPool with minimal cost.Also, DynOPool controls the complexity of the model by in-troducing an additional loss term that constrains computa-tional cost. Our experiments show that the models equipped with the proposed learnable resizing module outperform the baseline algorithms on multiple datasets in image classiﬁ-cation and semantic segmentation. 