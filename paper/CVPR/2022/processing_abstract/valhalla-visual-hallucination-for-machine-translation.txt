Designing better machine translation systems by consid-ering auxiliary inputs such as images has attracted much attention in recent years. While existing methods show promising performance over the conventional text-only trans-lation systems, they typically require paired text and image as input during inference, which limits their applicability to real-world scenarios. In this paper, we introduce a vi-sual hallucination framework, called VALHALLA, which requires only source sentences at inference time and in-stead uses hallucinated visual representations for multi-modal machine translation. In particular, given a source sentence an autoregressive hallucination transformer is used to predict a discrete visual representation from the input text, and the combined text and hallucinated repre-sentations are utilized to obtain the target translation. We train the hallucination transformer jointly with the trans-lation transformer using standard backpropagation with cross-entropy losses while being guided by an additional loss that encourages consistency between predictions us-ing either ground-truth or hallucinated visual representa-tions. Extensive experiments on three standard translation datasets with a diverse set of language pairs demonstrate the effectiveness of our approach over both text-only base-lines and state-of-the-art methods. Project page: http://www.svcl.ucsd.edu/projects/valhalla. 