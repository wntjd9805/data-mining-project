Visual degradations caused by motion blur, raindrop, rain, snow, illumination, and fog deteriorate image qual-ity and, subsequently, the performance of perception algo-rithms deployed in outdoor conditions. While degradation-specific image restoration techniques have been extensively studied, such algorithms are domain sensitive and fail in real scenarios where multiple degradations exist simulta-neously. This makes a case for blind image restoration and reconstruction algorithms as practically relevant. However, the absence of a dataset diverse enough to encapsulate all variations hinders development for such an algorithm. In this paper, we utilize a synthetic degradation model that recursively applies sets of random degradations to gener-ate naturalistic degradation images of varying complexity, which are used as input. Furthermore, as the degradation intensity can vary across an image, the spatially invariant convolutional filter cannot be applied for all degradations.Hence to enable spatial variance during image restoration and reconstruction, we design a transformer-based archi-tecture to benefit from the long-range dependencies. In ad-dition, to reduce the computational cost of transformers, we propose a multi-branch structure coupled with modifi-cations such as a complimentary feature selection mecha-nism and the replacement of a feed-forward network with lightweight multiscale convolutions. Finally, to improve restoration and reconstruction, we integrate an auxiliary decoder branch to predict the degradation mask to ensure the underlying network can localize the degradation infor-mation. From empirical analysis on 10 datasets covering rain drop removal, deraining, dehazing, image enhance-ment, and deblurring, we demonstrate the efficacy of the proposed approach while obtaining SoTA performance. 