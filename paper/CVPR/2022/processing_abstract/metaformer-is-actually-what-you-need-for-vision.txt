Transformers have shown great potential in computer vision tasks. A common belief is their attention-based to-ken mixer module contributes most to their competence.However, recent works show the attention-based module in transformers can be replaced by spatial MLPs and the re-sulted models still perform quite well. Based on this ob-servation, we hypothesize that the general architecture of the transformers, instead of the specific token mixer mod-ule, is more essential to the model’s performance. To verify this, we deliberately replace the attention module in trans-formers with an embarrassingly simple spatial pooling op-erator to conduct only basic token mixing. Surprisingly, we observe that the derived model, termed as PoolFormer, achieves competitive performance on multiple computer vi-∗Work done during an internship at Sea AI Lab. sion tasks. For example, on ImageNet-1K, PoolFormer achieves 82.1% top-1 accuracy, surpassing well-tuned vi-sion transformer/MLP-like baselines DeiT-B/ResMLP-B24 by 0.3%/1.1% accuracy with 35%/52% fewer parameters and 49%/61% fewer MACs. The effectiveness of Pool-Former verifies our hypothesis and urges us to initiate the concept of “MetaFormer”, a general architecture ab-stracted from transformers without specifying the token mixer. Based on the extensive experiments, we argue thatMetaFormer is the key player in achieving superior results for recent transformer and MLP-like models on vision tasks.This work calls for more future research dedicated to im-proving MetaFormer instead of focusing on the token mixer modules. Additionally, our proposed PoolFormer could serve as a starting baseline for future MetaFormer archi-tecture design.