Federated Learning (FL) framework brings privacy ben-efits to distributed learning systems by allowing multiple clients to participate in a learning task under the coordi-nation of a central server without exchanging their private data. However, recent studies have revealed that private in-formation can still be leaked through shared gradient infor-mation. To further protect userâ€™s privacy, several defense mechanisms have been proposed to prevent privacy leak-age via gradient information degradation methods, such as using additive noise or gradient compression before shar-ing it with the server.In this work, we validate that the private training data can still be leaked under certain de-fense settings with a new type of leakage, i.e., Genera-tive Gradient Leakage (GGL). Unlike existing methods that only rely on gradient information to reconstruct data, our method leverages the latent space of generative adversarial networks (GAN) learned from public image datasets as a prior to compensate for the informational loss during gradi-ent degradation. To address the nonlinearity caused by the gradient operator and the GAN model, we explore various gradient-free optimization methods (e.g., evolution strate-gies and Bayesian optimization) and empirically show their superiority in reconstructing high-quality images from gra-dients compared to gradient-based optimizers. We hope the proposed method can serve as a tool for empirically mea-suring the amount of privacy leakage to facilitate the design of more robust defense mechanisms. 