The key challenge of zero-shot learning (ZSL) is how to infer the latent semantic knowledge between visual and at-tribute features on seen classes, and thus achieving a de-sirable knowledge transfer to unseen classes. Prior works either simply align the global features of an image with its associated class semantic vector or utilize unidirectional at-tention to learn the limited latent semantic representations, which could not effectively discover the intrinsic semantic knowledge (e.g., attribute semantics) between visual and at-tribute features. To solve the above dilemma, we propose a Mutually Semantic Distillation Network (MSDN), which progressively distills the intrinsic semantic representations between visual and attribute features for ZSL. MSDN incor-porates an attribute→visual attention sub-net that learns attribute-based visual features, and a visual→attribute at-tention sub-net that learns visual-based attribute features.By further introducing a semantic distillation loss, the two mutual attention sub-nets are capable of learning collab-oratively and teaching each other throughout the training process. The proposed MSDN yields signiﬁcant improve-ments over the strong baselines, leading to new state-of-the-art performances on three popular challenging bench-marks. Our codes have been available at: https:// github.com/shiming-chen/MSDN . 