Large-scale unlabeled data has spurred recent progress in self-supervised learning methods that learn rich vi-sual representations. State-of-the-art self-supervised meth-ods for learning representations from images (e.g., MoCo,BYOL, MSF) use an inductive bias that random augmen-tations (e.g., random crops) of an image should produce similar embeddings. We show that such methods are vul-nerable to backdoor attacks â€” where an attacker poisons a small part of the unlabeled data by adding a trigger (image patch chosen by the attacker) to the images. The model performance is good on clean test images, but the attacker can manipulate the decision of the model by show-ing the trigger at test time. Backdoor attacks have been studied extensively in supervised learning and to the best of our knowledge, we are the first to study them for self-supervised learning. Backdoor attacks are more practi-cal in self-supervised learning, since the use of large un-labeled data makes data inspection to remove poisons pro-hibitive. We show that in our targeted attack, the attacker can produce many false positives for the target category by using the trigger at test time. We also propose a de-fense method based on knowledge distillation that succeeds in neutralizing the attack. Our code is available here: https://github.com/UMBCvision/SSL-Backdoor 