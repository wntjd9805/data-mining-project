It is difficult to construct a data collection including all possible combinations of human actions and interacting ob-jects due to the combinatorial nature of human-object inter-actions (HOI). In this work, we aim to develop a transfer-able HOI detector for unseen interactions. Existing HOI de-tectors often treat interactions as discrete labels and learn a classifier according to a predetermined category space.This is inherently inapt for detecting unseen interactions which are out of the predefined categories. Conversely, we treat independent HOI labels as the natural language su-pervision of interactions and embed them into a joint visual-and-text space to capture their correlations. More specifi-cally, we propose a new HOI visual encoder to detect the interacting humans and objects, and map them to a joint feature space to perform interaction recognition. Our vi-sual encoder is instantiated as a Vision Transformer with new learnable HOI tokens and a sequence parser to gen-erate unique HOI predictions. It distills and leverages the transferable knowledge from the pretrained CLIP model to perform the zero-shot interaction detection. Experiments on two datasets, SWIG-HOI and HICO-DET, validate that our proposed method can achieve a notable mAP improvement on detecting both seen and unseen HOIs. Our code is avail-able at https://github.com/scwangdyd/promting_ hoi. 