This paper presents a grounded language-image pre-training (GLIP) model for learning object-level, language-aware, and semantic-rich visual representations. GLIP uni-fies object detection and phrase grounding for pre-training.The unification brings two benefits: 1) it allows GLIP to learn from both detection and grounding data to im-prove both tasks and bootstrap a good grounding model; 2) GLIP can leverage massive image-text pairs by generat-ing grounding boxes in a self-training fashion, making the learned representations semantic-rich.In our experiments, we pre-train GLIP on 27M grounding data, including 3M human-annotated and 24M web-crawled image-text pairs.The learned representations demonstrate strong zero-shot and few-shot transferability to various object-level recogni-tion tasks. 1) When directly evaluated on COCO and LVIS (without seeing any images in COCO during pre-training),GLIP achieves 49.8 AP and 26.9 AP, respectively, surpass-ing many supervised baselines.1 2) After fine-tuned onCOCO, GLIP achieves 60.8 AP on val and 61.5 AP on test-dev, surpassing prior SoTA. 3) When transferred to 13 downstream object detection tasks, a 1-shot GLIP rivals with a fully-supervised Dynamic Head. Code will be re-leased at https://github.com/microsoft/GLIP. 