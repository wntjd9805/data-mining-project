Recent progress towards learning from limited supervi-sion has encouraged efforts towards designing models that can recognize novel classes at test time (generalized zero-shot learning or GZSL). GZSL approaches assume knowl-edge of all classes, with or without labeled data, before-hand. However, practical scenarios demand models that are adaptable and can handle dynamic addition of new seen and unseen classes on the ﬂy (i.e continual general-ized zero-shot learning or CGZSL). One solution is to se-quentially retrain and reuse conventional GZSL methods, however, such an approach suffers from catastrophic for-getting leading to suboptimal generalization performance.A few recent efforts towards tackling CGZSL have been lim-ited by difference in settings, practicality, data splits and protocols followed – inhibiting fair comparison and a clear direction forward. Motivated from these observations, in this work, we ﬁrstly consolidate the different CGZSL setting variants and propose a new Online-CGZSL setting which is more practical and ﬂexible. Secondly, we introduce a uniﬁed feature-generative framework for CGZSL that lever-ages bi-directional incremental alignment to dynamically adapt to addition of new classes, with or without labeled data, that arrive over time in any of these CGZSL settings.Our comprehensive experiments and analysis on ﬁve bench-mark datasets and comparison with baselines show that our approach consistently outperforms existing methods, espe-cially on the more practical Online setting. 