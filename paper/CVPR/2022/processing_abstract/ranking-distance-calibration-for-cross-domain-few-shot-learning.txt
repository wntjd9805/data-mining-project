Recent progress in few-shot learning promotes a more realistic cross-domain setting, where the source and tar-get datasets are in different domains. Due to the domain gap and disjoint label spaces between source and target datasets, their shared knowledge is extremely limited. This encourages us to explore more information in the target do-main rather than to overly elaborate training strategies on the source domain as in many existing methods. Hence, we start from a generic representation pre-trained by a cross-entropy loss and a conventional distance-based classifier, along with an image retrieval view, to employ a re-ranking process to calibrate a target distance matrix by discovering the k-reciprocal neighbours within the task. Assuming the pre-trained representation is biased towards the source, we construct a non-linear subspace to minimise task-irrelevant features therewithin while keep more transferrable discrim-inative information by a hyperbolic tangent transformation.The calibrated distance in this target-aware non-linear sub-space is complementary to that in the pre-trained repre-sentation. To impose such distance calibration informa-tion onto the pre-trained representation, a Kullback-Leibler divergence loss is employed to gradually guide the model towards the calibrated distance-based distribution. Exten-sive evaluations on eight target domains show that this tar-get ranking calibration process can improve conventional distance-based classifiers in few-shot learning. 