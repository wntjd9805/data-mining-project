Hyperbolic space can naturally embed hierarchies, un-like Euclidean space. Hyperbolic Neural Networks (HNNs) exploit such representational power by lifting Euclidean features into hyperbolic space for classification, outper-forming Euclidean neural networks (ENNs) on datasets with known semantic hierarchies. However, HNNs under-perform ENNs on standard benchmarks without clear hier-archies, greatly restricting HNNs’ applicability in practice.Our key insight is that HNNs’ poorer general classifica-tion performance results from vanishing gradients during backpropagation, caused by their hybrid architecture con-necting Euclidean features to a hyperbolic classifier. We propose an effective solution by simply clipping the Eu-clidean feature magnitude while training HNNs.Our experiments demonstrate that clipped HNNs become super-hyperbolic classifiers: They are not only consistently better than HNNs which already outperform ENNs on hi-erarchical data, but also on-par with ENNs on MNIST, CI-FAR10, CIFAR100 and ImageNet benchmarks, with better adversarial robustness and out-of-distribution detection. 