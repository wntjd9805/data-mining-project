The occlusion problem heavily degrades the localization performance of face alignment. Most current solutions for this problem focus on annotating new occlusion data, in-troducing boundary estimation, and stacking deeper mod-els to improve the robustness of neural networks. How-ever, the performance degradation of models remains un-der extreme occlusion (i.e. average occlusion of over 50%) because of missing a large amount of facial context infor-mation. We argue that exploring neural networks to model the facial hierarchies is a more promising method for deal-ing with extreme occlusion. Surprisingly, in recent stud-ies, little effort has been devoted to representing the facial hierarchies using neural networks. This paper proposes a new network architecture called GlomFace to model the fa-cial hierarchies against various occlusions, which draws in-spiration from the viewpoint-invariant hierarchy of facial structure. Specifically, GlomFace is functionally divided into two modules: the part-whole hierarchical module and the whole-part hierarchical module. The former captures the part-whole hierarchical dependencies of facial parts to suppress multi-scale occlusion information, whereas the latter injects structural reasoning into neural networks by building the whole-part hierarchical relations among facial parts. As a result, GlomFace has a clear topological inter-pretation due to its correspondence to the facial hierarchies.Extensive experimental results indicate that the proposedGlomFace performs comparably to existing state-of-the-art methods, especially in cases of extreme occlusion. Models are available at https://github.com/zhuccly/GlomFace-Face-Alignment. 