This paper strives for activity recognition under domain shift, for example caused by change of scenery or camera viewpoint. The leading approaches reduce the shift in activ-ity appearance by adversarial training and self-supervised learning. Different from these vision-focused works we lever-age activity sounds for domain adaptation as they have less variance across domains and can reliably indicate which ac-tivities are not happening. We propose an audio-adaptive en-coder and associated learning methods that discriminatively adjust the visual feature representation as well as address-ing shifts in the semantic distribution. To further eliminate domain-speciÔ¨Åc features and include domain-invariant ac-tivity sounds for recognition, an audio-infused recognizer is proposed, which effectively models the cross-modal inter-action across domains. We also introduce the new task of actor shift, with a corresponding audio-visual dataset, to challenge our method with situations where the activity ap-pearance changes dramatically. Experiments on this dataset,EPIC-Kitchens and CharadesEgo show the effectiveness of our approach. Project page: https://xiaobai1217. github.io/DomainAdaptation. 