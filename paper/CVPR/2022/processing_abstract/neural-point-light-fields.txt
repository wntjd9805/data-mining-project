We introduce Neural Point Light Fields that represent scenes implicitly with a light ﬁeld living on a sparse point cloud. Combining differentiable volume rendering with learned implicit density representations has made it pos-sible to synthesize photo-realistic images for novel views of small scenes. As neural volumetric rendering methods require dense sampling of the underlying functional scene representation, at hundreds of samples along a ray cast through the volume, they are fundamentally limited to small scenes with the same objects projected to hundreds of train-ing views. Promoting sparse point clouds to neural implicit light ﬁelds allows us to represent large scenes effectively with only a single radiance evaluation per ray. These point light ﬁelds are as a function of the ray direction, and local point feature neighborhood, allowing us to interpolate the light ﬁeld conditioned training images without dense object coverage and parallax. We assess the proposed method for novel view synthesis on large driving scenarios, where we synthesize realistic unseen views that existing implicit ap-proaches fail to represent. We validate that Neural PointLight Fields make it possible to predict videos along unseen trajectories previously only feasible to generate by explic-itly modeling the scene. 