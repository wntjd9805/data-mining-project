By leveraging contrastive learning, clustering, and other pretext tasks, unsupervised methods for learning image rep-resentations have reached impressive results on standard benchmarks. The result has been a crowded field â€“ many methods with substantially different implementations yield results that seem nearly identical on popular benchmarks, such as linear evaluation on ImageNet. However, a single result does not tell the whole story. In this paper, we com-pare methods using performance-based benchmarks such as linear evaluation, nearest neighbor classification, and clustering for several different datasets, demonstrating the lack of a clear front-runner within the current state-of-the-art.In contrast to prior work that performs only super-vised vs. unsupervised comparison, we compare several different unsupervised methods against each other. To en-rich this comparison, we analyze embeddings with mea-surements such as uniformity, tolerance, and centered ker-nel alignment (CKA), and propose two new metrics of our own: nearest neighbor graph similarity and linear predic-tion overlap. We reveal through our analysis that in iso-lation, single popular methods should not be treated as though they represent the field as a whole, and that future work ought to consider how to leverage the complimentary nature of these methods. We also leverage CKA to provide a framework to robustly quantify augmentation invariance, and provide a reminder that certain types of invariance will be undesirable for downstream tasks. 