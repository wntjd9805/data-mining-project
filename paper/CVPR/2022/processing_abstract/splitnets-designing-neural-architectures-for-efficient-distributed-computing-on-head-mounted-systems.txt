We design deep neural networks (DNNs) and corre-sponding networks’ splittings to distribute DNNs’ workload to camera sensors and a centralized aggregator on head mounted devices to meet system performance targets in in-ference accuracy and latency under the given hardware re-source constraints. To achieve an optimal balance among computation, communication, and performance, a split-aware neural architecture search framework, SplitNets, is introduced to conduct model designing, splitting, and com-munication reduction simultaneously. We further extend the framework to multi-view systems for learning to fuse inputs from multiple camera sensors with optimal performance and systemic efﬁciency. We validate SplitNets for single-view system on ImageNet as well as multi-view system on 3D classiﬁcation, and show that the SplitNets framework achieves state-of-the-art (SOTA) performance and system latency compared with existing approaches. 