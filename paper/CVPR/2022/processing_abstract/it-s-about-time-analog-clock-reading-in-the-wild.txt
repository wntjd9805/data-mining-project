In this paper, we present a framework for reading ana-log clocks in natural images or videos. Speciﬁcally, we make the following contributions: First, we create a scal-able pipeline for generating synthetic clocks, signiﬁcantly reducing the requirements for the labour-intensive annota-tions; Second, we introduce a clock recognition architec-ture based on spatial transformer networks (STN), which is trained end-to-end for clock alignment and recognition.We show that the model trained on the proposed synthetic dataset generalises towards real clocks with good accuracy, advocating a Sim2Real training regime; Third, to further reduce the gap between simulation and real data, we lever-age the special property of “time”, i.e. uniformity, to gener-ate reliable pseudo-labels on real unlabelled clock videos, and show that training on these videos offers further im-provements while still requiring zero manual annotations.Lastly, we introduce three benchmark datasets based onCOCO, Open Images, and The Clock movie, with full anno-tations for time, accurate to the minute. 