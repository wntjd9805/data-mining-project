Video summarization has recently engaged increasing attention in computer vision communities. However, the scarcity of annotated data has been a key obstacle in this task. To address it, this work explores a new solution for video summarization by transferring samples from a corre-lated task (i.e., video moment localization) equipped with abundant training data. Our main insight is that the anno-tated video moments also indicate the semantic highlights of a video, essentially similar to video summary. Approx-imately, the video summary can be treated as a sparse, redundancy-free version of the video moments.Inspired by this observation, we propose an importance Propaga-It tion based collaborative Teaching Network (iPTNet). consists of two separate modules that conduct video sum-marization and moment localization, respectively. Each module estimates a frame-wise importance map for indi-cating keyframes or moments. To perform cross-task sam-ple transfer, we devise an importance propagation module that realizes the conversion between summarization-guided and localization-guided importance maps. This way crit-ically enables optimizing one of the tasks using the data from the other task. Additionally, in order to avoid error ampliﬁcation caused by batch-wise joint training, we de-vise a collaborative teaching scheme, which adopts a cross-task mean teaching strategy to realize the joint optimiza-tion of the two tasks and provide robust frame-level teach-ing signals. Extensive experiments on video summarization benchmarks demonstrate that iPTNet signiﬁcantly outper-forms previous state-of-the-art video summarization meth-ods, serving as an effective solution that overcomes the data scarcity issue in video summarization. 