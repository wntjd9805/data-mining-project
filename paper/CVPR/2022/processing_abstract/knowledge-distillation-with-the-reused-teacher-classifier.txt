Knowledge distillation aims to compress a powerful yet cumbersome teacher model into a lightweight student model without much sacrifice of performance. For this purpose, various approaches have been proposed over the past few years, generally with elaborately designed knowledge rep-resentations, which in turn increase the difficulty of model development and interpretation.In contrast, we empiri-cally show that a simple knowledge distillation technique is enough to significantly narrow down the teacher-student performance gap. We directly reuse the discriminative clas-sifier from the pre-trained teacher model for student infer-ence and train a student encoder through feature alignment with a single â„“2 loss. In this way, the student model is able to achieve exactly the same performance as the teacher model provided that their extracted features are perfectly aligned.An additional projector is developed to help the student en-coder match with the teacher classifier, which renders our technique applicable to various teacher and student archi-tectures. Extensive experiments demonstrate that our tech-nique achieves state-of-the-art results at the modest cost of compression ratio due to the added projector. 