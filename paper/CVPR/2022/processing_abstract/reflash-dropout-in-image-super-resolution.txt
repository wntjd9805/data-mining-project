Dropout is designed to relieve the overfitting problem in high-level vision tasks but is rarely applied in low-level vi-sion tasks, like image super-resolution (SR). As a classic regression problem, SR exhibits a different behaviour as high-level tasks and is sensitive to the dropout operation.However, in this paper, we show that appropriate usage of dropout benefits SR networks and improves the generaliza-tion ability. Specifically, dropout is better embedded at the end of the network and is significantly helpful for the multi-degradation settings. This discovery breaks our common sense and inspires us to explore its working mechanism. We further use two analysis tools â€“ one is from a recent network interpretation work, and the other is specially designed for this task. The analysis results provide side proofs to our experimental findings and show us a new perspective to un-derstand SR networks. 