Humans can perceive multiple expressions, each one with varying intensity, in the picture of a face. We propose a methodology for collecting and modeling multidimensional modulated expression annotations from human annotators.Our data reveals that the perception of some expressions can be quite different across observers; thus, our model is designed to represent ambiguity alongside intensity. An em-pirical exploration of how many dimensions are necessary to capture the perception of facial expression suggests six principal expression dimensions are sufficient. Using our method, we collected multidimensional modulated expres-sion annotations for 1,000 images culled from the popularExpW in-the-wild dataset. As a proof of principle of our im-proved measurement technique, we used these annotations to benchmark four public domain algorithms for automated facial expression prediction. 