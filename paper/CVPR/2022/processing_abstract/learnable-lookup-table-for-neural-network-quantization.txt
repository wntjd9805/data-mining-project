Neural network quantization aims at reducing bit-widths of weights and activations for memory and computational efﬁciency. Since a linear quantizer (i.e., round(·) function) cannot well ﬁt the bell-shaped distributions of weights and activations, many existing methods use pre-deﬁned func-tions (e.g., exponential function) with learnable parame-ters to build the quantizer for joint optimization. How-ever, these complicated quantizers introduce considerable computational overhead during inference since activation quantization should be conducted online. In this paper, we formulate the quantization process as a simple lookup op-eration and propose to learn lookup tables as quantizers.Speciﬁcally, we develop differentiable lookup tables and in-troduce several training strategies for optimization. Our lookup tables can be trained with the network in an end-to-end manner to ﬁt the distributions in different layers and have very small additional computational cost. Compari-son with previous methods show that quantized networks us-ing our lookup tables achieve state-of-the-art performance on image classiﬁcation, image super-resolution, and point cloud classiﬁcation tasks. 