Vanilla unsupervised domain adaptation methods tend to optimize the model with fixed neural architecture, which is not very practical in real-world scenarios since the target data is usually processed by different resource-limited de-vices. It is therefore of great necessity to facilitate archi-tecture adaptation across various devices. In this paper, we introduce a simple framework, Slimmable Domain Adapta-tion, to improve cross-domain generalization with a weight-sharing model bank, from which models of different capac-ities can be sampled to accommodate different accuracy-efficiency trade-offs. The main challenge in this frame-work lies in simultaneously boosting the adaptation perfor-mance of numerous models in the model bank. To tackle this problem, we develop a Stochastic EnsEmble Distilla-tion method to fully exploit the complementary knowledge in the model bank for inter-model interaction. Nevertheless, considering the optimization conflict between inter-model interaction and intra-model adaptation, we augment the ex-isting bi-classifier domain confusion architecture into anOptimization-Separated Tri-Classifier counterpart. After optimizing the model bank, architecture adaptation is lever-aged via our proposed Unsupervised Performance Eval-uation Metric. Under various resource constraints, our framework surpasses other competing approaches by a very large margin on multiple benchmarks.It is also worth emphasizing that our framework can preserve the perfor-mance improvement against the source-only model even when the computing complexity is reduced to 1/64. Code will be available at https://github.com/HIK-LAB/SlimDA. 