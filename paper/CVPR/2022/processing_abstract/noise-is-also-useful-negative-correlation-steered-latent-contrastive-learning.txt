How to effectively handle label noise has been one of the most practical but challenging tasks in Deep Neural Net-works (DNNs). Recent popular methods for training DNNs with noisy labels mainly focus on directly filtering out sam-ples with low confidence or repeatedly mining valuable in-formation from low-confident samples. However, they can-not guarantee the robust generalization of models due to the ignorance of useful information hidden in noisy data.To address this issue, we propose a new effective method named as LaCoL (Latent Contrastive Learning) to leverage the negative correlations from the noisy data. Specifically, in label space, we exploit the weakly-augmented data to fil-ter samples and adopt classification loss on strong augmen-tations of the selected sample set, which can preserve the training diversity. While in metric space, we utilize weakly-supervised contrastive learning to excavate these negative correlations hidden in noisy data. Moreover, a cross-space similarity consistency regularization is provided to con-strain the gap between label space and metric space. Ex-tensive experiments have validated the superiority of our approach over existing state-of-the-art methods. 