Vision Transformers (ViTs) and their multi-scale and hi-erarchical variations have been successful at capturing im-age representations but their use has been generally stud-ied for low-resolution images (e.g. 256 × 256, 384 × 384).For gigapixel whole-slide imaging (WSI) in computational pathology, WSIs can be as large as 150000 × 150000 pixels at 20× magnification and exhibit a hierarchical structure of visual tokens across varying resolutions: from 16 × 16 im-ages capturing individual cells, to 4096×4096 images char-acterizing interactions within the tissue microenvironment.We introduce a new ViT architecture called the HierarchicalImage Pyramid Transformer (HIPT), which leverages the natural hierarchical structure inherent in WSIs using two levels of self-supervised learning to learn high-resolution image representations. HIPT is pretrained across 33 can-cer types using 10,678 gigapixel WSIs, 408,218 4096×4096 images, and 104M 256 × 256 images. We benchmark HIPT representations on 9 slide-level tasks, and demonstrate that: 1) HIPT with hierarchical pretraining outperforms current state-of-the-art methods for cancer subtyping and survival prediction, 2) self-supervised ViTs are able to model im-portant inductive biases about the hierarchical structure of phenotypes in the tumor microenvironment. 