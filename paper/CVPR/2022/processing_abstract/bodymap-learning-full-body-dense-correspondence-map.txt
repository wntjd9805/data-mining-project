Dense correspondence between humans carries powerful semantic information that can be utilized to solve funda-mental problems for full-body understanding such as in-the-wild surface matching, tracking and reconstruction. In this paper we present BodyMap, a new framework for obtain-ing high-definition full-body and continuous dense corre-spondence between in-the-wild images of clothed humans and the surface of a 3D template model. The correspon-dences cover fine details such as hands and hair, while capturing regions far from the body surface, such as loose clothing. Prior methods for estimating such dense surface correspondence i) cut a 3D body into parts which are un-wrapped to a 2D UV space, producing discontinuities along part seams, or ii) use a single surface for representing the whole body, but none handled body details. Here, we intro-duce a novel network architecture with Vision Transformers that learn fine-level features on a continuous body surface.BodyMap outperforms prior work on various metrics and datasets, including DensePose-COCO by a large margin.Furthermore, we show various applications ranging from multi-layer dense cloth correspondence, neural rendering with novel-view synthesis and appearance swapping. 