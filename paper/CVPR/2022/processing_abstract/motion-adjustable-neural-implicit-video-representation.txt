Implicit neural representation (INR) has been suc-cessful in representing static images. Contemporary image-based INR, with the use of Fourier-based posi-tional encoding, can be viewed as a mapping from si-nusoidal patterns with different frequencies to image content. Inspired by that view, we hypothesize that it is possible to generate temporally varying content with a single image-based INR model by displacing its input sinusoidal patterns over time. By exploiting the rela-tion between the phase information in sinusoidal func-tions and their displacements, we incorporate into the conventional image-based INR model a phase-varying positional encoding module, and couple it with a phase-shift generation module that determines the phase-shift values at each frame. The model is trained end-to-end on a video to jointly determine the phase-shift values at each time with the mapping from the phase-shifted sinusoidal functions to the corresponding frame, en-abling an implicit video representation. Experiments on a wide range of videos suggest that such a model is capable of learning to interpret phase-varying posi-tional embeddings into the corresponding time-varying content. More importantly, we found that the learned phase-shift vectors tend to capture meaningful temporal and motion information from the video. In particular, manipulating the phase-shift vectors induces meaning-ful changes in the temporal dynamics of the resulting video, enabling non-trivial temporal and motion editing effects such as temporal interpolation, motion magnifi-cation, motion smoothing, and video loop detection. 