The goal of this paper is speech separation and en-hancement in multi-speaker and noisy environments using a combination of different modalities. Previous works have shown good performance when conditioning on temporal or static visual evidence such as synchronised lip movements or face identity. In this paper, we present a unified frame-work for multi-modal speech separation and enhancement based on synchronous or asynchronous cues. To that end we make the following contributions: (i) we design a mod-ern Transformer-based architecture tailored to fuse differ-ent modalities to solve the speech separation task in the raw waveform domain; (ii) we propose conditioning on the tex-tual content of a sentence alone or in combination with vi-sual information; (iii) we demonstrate the robustness of our model to audio-visual synchronisation offsets; and, (iv) we obtain state-of-the-art performance on the well-established benchmark datasets LRS2 and LRS3. 