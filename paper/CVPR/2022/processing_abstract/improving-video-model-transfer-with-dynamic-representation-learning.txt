Temporal modeling is an essential element in video un-derstanding. While deep convolution-based architectures have been successful at solving large-scale video recog-nition datasets, recent work has pointed out that they are biased towards modeling short-range relations, often fail-ing to capture long-term temporal structures in the videos, leading to poor transfer and generalization to new datasets.In this work, the problem of dynamic representation learn-ing (DRL) is studied. We propose dynamic score, a mea-sure of video dynamic modeling that describes the addi-tional amount of information learned by a video network that cannot be captured by pure spatial student through knowledge distillation. DRL is then formulated as an ad-versarial learning problem between the video and spatial models, with the objective of maximizing the dynamic score of learned spatiotemporal classifier. The quality of learned video representations are evaluated on a diverse set of transfer learning problems concerning many-shot and few-shot action classification. Experimental results show that models learned with DRL outperform baselines in dynamic modeling, demonstrating higher transferability and gener-alization capacity to novel domains and tasks. 