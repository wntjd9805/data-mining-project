We present a new system (NPBG++) for the novel view synthesis (NVS) task that achieves high rendering realism with low scene ﬁtting time. Our method efﬁciently lever-ages the multiview observations and the point cloud of a static scene to predict a neural descriptor for each point, improving upon the pipeline of Neural Point-Based Graph-ics [1] in several important ways. By predicting the descrip-tors with a single pass through the source images, we lift the requirement of per-scene optimization while also mak-ing the neural descriptors view-dependent and more suit-able for scenes with strong non-Lambertian effects. In our comparisons, the proposed system outperforms previousNVS approaches in terms of ﬁtting and rendering runtimes while producing images of similar quality. Project page: https://rakhimovv.github.io/npbgpp/. 