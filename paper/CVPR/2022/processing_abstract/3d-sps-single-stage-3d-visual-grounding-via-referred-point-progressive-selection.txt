3D visual grounding aims to locate the referred target object in 3D point cloud scenes according to a free-form language description. Previous methods mostly follow a two-stage paradigm, i.e., language-irrelevant detection and cross-modal matching, which is limited by the isolated ar-chitecture. In such a paradigm, the detector needs to sample keypoints from raw point clouds due to the inherent proper-ties of 3D point clouds (irregular and large-scale), to gen-erate the corresponding object proposal for each keypoint.However, sparse proposals may leave out the target in de-tection, while dense proposals may confuse the matching model. Moreover, the language-irrelevant detection stage can only sample a small proportion of keypoints on the target, deteriorating the target prediction.In this paper, we propose a 3D Single-Stage Referred Point ProgressiveSelection (3D-SPS) method, which progressively selects keypoints with the guidance of language and directly lo-cates the target. Specifically, we propose a Description-aware Keypoint Sampling (DKS) module to coarsely focus on the points of language-relevant objects, which are sig-nificant clues for grounding. Besides, we devise a Target-oriented Progressive Mining (TPM) module to finely con-centrate on the points of the target, which is enabled by progressive intra-modal relation modeling and inter-modal target mining. 3D-SPS bridges the gap between detection and matching in the 3D visual grounding task, localizing the target at a single stage. Experiments demonstrate that 3D-SPS achieves state-of-the-art performance on both ScanRe-fer and Nr3D/Sr3D datasets. 