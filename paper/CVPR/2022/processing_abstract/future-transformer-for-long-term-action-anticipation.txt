The task of predicting future actions from a video is cru-cial for a real-world agent interacting with others. When anticipating actions in the distant future, we humans typi-cally consider long-term relations over the whole sequence of actions, i.e., not only observed actions in the past but also potential actions in the future. In a similar spirit, we pro-pose an end-to-end attention model for action anticipation, dubbed Future Transformer (FUTR), that leverages global attention over all input frames and output tokens to predict a minutes-long sequence of future actions. Unlike the pre-vious autoregressive models, the proposed method learns to predict the whole sequence of future actions in parallel de-coding, enabling more accurate and fast inference for long-term anticipation. We evaluate our method on two stan-dard benchmarks for long-term action anticipation, Break-fast and 50 Salads, achieving state-of-the-art results. 