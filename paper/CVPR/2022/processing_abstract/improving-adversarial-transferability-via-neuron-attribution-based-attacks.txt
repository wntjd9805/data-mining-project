Deep neural networks (DNNs) are known to be vulner-able to adversarial examples. It is thus imperative to de-vise effective attack algorithms to identify the deficiencies ofDNNs beforehand in security-sensitive applications. To effi-ciently tackle the black-box setting where the target model’s particulars are unknown, feature-level transfer-based at-tacks propose to contaminate the intermediate feature out-puts of local models, and then directly employ the crafted adversarial samples to attack the target model. Due to the transferability of features, feature-level attacks have shown promise in synthesizing more transferable adversarial sam-ples. However, existing feature-level attacks generally em-ploy inaccurate neuron importance estimations, which de-teriorates their transferability. To overcome such pitfalls, in this paper, we propose the Neuron Attribution-based At-tack (NAA), which conducts feature-level attacks with more accurate neuron importance estimations. Specifically, we first completely attribute a model’s output to each neuron in a middle layer. We then derive an approximation scheme of neuron attribution to tremendously reduce the computa-tion overhead. Finally, we weight neurons based on their attribution results and launch feature-level attacks. Exten-sive experiments confirm the superiority of our approach to the state-of-the-art benchmarks. Our code is available at: https://github.com/jpzhang1810/NAA . 