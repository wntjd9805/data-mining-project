Deep spatiotemporal models are used in a variety of computer vision tasks, such as action recognition and video object segmentation. Currently, there is a limited under-standing of what information is captured by these models in their intermediate representations. For example, while it has been observed that action recognition algorithms are heavily inﬂuenced by visual appearance in single static frames, there is no quantitative methodology for evaluat-ing such static bias in the latent representation compared to bias toward dynamic information (e.g. motion). We tackle this challenge by proposing a novel approach for quanti-fying the static and dynamic biases of any spatiotemporal model. To show the efﬁcacy of our approach, we anal-yse two widely studied tasks, action recognition and video object segmentation. Our key ﬁndings are threefold: (i)Most examined spatiotemporal models are biased toward static information; although, certain two-stream architec-tures with cross-connections show a better balance between the static and dynamic information captured. (ii) Some datasets that are commonly assumed to be biased toward dynamics are actually biased toward static information. (iii) Individual units (channels) in an architecture can be bi-ased toward static, dynamic or a combination of the two. 1 