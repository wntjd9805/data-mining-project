We consider the problem of reconstructing the depth of dynamic objects from videos. Recent progress in dynamic video depth prediction has focused on improving the out-put of monocular depth estimators by means of multi-view constraints while imposing little to no restrictions on the deformation of the dynamic parts of the scene. However, the theory of Non-Rigid Structure from Motion prescribes to constrain the deformations for 3D reconstruction. We thus propose a new model that departs signiﬁcantly from this prior work. The idea is to ﬁt a dynamic point cloud to the video data using Sinkhorn’s algorithm to associate the 3D points to 2D pixels and use a differentiable point renderer to ensure the compatibility of the 3D deformations with the measured optical ﬂow. In this manner, our algorithm, calledKeypoint Transporter, models the overall deformation of the object within the entire video, so it can constrain the re-construction correspondingly. Compared to weaker defor-mation models, this signiﬁcantly reduces the reconstruction ambiguity and, for dynamic objects, allows Keypoint Trans-porter to obtain reconstructions of the quality superior or at least comparable to prior approaches while being much faster and reliant on a pre-trained monocular depth esti-mator network. To assess the method, we evaluate on new datasets of synthetic videos depicting dynamic humans and animals with ground-truth depth. We also show qualitative results on crowd-sourced real-world videos of pets. 