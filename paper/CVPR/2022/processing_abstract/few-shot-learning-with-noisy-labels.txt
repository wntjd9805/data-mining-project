Few-shot learning (FSL) methods typically assume clean support sets with accurately labeled samples when training on novel classes. This assumption can often be unrealistic: support sets, no matter how small, can still include misla-beled samples. Robustness to label noise is therefore es-sential for FSL methods to be practical, but this problem surprisingly remains largely unexplored. To address misla-beled samples in FSL settings, we make several technical contributions. (1) We offer simple, yet effective, feature ag-gregation methods, improving the prototypes used by Pro-toNet, a popular FSL technique. (2) We describe a novelTransformer model for Noisy Few-Shot Learning (TraNFS).TraNFS leverages a transformerâ€™s attention mechanism to weigh mislabeled versus correct samples. (3) Finally, we extensively test these methods on noisy versions of MiniIm-ageNet and TieredImageNet. Our results show that TraNFS is on-par with leading FSL methods on clean support sets, yet outperforms them, by far, in the presence of label noise. 