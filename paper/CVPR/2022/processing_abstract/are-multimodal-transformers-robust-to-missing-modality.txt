Multimodal data collected from the real world are often imperfect due to missing modalities. Therefore multimodal models that are robust against modal-incomplete data are highly preferred. Recently, Transformer models have shown great success in processing multimodal data. However, existing work has been limited to either architecture de-signs or pre-training strategies; whether Transformer mod-els are naturally robust against missing-modal data has rarely been investigated. In this paper, we present the first-of-its-kind work to comprehensively investigate the behav-ior of Transformers in the presence of modal-incomplete data. Unsurprising, we find Transformer models are sen-sitive to missing modalities while different modal fusion strategies will significantly affect the robustness. What sur-prised us is that the optimal fusion strategy is dataset de-pendent even for the same Transformer model; there does not exist a universal strategy that works in general cases.Based on these findings, we propose a principle method to improve the robustness of Transformer models by automati-cally searching for an optimal fusion strategy regarding in-put data. Experimental validations on three benchmarks support the superior performance of the proposed method. 