Grouping and recognition are important components of visual scene understanding, e.g., for object detection and semantic segmentation. With end-to-end deep learning systems, grouping of image regions usually happens im-plicitly via top-down supervision from pixel-level recogni-tion labels.Instead, in this paper, we propose to bring back the grouping mechanism into deep networks, which allows semantic segments to emerge automatically with only text supervision. We propose a hierarchical Group-ing Vision Transformer (GroupViT), which goes beyond the regular grid structure representation and learns to group image regions into progressively larger arbitrary-shaped segments. We train GroupViT jointly with a text en-coder on a large-scale image-text dataset via contrastive losses. With only text supervision and without any pixel-level annotations, GroupViT learns to group together se-mantic regions and successfully transfers to the task of se-mantic segmentation in a zero-shot manner, i.e., without any further Ô¨Åne-tuning.It achieves a zero-shot accuracy of 52.3% mIoU on the PASCAL VOC 2012 and 22.4% mIoU on PASCAL Context datasets, and performs compet-itively to state-of-the-art transfer-learning methods requir-ing greater levels of supervision. We open-source our code at https://github.com/NVlabs/GroupViT. 