Generating speech-consistent body and gesture move-ments is a long-standing problem in virtual avatar creation.Previous studies often synthesize pose movement in a holis-tic manner, where poses of all joints are generated simul-taneously. Such a straightforward pipeline fails to gen-erate fine-grained co-speech gestures. One observation is that the hierarchical semantics in speech and the hierar-chical structures of human gestures can be naturally de-scribed into multiple granularities and associated together.To fully utilize the rich connections between speech audio and human gestures, we propose a novel framework namedHierarchical Audio-to-Gesture (HA2G) for co-speech ges-ture generation. In HA2G, a Hierarchical Audio Learner extracts audio representations across semantic granular-ities. A Hierarchical Pose Inferer subsequently renders the entire human pose gradually in a hierarchical man-ner. To enhance the quality of synthesized gestures, we de-velop a contrastive learning strategy based on audio-text alignment for better audio representations. Extensive ex-periments and human evaluation demonstrate that the pro-posed method renders realistic co-speech gestures and out-performs previous methods in a clear margin. Project page: https://alvinliu0.github.io/projects/HA2G. 