With the assumption that a video dataset is multimodal-ity annotated in which auditory and visual modalities both are labeled or class-relevant, current multimodal methods apply modality fusion or cross-modality attention. How-ever, effectively leveraging the audio modality in vision-specific annotated videos for action recognition is of par-ticular challenge.To tackle this challenge, we propose a novel audio-visual framework that effectively leverages the audio modality in any solely vision-specific annotated dataset. We adopt the language models (e.g., BERT) to build a semantic audio-video label dictionary (SAVLD) that maps each video la-bel to its most K-relevant audio labels in which SAVLD serves as a bridge between audio and video datasets. Then,SAVLD along with a pretrained audio multi-label model are used to estimate the audio-visual modality relevance dur-ing the training phase. Accordingly, a novel learnable ir-relevant modality dropout (IMD) is proposed to completely drop out the irrelevant audio modality and fuse only the rel-evant modalities. Moreover, we present a new two-stream video Transformer for efficiently modeling the visual modal-ities. Results on several vision-specific annotated datasets including Kinetics400 and UCF-101 validated our frame-work as it outperforms most relevant action recognition methods. 