Traditional 3D morphable face models (3DMMs) pro-vide fine-grained control over expression but cannot eas-ily capture geometric and appearance details. Neural volumetric representations approach photorealism but are hard to animate and do not generalize well to unseen ex-pressions. To tackle this problem, we propose IMavatar (Implicit Morphable avatar), a novel method for learn-ing implicit head avatars from monocular videos. Inspired by the fine-grained control mechanisms afforded by con-ventional 3DMMs, we represent the expression- and pose-related deformations via learned blendshapes and skinning fields. These attributes are pose-independent and can be used to morph the canonical geometry and texture fields given novel expression and pose parameters. We employ ray marching and iterative root-finding to locate the canon-ical surface intersection for each pixel. A key contribution is our novel analytical gradient formulation that enables end-to-end training of IMavatars from videos. We show quanti-tatively and qualitatively that our method improves geome-try and covers a more complete expression space compared to state-of-the-art methods. Code and data can be found at https://ait.ethz.ch/projects/2022/IMavatar/. 