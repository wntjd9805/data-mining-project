In this paper, we tackle the problem of few-shot class in-cremental learning (FSCIL). FSCIL aims to incrementally learn new classes with only a few samples in each class.Most existing methods only consider the incremental steps at test time. The learning objective of these methods is often hand-engineered and is not directly tied to the objective (i.e. incrementally learning new classes) during testing. Those methods are sub-optimal due to the misalignment between the training objectives and what the methods are expected to do during evaluation. In this work, we proposed a bi-level optimization based on meta-learning to directly op-timize the network to learn how to incrementally learn in the setting of FSCIL. Concretely, we propose to sample se-quences of incremental tasks from base classes for train-ing to simulate the evaluation protocol. For each task, the model is learned using a meta-objective such that it is ca-pable to perform fast adaptation without forgetting. Fur-thermore, we propose a bi-directional guided modulation, which is learned to automatically modulate the activations to reduce catastrophic forgetting. Extensive experimental results demonstrate that the proposed method outperforms the baseline and achieves the state-of-the-art results on CI-FAR100, MiniImageNet, and CUB200 datasets. 