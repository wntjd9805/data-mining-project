Humans can not only see the collection of objects in visual scenes, but also identify the relationship be-tween objects. The visual relationship in the scene can be abstracted into the semantic representation of a triple (cid:104)subject, predicate, object(cid:105) and thus results in a scene graph, which can convey a lot of information for visual un-derstanding. Due to the motion of objects, the visual re-lationship between two objects in videos may vary, which makes the task of dynamically generating scene graphs from videos more complicated and challenging than the conven-tional image-based static scene graph generation. Inspired by the ability of humans to infer the visual relationship, we propose a novel anticipatory pre-training paradigm based on Transformer to explicitly model the temporal correla-tion of visual relationships in different frames to improve dynamic scene graph generation.In pre-training stage, the model predicts the visual relationships of current frame based on the previous frames by extracting intra-frame spa-tial information with a spatial encoder and inter-frame tem-poral correlations with a progressive temporal encoder. In the Ô¨Åne-tuning stage, we reuse the spatial encoder and the progressive temporal encoder while the information of the current frame is combined for predicting the visual relation-ship. Extensive experiments demonstrate that our method achieves state-of-the-art performance on Action Genome dataset. 