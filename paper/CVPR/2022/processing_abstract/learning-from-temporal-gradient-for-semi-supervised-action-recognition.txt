Semi-supervised video action recognition tends to en-able deep neural networks to achieve remarkable perfor-mance even with very limited labeled data. However, ex-isting methods are mainly transferred from current image-based methods (e.g., FixMatch). Without specifically uti-lizing the temporal dynamics and inherent multimodal at-tributes, their results could be suboptimal. To better lever-age the encoded temporal information in videos, we intro-duce temporal gradient as an additional modality for more attentive feature extraction in this paper. To be specific, our method explicitly distills the fine-grained motion represen-tations from temporal gradient (TG) and imposes consis-tency across different modalities (i.e., RGB and TG). The performance of semi-supervised action recognition is sig-nificantly improved without additional computation or pa-rameters during inference. Our method achieves the state-of-the-art performance on three video action recognition benchmarks (i.e., Kinetics-400, UCF-101, and HMDB-51) under several typical semi-supervised settings (i.e., differ-ent ratios of labeled data). Code is made available at https://github.com/lambert-x/video-semisup. 