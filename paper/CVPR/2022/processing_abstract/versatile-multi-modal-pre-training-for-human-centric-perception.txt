are available at https://github.com/hongfz16/HCMoCo.Human-centric perception plays a vital role in vision and graphics. But their data annotations are prohibitively ex-pensive. Therefore, it is desirable to have a versatile pre-train model that serves as a foundation for data-efficient downstream tasks transfer. To this end, we propose theHuman-Centric Multi-Modal Contrastive Learning frame-work HCMoCo that leverages the multi-modal nature of hu-man data (e.g. RGB, depth, 2D keypoints) for effective rep-resentation learning. The objective comes with two main challenges: dense pre-train for multi-modality data, effi-cient usage of sparse human priors. To tackle the chal-lenges, we design the novel Dense Intra-sample ContrastiveLearning and Sparse Structure-aware Contrastive Learning targets by hierarchically learning a modal-invariant latent space featured with continuous and ordinal feature distri-bution and structure-aware semantic consistency. HCMoCo provides pre-train for different modalities by combining het-erogeneous datasets, which allows efficient usage of exist-ing task-specific human data. Extensive experiments on four downstream tasks of different modalities demonstrate the ef-fectiveness of HCMoCo, especially under data-efficient set-tings (7.16% and 12% improvement on DensePose Estima-tion and Human Parsing). Moreover, we demonstrate the versatility of HCMoCo by exploring cross-modality super-vision and missing-modality inference, validating its strong ability in cross-modal association and reasoning. Codes (cid:0) Corresponding author 