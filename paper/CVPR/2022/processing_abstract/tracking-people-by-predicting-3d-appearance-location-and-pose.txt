We present an approach for tracking people in monoc-ular videos by predicting their future 3D representations.To achieve this, we first lift people to 3D from a single frame in a robust manner. This lifting includes informa-tion about the 3D pose of the person, their location in the 3D space, and the 3D appearance. As we track a person, we collect 3D observations over time in a track-let representation. Given the 3D nature of our observa-tions, we build temporal models for each one of the previ-ous attributes. We use these models to predict the future state of the tracklet, including 3D appearance, 3D loca-tion, and 3D pose. For a future frame, we compute the similarity between the predicted state of a tracklet and the single frame observations in a probabilistic manner. As-sociation is solved with simple Hungarian matching, and the matches are used to update the respective tracklets. We evaluate our approach on various benchmarks and report state-of-the-art results. Code and models are available at: https://brjathu.github.io/PHALP. 