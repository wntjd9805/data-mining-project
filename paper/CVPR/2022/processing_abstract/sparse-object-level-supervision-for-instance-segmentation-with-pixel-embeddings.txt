Most state-of-the-art instance segmentation methods have to be trained on densely annotated images. While difficult in general, this requirement is especially daunt-ing for biomedical images, where domain expertise is of-ten required for annotation and no large public data collec-tions are available for pre-training. We propose to address the dense annotation bottleneck by introducing a proposal-free segmentation approach based on non-spatial embed-dings, which exploits the structure of the learned embed-ding space to extract individual instances in a differentiable way. The segmentation loss can then be applied directly to instances and the overall pipeline can be trained in a fully-or weakly supervised manner. We consider the challeng-ing case of positive-unlabeled supervision, where a novel self-supervised consistency loss is introduced for the un-labeled parts of the training data. We evaluate the pro-posed method on 2D and 3D segmentation problems in dif-ferent microscopy modalities as well as on the Cityscapes and CVPPP instance segmentation benchmarks, achieving state-of-the-art results on the latter. 