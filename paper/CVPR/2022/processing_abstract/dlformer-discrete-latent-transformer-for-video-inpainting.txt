Video inpainting remains a challenging problem to fill with plausible and coherent content in unknown ar-eas in video frames despite the prevalence of data-driven methods. Although various transformer-based architec-tures yield promising result for this task, they still suffer from hallucinating blurry contents and long-term spatial-temporal inconsistency. While noticing the capability of discrete representation for complex reasoning and predic-tive learning, we propose a novel Discrete Latent Trans-former (DLFormer) to reformulate video inpainting tasks into the discrete latent space rather the previous contin-uous feature space. Specifically, we first learn a unique compact discrete codebook and the corresponding autoen-coder to represent the target video. Built upon these rep-resentative discrete codes obtained from the entire target video, the subsequent discrete latent transformer is capa-ble to infer proper codes for unknown areas under a self-attention mechanism, and thus produces fine-grained con-tent with long-term spatial-temporal consistency. More-over, we further explicitly enforce the short-term consis-tency to relieve temporal visual jitters via a temporal aggre-gation block among adjacent frames. We conduct compre-hensive quantitative and qualitative evaluations to demon-strate that our method significantly outperforms other state-of-the-art approaches in reconstructing visually-plausible and spatial-temporal coherent content with fine-grained details.Code is available at https://github.com/JingjingRenabc/dlformer. 