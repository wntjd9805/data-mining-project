Autonomous driving requires the model to perceive the environment and (re)act within a low latency for safety.While past works ignore the inevitable changes in the envi-ronment after processing, streaming perception is proposed to jointly evaluate the latency and accuracy into a single metric for video online perception. In this paper, instead of searching trade-offs between accuracy and speed like pre-vious works, we point out that endowing real-time models with the ability to predict the future is the key to dealing with this problem. We build a simple and effective frame-It equips a novel Dual-work for streaming perception.Flow Perception module (DFP), which includes dynamic and static flows to capture the moving trend and basic detec-tion feature for streaming prediction. Further, we introduce a Trend-Aware Loss (TAL) combined with a trend factor to generate adaptive weights for objects with different mov-ing speeds. Our simple method achieves competitive per-formance on Argoverse-HD dataset and improves the AP by 4.9% compared to the strong baseline, validating its ef-fectiveness. Our code will be made available at https://github.com/yancie-yjr/StreamYOLO. 