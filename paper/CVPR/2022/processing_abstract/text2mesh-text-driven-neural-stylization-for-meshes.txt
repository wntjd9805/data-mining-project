In this work, we develop intuitive controls for edit-ing the style of 3D objects. Our framework, Text2Mesh, stylizes a 3D mesh by predicting color and local geo-metric details which conform to a target text prompt.We consider a disentangled representation of a 3D ob-ject using a ﬁxed mesh input (content) coupled with a learned neural network, which we term a neural styleﬁeld network (NSF). In order to modify style, we ob-tain a similarity score between a text prompt (describ-ing style) and a stylized mesh by harnessing the rep-resentational power of CLIP. Text2Mesh requires nei-ther a pre-trained generative model nor a specialized 3D mesh dataset.It can handle low-quality meshes (non-manifold, boundaries, etc.) with arbitrary genus, and does not require UV parameterization. We demon-strate the ability of our technique to synthesize a myr-iad of styles over a wide variety of 3D meshes. Our code and results are available in our project webpage: https://threedle.github.io/text2mesh/. 