Temporal action detection (TAD) is an important yet challenging task in video understanding. It aims to simulta-neously predict the semantic label and the temporal inter-val of every action instance in an untrimmed video. Rather than end-to-end learning, most existing methods adopt a head-only learning paradigm, where the video encoder is pre-trained for action classification, and only the detec-tion head upon the encoder is optimized for TAD. The ef-fect of end-to-end learning is not systematically evaluated.Besides, there lacks an in-depth study on the efficiency-accuracy trade-off in end-to-end TAD. In this paper, we present an empirical study of end-to-end temporal action detection. We validate the advantage of end-to-end learn-ing over head-only learning and observe up to 11% perfor-mance improvement. Besides, we study the effects of mul-tiple design choices that affect the TAD performance and speed, including detection head, video encoder, and reso-lution of input videos. Based on the findings, we build a mid-resolution baseline detector, which achieves the state-of-the-art performance of end-to-end methods while run-ning more than 4Ã— faster. We hope that this paper can serve as a guide for end-to-end learning and inspire future research in this field. Code and models are available at https://github.com/xlliu7/E2E-TAD. 