Federated learning (FL) is a distributed learning paradigm that enables multiple clients to collaboratively learn a shared global model. Despite the recent progress, it remains challenging to deal with heterogeneous data clients, as the discrepant data distributions usually pre-vent the global model from delivering good generalization ability on each participating client. In this paper, we pro-pose CD2-pFed, a novel Cyclic Distillation-guided ChannelDecoupling framework, to personalize the global model inFL, under various settings of data heterogeneity. Differ-ent from previous works which establish layer-wise per-sonalization to overcome the non-IID data across different clients, we make the first attempt at channel-wise assign-ment for model personalization, referred to as channel de-coupling. To further facilitate the collaboration between private and shared weights, we propose a novel cyclic dis-tillation scheme to impose a consistent regularization be-tween the local and global model representations during the federation. Guided by the cyclical distillation, our chan-nel decoupling framework can deliver more accurate and generalized results for different kinds of heterogeneity, such as feature skew, label distribution skew, and concept shift.Comprehensive experiments on four benchmarks, including natural image and medical image analysis tasks, demon-strate the consistent effectiveness of our method on both lo-cal and external validations. 