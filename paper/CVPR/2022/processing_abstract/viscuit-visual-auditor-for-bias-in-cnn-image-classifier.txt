CNN image classifiers are widely used, thanks to their efficiency and accuracy. However, they can suffer from bi-ases that impede their practical applications. Most existing bias investigation techniques are either inapplicable to gen-eral image classification tasks or require significant user efforts in perusing all data subgroups to manually specify which data attributes to inspect. We present VISCUIT, an interactive visualization system that reveals how and why a CNN classifier is biased. VISCUIT visually summa-rizes the subgroups on which the classifier underperforms and helps users discover and characterize the cause of the underperformances by revealing image concepts responsi-ble for activating neurons that contribute to misclassifica-tions. VISCUIT runs in modern browsers and is open-source, allowing people to easily access and extend the tool to other model architectures and datasets. VISCUIT is available at the following public demo link: https://poloclub.github.io/VisCUIT. A video demo is available at https://youtu.be/eNDbSyM4R_4. 