Correctly classifying adversarial examples is an essential but challenging requirement for safely deploying machine learning models. As reported in RobustBench, even the state-of-the-art adversarially trained models struggle to exceed 67% robust test accuracy on CIFAR-10, which is far from practical. A complementary way towards robustness is to introduce a rejection option, allowing the model to not re-turn predictions on uncertain inputs, where conﬁdence is a commonly used certainty proxy. Along with this routine, weﬁnd that conﬁdence and a rectiﬁed conﬁdence (R-Con) can form two coupled rejection metrics, which could provably distinguish wrongly classiﬁed inputs from correctly classiﬁed ones. This intriguing property sheds light on using coupling strategies to better detect and reject adversarial examples.We evaluate our rectiﬁed rejection (RR) module on CIFAR-10,CIFAR-10-C, and CIFAR-100 under several attacks includ-ing adaptive ones, and demonstrate that the RR module is compatible with different adversarial training frameworks on improving robustness, with little extra computation. 