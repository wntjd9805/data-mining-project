For Visible-Infrared person Re-IDentification (VI-ReID), existing modality-specific information compensation based models try to generate the images of missing modal-ity from existing ones for reducing cross-modality dis-crepancy. However, because of the large modality dis-crepancy between visible and infrared images, the gen-erated images usually have low qualities and introduce much more interfering information (e.g., color inconsis-tency).This greatly degrades the subsequent VI-ReID performance. Alternatively, we present a novel Feature-level Modality Compensation Network (FMCNet) for VI-ReID in this paper, which aims to compensate the miss-ing modality-specific information in the feature level rather than in the image level, i.e., directly generating those miss-ing modality-specific features of one modality from exist-ing modality-shared features of the other modality. This will enable our model to mainly generate some discrim-inative person related modality-specific features and dis-card those non-discriminative ones for benefiting VI-ReID.For that, a single-modality feature decomposition module is first designed to decompose single-modality features into modality-specific ones and modality-shared ones. Then, a feature-level modality compensation module is present to generate those missing modality-specific features from ex-isting modality-shared ones. Finally, a shared-specific fea-ture fusion module is proposed to combine the existing and generated features for VI-ReID. The effectiveness of our proposed model is verified on two benchmark datasets. 