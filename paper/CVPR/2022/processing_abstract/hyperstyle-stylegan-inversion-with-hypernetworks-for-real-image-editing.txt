The inversion of real images into StyleGAN’s latent space is a well-studied problem. Nevertheless, applying ex-isting approaches to real-world scenarios remains an open challenge, due to an inherent trade-off between reconstruc-tion and editability: latent space regions which can accu-rately represent real images typically suffer from degraded semantic control. Recent work proposes to mitigate this trade-off by ﬁne-tuning the generator to add the target im-age to well-behaved, editable regions of the latent space.While promising, this ﬁne-tuning scheme is impractical for prevalent use as it requires a lengthy training phase for each new image. In this work, we introduce this approach into the realm of encoder-based inversion. We propose HyperStyle, a hypernetwork that learns to modulate StyleGAN’s weights to faithfully express a given image in editable regions of the latent space. A naive modulation approach would re-quire training a hypernetwork with over three billion pa-rameters. Through careful network design, we reduce this to be in line with existing encoders. HyperStyle yields reconstructions comparable to those of optimization tech-niques with the near real-time inference capabilities of en-coders. Lastly, we demonstrate HyperStyle’s effectiveness on several applications beyond the inversion task, includ-ing the editing of out-of-domain images which were never seen during training. Code is available on our project page: https://yuval- alaluf.github.io/hyperstyle/. 