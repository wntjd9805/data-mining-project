track  - raceIMLVideo Question Answering (VideoQA) is the task of an-swering questions about a video. At its core is understand-ing the alignments between visual scenes in video and lin-guistic semantics in question to yield the answer. In lead-ing VideoQA models, the typical learning objective, empiri-cal risk minimization (ERM), latches on superﬁcial corre-lations between video-question pairs and answers as the alignments. However, ERM can be problematic, because it tends to over-exploit the spurious correlations between question-irrelevant scenes and answers, instead of inspect-ing the causal effect of question-critical scenes. As a result, the VideoQA models suffer from unreliable reasoning. answer (b)In this work, we ﬁrst take a causal look at VideoQA and argue that invariant grounding is the key to ruling out the spurious correlations. Towards this end, we propose a new learning framework, Invariant Grounding for VideoQA (IGV), to ground the question-critical scene, whose causal relations with answers are invariant across different inter-ventions on the complement. With IGV, the VideoQA mod-els are forced to shield the answering process from the negative inﬂuence of spurious correlations, which signiﬁ-cantly improves the reasoning ability. Experiments on three benchmark datasets validate the superiority of IGV in terms of accuracy, visual explainability, and generalization abil-ity over the leading baselines. Our code is available at https://github.com/yl3800/IGV . 