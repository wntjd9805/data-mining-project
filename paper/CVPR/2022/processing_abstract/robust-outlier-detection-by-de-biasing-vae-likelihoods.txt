Deep networks often make conﬁdent, yet, incorrect, pre-dictions when tested with outlier data that is far removed from their training distributions. Likelihoods computed by deep generative models (DGMs) are a candidate metric for outlier detection with unlabeled data. Yet, previous studies have shown that DGM likelihoods are unreliable and can be easily biased by simple transformations to in-put data. Here, we examine outlier detection with varia-tional autoencoders (VAEs), among the simplest of DGMs.We propose novel analytical and algorithmic approaches to ameliorate key biases with VAE likelihoods. Our bias corrections are sample-speciﬁc, computationally inexpen-sive, and readily computed for various decoder visible dis-tributions. Next, we show that a well-known image pre-processing technique – contrast stretching – extends the effectiveness of bias correction to further improve outlier detection. Our approach achieves state-of-the-art accura-cies with nine grayscale and natural image datasets, and demonstrates signiﬁcant advantages – both with speed and performance – over four recent, competing approaches.In summary, lightweight remedies sufﬁce to achieve robust outlier detection with VAEs.1 