Finding relevant moments and highlights in videos ac-cording to natural language queries is a natural and highly valuable common need in the current video content explo-sion era. Nevertheless, jointly conducting moment retrieval and highlight detection is an emerging research topic, even though its component problems and some related tasks have already been studied for a while. In this paper, we present the first unified framework, named Unified Multi-modalTransformers (UMT), capable of realizing such joint opti-mization while can also be easily degenerated for solving individual problems. As far as we are aware, this is the first scheme to integrate multi-modal (visual-audio) learn-ing for either joint optimization or the individual moment retrieval task, and tackles moment retrieval as a keypointâˆ—Corresponding author. detection problem using a novel query generator and query decoder. Extensive comparisons with existing methods and ablation studies on QVHighlights, Charades-STA, YouTubeHighlights, and TVSum datasets demonstrate the effective-ness, superiority, and flexibility of the proposed method un-der various settings. Source code and pre-trained models are available at https://github.com/TencentARC/UMT. 