Comprehending the rich semantics in an image and or-dering them in linguistic order are essential to compose a visually-grounded and linguistically coherent description for image captioning. Modern techniques commonly cap-italize on a pre-trained object detector/classiﬁer to mine the semantics in an image, while leaving the inherent lin-guistic ordering of semantics under-exploited. In this pa-per, we propose a new recipe of Transformer-style struc-ture, namely Comprehending and Ordering Semantics Net-works (COS-Net), that novelly uniﬁes an enriched seman-tic comprehending and a learnable semantic ordering pro-cesses into a single architecture. Technically, we initially utilize a cross-modal retrieval model to search the relevant sentences of each image, and all words in the searched sen-tences are taken as primary semantic cues. Next, a novel semantic comprehender is devised to ﬁlter out the irrele-vant semantic words in primary semantic cues, and mean-while infer the missing relevant semantic words visually grounded in the image. After that, we feed all the screened and enriched semantic words into a semantic ranker, which learns to allocate all semantic words in linguistic order as humans.Such sequence of ordered semantic words are further integrated with visual tokens of images to trig-ger sentence generation. Empirical evidences show thatCOS-Net clearly surpasses the state-of-the-art approaches on COCO and achieves to-date the best CIDEr score of 141.1% on Karpathy test split. Source code is available at https://github.com/YehLi/xmodaler/tree/ master/configs/image_caption/cosnet. 