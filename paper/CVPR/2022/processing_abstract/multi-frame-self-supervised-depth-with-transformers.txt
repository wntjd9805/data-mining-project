Multi-frame depth estimation improves over single-frame approaches by also leveraging geometric relation-ships between images via feature matching, in addition to learning appearance-based features. In this paper we re-visit feature matching for self-supervised monocular depth estimation, and propose a novel transformer architecture for cost volume generation. We use depth-discretized epipo-lar sampling to select matching candidates, and reﬁne pre-dictions through a series of self- and cross-attention lay-ers. These layers sharpen the matching probability between pixel features, improving over standard similarity metrics prone to ambiguities and local minima. The reﬁned cost vol-ume is decoded into depth estimates, and the whole pipeline is trained end-to-end from videos using only a photometric objective. Experiments on the KITTI and DDAD datasets show that our DepthFormer architecture establishes a new state of the art in self-supervised monocular depth estima-tion, and is even competitive with highly specialized su-pervised single-frame architectures. We also show that our learned cross-attention network yields representations transferable across datasets, increasing the effectiveness of pre-training strategies. Project page: https://sites. google.com/tri.global/depthformer. 