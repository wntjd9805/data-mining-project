Neural Architecture Search (NAS) has attracted in-creasingly more attention in recent years because of its capability to design deep neural network automatically.Among them, differential NAS approaches such as DARTS, have gained popularity for the search efficiency. How-ever, they suffer from two main issues, the weak robust-ness to the performance collapse and the poor general-ization ability of the searched architectures.To solve these two problems, a simple-but-efficient regularization method, termed as Beta-Decay, is proposed to regularize theDARTS-based NAS searching process. Specifically, Beta-Decay regularization can impose constraints to keep the value and variance of activated architecture parameters from too large. Furthermore, we provide in-depth theo-retical analysis on how it works and why it works. Ex-perimental results on NAS-Bench-201 show that our pro-posed method can help to stabilize the searching process and makes the searched network more transferable across different datasets.In addition, our search scheme shows an outstanding property of being less dependent on train-ing time and data. Comprehensive experiments on a va-riety of search spaces and datasets validate the effective-ness of the proposed method. The code is available at https://github.com/Sunshine-Ye/Beta-DARTS. 