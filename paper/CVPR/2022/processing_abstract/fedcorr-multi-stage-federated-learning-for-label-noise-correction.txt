Federated learning (FL) is a privacy-preserving dis-tributed learning paradigm that enables clients to jointly train a global model.In real-world FL implementations, client data could have label noise, and different clients could have vastly different label noise levels. Although there exist methods in centralized learning for tackling la-bel noise, such methods do not perform well on heteroge-neous label noise in FL settings, due to the typically smaller sizes of client datasets and data privacy requirements in FL.In this paper, we propose FedCorr, a general multi-stage framework to tackle heterogeneous label noise in FL, with-out making any assumptions on the noise models of local clients, while still maintaining client data privacy. In par-ticular, (1) FedCorr dynamically identiﬁes noisy clients by exploiting the dimensionalities of the model prediction subspaces independently measured on all clients, and then identiﬁes incorrect labels on noisy clients based on per-sample losses. To deal with data heterogeneity and to in-crease training stability, we propose an adaptive local prox-imal regularization term that is based on estimated local noise levels. (2) We further ﬁnetune the global model on identiﬁed clean clients and correct the noisy labels for the remaining noisy clients after ﬁnetuning. (3) Finally, we ap-ply the usual training on all clients to make full use of all local data. Experiments conducted on CIFAR-10/100 with federated synthetic label noise, and on a real-world noisy dataset, Clothing1M, demonstrate that FedCorr is robust to label noise and substantially outperforms the state-of-the-art methods at multiple noise levels. 