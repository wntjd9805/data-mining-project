Referring video object segmentation aims to predict fore-ground labels for objects referred by natural language ex-pressions in videos. Previous methods either depend on 3DConvNets or incorporate additional 2D ConvNets as en-coders to extract mixed spatial-temporal features. However, these methods suffer from spatial misalignment or false dis-tractors due to delayed and implicit spatial-temporal inter-action occurring in the decoding phase. To tackle these limitations, we propose a Language-Bridged Duplex Trans-fer (LBDT) module which utilizes language as an interme-diary bridge to accomplish explicit and adaptive spatial-temporal interaction earlier in the encoding phase. Con-cretely, cross-modal attention is performed among the tem-poral encoder, referring words and the spatial encoder to aggregate and transfer language-relevant motion and ap-pearance information. In addition, we also propose a Bi-lateral Channel Activation (BCA) module in the decoding phase for further denoising and highlighting the spatial-temporal consistent features via channel-wise activation.Extensive experiments show our method achieves new state-of-the-art performances on four popular benchmarks with 6.8% and 6.9% absolute AP gains on A2D Sentences andJ-HMDB Sentences respectively, while consuming around 7Ã— less computational overhead 1. 