Deep Neural Networks (DNNs) usually have a large number of parameters and consume a huge volume of storage space, which limits the application of DNNs on memory-constrained devices. Network quantization is an appealing way to compress DNNs. However, most of exist-ing quantization methods require the training dataset and a fine-tuning procedure to preserve the quality of a full-precision model. These are unavailable for the confiden-tial scenarios due to personal privacy and security prob-lems. Focusing on this issue, we propose a novel data-free method for network compression called PNMQ, which em-ploys the Parametric Non-uniform Mixed precision Quan-tization to generate a quantized network. During the com-pression stage, the optimal parametric non-uniform quan-tization grid is calculated directly for each layer to mini-mize the quantization error. User can directly specify the re-quired compression ratio of a network, which is used by thePNMQ algorithm to select bitwidths of layers. This method does not require any model retraining or expensive cal-culations, which allows efficient implementations for net-work compression on edge devices. Extensive experiments have been conducted on various computer vision tasks and the results demonstrate that PNMQ achieves better perfor-mance than other state-of-the-art methods of network com-pression. 