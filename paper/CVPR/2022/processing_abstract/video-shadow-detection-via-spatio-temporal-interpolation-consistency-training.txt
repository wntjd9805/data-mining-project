It is challenging to annotate large-scale datasets for su-pervised video shadow detection methods. Using a model trained on labeled images to the video frames directly may lead to high generalization error and temporal inconsis-In this paper, we address these challenges tent results. by proposing a Spatio-Temporal Interpolation ConsistencyTraining (STICT) framework to rationally feed the unla-beled video frames together with the labeled images into an image shadow detection network training. Specifically, we propose the Spatial and Temporal ICT, in which we de-fine two new interpolation schemes, i.e., the spatial interpo-lation and the temporal interpolation. We then derive the spatial and temporal interpolation consistency constraints accordingly for enhancing generalization in the pixel-wise classification task and for encouraging temporal consistent predictions, respectively. In addition, we design a Scale-Aware Network for multi-scale shadow knowledge learn-ing in images, and propose a scale-consistency constraint to minimize the discrepancy among the predictions at dif-ferent scales. Our proposed approach is extensively val-idated on the ViSha dataset and a self-annotated dataset.Experimental results show that, even without video labels, our approach is better than most state of the art supervised, semi-supervised or unsupervised image/video shadow de-tection methods and other methods in related tasks. Code and dataset are available at https://github.com/ yihong-97/STICT.*Corresponding author.Figure 1. Shadow maps produced by our image shadow detection network SANet (a) trained on labeled images and (b) trained on both labeled images and unlabeled videos with STICT. 