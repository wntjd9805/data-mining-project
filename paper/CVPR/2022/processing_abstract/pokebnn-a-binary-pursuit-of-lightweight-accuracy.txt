Optimization of Top-1 ImageNet promotes enormous net-works that may be impractical in inference settings. Binary neural networks (BNNs) have the potential to signiﬁcantly lower the compute intensity but existing models suffer from low quality. To overcome this deﬁciency, we propose Poke-Conv, a binary convolution block which improves quality of BNNs by techniques such as adding multiple residual paths, and tuning the activation function. We apply it toResNet-50 and optimize ResNet’s initial convolutional layer which is hard to binarize. We name the resulting network family PokeBNN1. These techniques are chosen to yield fa-vorable improvements in both top-1 accuracy and the net-work’s cost.In order to enable joint optimization of the cost together with accuracy, we deﬁne arithmetic compu-tation effort (ACE), a hardware- and energy-inspired cost metric for quantized and binarized networks. We also iden-tify a need to optimize an under-explored hyper-parameter controlling the binarization gradient approximation.We establish a new, strong state-of-the-art (SOTA) on top-1 accuracy together with commonly-used CPU64 cost,ACE cost and network size metrics. ReActNet-Adam [33], the previous SOTA in BNNs, achieved a 70.5% top-1 ac-curacy with 7.9 ACE. A small variant of PokeBNN achieves 70.5% top-1 with 2.6 ACE, more than 3x reduction in cost; a larger PokeBNN achieves 75.6% top-1 with 7.8 ACE, more than 5% improvement in accuracy without increasing the cost. PokeBNN implementation in JAX/Flax [6, 18] and re-production instructions are open sourced.2 