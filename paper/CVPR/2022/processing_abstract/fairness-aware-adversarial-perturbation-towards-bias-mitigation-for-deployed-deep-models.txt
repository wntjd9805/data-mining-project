Prioritizing fairness is of central importance in artifi-cial intelligence (AI) systems, especially for those societal applications, e.g., hiring systems should recommend appli-cants equally from different demographic groups, and risk assessment systems must eliminate racism in criminal jus-tice. Existing efforts towards the ethical development of AI systems have leveraged data science to mitigate biases in the training set or introduced fairness principles into the training process. For a deployed AI system, however, it may not allow for retraining or tuning in practice. By contrast, we propose a more flexible approach, i.e., fairness-aware adversarial perturbation (FAAP), which learns to perturb input data to blind deployed models on fairness-related fea-tures, e.g., gender and ethnicity. The key advantage is thatFAAP does not modify deployed models in terms of param-eters and structures. To achieve this, we design a discrimi-nator to distinguish fairness-related attributes based on la-tent representations from deployed models. Meanwhile, a perturbation generator is trained against the discrimina-tor, such that no fairness-related features could be extracted from perturbed inputs. Exhaustive experimental evaluation demonstrates the effectiveness and superior performance of the proposed FAAP. In addition, FAAP is validated on real-world commercial deployments (inaccessible to model pa-rameters), which shows the transferability of FAAP, foresee-ing the potential of black-box adaptation. 