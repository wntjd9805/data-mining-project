Attention-based neural networks such as the Vision Trans-former (ViT) have recently attained state-of-the-art results on many computer vision benchmarks. Scale is a primary ingredient in attaining excellent results, therefore, under-standing a model’s scaling properties is a key to designing future generations effectively. While the laws for scalingTransformer language models have been studied, it is un-known how Vision Transformers scale. To address this, we scale ViT models and data, both up and down, and character-ize the relationships between error rate, data, and compute.Along the way, we reﬁne the architecture and training of ViT, reducing memory consumption and increasing accuracy of the resulting models. As a result, we successfully train aViT model with two billion parameters, which attains a new state-of-the-art on ImageNet of 90.45% top-1 accuracy. The model also performs well for few-shot transfer, for example, reaching 84.86% top-1 accuracy on ImageNet with only 10 examples per class. 