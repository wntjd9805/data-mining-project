Indoor scenes exhibit signiﬁcant appearance variations due to myriad interactions between arbitrarily diverse object shapes, spatially-changing materials, and complex light-ing. Shadows, highlights, and inter-reﬂections caused by visible and invisible light sources require reasoning about long-range interactions for inverse rendering, which seeks to recover the components of image formation, namely, shape, material, and lighting. In this work, our intuition is that the long-range attention learned by transformer architec-tures is ideally suited to solve longstanding challenges in single-image inverse rendering. We demonstrate with a spe-ciﬁc instantiation of a dense vision transformer, IRISformer, that excels at both single-task and multi-task reasoning re-quired for inverse rendering. Speciﬁcally, we propose a transformer architecture to simultaneously estimate depths, normals, spatially-varying albedo, roughness and lighting from a single image of an indoor scene. Our extensive evalu-ations on benchmark datasets demonstrate state-of-the-art results on each of the above tasks, enabling applications like object insertion and material editing in a single uncon-strained real image, with greater photorealism than prior works. Code and data are publicly released.1 