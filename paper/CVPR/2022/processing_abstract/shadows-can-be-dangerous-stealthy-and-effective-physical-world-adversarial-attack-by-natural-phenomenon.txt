Estimating the risk level of adversarial examples is es-sential for safely deploying machine learning models in the real world. One popular approach for physical-world at-tacks is to adopt the “sticker-pasting” strategy, which how-ever suffers from some limitations, including difﬁculties in access to the target or printing by valid colors. A new type of non-invasive attacks emerged recently, which attempt to cast perturbation onto the target by optics based tools, such as laser beam and projector. However, the added optical patterns are artiﬁcial but not natural. Thus, they are still conspicuous and attention-grabbed, and can be easily no-ticed by humans. In this paper, we study a new type of op-tical adversarial examples, in which the perturbations are generated by a very common natural phenomenon, shadow, to achieve naturalistic and stealthy physical-world adver-sarial attack under the black-box setting. We extensively evaluate the effectiveness of this new attack on both simu-lated and real-world environments. Experimental results on trafﬁc sign recognition demonstrate that our algorithm can generate adversarial examples effectively, reaching 98.23% and 90.47% success rates on LISA and GTSRB test sets re-spectively, while continuously misleading a moving camera over 95% of the time in real-world scenarios. We also offer discussions about the limitations and the defense mecha-nism of this attack1. 