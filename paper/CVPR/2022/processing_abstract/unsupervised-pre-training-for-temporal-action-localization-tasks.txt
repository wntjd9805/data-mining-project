Unsupervised video representation learning has made remarkable achievements in recent years. However, most existing methods are designed and optimized for video clas-sification. These pre-trained models can be sub-optimal for temporal localization tasks due to the inherent discrep-ancy between video-level classification and clip-level local-ization. To bridge this gap, we make the first attempt to propose a self-supervised pretext task, coined as PseudoAction Localization (PAL) to Unsupervisedly Pre-train fea-ture encoders for Temporal Action Localization tasks (UP-TAL). Specifically, we first randomly select temporal re-gions, each of which contains multiple clips, from one video as pseudo actions and then paste them onto different tem-poral positions of the other two videos. The pretext task is to align the features of pasted pseudo action regions from two synthetic videos and maximize the agreement between them. Compared to the existing unsupervised video rep-resentation learning approaches, our PAL adapts better to downstream TAL tasks by introducing a temporal equivari-ant contrastive learning paradigm in a temporally dense and scale-aware manner. Extensive experiments show thatPAL can utilize large-scale unlabeled video data to sig-nificantly boost the performance of existing TAL methods.Our codes and models will be made publicly available at https://github.com/zhang-can/UP-TAL. 