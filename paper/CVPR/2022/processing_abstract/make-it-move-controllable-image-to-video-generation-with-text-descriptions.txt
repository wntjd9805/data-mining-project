Generating controllable videos conforming to user in-tentions is an appealing yet challenging topic in computer vision. To enable maneuverable control in line with user in-tentions, a novel video generation task, named Text-Image-to-Video generation (TI2V), is proposed. With both con-trollable appearance and motion, TI2V aims at generat-ing videos from a static image and a text description. The key challenges of TI2V task lie both in aligning appear-ance and motion from different modalities, and in handling uncertainty in text descriptions. To address these chal-lenges, we propose a Motion Anchor-based video GEnera-tor (MAGE) with an innovative motion anchor (MA) struc-ture to store appearance-motion aligned representation. To model the uncertainty and increase the diversity, it further allows the injection of explicit condition and implicit ran-domness. Through three-dimensional axial transformers,MA is interacted with given image to generate next frames recursively with satisfying controllability and diversity. Ac-companying the new task, we build two new video-text paired datasets based on MNIST and CATER for evalua-tion. Experiments conducted on these datasets verify the ef-fectiveness of MAGE and show appealing potentials of TI2V task. Datasets are available at https:// github.com/ Youncy-Hu/ MAGE. 