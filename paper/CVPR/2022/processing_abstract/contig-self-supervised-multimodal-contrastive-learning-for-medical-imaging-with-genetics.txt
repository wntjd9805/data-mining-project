High annotation costs are a substantial bottleneck in ap-plying modern deep learning architectures to clinically rel-evant medical use cases, substantiating the need for novel algorithms to learn from unlabeled data. In this work, we propose ContIG, a self-supervised method that can learn from large datasets of unlabeled medical images and ge-netic data. Our approach aligns images and several ge-netic modalities in the feature space using a contrastive loss. We design our method to integrate multiple modal-ities of each individual person in the same model end-to-end, even when the available modalities vary across indi-viduals. Our procedure outperforms state-of-the-art self-supervised methods on all evaluated downstream bench-mark tasks. We also adapt gradient-based explainability algorithms to better understand the learned cross-modal as-sociations between the images and genetic modalities. Fi-nally, we perform genome-wide association studies on the features learned by our models, uncovering interesting re-lationships between images and genetic data.1 