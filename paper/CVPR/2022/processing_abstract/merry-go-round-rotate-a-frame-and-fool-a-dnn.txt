A large proportion of videos captured today are first per-son videos shot from wearable cameras. Similar to other computer vision tasks, Deep Neural Networks (DNNs) are the workhorse for most state-of-the-art (SOTA) egocentric vision techniques. On the other hand DNNs are known to be susceptible to Adversarial Attacks (AAs) which add im-perceptible noise to the input. Both black-box, as well as white-box attacks on image as well as video analysis tasks have been shown. We observe that most AA techniques ba-sically add intensity perturbation to an image. Even for videos, the same process is essentially repeated for each frame independently. We note that definition of impercep-tibility used for images may not be applicable for videos, where a small intensity change happening randomly in two consecutive frames may still be perceptible. In this paper we make a key novel suggestion to use perturbation in opti-cal flow to carry out AAs on a video analysis system. Such perturbation is especially useful for egocentric videos, be-cause there is lot of shake in the egocentric videos anyways, and adding a little more, keeps it highly imperceptible. In general our idea can be seen as adding structured, para-metric noise as the adversarial perturbation. Our imple-mentation of the idea by adding 3D rotations to the frames, reveal that using our technique, one can mount a black-boxAA on an egocentric activity detection system in one-third of the queries compared to the SOTA AA technique. 