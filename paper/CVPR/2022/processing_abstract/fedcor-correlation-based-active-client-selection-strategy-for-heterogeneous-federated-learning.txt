Client-wise data heterogeneity is one of the major is-sues that hinder effective training in federated learning (FL).Since the data distribution on each client may vary dramat-ically, the client selection strategy can significantly influ-ence the convergence rate of the FL process. Active client selection strategies are popularly proposed in recent stud-ies. However, they neglect the loss correlations between the clients and achieve only marginal improvement com-pared to the uniform selection strategy. In this work, we propose FedCor—an FL framework built on a correlation-based client selection strategy, to boost the convergence rate of FL. Specifically, we first model the loss correlations be-tween the clients with a Gaussian Process (GP). Based on the GP model, we derive a client selection strategy with a significant reduction of expected global loss in each round.Besides, we develop an efficient GP training method with a low communication overhead in the FL scenario by uti-lizing the covariance stationarity. Our experimental results show that compared to the state-of-the-art method, FedCorr can improve the convergence rates by 34% ∼ 99% and 26% ∼ 51% on FMNIST and CIFAR-10, respectively. 