Implicit neural rendering, especially Neural RadianceField (NeRF), has shown great potential in novel view syn-thesis of a scene. However, current NeRF-based methods cannot enable users to perform user-controlled shape de-formation in the scene. While existing works have pro-posed some approaches to modify the radiance ﬁeld ac-cording to the user’s constraints, the modiﬁcation is limited to color editing or object translation and rotation. In this paper, we propose a method that allows users to perform controllable shape deformation on the implicit representa-tion of the scene, and synthesizes the novel view images of the edited scene without re-training the network. Speciﬁ-cally, we establish a correspondence between the extracted explicit mesh representation and the implicit neural repre-sentation of the target scene. Users can ﬁrst utilize well-developed mesh-based deformation methods to deform the mesh representation of the scene. Our method then utilizes user edits from the mesh representation to bend the camera rays by introducing a tetrahedra mesh as a proxy, obtaining the rendering results of the edited scene. Extensive exper-iments demonstrate that our framework can achieve ideal editing results not only on synthetic data, but also on real scenes captured by users. 