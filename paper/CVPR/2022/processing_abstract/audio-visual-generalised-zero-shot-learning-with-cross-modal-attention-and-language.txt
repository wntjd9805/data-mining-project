i.e.Learning to classify video data from classes not in-cluded in the training data, video-based zero-shot learning, is challenging. We conjecture that the natural alignment between the audio and visual modalities in video data provides a rich training signal for learning discrim-inative multi-modal representations. Focusing on the rel-atively underexplored task of audio-visual zero-shot learn-ing, we propose to learn multi-modal representations from audio-visual data using cross-modal attention and exploit textual label embeddings for transferring knowledge from seen classes to unseen classes. Taking this one step fur-ther, in our generalised audio-visual zero-shot learning set-ting, we include all the training classes in the test-time search space which act as distractors and increase the dif-ficulty while making the setting more realistic. Due to the lack of a unified benchmark in this domain, we introduce a (generalised) zero-shot learning benchmark on three audio-visual datasets of varying sizes and difficulty, VGGSound,UCF, and ActivityNet, ensuring that the unseen test classes do not appear in the dataset used for supervised training of the backbone deep models. Comparing multiple rele-vant and recent methods, we demonstrate that our proposedAVCA model achieves state-of-the-art performance on all three datasets. Code and data are available at https://github.com/ExplainableML/AVCA-GZSL. 