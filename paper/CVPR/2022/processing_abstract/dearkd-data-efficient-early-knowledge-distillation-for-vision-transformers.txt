Transformers are successfully applied to computer vi-sion due to their powerful modeling capacity with self-attention. However, the excellent performance of transform-ers heavily depends on enormous training images. Thus, a data-efficient transformer solution is urgently needed.In this work, we propose an early knowledge distillation framework, which is termed as DearKD, to improve the data efficiency required by transformers. Our DearKD is a two-stage framework that first distills the inductive bi-ases from the early intermediate layers of a CNN and then gives the transformer full play by training without distil-lation. Further, our DearKD can be readily applied to the extreme data-free case where no real images are avail-able. In this case, we propose a boundary-preserving intra-divergence loss based on DeepInversion to further close the performance gap against the full-data counterpart. Exten-sive experiments on ImageNet, partial ImageNet, data-free setting and other downstream tasks prove the superiority ofDearKD over its baselines and state-of-the-art methods. 