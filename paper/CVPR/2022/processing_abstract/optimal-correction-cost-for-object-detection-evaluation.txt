Mean Average Precision (mAP) is the primary evalua-tion measure for object detection. Although object detec-tion has a broad range of applications, mAP evaluates de-tectors in terms of the performance of ranked instance re-trieval. Such the assumption for the evaluation task does not suit some downstream tasks. To alleviate the gap between downstream tasks and the evaluation scenario, we proposeOptimal Correction Cost (OC-cost), which assesses detec-tion accuracy at image level. OC-cost computes the cost of correcting detections to ground truths as a measure of accuracy. The cost is obtained by solving an optimal trans-portation problem between the detections and the ground truths. Unlike mAP, OC-cost is designed to penalize false positive and false negative detections properly, and every image in a dataset is treated equally. Our experimental re-sult validates that OC-cost has better agreement with hu-man preference than a ranking-based measure, i.e., mAP for a single image. We also show that detectorsâ€™ rankings by OC-cost are more consistent on different data splits than mAP. Our goal is not to replace mAP with OC-cost but pro-vide an additional tool to evaluate detectors from another aspect. To help future researchers and developers choose a target measure, we provide a series of experiments to clarify how mAP and OC-cost differ. 