Backdoor attacks aim to cause misclassiﬁcation of a sub-ject model by stamping a trigger to inputs. Backdoors could be injected through malicious training and naturally exist.Deriving backdoor trigger for a subject model is critical to both attack and defense. A popular trigger inversion method is by optimization. Existing methods are based on ﬁnding a smallest trigger that can uniformly ﬂip a set of input sam-ples by minimizing a mask. The mask deﬁnes the set of pixels that ought to be perturbed. We develop a new op-timization method that directly minimizes individual pixel changes, without using a mask. Our experiments show that compared to existing methods, the new one can generate triggers that require a smaller number of input pixels to be perturbed, have a higher attack success rate, and are more robust. They are hence more desirable when used in real-world attacks and more effective when used in defense. Our method is also more cost-effective. 