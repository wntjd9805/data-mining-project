Deep Metric Learning (DML) aims to learn represen-tation spaces on which semantic relations can simply be expressed through predeﬁned distance metrics. Best per-forming approaches commonly leverage class proxies as sample stand-ins for better convergence and generalization.However, these proxy-methods solely optimize for sample-proxy distances. Given the inherent non-bijectiveness of used distance functions, this can induce locally isotropic sample distributions, leading to crucial semantic context being missed due to difﬁculties resolving local structures and intraclass relations between samples. To alleviate this problem, we propose non-isotropy regularization (NIR) for proxy-based Deep Metric Learning. By leveraging Nor-malizing Flows, we enforce unique translatability of sam-ples from their respective class proxies. This allows us to explicitly induce a non-isotropic distribution of samples around a proxy to optimize for. In doing so, we equip proxy-based objectives to better learn local structures. Exten-sive experiments highlight consistent generalization bene-ﬁts of NIR while achieving competitive and state-of-the-art performance on the standard benchmarks CUB200-In addi-2011, Cars196 and Stanford Online Products. tion, we ﬁnd the superior convergence properties of proxy-based methods to still be retained or even improved, makingNIR very attractive for practical usage. Code available at github.com/ExplainableML/NonIsotropicProxyDML. 