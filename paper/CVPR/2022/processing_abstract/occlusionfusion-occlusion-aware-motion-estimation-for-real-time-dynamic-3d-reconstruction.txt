RGBD-based real-time dynamic 3D reconstruction suf-fers from inaccurate inter-frame motion estimation as er-rors may accumulate with online tracking. This problem is even more severe for single-view-based systems due to strong occlusions. Based on these observations, we proposeOcclusionFusion, a novel method to calculate occlusion-aware 3D motion to guide the reconstruction. In our tech-nique, the motion of visible regions is first estimated and combined with temporal information to infer the motion of the occluded regions through an LSTM-involved graph neu-ral network. Furthermore, our method computes the con-fidence of the estimated motion by modeling the network output with a probabilistic model, which alleviates untrust-worthy motions and enables robust tracking. Experimental results on public datasets and our own recorded data show that our technique outperforms existing single-view-based real-time methods by a large margin. With the reduction of the motion errors, the proposed technique can handle long and challenging motion sequences. Please check out the project page for sequence results: https://wenbin-lin.github.io/OcclusionFusion. 