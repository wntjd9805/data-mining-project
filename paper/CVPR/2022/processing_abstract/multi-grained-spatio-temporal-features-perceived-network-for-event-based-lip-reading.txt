Automatic lip-reading (ALR) aims to recognize words us-ing visual information from the speakerâ€™s lip movements.In this work, we introduce a novel type of sensing de-vice, event cameras, for the task of ALR. Event cameras have both technical and application advantages over con-ventional cameras for the ALR task because they have higher temporal resolution, less redundant visual informa-tion, and lower power consumption. To recognize words from the event data, we propose a novel Multi-grainedSpatio-Temporal Features Perceived Network (MSTP) to perceive fine-grained spatio-temporal features from mi-crosecond time-resolved event data. Specifically, a multi-branch network architecture is designed, in which differ-ent grained spatio-temporal features are learned by oper-ating at different frame rates. The branch operating on the low frame rate can perceive spatial complete but tem-poral coarse features. While the branch operating on the high frame rate can perceive spatial coarse but temporal refinement features. And a message flow module is devised to integrate the features from different branches, leading to perceiving more discriminative spatio-temporal features.In addition, we present the first event-based lip-reading dataset (DVS-Lip) captured by the event camera. Experi-mental results demonstrated the superiority of the proposed model compared to the state-of-the-art event-based action recognition models and video-based lip-reading models. 