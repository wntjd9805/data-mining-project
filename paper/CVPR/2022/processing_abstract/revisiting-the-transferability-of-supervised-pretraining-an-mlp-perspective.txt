The pretrain-finetune paradigm is a classical pipeline in visual learning. Recent progress on unsupervised pre-training methods shows superior transfer performance to their supervised counterparts. This paper revisits this phe-nomenon and sheds new light on understanding the trans-ferability gap between unsupervised and supervised pre-training from a multilayer perceptron (MLP) perspective.While previous works [6, 8, 17] focus on the effectiveness of MLP on unsupervised image classification where pre-training and evaluation are conducted on the same dataset, we reveal that the MLP projector is also the key factor to better transferability of unsupervised pretraining methods than supervised pretraining methods. Based on this ob-servation, we attempt to close the transferability gap be-tween supervised and unsupervised pretraining by adding an MLP projector before the classifier in supervised pre-training. Our analysis indicates that the MLP projector can help retain intra-class variation of visual features, decrease the feature distribution distance between pretraining and evaluation datasets, and reduce feature redundancy. Ex-tensive experiments on public benchmarks demonstrate that the added MLP projector significantly boosts the transfer-ability of supervised pretraining, e.g. +7.2% top-1 accuracy on the concept generalization task, +5.8% top-1 accuracy for linear evaluation on 12-domain classification tasks, and+0.8% AP on COCO object detection task, making super-vised pretraining comparable or even better than unsuper-vised pretraining. 