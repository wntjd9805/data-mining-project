Image-text matching, as a fundamental task, bridges the gap between vision and language. The key of this task is to accurately measure similarity between these two modal-ities. Prior work measuring this similarity mainly based on matched fragments (i.e., word/region with high rele-vance), while underestimating or even ignoring the effect of mismatched fragments (i.e., word/region with low rel-evance), e.g., via a typical LeaklyReLU or ReLU opera-tion that forces negative scores close or exact to zero in attention. This work argues that mismatched textual frag-ments, which contain rich mismatching clues, are also cru-cial for image-text matching. We thereby propose a novelNegative-Aware Attention Framework (NAAF), which ex-plicitly exploits both the positive effect of matched frag-ments and the negative effect of mismatched fragments to jointly infer image-text similarity. NAAF (1) delicately de-signs an iterative optimization method to maximally mine the mismatched fragments, facilitating more discrimina-tive and robust negative effects, and (2) devises the two-branch matching mechanism to precisely calculate similar-ity/dissimilarity degrees for matched/mismatched fragments with different masks. Extensive experiments on two bench-mark datasets, i.e., Flickr30K and MSCOCO, demonstrate the superior effectiveness of our NAAF, achieving state-of-the-art performance. Code will be released at: https://github.com/CrossmodalGroup/NAAF. 