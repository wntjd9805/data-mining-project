Unpaired image-to-image (I2I) translation often requires to maximize the mutual information between the source and the translated images across different domains, which is critical for the generator to keep the source content and pre-vent it from unnecessary modiﬁcations. The self-supervised contrastive learning has already been successfully applied in the I2I. By constraining features from the same loca-tion to be closer than those from different ones, it implicitly ensures the result to take content from the source. How-ever, previous work uses the features from random loca-tions to impose the constraint, which may not be appropri-ate since some locations contain less information of source domain. Moreover, the feature itself does not reﬂect the relation with others. This paper deals with these prob-lems by intentionally selecting signiﬁcant anchor points for contrastive learning. We design a query-selected attention (QS-Attn) module, which compares feature distances in the source domain, giving an attention matrix with a prob-ability distribution in each row. Then we select queries according to their measurement of signiﬁcance, computed from the distribution. The selected ones are regarded as anchors for contrastive loss. At the same time, the re-duced attention matrix is employed to route features in both domains, so that source relations maintain in the synthe-sis. We validate our proposed method in three different I2I datasets, showing that it increases the image quality with-out adding learnable parameters. Codes are available at https://github.com/sapphire497/query-selected-attention.Source GTargetEEDQS-AttnModuleContrastive Loss real or  fake anchor:  positive:  negative: Figure 1. The overall structure of our model. The source domain image Ix is translated by the generator G into a target domain image G(Ix). The encoder E extracts features from these two images, then the QS-Attn module selects signiﬁcant features to establish the contrastive loss. We also use a discriminator D to construct the adversarial loss. 