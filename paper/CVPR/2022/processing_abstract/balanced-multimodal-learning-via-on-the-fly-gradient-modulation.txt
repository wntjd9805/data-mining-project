Multimodal learning helps to comprehensively under-stand the world, by integrating different senses. Accord-ingly, multiple input modalities are expected to boost model performance, but we actually find that they are not fully exploited even when the multimodal model outperforms its uni-modal counterpart. Specifically, in this paper we point out that existing multimodal discriminative models, in which uniform objective is designed for all modalities, could remain under-optimized uni-modal representations, caused by another dominated modality in some scenar-ios, e.g., sound in blowing wind event, vision in drawing picture event, etc. To alleviate this optimization imbal-ance, we propose on-the-fly gradient modulation to adap-tively control the optimization of each modality, via mon-itoring the discrepancy of their contribution towards the learning objective. Further, an extra Gaussian noise that changes dynamically is introduced to avoid possible gen-eralization drop caused by gradient modulation. As a re-sult, we achieve considerable improvement over common fusion methods on different multimodal tasks, and this sim-ple strategy can also boost existing multimodal methods, which illustrates its efficacy and versatility. The source code is available at https://github.com/GeWu-Lab/OGM-GE_CVPR2022. 