Transfer learning, where the goal is to transfer the well-trained deep learning models from a primary source task to a new task, is a crucial learning scheme for on-device machine learning, due to the fact that IoT/edge devices col-lect and then process massive data in our daily life. How-ever, due to the tiny memory constraint in IoT/edge devices, such on-device learning requires ultra-small training mem-ory footprint, bringing new challenges for memory-efficient learning. Many existing works solve this problem by re-ducing the number of trainable parameters. However, this doesnâ€™t directly translate to memory saving since the ma-jor bottleneck is the activations, not parameters. To de-velop memory-efficient on-device transfer learning, in this work, we are the first to approach the concept of transfer learning from a new perspective of intermediate feature re-programming of a pre-trained model (i.e., backbone). To perform this lightweight and memory-efficient reprogram-ming, we propose to train a tiny Reprogramming Network (Rep-Net) directly from the new task input data, while freez-ing the backbone model. The proposed Rep-Net model in-terchanges the features with the backbone model using an activation connector at regular intervals to mutually bene-fit both the backbone model and Rep-Net model features.Through extensive experiments, we validate each design specs of the proposed Rep-Net model in achieving highly memory-efficient on-device reprogramming. Our experi-ments establish the superior performance (i.e., low training memory and high accuracy) of Rep-Net compared to SOTA on-device transfer learning schemes across multiple bench-marks. Code is available at https://github.com/ASU-ESIC-FAN-Lab/RepNet. 