Training Deep Neural Networks (DNNs) is inherently subject to sensitive hyper-parameters and untimely feed-backs of performance evaluation. To solve these two dif-ﬁculties, an efﬁcient parallel hyper-parameter optimization model is proposed under the framework of Deep Reinforce-ment Learning (DRL). Technically, we develop Attention and Memory Enhancement (AME), that includes multi-head attention and memory mechanism to enhance the ability to capture both the short-term and long-term relationship-s between different hyper-parameter conﬁgurations, yield-ing an attentive sampling mechanism for searching high-performance conﬁgurations embedded into a huge search space. During the optimization of transformer-structured conﬁguration searcher, a conceptually intuitive yet power-ful strategy is applied to solve the problem of insufﬁcient number of samples due to the untimely feedback. Experi-ments on three visual tasks, including image classiﬁcation, object detection, semantic segmentation, demonstrate the effectiveness of AME. 