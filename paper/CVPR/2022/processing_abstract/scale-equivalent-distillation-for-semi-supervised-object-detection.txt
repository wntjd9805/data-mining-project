Recent Semi-Supervised Object Detection (SS-OD) methods are mainly based on self-training, i.e., generat-ing hard pseudo-labels by a teacher model on unlabeled data as supervisory signals. Although they achieved cer-tain success, the limited labeled data in semi-supervised learning scales up the challenges of object detection. We analyze the challenges these methods meet with the em-pirical experiment results. We find that the massive FalseNegative samples and inferior localization precision lack consideration. Besides, the large variance of object sizes and class imbalance (i.e., the extreme ratio between back-ground and object) hinder the performance of prior arts.Further, we overcome these challenges by introducing a novel approach, Scale-Equivalent Distillation (SED), which is a simple yet effective end-to-end knowledge distillation framework robust to large object size variance and class im-balance. SED has several appealing benefits compared to the previous works. (1) SED imposes a consistency regular-ization to handle the large scale variance problem. (2) SED alleviates the noise problem from the False Negative sam-ples and inferior localization precision. (3) A re-weighting strategy can implicitly screen the potential foreground re-gions of the unlabeled data to reduce the effect of class im-balance. Extensive experiments show that SED consistently outperforms the recent state-of-the-art methods on different datasets with significant margins. For example, it surpasses the supervised counterpart by more than 10 mAP when us-ing 5% and 10% labeled data on MS-COCO. 