Appearance of dressed humans undergoes a complex ge-ometric transformation induced not only by the static pose but also by its dynamics, i.e., there exists a number of cloth geometric configurations given a pose depending on the way it has moved. Such appearance modeling con-ditioned on motion has been largely neglected in existing human rendering methods, resulting in rendering of phys-ically implausible motion. A key challenge of learning the dynamics of the appearance lies in the requirement ofIn this pa-a prohibitively large amount of observations. per, we present a compact motion representation by enforc-ing equivarianceâ€”a representation is expected to be trans-formed in the way that the pose is transformed. We model an equivariant encoder that can generate the generalizable representation from the spatial and temporal derivatives of the 3D body surface. This learned representation is de-coded by a compositional multi-task decoder that renders high fidelity time-varying appearance. Our experiments show that our method can generate a temporally coherent video of dynamic humans for unseen body poses and novel views given a single view video. 