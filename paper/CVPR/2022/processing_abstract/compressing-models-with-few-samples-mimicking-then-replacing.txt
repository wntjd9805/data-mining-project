Few-sample compression aims to compress a big redun-dant model into a small compact one with only few sam-ples. If we fine-tune models with these limited few samples directly, models will be vulnerable to overfit and learn al-most nothing. Hence, previous methods optimize the com-pressed model layer-by-layer and try to make every layer have the same outputs as the corresponding layer in the teacher model, which is cumbersome. In this paper, we pro-pose a new framework named Mimicking then Replacing (MiR) for few-sample compression, which firstly urges the pruned model to output the same features as the teacher’s in the penultimate layer, and then replaces teacher’s layers be-fore penultimate with a well-tuned compact one. Unlike pre-vious layer-wise reconstruction methods, our MiR optimizes the entire network holistically, which is not only simple and effective, but also unsupervised and general. MiR outper-forms previous methods with large margins. Codes is avail-able at https://github.com/cjnjuwhy/MiR. 