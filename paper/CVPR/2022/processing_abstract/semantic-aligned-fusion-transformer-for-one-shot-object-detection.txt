One-shot object detection aims at detecting novel ob-jects according to merely one given instance. With extreme data scarcity, current approaches explore various feature fusions to obtain directly transferable meta-knowledge. Yet, their performances are often unsatisfactory. In this paper, we attribute this to inappropriate correlation methods that misalign query-support semantics by overlooking spatial structures and scale variances. Upon analysis, we lever-age the attention mechanism and propose a simple but ef-fective architecture named Semantic-aligned Fusion Trans-former (SaFT) to resolve these issues. Specifically, we equip SaFT with a vertical fusion module (VFM) for cross-scale semantic enhancement and a horizontal fusion mod-ule (HFM) for cross-sample feature fusion. Together, they broaden the vision for each feature point from the support to a whole augmented feature pyramid from the query, fa-cilitating semantic-aligned associations. Extensive exper-iments on multiple benchmarks demonstrate the superior-ity of our framework. Without fine-tuning on novel classes, it brings significant performance gains to one-stage base-lines, lifting state-of-the-art results to a higher level. 