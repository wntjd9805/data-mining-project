Event-based cameras bring a unique capability to track-ing, being able to function in challenging real-world condi-tions as a direct result of their high temporal resolution and high dynamic range. These imagers capture events asyn-chronously that encode rich temporal and spatial informa-tion. However, effectively extracting this information from events remains an open challenge.In this work, we pro-pose a spiking transformer network, STNet, for single ob-ject tracking. STNet dynamically extracts and fuses infor-In par-mation from both temporal and spatial domains. ticular, the proposed architecture features a transformer module to provide global spatial information and a spik-ing neural network (SNN) module for extracting temporal cues. The spiking threshold of the SNN module is dynami-cally adjusted based on the statistical cues of the spatial in-formation, which we ﬁnd essential in providing robust SNN features. We fuse both feature branches dynamically with a novel cross-domain attention fusion algorithm. Extensive experiments on three event-based datasets, FE240hz, EED and VisEvent validate that the proposed STNet outperforms existing state-of-the-art methods in both tracking accuracy and speed with a signiﬁcant margin. The code and pre-trained models are at https://github.com/Jee-King/CVPR2022_STNet. 