Spatial-Temporal Video Super-Resolution (ST-VSR) aims to generate super-resolved videos with higher resolution (HR) and higher frame rate (HFR). Quite intuitively, pi-oneering two-stage based methods complete ST-VSR by directly combining two sub-tasks: Spatial Video Super-Resolution (S-VSR) and Temporal Video Super-Resolution (T-VSR) but ignore the reciprocal relations among them.Specifically, 1) T-VSR to S-VSR: temporal correlations help accurate spatial detail representation with more clues; 2)S-VSR to T-VSR: abundant spatial information contributes to the refinement of temporal prediction. To this end, we propose a one-stage based Cycle-projected Mutual learn-ing network (CycMu-Net) for ST-VSR, which makes full use of spatial-temporal correlations via the mutual learn-ing between S-VSR and T-VSR. Specifically, we propose to exploit the mutual information among them via itera-tive up-and-down projections, where the spatial and tem-poral features are fully fused and distilled, helping the high-quality video reconstruction. Besides extensive exper-iments on benchmark datasets, we also compare our pro-posed CycMu-Net with S-VSR and T-VSR tasks, demonstrat-ing that our method significantly outperforms state-of-the-art methods. Codes are publicly available at: https://github.com/hhhhhumengshun/CycMuNet. 