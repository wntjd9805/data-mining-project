Channel (or 3D filter) pruning serves as an effective way to accelerate the inference of neural networks. There has been a flurry of algorithms that try to solve this practical problem, each being claimed effective in some ways. Yet, a benchmark to compare those algorithms directly is lacking, mainly due to the complexity of the algorithms and some custom settings such as the particular network configura-tion or training procedure. A fair benchmark is important for the further development of channel pruning.Meanwhile, recent investigations reveal that the chan-nel configurations discovered by pruning algorithms are at least as important as the pre-trained weights. This gives channel pruning a new role, namely searching the optimal channel configuration. In this paper, we try to determine the channel configuration of the pruned models by ran-dom search. The proposed approach provides a new way to compare different methods, namely how well they be-have compared with random pruning. We show that this simple strategy works quite well compared with other chan-nel pruning methods. We also show that under this setting, there are surprisingly no clear winners among different channel importance evaluation methods, which then may tilt the research efforts into advanced channel configura-tion searching methods. Code will be released at https://github.com/ofsoundof/random_channel_ pruning. 