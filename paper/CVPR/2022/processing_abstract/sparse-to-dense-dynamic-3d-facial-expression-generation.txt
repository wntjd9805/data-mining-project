In this paper, we propose a solution to the task of generating dynamic 3D facial expressions from a neutral 3D face and an expression label. This involves solving two sub-problems: (i) modeling the temporal dynamics of expressions, and (ii) deforming the neutral mesh to ob-tain the expressive counterpart. We represent the tempo-ral evolution of expressions using the motion of a sparse set of 3D landmarks that we learn to generate by train-ing a manifold-valued GAN (Motion3DGAN). To better en-code the expression-induced deformation and disentangle it from the identity information, the generated motion is represented as per-frame displacement from a neutral con-figuration. To generate the expressive meshes, we train a Sparse2Dense mesh Decoder (S2D-Dec) that maps the landmark displacements to a dense, per-vertex displace-ment. This allows us to learn how the motion of a sparse set of landmarks influences the deformation of the overall face surface, independently from the identity. Experimen-tal results on the CoMA and D3DFACS datasets show that our solution brings significant improvements with respect to previous solutions in terms of both dynamic expression gen-eration and mesh reconstruction, while retaining good gen-eralization to unseen data. Code and models are available at https://github.com/CRISTAL-3DSAM/Sparse2Dense.Figure 1. 3D dynamic facial expression generation: A GAN generates the motion of 3D landmarks from an expression label and noise; A decoder expands the animation from the landmarks to a dense mesh, while keeping the identity of a neutral 3D face, 