Consistency-based Semi-supervised learning (SSL) has achieved promising performance recently. However, the success largely depends on the assumption that the labeled and unlabeled data share an identical class distribution, which is hard to meet in real practice. The distribution mis-match between the labeled and unlabeled sets can cause severe bias in the pseudo-labels of SSL, resulting in signif-icant performance degradation. To bridge this gap, we put forward a new SSL learning framework, named Distribu-tion Consistency SSL (DC-SSL), which rectifies the pseudo-labels from a distribution perspective. The basic idea is to directly estimate a reference class distribution (RCD), which is regarded as a surrogate of the ground truth class distribution about the unlabeled data, and then improve the pseudo-labels by encouraging the predicted class distribu-tion (PCD) of the unlabeled data to approach RCD gradu-ally. To this end, this paper revisits the Exponentially Mov-ing Average (EMA) model and utilizes it to estimate RCD in an iteratively improved manner, which is achieved with a momentum-update scheme throughout the training proce-dure. On top of this, two strategies are proposed for RCD to rectify the pseudo-label prediction, respectively. They cor-respond to an efficient training-free scheme and a training-based alternative that generates more accurate and reli-able predictions. DC-SSL is evaluated on multiple SSL benchmarks and demonstrates remarkable performance im-provement over competitive methods under matched- and mismatched-distribution scenarios. 