Volumetric neural rendering methods like NeRF [34] generate high-quality view synthesis results but are opti-mized per-scene leading to prohibitive reconstruction time.On the other hand, deep multi-view stereo methods can quickly reconstruct scene geometry via direct network in-ference. Point-NeRF combines the advantages of these two approaches by using neural 3D point clouds, with associ-ated neural features, to model a radiance field. Point-NeRF can be rendered efficiently by aggregating neural point fea-tures near scene surfaces, in a ray marching-based render-ing pipeline. Moreover, Point-NeRF can be initialized via direct inference of a pre-trained deep network to produce a neural point cloud; this point cloud can be finetuned to surpass the visual quality of NeRF with 30Ã— faster train-ing time. Point-NeRF can be combined with other 3D re-construction methods and handles the errors and outliers in such methods via a novel pruning and growing mechanism. 