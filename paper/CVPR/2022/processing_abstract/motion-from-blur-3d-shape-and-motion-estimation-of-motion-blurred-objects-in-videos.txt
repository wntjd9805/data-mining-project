We propose a method for jointly estimating the 3D mo-tion, 3D shape, and appearance of highly motion-blurred objects from a video. To this end, we model the blurred appearance of a fast moving object in a generative fashion by parametrizing its 3D position, rotation, velocity, accel-eration, bounces, shape, and texture over the duration of a predefined time window spanning multiple frames. Us-ing differentiable rendering, we are able to estimate all parameters by minimizing the pixel-wise reprojection er-ror to the input video via backpropagating through a ren-dering pipeline that accounts for motion blur by averag-ing the graphics output over short time intervals. For that purpose, we also estimate the camera exposure gap time within the same optimization. To account for abrupt mo-tion changes like bounces, we model the motion trajectory as a piece-wise polynomial, and we are able to estimate the specific time of the bounce at sub-frame accuracy. Experi-ments on established benchmark datasets demonstrate that our method outperforms previous methods for fast moving object deblurring and 3D reconstruction. 