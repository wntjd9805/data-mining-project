Generalized zero-shot learning (GZSL) aims to recog-nize samples whose categories may not have been seen at training. Recognizing unseen classes as seen ones or vice versa often leads to poor performance in GZSL. There-fore, distinguishing seen and unseen domains is naturally an effective yet challenging solution for GZSL. In this pa-per, we present a novel method which leverages both vi-sual and semantic modalities to distinguish seen and un-seen categories. Speciﬁcally, our method deploys two vari-ational autoencoders to generate latent representations for visual and semantic modalities in a shared latent space, in which we align latent representations of both modalities byWasserstein distance and reconstruct two modalities with the representations of each other. In order to learn a clearer boundary between seen and unseen classes, we propose a two-stage training strategy which takes advantage of seen and unseen semantic descriptions and searches a threshold to separate seen and unseen visual samples. At last, a seen expert and an unseen expert are used for ﬁnal classiﬁca-tion. Extensive experiments on ﬁve widely used benchmarks verify that the proposed method can signiﬁcantly improve the results of GZSL. For instance, our method correctly rec-ognizes more than 99% samples when separating domains and improves the ﬁnal classiﬁcation accuracy from 72.6% to 82.9% on AWA1. 