Variational autoencoder (VAE) is a very successful gen-erative model whose key element is the so-called amor-tized inference network, which can perform test time in-ference using a single feed forward pass. Unfortunately, this comes at the cost of degraded accuracy in posterior ap-proximation, often underperforming the instance-wise vari-ational optimization. Although the latest semi-amortized approaches mitigate the issue by performing a few varia-tional optimization updates starting from the VAE’s amor-tized inference output, they inherently suffer from compu-tational overhead for inference at test time. In this paper, we address the problem in a completely different way by considering a random inference model, where we model the mean and variance functions of the variational posterior as random Gaussian processes (GP). The motivation is that the deviation of the VAE’s amortized posterior distribution from the true posterior can be regarded as random noise, which allows us to view the approximation error as uncer-tainty in posterior approximation that can be dealt with in a principled GP manner. In particular, our model can quan-tify the difficulty in posterior approximation by a Gaussian variational density. Inference in our GP model is done by a single feed forward pass through the network, significantly faster than semi-amortized methods. We show that our ap-proach attains higher test data likelihood than the state-of-the-arts on several benchmark datasets. 