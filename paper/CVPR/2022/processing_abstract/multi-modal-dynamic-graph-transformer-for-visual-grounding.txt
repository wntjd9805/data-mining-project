Visual grounding (VG) aims to align the correct regions of an image with a natural language query about that im-age. We found that existing VG methods are trapped by the single-stage grounding process that performs a sole evaluate-and-rank for meticulously prepared regions. Their performance depends on the density and quality of the can-didate regions, and is capped by the inability to optimize the located regions continuously. To address these issues, we propose to remodel VG into a progressively optimized visual semantic alignment process. Our proposed multi-modal dynamic graph transformer (M-DGT) achieves this by building upon the dynamic graph structure with regions as nodes and their semantic relations as edges. Starting from a few randomly initialized regions, M-DGT is able to make sustainable adjustments (i.e., 2D spatial transforma-tion and deletion) to the nodes and edges of the graph based on multi-modal information and the graph feature, thereby efﬁciently shrinking the graph to approach the ground truth regions. Experiments show that with an average of 48 boxes as initialization, the performance of M-DGT on theFlickr30k Entities and RefCOCO datasets outperforms ex-isting state-of-the-art methods by a substantial margin, in terms of both accuracy and Intersect over Union (IOU) scores. Furthermore, introducing M-DGT to optimize the predicted regions of existing methods can further signiﬁ-cantly improve their performance. The source codes are available at https://github.com/iQua/M-DGT. 