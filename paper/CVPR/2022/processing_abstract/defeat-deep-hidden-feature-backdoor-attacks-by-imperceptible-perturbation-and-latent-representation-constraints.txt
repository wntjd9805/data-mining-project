Backdoor attack is a type of serious security threat to deep learning models. An adversary can provide users with a model trained on poisoned data to manipulate prediction behavior in test stage using a backdoor. The backdoored models behave normally on clean images, yet can be acti-vated and output incorrect prediction if the input is stamped with a speciﬁc trigger pattern. Most existing backdoor at-tacks focus on manually deﬁning imperceptible triggers in input space without considering the abnormality of trig-gers’ latent representations in the poisoned model. These attacks are susceptible to backdoor detection algorithms and even visual inspection.In this paper, We propose a novel and stealthy backdoor attack - DEFEAT. It poisons the clean data using adaptive imperceptible perturbation and restricts latent representation during training process to strengthen our attack’s stealthiness and resistance to de-fense algorithms. We conduct extensive experiments on mul-tiple image classiﬁers using real-world datasets to demon-strate that our attack can 1) hold against the state-of-the-art defenses, 2) deceive the victim model with high attack success without jeopardizing model utility, and 3) provide practical stealthiness on image data. 