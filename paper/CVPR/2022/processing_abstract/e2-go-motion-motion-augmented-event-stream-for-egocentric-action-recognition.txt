Event cameras are novel bio-inspired sensors, which asynchronously capture pixel-level intensity changes in the form of “events”. Due to their sensing mechanism, event cameras have little to no motion blur, a very high temporal resolution and require signiﬁcantly less power and mem-ory than traditional frame-based cameras. These charac-teristics make them a perfect ﬁt to several real-world appli-cations such as egocentric action recognition on wearable devices, where fast camera motion and limited power chal-lenge traditional vision sensors. However, the ever-growingﬁeld of event-based vision has, to date, overlooked the po-tential of event cameras in such applications. In this pa-per, we show that event data is a very valuable modality for egocentric action recognition. To do so, we introduceN-EPIC-Kitchens, the ﬁrst event-based camera extension of the large-scale EPIC-Kitchens dataset. In this context, we propose two strategies: (i) directly processing event-camera data with traditional video-processing architectures (E2(GO)) and (ii) using event-data to distill optical ﬂow in-formation (E2(GO)MO). On our proposed benchmark, we show that event data provides a comparable performance to RGB and optical ﬂow, yet without any additional ﬂow computation at deploy time, and an improved performance of up to 4% with respect to RGB only information. The N-EPIC-Kitchens dataset is available at https://github. com/EgocentricVision/N-EPIC-Kitchens. 