Knowledge distillation has been applied to image clas-sification successfully. However, object detection is much more sophisticated and most knowledge distillation meth-ods have failed on it.In this paper, we point out that in object detection, the features of the teacher and stu-dent vary greatly in different areas, especially in the fore-ground and background. If we distill them equally, the un-even differences between feature maps will negatively af-fect the distillation. Thus, we propose Focal and GlobalDistillation (FGD). Focal distillation separates the fore-ground and background, forcing the student to focus on the teacherâ€™s critical pixels and channels. Global distilla-tion rebuilds the relation between different pixels and trans-fers it from teachers to students, compensating for missing global information in focal distillation. As our method only needs to calculate the loss on the feature map, FGD can be applied to various detectors. We experiment on vari-ous detectors with different backbones and the results show that the student detector achieves excellent mAP improve-ment. For example, ResNet-50 based RetinaNet, FasterRCNN, RepPoints and Mask RCNN with our distillation method achieve 40.7%, 42.0%, 42.0% and 42.1% mAP on COCO2017, which are 3.3, 3.6, 3.4 and 2.9 higher than the baseline, respectively. Our codes are available at https://github.com/yzd-v/FGD. 