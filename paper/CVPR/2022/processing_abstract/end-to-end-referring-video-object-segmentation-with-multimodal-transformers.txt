The referring video object segmentation task (RVOS) in-volves segmentation of a text-referred object instance in the frames of a given video. Due to the complex nature of this multimodal task, which combines text reasoning, video understanding, instance segmentation and tracking, exist-ing approaches typically rely on sophisticated pipelinesIn this paper, we propose a sim-in order to tackle it. ple Transformer-based approach to RVOS. Our framework, termed Multimodal Tracking Transformer (MTTR), mod-els the RVOS task as a sequence prediction problem. Fol-lowing recent advancements in computer vision and nat-ural language processing, MTTR is based on the realiza-tion that video and text can be processed together effec-tively and elegantly by a single multimodal Transformer model. MTTR is end-to-end trainable, free of text-related inductive bias components and requires no additional mask-reﬁnement post-processing steps. As such, it simpliﬁes theRVOS pipeline considerably compared to existing meth-ods. Evaluation on standard benchmarks reveals that MTTR signiﬁcantly outperforms previous art across multiple met-rics. In particular, MTTR shows impressive +5.7 and +5.0 mAP gains on the A2D-Sentences and JHMDB-Sentences datasets respectively, while processing 76 frames per sec-ond.In addition, we report strong results on the public validation set of Refer-YouTube-VOS, a more challengingRVOS dataset that has yet to receive the attention of re-searchers. The code to reproduce our experiments is avail-able at https://github.com/mttr2021/MTTR. 