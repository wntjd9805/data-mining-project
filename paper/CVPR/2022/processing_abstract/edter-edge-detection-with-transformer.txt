Convolutional neural networks have made significant progresses in edge detection by progressively exploring the context and semantic features. However, local details are gradually suppressed with the enlarging of receptive fields. Recently, vision transformer has shown excellent capability in capturing long-range dependencies. Inspired by this, we propose a novel transformer-based edge de-tector, Edge Detection TransformER (EDTER), to extract clear and crisp object boundaries and meaningful edges by exploiting the full image context information and de-tailed local cues simultaneously. EDTER works in two stages.In Stage I, a global transformer encoder is used to capture long-range global context on coarse-grained im-age patches. Then in Stage II, a local transformer encoder works on fine-grained patches to excavate the short-range local cues. Each transformer encoder is followed by an elaborately designed Bi-directional Multi-Level Aggrega-tion decoder to achieve high-resolution features. Finally, the global context and local cues are combined by a FeatureFusion Module and fed into a decision head for edge pre-diction. Extensive experiments on BSDS500, NYUDv2, andMulticue demonstrate the superiority of EDTER in compar-ison with state-of-the-arts. The source code is available at https://github.com/MengyangPu/EDTER. 