The way an object looks and sounds provide complemen-tary reﬂections of its physical properties. In many settings cues from vision and audition arrive asynchronously but must be integrated, as when we hear an object dropped on the ﬂoor and then must ﬁnd it. In this paper, we introduce a setting in which to study multi-modal object localization in 3D virtual environments. An object is dropped somewhere in a room. An embodied robot agent, equipped with cam-era and microphone, must determine what object has been dropped – and where – by combining audio and visual sig-nals with knowledge of the underlying physics. To study this problem, we have generated a large-scale dataset – theFallen Objects dataset – that includes 8000 instances of 30 physical object categories in 64 rooms. The dataset uses the ThreeDWorld Platform that can simulate physics-based impact sounds and complex physical interactions between objects in a photorealistic setting. As a ﬁrst step toward ad-dressing this challenge, we develop a set of embodied agent baselines, based on imitation learning, reinforcement learn-ing, and modular planning, and perform an in-depth analy-sis of the challenge of this new task. This dataset is publicly available 1. 