Visual grounding is a task to locate the target indicated by a natural language expression. Existing methods ex-tend the generic object detection framework to this prob-lem. They base the visual grounding on the features from pre-generated proposals or anchors, and fuse these features with the text embeddings to locate the target mentioned by the text. However, modeling the visual features from these predefined locations may fail to fully exploit the visual con-text and attribute information in the text query, which limits their performance. In this paper, we propose a transformer-based framework for accurate visual grounding by estab-lishing text-conditioned discriminative features and per-forming multi-stage cross-modal reasoning. Specifically, we develop a visual-linguistic verification module to focus the visual features on regions relevant to the textual descrip-tions while suppressing the unrelated areas. A language-guided feature encoder is also devised to aggregate the vi-sual contexts of the target object to improve the objectâ€™s dis-tinctiveness. To retrieve the target from the encoded visual features, we further propose a multi-stage cross-modal de-coder to iteratively speculate on the correlations between the image and text for accurate target localization. Exten-sive experiments on five widely used datasets validate the efficacy of our proposed components and demonstrate state-of-the-art performance. 