Visual place recognition is a challenging task for appli-cations such as autonomous driving navigation and mobile robot localization. Distracting elements presenting in com-plex scenes often lead to deviations in the perception of vi-sual place. To address this problem, it is crucial to integrate information from only task-relevant regions into image rep-In this paper, we introduce a novel holistic resentations. place recognition model, TransVPR, based on vision Trans-formers. It benefits from the desirable property of the self-attention operation in Transformers which can naturally ag-gregate task-relevant features. Attentions from multiple lev-els of the Transformer, which focus on different regions of interest, are further combined to generate a global image representation. In addition, the output tokens from Trans-former layers filtered by the fused attention mask are con-sidered as key-patch descriptors, which are used to perform spatial matching to re-rank the candidates retrieved by the global image features. The whole model allows end-to-end training with a single objective and image-level supervi-sion. TransVPR achieves state-of-the-art performance on several real-world benchmarks while maintaining low com-putational time and storage requirements. 