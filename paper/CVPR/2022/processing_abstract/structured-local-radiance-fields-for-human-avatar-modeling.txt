It is extremely challenging to create an animatable clothed human avatar from RGB videos, especially for loose clothes due to the difficulties in motion modeling. To ad-dress this problem, we introduce a novel representation on the basis of recent neural scene rendering techniques. The core of our representation is a set of structured local ra-diance fields, which are anchored to the pre-defined nodes sampled on a statistical human body template. These local radiance fields not only leverage the flexibility of implicit representation in shape and appearance modeling, but also factorize cloth deformations into skeleton motions, node residual translations and the dynamic detail variations in-side each individual radiance field. To learn our represen-tation from RGB data and facilitate pose generalization, we propose to learn the node translations and the detail varia-tions in a conditional generative latent space. Overall, our method enables automatic construction of animatable hu-man avatars for various types of clothes without the need for scanning subject-specific templates, and can generate realistic images with dynamic details for novel poses. Ex-periment show that our method outperforms state-of-the-art methods both qualitatively and quantitatively. 