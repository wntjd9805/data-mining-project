Motion, as the uniqueness of a video, has been critical to the development of video understanding models. Mod-ern deep learning models leverage motion by either ex-ecuting spatio-temporal 3D convolutions, factorizing 3D convolutions into spatial and temporal convolutions sepa-rately, or computing self-attention along temporal dimen-sion. The implicit assumption behind such successes is that the feature maps across consecutive frames can be nicely aggregated. Nevertheless, the assumption may not always hold especially for the regions with large defor-mation.In this paper, we present a new recipe of inter-frame attention block, namely Stand-alone Inter-Frame At-tention (SIFA), that novelly delves into the deformation across frames to estimate local self-attention on each spa-tial location. Technically, SIFA remoulds the deformable design via re-scaling the offset predictions by the difference between two frames. Taking each spatial location in the cur-rent frame as the query, the locally deformable neighbors in the next frame are regarded as the keys/values. Then,SIFA measures the similarity between query and keys as stand-alone attention to weighted average the values for temporal aggregation. We further plug SIFA block into Con-vNets and Vision Transformer, respectively, to devise SIFA-Net and SIFA-Transformer. Extensive experiments con-ducted on four video datasets demonstrate the superiority of SIFA-Net and SIFA-Transformer as stronger backbones.More remarkably, SIFA-Transformer achieves an accuracy of 83.1% on Kinetics-400 dataset. Source code is available at https://github.com/FuchenUSTC/SIFA. 