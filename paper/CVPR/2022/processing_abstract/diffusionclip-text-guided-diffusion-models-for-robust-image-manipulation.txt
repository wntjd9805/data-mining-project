Recently, GAN inversion methods combined with Con-trastive Language-Image Pretraining (CLIP) enables zero-shot image manipulation guided by text prompts. However, their applications to diverse real images are still difficult due to the limited GAN inversion capability. Specifically, these approaches often have difficulties in reconstructing images with novel poses, views, and highly variable contents compared to the training data, altering object identity, or pro-ducing unwanted image artifacts. To mitigate these problemsThis research was supported by Field-oriented Technology Develop-ment Project for Customs Administration through the National ResearchFoundation of Korea(NRF) funded by the Ministry of Science & ICT andKorea Customs Service (NRF-2021M3I1A1097938), and supported by the Institute of Information & communications Technology Planning &Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2019-0-00075, Artificial Intelligence Graduate School Program (KAIST)). and enable faithful manipulation of real images, we propose a novel method, dubbed DiffusionCLIP, that performs text-driven image manipulation using diffusion models. Based on full inversion capability and high-quality image generation power of recent diffusion models, our method performs zero-shot image manipulation successfully even between unseen domains and takes another step towards general applica-tion by manipulating images from a widely varying ImageNet dataset. Furthermore, we propose a novel noise combination method that allows straightforward multi-attribute manipula-tion. Extensive experiments and human evaluation confirmed robust and superior manipulation performance of our meth-ods compared to the existing baselines. Code is available at https://github.com/gwang-kim/DiffusionCLIP.git 