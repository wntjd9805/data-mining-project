Self- and cross-attention in Transformers provide for high model capacity, making them viable models for ob-ject detection. However, Transformers still lag in per-formance behind CNN-based detectors. This is, we be-lieve, because: (a) Cross-attention is used for both clas-sification and bounding-box regression tasks; (b) Trans-former’s decoder poorly initializes content queries; and (c)Self-attention poorly accounts for certain prior knowledge which could help improve inductive bias. These limitations are addressed with the corresponding three contributions.First, we propose a new Detection Split Transformer (DE-STR) that separates estimation of cross-attention into two independent branches – one tailored for classification and the other for box regression. Second, we use a mini-detector to initialize the content queries in the decoder with classi-fication and regression embeddings of the respective heads in the mini-detector. Third, we augment self-attention in the decoder to additionally account for pairs of adjacent object queries. Our experiments on the MS-COCO dataset show that DESTR outperforms DETR and its successors. 