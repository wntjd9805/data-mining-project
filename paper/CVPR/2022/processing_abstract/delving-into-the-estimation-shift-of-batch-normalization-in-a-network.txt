Batch normalization (BN) is a milestone technique in deep learning. It normalizes the activation using mini-batch statistics during training but the estimated population statis-tics during inference. This paper focuses on investigating the estimation of population statistics. We deﬁne the estimation shift magnitude of BN to quantitatively measure the differ-ence between its estimated population statistics and expected ones. Our primary observation is that the estimation shift can be accumulated due to the stack of BN in a network, which has detriment effects for the test performance. We fur-ther ﬁnd a batch-free normalization (BFN) can block such an accumulation of estimation shift. These observations mo-tivate our design of XBNBlock that replace one BN with BFN in the bottleneck block of residual-style networks. Experi-ments on the ImageNet and COCO benchmarks show thatXBNBlock consistently improves the performance of different architectures, including ResNet and ResNeXt, by a signiﬁ-cant margin and seems to be more robust to distribution shift. 