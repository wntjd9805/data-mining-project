We present Block-NeRF, a variant of Neural RadianceFields that can represent large-scale environments. Specif-ically, we demonstrate that when scaling NeRF to render city-scale scenes spanning multiple blocks, it is vital to de-compose the scene into individually trained NeRFs. This decomposition decouples rendering time from scene size, en-ables rendering to scale to arbitrarily large environments, and allows per-block updates of the environment. We adopt several architectural changes to make NeRF robust to data captured over months under different environmental condi-tions. We add appearance embeddings, learned pose reÔ¨Åne-ment, and controllable exposure to each individual NeRF, and introduce a procedure for aligning appearance between adjacent NeRFs so that they can be seamlessly combined. We build a grid of Block-NeRFs from 2.8 million images to cre-ate the largest neural scene representation to date, capable of rendering an entire neighborhood of San Francisco. 