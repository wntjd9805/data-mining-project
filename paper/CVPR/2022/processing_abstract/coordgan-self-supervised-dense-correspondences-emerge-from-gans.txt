Recent advances show that Generative Adversarial Net-works (GANs) can synthesize images with smooth varia-tions along semantically meaningful latent directions, such as pose, expression, layout, etc. While this indicates thatGANs implicitly learn pixel-level correspondences across images, few studies explored how to extract them explicitly.In this work, we introduce Coordinate GAN (CoordGAN), a structure-texture disentangled GAN that learns a dense correspondence map for each generated image. We repre-sent the correspondence maps of different images as warped coordinate frames transformed from a canonical coordi-nate frame, i.e., the correspondence map, which describes the structure (e.g., the shape of a face), is controlled via a transformation. Hence, Ô¨Ånding correspondences boils down to locating the same coordinate in different corre-In CoordGAN, we sample a transfor-spondence maps. mation to represent the structure of a synthesized instance, while an independent texture branch is responsible for ren-*Work done while an intern at Nvidia. dering appearance details orthogonal to the structure. Our approach can also extract dense correspondence maps for real images by adding an encoder on top of the genera-tor. We quantitatively demonstrate the quality of the learned dense correspondences through segmentation mask trans-fer on multiple datasets. We also show that the proposed generator achieves better structure and texture disentan-glement compared to existing approaches. Project page: https://jitengmu.github.io/CoordGAN/ 