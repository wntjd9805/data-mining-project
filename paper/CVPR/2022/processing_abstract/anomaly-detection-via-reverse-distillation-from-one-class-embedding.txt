Knowledge distillation (KD) achieves promising results on the challenging problem of unsupervised anomaly detec-tion (AD). The representation discrepancy of anomalies in the teacher-student (T-S) model provides essential evidence for AD. However, using similar or identical architectures to build the teacher and student models in previous stud-ies hinders the diversity of anomalous representations. To tackle this problem, we propose a novel T-S model consist-ing of a teacher encoder and a student decoder and intro-duce a simple yet effective ”reverse distillation” paradigm accordingly. Instead of receiving raw images directly, the student network takes teacher model’s one-class embed-ding as input and targets to restore the teacher’s multi-scale representations. Inherently, knowledge distillation in this study starts from abstract, high-level presentations to low-level features.In addition, we introduce a trainable one-class bottleneck embedding (OCBE) module in our T-S model. The obtained compact embedding effectively pre-serves essential information on normal patterns, but aban-dons anomaly perturbations. Extensive experimentation onAD and one-class novelty detection benchmarks shows that our method surpasses SOTA performance, demonstrating our proposed approach’s effectiveness and generalizability. 