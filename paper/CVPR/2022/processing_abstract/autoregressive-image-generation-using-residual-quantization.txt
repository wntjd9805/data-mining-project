For autoregressive (AR) modeling of high-resolution im-ages, vector quantization (VQ) represents an image as a se-quence of discrete codes. A short sequence length is im-portant for an AR model to reduce its computational costs to consider long-range interactions of codes. However, we postulate that previous VQ cannot shorten the code se-quence and generate high-ﬁdelity images together in terms of the rate-distortion trade-off. In this study, we propose the two-stage framework, which consists of Residual-QuantizedVAE (RQ-VAE) and RQ-Transformer, to effectively generate high-resolution images. Given a ﬁxed codebook size, RQ-VAE can precisely approximate a feature map of an image and represent the image as a stacked map of discrete codes.Then, RQ-Transformer learns to predict the quantized fea-ture vector at the next position by predicting the next stack of codes. Thanks to the precise approximation of RQ-VAE, we can represent a 256×256 image as 8×8 resolution of the feature map, and RQ-Transformer can efﬁciently reduce the computational costs. Consequently, our framework out-performs the existing AR models on various benchmarks of unconditional and conditional image generation. Our ap-proach also has a signiﬁcantly faster sampling speed than previous AR models to generate high-quality images.Figure 1. Examples of our conditional generation for 256×256 images. The images in the ﬁrst row are generated from the classes of ImageNet. The images in the second row are generated from text conditions (“A cheeseburger in front of a mountain range cov-ered with snow.” and “a cherry blossom tree on the blue ocean”).The text conditions are unseen during the training. 