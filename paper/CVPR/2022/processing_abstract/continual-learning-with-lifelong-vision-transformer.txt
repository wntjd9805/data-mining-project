Continual learning methods aim at training a neural net-work from sequential data with streaming labels, reliev-ing catastrophic forgetting. However, existing methods are based on and designed for convolutional neural networks (CNNs), which have not utilized the full potential of newly emerged powerful vision transformers.In this paper, we propose a novel attention-based framework Lifelong VisionTransformer (LVT), to achieve a better stability-plasticity trade-off for continual learning. Specifically, an inter-task attention mechanism is presented in LVT, which implicitly absorbs the previous tasksâ€™ information and slows down the drift of important attention between previous tasks and the current task. LVT designs a dual-classifier structure that independently injects new representation to avoid catas-trophic interference and accumulates the new and previous knowledge in a balanced manner to improve the overall per-formance. Moreover, we develop a confidence-aware mem-ory update strategy to deepen the impression of the previ-ous tasks. The extensive experimental results show that our approach achieves state-of-the-art performance with even fewer parameters on continual learning benchmarks. 