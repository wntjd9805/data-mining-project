Defense models against adversarial attacks have grown significantly, but the lack of practical evaluation methods has hindered progress. Evaluation can be defined as look-ing for defense models’ lower bound of robustness given a budget number of iterations and a test dataset. A practical evaluation method should be convenient (i.e., parameter-free), efficient (i.e., fewer iterations) and reliable (i.e., ap-proaching the lower bound of robustness). Towards this tar-get, we propose a parameter-free Adaptive Auto Attack (A3) evaluation method which addresses the efficiency and relia-bility in a test-time-training fashion. Specifically, by observ-ing that adversarial examples to a specific defense model follow some regularities in their starting points, we design an Adaptive Direction Initialization strategy to speed up the evaluation. Furthermore, to approach the lower bound of robustness under the budget number of iterations, we pro-pose an online statistics-based discarding strategy that au-tomatically identifies and abandons hard-to-attack images.Extensive experiments on nearly 50 widely-used defense models demonstrate the effectiveness of our A3. By consum-ing much fewer iterations than existing methods, i.e., 1/10 on average (10× speed up), we achieve lower robust accu-racy in all cases. Notably, we won first place out of 1681 teams in CVPR 2021 White-box Adversarial Attacks on De-fense Models competitions with this method. Code is avail-able at: https://github.com/liuye6666/adaptive auto attack 