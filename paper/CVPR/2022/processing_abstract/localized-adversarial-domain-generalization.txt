Deep learning methods can struggle to handle domain shifts not seen in training data, which can cause them to not generalize well to unseen domains. This has led to research attention on domain generalization (DG), which aims to the modelâ€™s generalization ability to out-of-distribution. Ad-versarial domain generalization is a popular approach toDG, but conventional approaches (1) struggle to sufficiently align features so that local neighborhoods are mixed across domains; and (2) can suffer from feature space over col-lapse which can threaten generalization performance. To address these limitations, we propose localized adversar-ial domain generalization with space compactness mainte-nance (LADG) which constitutes two major contributions.First, we propose an adversarial localized classifier as the domain discriminator, along with a principled primary branch. This constructs a min-max game whereby the aim of the featurizer is to produce locally mixed domains. Sec-ond, we propose to use a coding-rate loss to alleviate fea-ture space over collapse. We conduct comprehensive ex-periments on the Wilds DG benchmark to validate our ap-proach, where LADG outperforms leading competitors on most datasets. 