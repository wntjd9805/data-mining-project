Structural re-parameterization (Rep) methods achieve noticeable improvements on simple VGG-style networks.Despite the prevalence, current Rep methods simply re-parameterize all operations into an augmented network, in-cluding those that rarely contribute to the modelâ€™s perfor-mance. As such, the price to pay is an expensive com-putational overhead to manipulate these unnecessary be-haviors. To eliminate the above caveats, we aim to boot-strap the training with minimal cost by devising a dy-namic re-parameterization (DyRep) method, which encodesRep technique into the training process that dynamically evolves the network structures. Concretely, our proposal adaptively finds the operations which contribute most to the loss in the network, and applies Rep to enhance their representational capacity. Besides, to suppress the noisy and redundant operations introduced by Rep, we devise a de-parameterization technique for a more compact re-parameterization. With this regard, DyRep is more efficient than Rep since it smoothly evolves the given network instead of constructing an over-parameterized network. Experi-mental results demonstrate our effectiveness, e.g., DyRep improves the accuracy of ResNet-18 by 2.04% on ImageNet and reduces 22% runtime over the baseline. Code is avail-able at: https://github.com/hunto/DyRep. 