As convolution has empowered many smart applications, dynamic convolution further equips it with the ability to adapt to diverse inputs. However, the static and dynamic convolutions are either layout-agnostic or computation-heavy, making it inappropriate for layout-specific applica-tions, e.g., face recognition and medical image segmenta-tion. We observe that these applications naturally exhibit the characteristics of large intra-image (spatial) variance and small cross-image variance. This observation moti-vates our efficient translation variant convolution (TVConv) for layout-aware visual processing. Technically, TVConv is composed of affinity maps and a weight-generating block.While affinity maps depict pixel-paired relationships grace-fully, the weight-generating block can be explicitly over-parameterized for better training while maintaining effi-cient inference. Although conceptually simple, TVConv sig-nificantly improves the efficiency of the convolution and can be readily plugged into various network architectures.Extensive experiments on face recognition show that TV-Conv reduces the computational cost by up to 3.1× and im-proves the corresponding throughput by 2.3× while main-taining a high accuracy compared to the depthwise convo-lution. Moreover, for the same computation cost, we boost the mean accuracy by up to 4.21%. We also conduct ex-periments on the optic disc/cup segmentation task and ob-tain better generalization performance, which helps miti-gate the critical data scarcity issue. Code is available at https://github.com/JierunChen/TVConv. 