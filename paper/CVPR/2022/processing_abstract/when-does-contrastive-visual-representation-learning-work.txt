Recent self-supervised representation learning tech-niques have largely closed the gap between supervised and unsupervised learning on ImageNet classiﬁcation. While the particulars of pretraining on ImageNet are now rela-tively well understood, the ﬁeld still lacks widely accepted best practices for replicating this success on other datasets.As a ﬁrst step in this direction, we study contrastive self-supervised learning on four diverse large-scale datasets.By looking through the lenses of data quantity, data do-main, data quality, and task granularity, we provide new insights into the necessary conditions for successful self-supervised learning. Our key ﬁndings include observations such as: (i) the beneﬁt of additional pretraining data beyond 500k images is modest, (ii) adding pretraining images from another domain does not lead to more general representa-tions, (iii) corrupted pretraining images have a disparate impact on supervised and self-supervised pretraining, and (iv) contrastive learning lags far behind supervised learn-ing on ﬁne-grained visual classiﬁcation tasks. 