We propose SelfRecon, a clothed human body recon-struction method that combines implicit and explicit repre-sentations to recover space-time coherent geometries from a monocular self-rotating human video. Explicit methods require a predeﬁned template mesh for a given sequence, while the template is hard to acquire for a speciﬁc subject.Meanwhile, the ﬁxed topology limits the reconstruction ac-curacy and clothing types. Implicit representation supports arbitrary topology and can represent high-ﬁdelity geometry shapes due to its continuous nature. However, it is difﬁcult to integrate multi-frame information to produce a consistent registration sequence for downstream applications. We pro-pose to combine the advantages of both representations. We utilize differential mask loss of the explicit mesh to obtain the coherent overall shape, while the details on the implicit surface are reﬁned with the differentiable neural rendering.Meanwhile, the explicit mesh is updated periodically to ad-just its topology changes, and a consistency loss is designed to match both representations. Compared with existing methods, SelfRecon can produce high-ﬁdelity surfaces for arbitrary clothed humans with self-supervised optimization.Extensive experimental results demonstrate its effectiveness on real captured monocular videos. The source code is available at https://github.com/jby1993/SelfReconCode. 