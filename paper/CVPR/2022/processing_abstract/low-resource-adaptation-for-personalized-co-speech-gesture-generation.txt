Personalizing an avatar for co-speech gesture generation from spoken language requires learning the idiosyncrasies of a personâ€™s gesture style from a small amount of data. Pre-vious methods in gesture generation require large amounts of data for each speaker, which is often infeasible. We pro-pose an approach, named DiffGAN, that efficiently personal-izes co-speech gesture generation models of a high-resource source speaker to target speaker with just 2 minutes of target training data. A unique characteristic of DiffGAN is its abil-ity to account for the crossmodal grounding shift, while also addressing the distribution shift in the output domain. We substantiate the effectiveness of our approach a large scale publicly available dataset through quantitative, qualitative and user studies, which show that our proposed methodology significantly outperforms prior approaches for low-resource adaptation of gesture generation. Code and videos can be found at https://chahuja.com/diffgan. 