Modeling latent variables with priors and hyperpriors is an essential problem in variational image compression.Formally, trade-off between rate and distortion is handled well if priors and hyperpriors precisely describe latent vari-ables. Current practices only adopt univariate priors and process each variable individually. However, we ﬁnd inter-correlations and intra-correlations exist when observing la-tent variables in a vectorized perspective. These ﬁndings reveal visual redundancies to improve rate-distortion per-formance and parallel processing ability to speed up com-pression. This encourages us to propose a novel vectorized prior. Speciﬁcally, a multivariate Gaussian mixture is pro-posed with means and covariances to be estimated. Then, a novel probabilistic vector quantization is utilized to effec-tively approximate means, and remaining covariances are further induced to a uniﬁed mixture and solved by cascaded estimation without context models involved. Furthermore, codebooks involved in quantization are extended to multi-codebooks for complexity reduction, which formulates an efﬁcient compression procedure. Extensive experiments on benchmark datasets against state-of-the-art indicate our model has better rate-distortion performance and an im-pressive 3.18× compression speed up, giving us the ability to perform real-time, high-quality variational image com-pression in practice. Our source code is publicly available at https://github.com/xiaosu-zhu/McQuic. 