Visual appearance is considered to be the most impor-tant cue to understand images for cross-modal retrieval, while sometimes the scene text appearing in images can provide valuable information to understand the visual se-mantics. Most of existing cross-modal retrieval approaches ignore the usage of scene text information and directly adding this information may lead to performance degra-dation in scene text free scenarios. To address this is-sue, we propose a full transformer architecture to unify these cross-modal retrieval scenarios in a single Vision andScene Text Aggregation framework (ViSTA). Speciﬁcally,ViSTA utilizes transformer blocks to directly encode image patches and fuse scene text embedding to learn an aggre-gated visual representation for cross-modal retrieval. To tackle the modality missing problem of scene text, we pro-pose a novel fusion token based transformer aggregation approach to exchange the necessary scene text information only through the fusion token and concentrate on the most important features in each modality. To further strengthen the visual modality, we develop dual contrastive learning losses to embed both image-text pairs and fusion-text pairs into a common cross-modal space. Compared to existing methods, ViSTA enables to aggregate relevant scene text semantics with visual appearance, and hence improve re-sults under both scene text free and scene text aware sce-narios. Experimental results show that ViSTA outperforms other methods by at least 8.4% at Recall@1 for scene text aware retrieval task. Compared with state-of-the-art scene text free retrieval methods, ViSTA can achieve better accu-racy on Flicker30K and MSCOCO while running at least three times faster during the inference stage, which vali-dates the effectiveness of the proposed framework.*Equal Contributions. This work is done when Mengjun Cheng is a research intern at Baidu Inc.†Corresponding author.Figure 1. Given a text query, two images are close in visual se-mantics for (a) conventional cross-modal retrieval. By consider-ing visual appearance and scene text information, e.g.,“gummy hotdog”, into one framework, (b) the proposed Vision and SceneText Aggregation (ViSTA) approach enables to distinguish the se-mantic difference between images I1 and I2 (θ2 < θ1), and can be also adapted to conventional scene text free scenarios. 