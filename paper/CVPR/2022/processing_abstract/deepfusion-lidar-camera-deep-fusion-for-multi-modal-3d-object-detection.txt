Lidars and cameras are critical sensors that pro-vide complementary information for 3D detection in au-tonomous driving. While prevalent multi-modal meth-ods [34, 36] simply decorate raw lidar point clouds with camera features and feed them directly to existing 3D de-tection models, our study shows that fusing camera features with deep lidar features instead of raw points, can lead to better performance. However, as those features are of-ten augmented and aggregated, a key challenge in fusion is how to effectively align the transformed features from two modalities. In this paper, we propose two novel techniques:InverseAug that inverses geometric-related augmentations, e.g., rotation, to enable accurate geometric alignment be-tween lidar points and image pixels, and LearnableAlign that leverages cross-attention to dynamically capture the correlations between image and lidar features during fu-sion. Based on InverseAug and LearnableAlign, we de-velop a family of generic multi-modal 3D detection mod-els named DeepFusion, which is more accurate than pre-vious methods. For example, DeepFusion improves Point-Pillars, CenterPoint, and 3D-MAN baselines on Pedestrian detection for 6.7, 8.9, and 6.2 LEVEL 2 APH, respectively.Notably, our models achieve state-of-the-art performance on Waymo Open Dataset, and show strong model robust-ness against input corruptions and out-of-distribution data.Code will be publicly available at https://github. com/tensorflow/lingvo.Figure 1. Our method fuses two modalities on deep feature level, while previous state-of-the-art methods (PointPainting [34] andPointAugmenting [36] as examples) decorate lidar points with camera features on input level. To address the modality align-ment issue (see Section 1) for deep feature fusion, we propose two techniques InverseAug (see Figure 2 and 3) and LearnableAlign, a cross-attention-based feature-level alignment technique. 