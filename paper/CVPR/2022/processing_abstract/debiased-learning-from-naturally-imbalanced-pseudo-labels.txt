Pseudo-labels are conﬁdent predictions made on unla-beled target data by a classiﬁer trained on labeled source data. They are widely used for adapting a model to unlabeled data, e.g., in a semi-supervised learning setting.Our key insight is that pseudo-labels are naturally imbal-anced due to intrinsic data similarity, even when a model is trained on balanced source data and evaluated on balanced target data. If we address this previously unknown imbal-anced classiﬁcation problem arising from pseudo-labels in-stead of ground-truth training labels, we could remove model biases towards false majorities created by pseudo-labels.We propose a novel and effective debiased learning method with pseudo-labels, based on counterfactual rea-soning and adaptive margins: The former removes the clas-siﬁer response bias, whereas the latter adjusts the margin of each class according to the imbalance of pseudo-labels.Validated by extensive experimentation, our simple debiased learning delivers signiﬁcant accuracy gains over the state-of-the-art on ImageNet-1K: 26% for semi-supervised learning with 0.2% annotations and 9% for zero-shot learning. Our code is available at: https://github.com/frank-xwang/debiased-pseudo-labeling. 