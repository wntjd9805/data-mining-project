Representing scenes with multiple semitransparent col-ored layers has been a popular and successful choice for real-time novel view synthesis. Existing approaches infer colors and transparency values over regularly spaced lay-ers of planar or spherical shape. In this work, we introduce a new view synthesis approach based on multiple semitrans-parent layers with scene-adapted geometry. Our approach infers such representations from stereo pairs in two stages.The ﬁrst stage produces the geometry of a small number of data-adaptive layers from a given pair of views. The second stage infers the color and transparency values for these lay-ers, producing the ﬁnal representation for novel view syn-thesis.Importantly, both stages are connected through a differentiable renderer and are trained end-to-end. In the experiments, we demonstrate the advantage of the proposed approach over the use of regularly spaced layers without adaptation to scene geometry. Despite being orders of mag-nitude faster during rendering, our approach also outper-forms the recently proposed IBRNet system based on im-plicit geometry representation. 