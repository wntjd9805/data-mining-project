Although progress has been made for text-to-image syn-thesis, previous methods fall short of generalizing to unseen or underrepresented attribute compositions in the input text.Lacking compositionality could have severe implications for robustness and fairness, e.g., inability to synthesize the face images of underrepresented demographic groups. In this paper, we introduce a new framework, StyleT2I, to improve the compositionality of text-to-image synthesis. Specifically, we propose a CLIP-guided Contrastive Loss to better dis-tinguish different compositions among different sentences.To further improve the compositionality, we design a novelSemantic Matching Loss and a Spatial Constraint to identify attributes’ latent directions for intended spatial region ma-nipulations, leading to better disentangled latent representa-tions of attributes. Based on the identified latent directions of attributes, we propose Compositional Attribute Adjustment to adjust the latent code, resulting in better compositionality of image synthesis. In addition, we leverage the ℓ2-norm regularization of identified latent directions (norm penalty) to strike a nice balance between image-text alignment and image fidelity. In the experiments, we devise a new dataset split and an evaluation metric to evaluate the compositional-ity of text-to-image synthesis models. The results show thatStyleT2I outperforms previous approaches in terms of the consistency between the input text and synthesized images and achieves higher fidelity. 