The nonuniform quantization strategy for compressing neural networks usually achieves better performance than its counterpart, i.e., uniform strategy, due to its supe-rior representational capacity. However, many nonuni-form quantization methods overlook the complicated pro-jection process in implementing the nonuniformly quantized weights/activations, which incurs non-negligible time and space overhead in hardware deployment. In this study, we propose Nonuniform-to-Uniform Quantization (N2UQ), a method that can maintain the strong representation abil-ity of nonuniform methods while being hardware-friendly and efficient as the uniform quantization for model infer-ence. We achieve this through learning the flexible in-equidistant input thresholds to better fit the underlying distribution while quantizing these real-valued inputs into equidistant output levels. To train the quantized network with learnable input thresholds, we introduce a gener-alized straight-through estimator (G-STE) for intractable backward derivative calculation w.r.t. threshold param-eters. Additionally, we consider entropy preserving reg-ularization to further reduce information loss in weight quantization. Even under this adverse constraint of im-posing uniformly quantized weights and activations, ourN2UQ outperforms state-of-the-art nonuniform quantiza-tion methods by 0.5 âˆ¼ 1.7% on ImageNet, demonstrating the contribution of N2UQ design. Code and models are available at: https://github.com/liuzechun/Nonuniform-to-Uniform-Quantization. 