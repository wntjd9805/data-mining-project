In this paper, we study a problem of egocentric scene understanding, i.e., predicting depths and surface normals from an egocentric image. Egocentric scene understand-ing poses unprecedented challenges: (1) due to large head movements, the images are taken from non-canonical view-points (i.e., tilted images) where existing models of geom-etry prediction do not apply; (2) dynamic foreground ob-jects including hands constitute a large proportion of vi-sual scenes. These challenges limit the performance of the existing models learned from large indoor datasets, such as ScanNet [6] and NYUv2 [36], which comprise predom-inantly upright images of static scenes. We present a mul-timodal spatial rectifier that stabilizes the egocentric im-ages to a set of reference directions, which allows learning a coherent visual representation. Unlike unimodal spatial rectifier that often produces excessive perspective warp for egocentric images, the multimodal spatial rectifier learns from multiple directions that can minimize the impact of the perspective warp. To learn visual representations of the dy-namic foreground objects, we present a new dataset calledEDINA (Egocentric Depth on everyday INdoor Activities) that comprises more than 500K synchronized RGBD frames and gravity directions. Equipped with the multimodal spa-tial rectifier and the EDINA dataset, our proposed method on single-view depth and surface normal estimation sig-nificantly outperforms the baselines not only on our ED-INA dataset, but also on other popular egocentric datasets, such as First Person Hand Action (FPHA) [18] and EPIC-KITCHENS [7]. 