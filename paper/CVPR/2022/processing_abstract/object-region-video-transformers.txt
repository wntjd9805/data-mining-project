Recently, video transformers have shown great success in video understanding, exceeding CNN performance; yet existing video transformer models do not explicitly model objects, although objects can be essential for recognizingIn this work, we present Object-Region Video actions.Transformers (ORViT), an object-centric approach that ex-tends video transformer layers with a block that directly incorporates object representations. The key idea is to fuse object-centric representations starting from early lay-ers and propagate them into the transformer-layers, thus af-fecting the spatio-temporal representations throughout the network. Our ORViT block consists of two object-levelIn the appearance streams: appearance and dynamics. stream, an “Object-Region Attention” module applies self-attention over the patches and object regions. In this way, visual object regions interact with uniform patch tokens and enrich them with contextualized object information.We further model object dynamics via a separate “Object-Dynamics Module”, which captures trajectory interactions, and show how to integrate the two streams. We evalu-ate our model on four tasks and five datasets: composi-tional and few-shot action recognition on SomethingElse, spatio-temporal action detection on AVA, and standard ac-tion recognition on Something-Something V2, Diving48 andEpic-Kitchen100. We show strong performance improve-ment across all tasks and datasets considered, demonstrat-ing the value of a model that incorporates object repre-sentations into a transformer architecture. For code and pretrained models, visit the project page at https:// roeiherz.github.io/ORViT/ 