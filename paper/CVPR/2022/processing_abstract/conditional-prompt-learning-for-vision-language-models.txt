With the rise of powerful pre-trained vision-language models like CLIP, it becomes essential to investigate ways to adapt these models to downstream datasets. A recently proposed method named Context Optimization (CoOp) in-troduces the concept of prompt learning—a recent trend inNLP—to the vision domain for adapting pre-trained vision-language models. Specifically, CoOp turns context words in a prompt into a set of learnable vectors and, with only a few labeled images for learning, can achieve huge improve-ments over intensively-tuned manual prompts. In our study we identify a critical problem of CoOp: the learned con-text is not generalizable to wider unseen classes within the same dataset, suggesting that CoOp overfits base classes observed during training. To address the problem, we pro-pose Conditional Context Optimization (CoCoOp), which extends CoOp by further learning a lightweight neural net-work to generate for each image an input-conditional token (vector). Compared to CoOp’s static prompts, our dynamic prompts adapt to each instance and are thus less sensitive to class shift. Extensive experiments show that CoCoOp generalizes much better than CoOp to unseen classes, even showing promising transferability beyond a single dataset; and yields stronger domain generalization performance as well. Code is available at https://github.com/KaiyangZhou/CoOp. 