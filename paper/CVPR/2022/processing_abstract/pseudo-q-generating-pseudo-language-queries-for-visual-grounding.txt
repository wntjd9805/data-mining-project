Visual grounding, i.e., localizing objects in images ac-cording to natural language queries, is an important topic in visual language understanding. The most effective ap-proaches for this task are based on deep learning, which generally require expensive manually labeled image-query or patch-query pairs.To eliminate the heavy depen-dence on human annotations, we present a novel method, named Pseudo-Q, to automatically generate pseudo lan-guage queries for supervised training. Our method lever-ages an off-the-shelf object detector to identify visual ob-jects from unlabeled images, and then language queries for these objects are obtained in an unsupervised fashion with a pseudo-query generation module. Then, we design a task-related query prompt module to speciﬁcally tailor generated pseudo language queries for visual grounding tasks. Further, in order to fully capture the contextual re-lationships between images and language queries, we de-velop a visual-language model equipped with multi-level cross-modality attention mechanism. Extensive experimen-tal results demonstrate that our method has two notable beneﬁts: (1) it can reduce human annotation costs signiﬁ-cantly, e.g., 31% on RefCOCO [65] without degrading orig-inal model’s performance under the fully supervised set-ting, and (2) without bells and whistles, it achieves supe-rior or comparable performance compared to state-of-the-art weakly-supervised visual grounding methods on all theﬁve datasets we have experimented. Code is available at https://github.com/LeapLabTHU/Pseudo-Q. 