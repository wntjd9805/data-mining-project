The best performing learning algorithms devised for event cameras work by first converting events into dense representations that are then processed using standardCNNs. However, these steps discard both the sparsity and high temporal resolution of events, leading to high compu-tational burden and latency. For this reason, recent works have adopted Graph Neural Networks (GNNs), which pro-cess events as “static” spatio-temporal graphs, which are inherently ”sparse”. We take this trend one step further by introducing Asynchronous, Event-based Graph Neural Net-works (AEGNNs), a novel event-processing paradigm that generalizes standard GNNs to process events as “evolving” spatio-temporal graphs. AEGNNs follow efficient update rules that restrict recomputation of network activations only to the nodes affected by each new event, thereby signif-icantly reducing both computation and latency for event-by-event processing. AEGNNs are easily trained on syn-chronous inputs and can be converted to efficient, ”asyn-chronous” networks at test time. We thoroughly validate our method on object classification and detection tasks,*these authors contributed equally where we show an up to a 200-fold reduction in compu-tational complexity (FLOPs), with similar or even better performance than state-of-the-art asynchronous methods.This reduction in computation directly translates to an 8-fold reduction in computational latency when compared to standard GNNs, which opens the door to low-latency event-based processing.Multimedia MaterialFor videos, code and more, visit our project page https://uzh-rpg.github.io/aegnn/. 