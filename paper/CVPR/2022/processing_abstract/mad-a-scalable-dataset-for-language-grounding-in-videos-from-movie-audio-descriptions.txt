The recent and increasing interest in video-language re-search has driven the development of large-scale datasets that enable data-intensive machine learning techniques. In comparison, limited effort has been made at assessing theﬁtness of these datasets for the video-language ground-ing task. Recent works have begun to discover signiﬁ-cant limitations in these datasets, suggesting that state-of-the-art techniques commonly overﬁt to hidden dataset bi-ases. In this work, we present MAD (Movie Audio Descrip-tions), a novel benchmark that departs from the paradigm of augmenting existing video datasets with text annota-tions and focuses on crawling and aligning available au-dio descriptions of mainstream movies. MAD contains over 384, 000 natural language sentences grounded in over 1, 200 hours of videos and exhibits a signiﬁcant reduction in the currently diagnosed biases for video-language ground-ing datasets. MAD’s collection strategy enables a novel and more challenging version of video-language ground-ing, where short temporal moments (typically seconds long) must be accurately grounded in diverse long-form videos that can last up to three hours. We have released MAD’s data and baselines code at https://github.com/Soldelli/MAD. 