Diffusion probabilistic models (DPMs) have achieved re-markable quality in image generation that rivals GANs’.But unlike GANs, DPMs use a set of latent variables that lack semantic meaning and cannot serve as a useful rep-resentation for other tasks. This paper explores the possi-bility of using DPMs for representation learning and seeks to extract a meaningful and decodable representation of an input image via autoencoding. Our key idea is to use a learnable encoder for discovering the high-level seman-tics, and a DPM as the decoder for modeling the remaining stochastic variations. Our method can encode any image into a two-part latent code where the ﬁrst part is semanti-cally meaningful and linear, and the second part captures stochastic details, allowing near-exact reconstruction. This capability enables challenging applications that currently foil GAN-based methods, such as attribute manipulation on real images. We also show that this two-level encoding im-proves denoising efﬁciency and naturally facilitates various downstream tasks including few-shot conditional sampling.Please visit our page: https://Diff-AE.github.io/ 