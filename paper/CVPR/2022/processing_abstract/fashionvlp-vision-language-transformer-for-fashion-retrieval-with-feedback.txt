Fashion image retrieval based on a query pair of refer-ence image and natural language feedback is a challenging task that requires models to assess fashion related informa-tion from visual and textual modalities simultaneously. We propose a new vision-language transformer based model,FashionVLP, that brings the prior knowledge contained in large image-text corpora to the domain of fashion image re-trieval, and combines visual information from multiple lev-els of context to effectively capture fashion-related informa-tion.While queries are encoded through the transformer lay-ers, our asymmetric design adopts a novel attention-based approach for fusing target image features without involving text or transformer layers in the process. Extensive results show that FashionVLP achieves the state-of-the-art perfor-mance on benchmark datasets, with a large 23% relative improvement on the challenging FashionIQ dataset, which contains complex natural language feedback. 