Diffusion models have recently attained significant interest within the community owing to their strong performance as generative models. Furthermore, its application to inverse problems have demonstrated state-of-the-art performance.Unfortunately, diffusion models have a critical downside- they are inherently slow to sample from, needing few thousand steps of iteration to generate images from pureIn this work, we show that startingGaussian noise. from Gaussian noise is unnecessary.Instead, starting from a single forward diffusion with better initialization significantly reduces the number of sampling steps in theThis work was supported by Institute of Information & communica-tions Technology Planning & Evaluation (IITP) grant funded by the Ko-rea government(MSIT) (No.2019-0-00075, Artificial Intelligence Gradu-ate School Program(KAIST)), and by National Research Foundation(NRF) of Korea grant NRF-2021M3I1A1097938 reverse conditional diffusion. This phenomenon is formally explained by the contraction theory of the stochastic differ-ence equations like our conditional diffusion strategy - the alternating applications of reverse diffusion followed by a non-expansive data consistency step. The new sampling strategy, dubbed Come-Closer-Diffuse-Faster (CCDF), also reveals a new insight on how the existing feed-forward neural network approaches for inverse problems can be synergistically combined with the diffusion models. Ex-perimental results with super-resolution, image inpainting, and compressed sensing MRI demonstrate that our method can achieve state-of-the-art reconstruction performance at significantly reduced sampling steps. 