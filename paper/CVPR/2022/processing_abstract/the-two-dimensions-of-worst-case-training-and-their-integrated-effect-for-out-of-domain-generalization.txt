Training with an emphasis on “hard-to-learn” compo-nents of the data has been proven as an effective method to improve the generalization of machine learning models, especially in the settings where robustness (e.g., generaliza-tion across distributions) is valued. Existing literature dis-cussing this “hard-to-learn” concept are mainly expanded either along the dimension of the samples or the dimension of the features. In this paper, we aim to introduce a simple view merging these two dimensions, leading to a new, sim-ple yet effective, heuristic to train machine learning mod-els by emphasizing the worst-cases on both the sample and the feature dimensions. We name our method W2D follow-ing the concept of “Worst-case along Two Dimensions”.We validate the idea and demonstrate its empirical strength over standard benchmarks. 