Optimization within a layer of a deep-net has emerged as a new direction for deep-net layer design. However, there are two main challenges when applying these layers to com-puter vision tasks: (a) which optimization problem within a layer is useful?; (b) how to ensure that computation within a layer remains efﬁcient? To study question (a), in this work, we propose total variation (TV) minimization as a layer for computer vision. Motivated by the success of total variation in image processing, we hypothesize that TV as a layer pro-vides useful inductive bias for deep-nets too. We study this hypothesis on ﬁve computer vision tasks: image classiﬁca-tion, weakly supervised object localization, edge-preserving smoothing, edge detection, and image denoising, improving over existing baselines. To achieve these results we had to address question (b): we developed a GPU-based projected-Newton method which is 37× faster than existing solutions. 