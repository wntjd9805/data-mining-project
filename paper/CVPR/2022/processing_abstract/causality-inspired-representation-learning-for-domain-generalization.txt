Domain generalization (DG) is essentially an out-of-distribution problem, aiming to generalize the knowledge learned from multiple source domains to an unseen target domain. The mainstream is to leverage statistical models to model the dependence between data and labels, intending to learn representations independent of domain. Nevertheless, the statistical models are superﬁcial descriptions of reality since they are only required to model dependence instead of the intrinsic causal mechanism. When the dependence changes with the target distribution, the statistic models may fail to generalize. In this regard, we introduce a gen-eral structural causal model to formalize the DG problem.Speciﬁcally, we assume that each input is constructed from a mix of causal factors (whose relationship with the label is invariant across domains) and non-causal factors (category-independent), and only the former cause the classiﬁcation judgments. Our goal is to extract the causal factors from inputs and then reconstruct the invariant causal mechanisms.However, the theoretical idea is far from practical of DG since the required causal/non-causal factors are unobserved.We highlight that ideal causal factors should meet three basic properties: separated from the non-causal ones, jointly inde-pendent, and causally sufﬁcient for the classiﬁcation. Based on that, we propose a Causality Inspired RepresentationLearning (CIRL) algorithm that enforces the representations to satisfy the above properties and then uses them to simu-late the causal factors, which yields improved generalization ability. Extensive experimental results on several widely used datasets verify the effectiveness of our approach. 1 