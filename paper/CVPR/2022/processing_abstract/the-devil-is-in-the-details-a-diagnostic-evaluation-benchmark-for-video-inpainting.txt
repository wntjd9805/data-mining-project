Quantitative evaluation has increased dramatically among recent video inpainting work, but the video and mask content used to gauge performance has received relatively little attention. Although attributes such as camera and background scene motion inherently change the difﬁculty of the task and affect methods differently, existing evalu-ation schemes fail to control for them, thereby providing minimal insight into inpainting failure modes. To address this gap, we propose the Diagnostic Evaluation of VideoInpainting on Landscapes (DEVIL) benchmark, which con-sists of two contributions: (i) a novel dataset of videos and masks labeled according to several key inpainting failure modes, and (ii) an evaluation scheme that samples slices of the dataset characterized by a ﬁxed content attribute, and scores performance on each slice according to recon-struction, realism, and temporal consistency quality. By re-vealing systematic changes in performance induced by par-ticular characteristics of the input content, our challeng-ing benchmark enables more insightful analysis into video inpainting methods and serves as an invaluable diagnos-tic tool for the ﬁeld. Our code and data are available at github.com/MichiganCOG/devil. 