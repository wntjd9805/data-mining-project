Neural Radiance Fields (NeRF) have emerged as a pow-erful representation for the task of novel view synthesis due to their simplicity and state-of-the-art performance.Though NeRF can produce photorealistic renderings of un-seen viewpoints when many input views are available, its performance drops significantly when this number is re-duced. We observe that the majority of artifacts in sparse input scenarios are caused by errors in the estimated scene geometry, and by divergent behavior at the start of training.We address this by regularizing the geometry and appear-ance of patches rendered from unobserved viewpoints, and annealing the ray sampling space during training. We ad-ditionally use a normalizing flow model to regularize the color of unobserved viewpoints. Our model outperforms not only other methods that optimize over a single scene, but in many cases also conditional models that are exten-sively pre-trained on large multi-view datasets. 