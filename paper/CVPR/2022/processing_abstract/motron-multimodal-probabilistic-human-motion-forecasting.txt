Autonomous systems and humans are increasingly shar-ing the same space. Robots work side by side or even hand in hand with humans to balance each other’s limi-tations. Such cooperative interactions are ever more so-phisticated. Thus, the ability to reason not just about a human’s center of gravity position, but also its granular motion is an important prerequisite for human-robot inter-action. Though, many algorithms ignore the multimodal nature of humans or neglect uncertainty in their motion forecasts. We present Motron, a multimodal, probabilistic, graph-structured model, that captures human’s multimodal-ity using probabilistic methods while being able to out-put deterministic maximum-likelihood motions and corre-sponding conﬁdence values for each mode. Our model aims to be tightly integrated with the robotic planning-control-interaction loop; outputting physically feasible human mo-tions and being computationally efﬁcient. We demonstrate the performance of our model on several challenging real-world motion forecasting datasets, outperforming a wide array of generative/variational methods while providing state-of-the-art single-output motions if required. Both us-ing signiﬁcantly less computational power than state-of-the art algorithms. 