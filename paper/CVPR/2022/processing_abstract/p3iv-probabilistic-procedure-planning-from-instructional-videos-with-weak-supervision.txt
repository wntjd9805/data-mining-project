In this paper, we study the problem of procedure plan-ning in instructional videos. Here, an agent must pro-duce a plausible sequence of actions that can transform the environment to a desired goal from a given start state. When learning procedure planning from instructional videos, most recent work leverages intermediate visual ob-servations as supervision, which requires expensive anno-tation efforts to localize precisely all the instructional steps in training videos. In contrast, we remove the need for ex-pensive temporal video annotations and propose a weakly supervised approach by learning from natural language in-structions. Our model is based on a transformer equipped with a memory module, which maps the start and goal ob-servations to a sequence of plausible actions. Furthermore, we augment our model with a probabilistic generative mod-ule to capture the uncertainty inherent to procedure plan-ning, an aspect largely overlooked by previous work. We evaluate our model on three datasets and show our weakly-supervised approach outperforms previous fully supervised state-of-the-art models on multiple metrics. 