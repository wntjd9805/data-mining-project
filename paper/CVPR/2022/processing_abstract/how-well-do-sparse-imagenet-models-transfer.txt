Transfer learning is a classic paradigm by which mod-els pretrained on large “upstream” datasets are adapted to yield good results on “downstream” specialized datasets.Generally, more accurate models on the “upstream” dataset tend to provide better transfer accuracy “down-stream”.In this work, we perform an in-depth investi-gation of this phenomenon in the context of convolutional neural networks (CNNs) trained on the ImageNet dataset, which have been pruned—that is, compressed by sparsiﬁy-ing their connections. We consider transfer using unstruc-tured pruned models obtained by applying several state-including magnitude-based, of-the-art pruning methods, second-order, re-growth, lottery-ticket, and regularization approaches, in the context of twelve standard transfer tasks.In a nutshell, our study shows that sparse models can match or even outperform the transfer performance of dense mod-els, even at high sparsities, and, while doing so, can lead to signiﬁcant inference and even training speedups. At the same time, we observe and analyze signiﬁcant differ-ences in the behaviour of different pruning methods. The code is available at: https://github.com/IST-DASLab/sparse-imagenet-transfer. 