Augmented reality devices have the potential to enhance human perception and enable other assistive functionalities in complex conversational environments. Effectively cap-turing the audio-visual context necessary for understanding these social interactions first requires detecting and local-izing the voice activities of the device wearer and the sur-rounding people. These tasks are challenging due to their egocentric nature: the wearer’s head motion may cause mo-tion blur, surrounding people may appear in difficult view-ing angles, and there may be occlusions, visual clutter, au-dio noise, and bad lighting. Under these conditions, pre-vious state-of-the-art active speaker detection methods do not give satisfactory results. Instead, we tackle the prob-lem from a new setting using both video and multi-channel microphone array audio. We propose a novel end-to-end deep learning approach that is able to give robust voice ac-tivity detection and localization results. In contrast to pre-vious methods, our method localizes active speakers from all possible directions on the sphere, even outside the cam-era’s field of view, while simultaneously detecting the de-vice wearer’s own voice activity. Our experiments show that the proposed method gives superior results, can run in real time, and is robust against noise and clutter. 