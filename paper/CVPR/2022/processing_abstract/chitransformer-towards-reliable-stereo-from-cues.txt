Current stereo matching techniques are challenged by restricted searching space, occluded regions and sheer size.While single image depth estimation is spared from these challenges and can achieve satisfactory results with the ex-tracted monocular cues, the lack of stereoscopic relation-ship renders the monocular prediction less reliable on its own especially in highly dynamic or cluttered environments.To address these issues in both scenarios, we present an optic-chiasm-inspired self-supervised binocular depth es-timation method, wherein vision transformer (ViT) with a gated positional cross-attention (GPCA) layer is designed to enable feature-sensitive pattern retrieval between views, while retaining the extensive context information aggre-gated through self-attentions. Monocular cues from a sin-gle view are thereafter conditionally rectiﬁed by a blending layer with the retrieved pattern pairs. This crossover de-sign is biologically analogous to the optic-chasma struc-ture in human visual system and hence the name, Chi-Transformer. Our experiments show that this architecture yields substantial improvements over state-of-the-art self-supervised stereo approaches by 11%, and can be used on both rectilinear and non-rectilinear (e.g., ﬁsheye) images. 1 