Optical sensors and learning algorithms for autonomous vehicles have dramatically advanced in the past few years.Nonetheless, the reliability of today’s autonomous vehicles is hindered by the limited line-of-sight sensing capability and the brittleness of data-driven methods in handling ex-treme situations. With recent developments of telecommuni-cation technologies, cooperative perception with vehicle-to-vehicle communications has become a promising paradigm to enhance autonomous driving in dangerous or emer-gency situations. We introduce COOPERNAUT, an end-to-end learning model that uses cross-vehicle perception for vision-based cooperative driving. Our model encodes Li-DAR information into compact point-based representations that can be transmitted as messages between vehicles via realistic wireless channels. To evaluate our model, we de-velop AUTOCASTSIM, a network-augmented driving sim-ulation framework with example accident-prone scenarios.Our experiments on AUTOCASTSIM suggest that our coop-erative perception driving models lead to a 40% improve-ment in average success rate over egocentric driving mod-els in these challenging driving situations and a 5× smaller bandwidth requirement than prior work V2VNet. COOPER-NAUT and AUTOCASTSIM are available at https:// ut-austin-rpl.github.io/Coopernaut/. 