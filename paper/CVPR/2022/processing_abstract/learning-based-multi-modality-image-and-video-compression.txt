Multi-modality (i.e., multi-sensor) data is widely used in various vision tasks for more accurate or robust percep-tion. However, the increased data modalities bring new challenges for data storage and transmission. The exist-ing data compression approaches usually adopt individual codecs for each modality without considering the corre-lation between different modalities. This work proposes a multi-modality compression framework for infrared and visible image pairs by exploiting the cross-modality redun-dancy. Specifically, given the image in the reference modal-ity (e.g., the infrared image), we use the channel-wise align-ment module to produce the aligned features based on the affine transform. Then the aligned feature is used as the context information for compressing the image in the cur-rent modality (e.g., the visible image), and the correspond-ing affine coefficients are losslessly compressed at negligi-ble cost. Furthermore, we introduce the Transformer-based spatial alignment module to exploit the correlation between the intermediate features in the decoding procedures for dif-ferent modalities. Our framework is very flexible and eas-ily extended for multi-modality video compression. Experi-mental results show our proposed framework outperforms the traditional and learning-based single modality com-pression methods on the FLIR and KAIST datasets. 