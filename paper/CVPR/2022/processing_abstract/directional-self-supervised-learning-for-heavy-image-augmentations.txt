Despite the large augmentation family, only a few cherry-picked robust augmentation policies are beneficial to self-supervised image representation learning.In this paper, we propose a directional self-supervised learning paradigm (DSSL), which is compatible with significantly more augmentations. Specifically, we adapt heavy augmen-tation policies after the views lightly augmented by stan-dard augmentations, to generate harder view (HV). HV usu-ally has a higher deviation from the original image than the lightly augmented standard view (SV). Unlike previous methods equally pairing all augmented views to symmet-rically maximize their similarities, DSSL treats augmented views of the same instance as a partially ordered set (with directions as SV↔SV, SV←HV), and then equips a direc-tional objective function respecting to the derived relation-ships among views. DSSL can be easily implemented with a few lines of codes and is highly flexible to popular self-supervised learning frameworks, including SimCLR, Sim-Siam, BYOL. Extensive experimental results on CIFAR andImageNet demonstrated that DSSL can stably improve var-ious baselines with compatibility to a wider range of aug-mentations. Code is available at: https://github. com/Yif-Yang/DSSL. 