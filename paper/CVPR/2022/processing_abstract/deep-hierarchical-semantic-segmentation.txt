Humans are able to recognize structured relations in ob-servation, allowing us to decompose complex scenes into simpler parts and abstract the visual world in multiple lev-els. However, such hierarchical reasoning ability of human perception remains largely unexplored in current literature of semantic segmentation. Existing work is often aware ofﬂatten labels and predicts target classes exclusively for each pixel. In this paper, we instead address hierarchical seman-tic segmentation (HSS), which aims at structured, pixel-wise description of visual observation in terms of a class hierar-chy. We devise HSSN, a general HSS framework that tackles two critical issues in this task: i) how to efﬁciently adapt ex-isting hierarchy-agnostic segmentation networks to the HSS setting, and ii) how to leverage the hierarchy information to regularize HSS network learning. To address i), HSSN dire-ctly casts HSS as a pixel-wise multi-label classiﬁcation task, only bringing minimal architecture change to current seg-mentation models. To solve ii), HSSN ﬁrst explores inherent properties of the hierarchy as a training objective, which en-forces segmentation predictions to obey the hierarchy stru-cture. Further, with hierarchy-induced margin constraints,HSSN reshapes the pixel embedding space, so as to generate well-structured pixel representations and improve segmen-tation eventually. We conduct experiments on four seman-tic segmentation datasets (i.e., Mapillary Vistas 2.0, City-scapes, LIP, and PASCAL-Person-Part), with different class hierarchies, segmentation network architectures and back-bones, showing the generalization and superiority of HSSN. 