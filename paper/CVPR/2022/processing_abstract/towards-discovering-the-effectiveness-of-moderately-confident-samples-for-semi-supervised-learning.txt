Semi-supervised learning (SSL) has been studied for a long time to solve vision tasks in data-efficient application scenarios. SSL aims to learn a good classification model us-ing a few labeled data together with large-scale unlabeled data. Recent advances achieve the goal by combining mul-tiple SSL techniques, e.g., self-training and consistency reg-ularization. From unlabeled samples, they usually adopt a confidence filter (CF) to select reliable ones with high pre-diction confidence. In this work, we study whether the mod-erately confident samples are useless and how to select the useful ones to improve model optimization. To answer these problems, we propose a novel Taylor expansion inspired filtration (TEIF) framework, which admits the samples of moderate confidence with similar feature or gradient to the respective one averaged over the labeled and highly confi-dent unlabeled data. It can produce a stable and new infor-mation induced network update, leading to better general-ization. Two novel filters are derived from this framework and can be naturally explained in two perspectives. One is gradient synchronization filter (GSF), which strengthens the optimization dynamic of fully-supervised learning; it se-lects the samples whose gradients are similar to class-wise majority gradients. The other is prototype proximity filter (PPF), which involves more prototypical samples in train-ing to learn better semantic representations; it selects the samples near class-wise prototypes. They can be integrated into SSL methods with CF. We use the state-of-the-art Fix-Match as the baseline. Experiments on popular SSL bench-marks show that we achieve the new state of the art. 