(cid:6)(cid:11)(cid:18)(cid:22)(cid:11)(cid:18) (cid:4)(cid:9)(cid:20)(cid:9)(cid:18)(cid:16)(cid:10)(cid:9)(cid:15)(cid:9)(cid:12)(cid:20)(cid:21)(cid:1)(cid:5)(cid:18)(cid:16)(cid:7)(cid:13)(cid:9)(cid:14)Federated learning has emerged as an important dis-tributed learning paradigm, which normally involves col-laborative updating with others and local updating on pri-vate data. However, heterogeneity problem and catas-trophic forgetting bring distinctive challenges. First, due to non-i.i.d (identically and independently distributed) data and heterogeneous architectures, models suffer perfor-mance degradation on other domains and communication barrier with participants models. Second, in local updat-ing, model is separately optimized on private data, which is prone to overﬁt current data distribution and forgets pre-viously acquired knowledge, resulting in catastrophic for-getting. In this work, we propose FCCL (Federated Cross-Correlation and Continual Learning). For heterogeneity problem, FCCL leverages unlabeled public data for com-munication and construct cross-correlation matrix to learn a generalizable representation under domain shift. Mean-while, for catastrophic forgetting, FCCL utilizes knowledge distillation in local updating, providing inter and intra do-main information without leaking privacy. Empirical re-sults on various image classiﬁcation tasks demonstrate the effectiveness of our method and the efﬁciency of modules. 