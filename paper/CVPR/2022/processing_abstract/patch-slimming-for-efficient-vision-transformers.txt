This paper studies the efﬁciency problem for visual transformers by excavating redundant calculation in given networks. The recent transformer architecture has demon-strated its effectiveness for achieving excellent performance on a series of computer vision tasks. However, similar to that of convolutional neural networks, the huge com-putational cost of vision transformers is still a severe is-sue. Considering that the attention mechanism aggregates different patches layer-by-layer, we present a novel patch slimming approach that discards useless patches in a top-down paradigm. We ﬁrst identify the effective patches in the last layer and then use them to guide the patch selec-tion process of previous layers. For each layer, the impact of a patch on the ﬁnal output feature is approximated and patches with less impacts will be removed. Experimental re-sults on benchmark datasets demonstrate that the proposed method can signiﬁcantly reduce the computational costs of vision transformers without affecting their performances.For example, over 45% FLOPs of the ViT-Ti model can be reduced with only 0.2% top-1 accuracy drop on the Ima-geNet dataset. 