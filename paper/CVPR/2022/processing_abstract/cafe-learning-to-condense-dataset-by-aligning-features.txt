Dataset condensation aims at reducing the network training effort through condensing a cumbersome train-ing set into a compact synthetic one. State-of-the-art ap-proaches largely rely on learning the synthetic data by matching the gradients between the real and synthetic data batches. Despite the intuitive motivation and promising re-sults, such gradient-based methods, by nature, easily over-ft to a biased set of samples that produce dominant gra-dients, and thus lack a global supervision of data distribu-tion. In this paper, we propose a novel scheme to Condense dataset by Aligning FEatures (CAFE), which explicitly at-tempts to preserve the real-feature distribution as well as the discriminant power of the resulting synthetic set, lend-ing itself to strong generalization capability to various ar-chitectures. At the heart of our approach is an effective strategy to align features from the real and synthetic data across various scales, while accounting for the classifca-tion of real samples. Our scheme is further backed up by a novel dynamic bi-level optimization, which adaptively ad-justs parameter updates to prevent over-/under-ftting. We validate the proposed CAFE across various datasets, and demonstrate that it generally outperforms the state of the art: on the SVHN dataset, for example, the performance gain is up to 11%. Extensive experiments and analysis ver-ify the effectiveness and necessity of proposed designs. (a) The gradient distribution changes from a uniform to long-tailed distribution during the training. Meanwhile, the overlap of large-gradient samples are small among different architectures. (b) The visualization of synthetic images and their distributions generated by gradient matching and CAFE. ConvNet is used.Figure 1: (a) At the later training stage, most examples do not contribute meaningful gradients, making the syn-thetic set learned by gradient matching extremely bias to-wards those large-gradient samples, which downgrades its generalization to unseen architectures. (b) Compared with gradient-based method [53], the synthetic set learned by our approach effectively captures the whole distribution thus generalizes well to other network architectures. 