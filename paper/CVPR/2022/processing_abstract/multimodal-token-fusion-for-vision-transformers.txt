Many adaptations of transformers have emerged to ad-dress the single-modal vision tasks, where self-attention modules are stacked to handle input sources like images.Intuitively, feeding multiple modalities of data to vision transformers could improve the performance, yet the inner-modal attentive weights may be diluted, which could thus greatly undermine the final performance. In this paper, we propose a multimodal token fusion method (TokenFusion), tailored for transformer-based vision tasks. To effectively fuse multiple modalities, TokenFusion dynamically detects uninformative tokens and substitute these tokens with pro-jected and aggregated inter-modal features. Residual posi-tional alignment is also adopted to enable explicit utiliza-tion of the inter-modal alignments after fusion. The design of TokenFusion allows the transformer to learn correlations among multimodal features, while the single-modal trans-former architecture remains largely intact. Extensive exper-iments are conducted on a variety of homogeneous and het-erogeneous modalities and demonstrate that TokenFusion surpasses state-of-the-art methods in three typical vision tasks: multimodal image-to-image translation, RGB-depth semantic segmentation, and 3D object detection with point cloud and images. Code will be released 1 2. 