Knowledge distillation becomes a de facto standard to improve the performance of small neural networks. Most of the previous works propose to regress the representa-tional features from the teacher to the student in a one-to-one spatial matching fashion. However, people tend to overlook the fact that, due to the architecture differences, the semantic information on the same spatial location usu-ally vary. This greatly undermines the underlying assump-tion of the one-to-one distillation approach. To this end, we propose a novel one-to-all spatial matching knowledge distillation approach. Specifically, we allow each pixel of the teacher feature to be distilled to all spatial locations of the student features given its similarity, which is gener-ated from a target-aware transformer. Our approach sur-passes the state-of-the-art methods by a significant mar-gin on various computer vision benchmarks, such as Im-ageNet, Pascal VOC and COCOStuff10k. Code is available at https://github.com/sihaoevery/TaT. 