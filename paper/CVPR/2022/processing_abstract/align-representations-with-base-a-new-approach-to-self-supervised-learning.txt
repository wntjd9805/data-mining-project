Existing symmetric contrastive learning methods suffer from collapses (complete and dimensional) or quadratic complexity of objectives. Departure from these methods which maximize mutual information of two generated views, along either instance or feature dimension, the proposed paradigm introduces intermediate variables at the feature level, and maximizes the consistency between variables and representations of each view. Specifically, the proposed in-termediate variables are the nearest group of base vectors to representations. Hence, we call the proposed methodARB (Align Representations with Base). Compared with other symmetric approaches, ARB 1) does not require neg-ative pairs, which leads the complexity of the overall ob-jective function is in linear order, 2) reduces feature redun-dancy, increasing the information density of training sam-ples, 3) is more robust to output dimension size, which out-performs previous feature-wise arts over 28% Top-1 accu-racy on ImageNet-100 under low-dimension settings. 