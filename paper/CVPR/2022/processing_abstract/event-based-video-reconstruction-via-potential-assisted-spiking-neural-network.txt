Neuromorphic vision sensor is a new bio-inspired imag-ing paradigm that reports asynchronous, continuously per-pixel brightness changes called ‘events’ with high tempo-ral resolution and high dynamic range. So far, the event-based image reconstruction methods are based on artiﬁ-cial neural networks (ANN) or hand-crafted spatiotempo-ral smoothing techniques.In this paper, we ﬁrst imple-ment the image reconstruction work via deep spiking neu-ral network (SNN) architecture. As the bio-inspired neural networks, SNNs operating with asynchronous binary spikes distributed over time, can potentially lead to greater com-putational efﬁciency on event-driven hardware. We pro-pose a novel Event-based Video reconstruction framework based on a fully Spiking Neural Network (EVSNN), which utilizes Leaky-Integrate-and-Fire (LIF) neuron and Mem-brane Potential (MP) neuron. We ﬁnd that the spiking neu-rons have the potential to store useful temporal information (memory) to complete such time-dependent tasks. Further-more, to better utilize the temporal information, we propose a hybrid potential-assisted framework (PA-EVSNN) using the membrane potential of spiking neuron. The proposed neuron is referred as Adaptive Membrane Potential (AM-P) neuron, which adaptively updates the membrane poten-tial according to the input spikes. The experimental results demonstrate that our models achieve comparable perfor-mance to ANN-based models on IJRR, MVSEC, and HQF datasets. The energy consumptions of EVSNN and PA-EVSNN are 19.36(cid:2) and 7.75(cid:2) more computationally ef-ﬁcient than their ANN architectures, respectively. The code and pretrained model are available at https://sites. google.com/view/evsnn. 