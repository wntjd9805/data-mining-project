The task of Human-Object Interaction (HOI) detection could be divided into two core problems, i.e., human-object association and interaction understanding.In this paper, we reveal and address the disadvantages of the conventional query-driven HOI detectors from the two aspects. For the association, previous two-branch methods suffer from com-plex and costly post-matching, while single-branch methods ignore the features distinction in different tasks. We pro-pose Guided-Embedding Network (GEN) to attain a two-branch pipeline without post-matching.In GEN, we de-sign an instance decoder to detect humans and objects with two independent query sets and a position Guided Embed-ding (p-GE) to mark the human and object in the same position as a pair. Besides, we design an interaction de-coder to classify interactions, where the interaction queries are made of instance Guided Embeddings (i-GE) gener-ated from the outputs of each instance decoder layer. For the interaction understanding, previous methods suffer from long-tailed distribution and zero-shot discovery. This pa-per proposes Visual-Linguistic Knowledge Transfer (VLKT) training strategy to enhance interaction understanding by transferring knowledge from a visual-linguistic pre-trained model CLIP. In specific, we extract text embeddings for all labels with CLIP to initialize the classifier and adopt a mimic loss to minimize the visual feature distance betweenGEN and CLIP. As a result, GEN-VLKT outperforms the state of the art by large margins on multiple datasets, e.g.,+5.05 mAP on HICO-Det. The source codes are available at https://github.com/YueLiao/gen-vlkt. 