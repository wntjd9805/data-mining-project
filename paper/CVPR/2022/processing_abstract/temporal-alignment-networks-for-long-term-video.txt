The objective of this paper is a temporal alignment net-work that ingests long term video sequences, and associ-ated text sentences, in order to: (1) determine if a sentence is alignable with the video; and (2) if it is alignable, then determine its alignment. The challenge is to train such networks from large-scale datasets, such as HowTo100M, where the associated text sentences have significant noise, and are only weakly aligned when relevant.Apart from proposing the alignment network, we also make four contributions: (i) we describe a novel co-training method that enables to denoise and train on raw instructional videos without using manual annotation, de-spite the considerable noise; (ii) to benchmark the align-ment performance, we manually curate a 10-hour subset ofHowTo100M, totalling 80 videos, with sparse temporal de-scriptions. Our proposed model, trained on HowTo100M, outperforms strong baselines (CLIP, MIL-NCE) on this alignment dataset by a significant margin; (iii) we ap-ply the trained model in the zero-shot settings to mul-tiple downstream video understanding tasks and achieve including text-video retrieval on state-of-the-art results,YouCook2, and weakly supervised video action segmenta-tion on Breakfast-Action. (iv) we use the automatically-aligned HowTo100M annotations for end-to-end finetuning of the backbone model, and obtain improved performance on downstream action recognition tasks. 