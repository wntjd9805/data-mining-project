Continuous normalizing flows (CNFs) construct invert-ible mappings between an arbitrary complex distribution and an isotropic Gaussian distribution using Neural Ordi-nary Differential Equations (neural ODEs). It has not been tractable on large datasets due to the incremental complex-ity of the neural ODE training. Optimal Transport theory has been applied to regularize the dynamics of the ODE to speed up training in recent works. In this paper, a tempo-ral optimization is proposed by optimizing the evolutionary time for forward propagation of the neural ODE training.In this appoach, we optimize the network weights of theCNF alternately with evolutionary time by coordinate de-scent. Further with temporal regularization, stability of the evolution is ensured. This approach can be used in conjunc-tion with the original regularization approach. We have ex-perimentally demonstrated that the proposed approach can significantly accelerate training without sacrifying perfor-mance over baseline models. 