In recent years, we have witnessed significant perfor-mance boost in the image captioning task based on vision-language pre-training (VLP). Scale is believed to be an im-portant factor for this advance. However, most existing work only focuses on pre-training transformers with mod-erate sizes (e.g., 12 or 24 layers) on roughly 4 million im-, a LargE-scale ages. In this paper, we present LEMON iMage captiONer, and provide the first empirical study on the scaling behavior of VLP for image captioning. We use the state-of-the-art VinVL model as our reference model, which consists of an image feature extractor and a trans-former model, and scale the transformer both up and down, with model sizes ranging from 13 to 675 million parame-ters. In terms of data, we conduct experiments with up to 200 million image-text pairs which are automatically col-lected from web based on the alt attribute of the image (dubbed as ALT200M1). Extensive analysis helps to char-acterize the performance trend as the model size and the pre-training data size increase. We also compare different training recipes, especially for training on large-scale noisy data. As a result, LEMON achieves new state of the arts on several major image captioning benchmarks, includingCOCO Caption, nocaps, and Conceptual Captions. We also show LEMON can generate captions with long-tail vi-sual concepts when used in a zero-shot manner. 