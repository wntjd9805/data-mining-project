Fully unsupervised 3D representation learning has gained attention owing to its advantages in data collec-tion. A successful approach involves a viewpoint-aware ap-proach that learns an image distribution based on genera-tive models (e.g., generative adversarial networks (GANs)) while generating various view images based on 3D-aware models (e.g., neural radiance fields (NeRFs)). However, they require images with various views for training, and consequently, their application to datasets with few or lim-ited viewpoints remains a challenge. As a complementary approach, an aperture rendering GAN (AR-GAN) that em-ploys a defocus cue was proposed. However, an AR-GAN is a CNN-based model and represents a defocus independently from a viewpoint change despite its high correlation, which is one of the reasons for its performance. As an alternative to an AR-GAN, we propose an aperture rendering NeRF (AR-NeRF), which can utilize viewpoint and defocus cues in a unified manner by representing both factors in a com-mon ray-tracing framework. Moreover, to learn defocus-aware and defocus-independent representations in a disen-tangled manner, we propose aperture randomized training, for which we learn to generate images while randomizing the aperture size and latent codes independently. During our experiments, we applied AR-NeRF to various natural image datasets, including flower, bird, and face images, the results of which demonstrate the utility of AR-NeRF for un-supervised learning of the depth and defocus effects. 