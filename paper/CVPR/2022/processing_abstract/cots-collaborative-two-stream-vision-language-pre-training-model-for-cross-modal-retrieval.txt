Large-scale single-stream pre-training has shown dra-matic performance in image-text retrieval. Regrettably, it faces low inference efﬁciency due to heavy attention layers.Recently, two-stream methods like CLIP and ALIGN with high inference efﬁciency have also shown promising per-formance, however, they only consider instance-level align-ment between the two streams (thus there is still room for improvement). To overcome these limitations, we propose a novel COllaborative Two-Stream vision-language pre-training model termed COTS for image-text retrieval by en-hancing cross-modal interaction. In addition to instance-level alignment via momentum contrastive learning, we leverage two extra levels of cross-modal interactions in our COTS: (1) Token-level interaction – a masked vision-language modeling (MVLM) learning objective is devised without using a cross-stream network module, where varia-tional autoencoder is imposed on the visual encoder to gen-erate visual tokens for each image. (2) Task-level interac-tion – a KL-alignment learning objective is devised between text-to-image and image-to-text retrieval tasks, where the probability distribution per task is computed with the nega-tive queues in momentum contrastive learning. Under a fair comparison setting, our COTS achieves the highest perfor-mance among all two-stream methods and comparable per-formance (but with 10,800× faster in inference) w.r.t. the latest single-stream methods. Importantly, our COTS is also applicable to text-to-video retrieval, yielding new state-of-the-art on the widely-used MSR-VTT dataset. 