Optical ﬂow, which captures motion information across frames, is exploited in recent video inpainting methods through propagating pixels along its trajectories. However, the hand-crafted ﬂow-based processes in these methods are applied separately to form the whole inpainting pipeline.Thus, these methods are less efﬁcient and rely heavily on the intermediate results from earlier stages.In this pa-per, we propose an End-to-End framework for Flow-GuidedVideo Inpainting (E2FGVI) through elaborately designed three trainable modules, namely, ﬂow completion, feature propagation, and content hallucination modules. The three modules correspond with the three stages of previous ﬂow-based methods but can be jointly optimized, leading to a more efﬁcient and effective inpainting process. Experimen-tal results demonstrate that the proposed method outper-forms state-of-the-art methods both qualitatively and quan-titatively and shows promising efﬁciency. The code is avail-able at https://github.com/MCG-NKU/E2FGVI. 