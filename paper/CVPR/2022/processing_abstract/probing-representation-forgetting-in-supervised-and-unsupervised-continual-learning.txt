Continual Learning (CL) research typically focuses on tackling the phenomenon of catastrophic forgetting in neu-ral networks. Catastrophic forgetting is associated with an abrupt loss of knowledge previously learned by a model when the task, or more broadly the data distribution, be-ing trained on changes.In supervised learning problems this forgetting, resulting from a change in the model’s rep-resentation, is typically measured or observed by evaluating the decrease in old task performance. However, a model’s representation can change without losing knowledge about prior tasks. In this work we consider the concept of rep-resentation forgetting, observed by using the difference in performance of an optimal linear classiﬁer before and after a new task is introduced. Using this tool we revisit a num-ber of standard continual learning benchmarks and observe that, through this lens, model representations trained with-out any explicit control for forgetting often experience small representation forgetting and can sometimes be comparable to methods which explicitly control for forgetting, especially in longer task sequences. We also show that representation forgetting can lead to new insights on the effect of model ca-pacity and loss function used in continual learning. Based on our results, we show that a simple yet competitive ap-proach is to learn representations continually with standard supervised contrastive learning while constructing proto-types of class samples when queried on old samples.1 