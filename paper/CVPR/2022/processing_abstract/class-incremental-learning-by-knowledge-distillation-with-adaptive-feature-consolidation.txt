We present a novel class incremental learning approach based on deep neural networks, which continually learns new tasks with limited memory for storing examples in the previous tasks. Our algorithm is based on knowledge dis-tillation and provides a principled way to maintain the rep-resentations of old models while adjusting to new tasks ef-fectively. The proposed method estimates the relationship between the representation changes and the resulting loss increases incurred by model updates. It minimizes the up-per bound of the loss increases using the representations, which exploits the estimated importance of each feature map within a backbone model. Based on the importance, the model restricts updates of important features for robust-ness while allowing changes in less critical features for flex-ibility. This optimization strategy effectively alleviates the notorious catastrophic forgetting problem despite the lim-ited accessibility of data in the previous tasks. The exper-imental results show significant accuracy improvement of the proposed algorithm over the existing methods on the standard datasets. Code is available.1 