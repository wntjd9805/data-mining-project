Generic event boundary detection aims to localize the taxonomy-free event boundaries that segment generic, videos into chunks. Existing methods typically require video frames to be decoded before feeding into the network, which demands considerable computational power and storage space. To that end, we propose a new end-to-end com-pressed video representation learning for event boundary detection that leverages the rich information in the com-pressed domain, i.e., RGB, motion vectors, residuals, and the internal group of pictures (GOP) structure, without fully decoding the video. Specifically, we first use the Con-vNets to extract features of the I-frames in the GOPs. Af-ter that, a light-weight spatial-channel compressed encoder is designed to compute the feature representations of the P-frames based on the motion vectors, residuals and represen-tations of their dependent I-frames. A temporal contrastive module is proposed to determine the event boundaries of video sequences. To remedy the ambiguities of annotations and speed up the training process, we use the Gaussian ker-nel to preprocess the ground-truth event boundaries. Exten-sive experiments conducted on the Kinetics-GEBD dataset demonstrate that the proposed method achieves compara-ble results to the state-of-the-art methods with 4.5Ã— faster running speed. 