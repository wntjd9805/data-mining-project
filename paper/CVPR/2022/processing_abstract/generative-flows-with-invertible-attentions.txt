Flow-based generative models have shown an excellent ability to explicitly learn the probability density function of data via a sequence of invertible transformations. Yet, learning attentions in generative ﬂows remains understud-ied, while it has made breakthroughs in other domains. Toﬁll the gap, this paper introduces two types of invertible at-tention mechanisms, i.e., map-based and transformer-based attentions, for both unconditional and conditional genera-tive ﬂows. The key idea is to exploit a masked scheme of these two attentions to learn long-range data dependencies in the context of generative ﬂows. The masked scheme al-lows for invertible attention modules with tractable Jaco-bian determinants, enabling its seamless integration at any positions of the ﬂow-based models. The proposed attention mechanisms lead to more efﬁcient generative ﬂows, due to their capability of modeling the long-term data dependen-cies. Evaluation on multiple image synthesis tasks shows that the proposed attention ﬂows result in efﬁcient models and compare favorably against the state-of-the-art uncon-ditional and conditional generative ﬂows. 