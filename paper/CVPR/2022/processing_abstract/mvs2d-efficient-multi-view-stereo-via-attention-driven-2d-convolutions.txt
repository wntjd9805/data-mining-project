Deep learning has made signiﬁcant impacts on multi-view stereo systems. State-of-the-art approaches typically involve building a cost volume, followed by multiple 3D convolution operations to recover the input image’s pixel-wise depth. While such end-to-end learning of plane-sweeping stereo advances public benchmarks’ accuracy, they are typically very slow to compute. We present MVS2D, a highly efﬁcient multi-view stereo algorithm that seam-lessly integrates multi-view constraints into single-view net-works via an attention mechanism. Since MVS2D only builds on 2D convolutions, it is at least 2 faster than all the notable counterparts. Moreover, our algorithm pro-duces precise depth estimations and 3D reconstructions, achieving state-of-the-art results on challenging bench-marks ScanNet, SUN3D, RGBD, and the classical DTU dataset. our algorithm also out-performs all other algo-rithms in the setting of inexact camera poses. Our code is released at https://github.com/zhenpeiyang/MVS2D× 