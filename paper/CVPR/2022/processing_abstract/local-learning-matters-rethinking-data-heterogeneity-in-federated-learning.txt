Federated learning (FL) is a promising strategy for per-forming privacy-preserving, distributed learning with a net-work of clients (i.e., edge devices). However, the data dis-tribution among clients is often non-IID in nature, making efﬁcient optimization difﬁcult. To alleviate this issue, manyFL algorithms focus on mitigating the effects of data hetero-geneity across clients by introducing a variety of proximal terms, some incurring considerable compute and/or mem-ory overheads, to restrain local updates with respect to the global model. Instead, we consider rethinking solutions to data heterogeneity in FL with a focus on local learning gen-erality rather than proximal restriction. To this end, we ﬁrst present a systematic study informed by second-order indi-cators to better understand algorithm effectiveness in FL.Interestingly, we ﬁnd that standard regularization methods are surprisingly strong performers in mitigating data het-erogeneity effects. Based on our ﬁndings, we further pro-pose a simple and effective method, FedAlign, to overcome data heterogeneity and the pitfalls of previous methods.FedAlign achieves competitive accuracy with state-of-the-art FL methods across a variety of settings while minimiz-ing computation and memory overhead. Code is available at https://github.com/mmendiet/FedAlign. 