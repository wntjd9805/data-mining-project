Recent Vision Transformer (ViT) models have demon-strated encouraging results across various computer vision tasks, thanks to its competence in modeling long-range de-pendencies of image patches or tokens via self-attention.These models, however, usually designate the similar recep-tive fields of each token feature within each layer. Such a constraint inevitably limits the ability of each self-attention layer in capturing multi-scale features, thereby leading to performance degradation in handling images with multi-ple objects of different scales. To address this issue, we propose a novel and generic strategy, termed shunted self-attention (SSA), that allows ViTs to model the attentions at hybrid scales per attention layer. The key idea of SSA is to inject heterogeneous receptive field sizes into tokens: before computing the self-attention matrix, it selectively merges to-kens to represent larger object features while keeping cer-tain tokens to preserve fine-grained features. This novel merging scheme enables the self-attention to learn rela-tionships between objects with different sizes, and simul-taneously reduces the token numbers and the computa-tional cost. Extensive experiments across various tasks demonstrate the superiority of SSA. Specifically, the SSA-based transformer achieve 84.0% Top-1 accuracy and out-performs the state-of-the-art Focal Transformer on Ima-geNet with only half of the model size and computation cost, and surpasses Focal Transformer by 1.3 mAP onCOCO and 2.9 mIOU on ADE20K under similar param-eter and computation cost. Code has been released at https://github.com/OliverRensu/Shunted-Transformer. 