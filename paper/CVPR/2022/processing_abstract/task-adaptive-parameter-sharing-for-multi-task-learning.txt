Adapting pre-trained models with broad capabilities has become standard practice for learning a wide range of downstream tasks. The typical approach of ﬁne-tuning dif-ferent models for each task is performant, but incurs a sub-stantial memory cost. To efﬁciently learn multiple down-stream tasks we introduce Task Adaptive Parameter Shar-ing (TAPS), a simple method for tuning a base model to a new task by adaptively modifying a small, task-speciﬁc sub-set of layers. This enables multi-task learning while min-imizing the resources used and avoids catastrophic forget-ting and competition between tasks. TAPS solves a joint optimization problem which determines both the layers that are shared with the base model and the value of the task-speciﬁc weights. Further, a sparsity penalty on the num-ber of active layers promotes weight sharing with the base model. Compared to other methods, TAPS retains a high accuracy on the target tasks while still introducing only a small number of task-speciﬁc parameters. Moreover, TAPS is agnostic to the particular architecture used and requires only minor changes to the training scheme. We evaluate our method on a suite of ﬁne-tuning tasks and architectures (ResNet,DenseNet,ViT) and show that it achieves state-of-the-art performance while being simple to implement. 