Prior works on action representation learning mainly fo-cus on designing various architectures to extract the global representations for short video clips.In contrast, many practical applications such as video alignment have strong demand for learning dense representations for long videos.In this paper, we introduce a novel contrastive action repre-sentation learning (CARL) framework to learn frame-wise action representations, especially for long videos, in a self-supervised manner. Concretely, we introduce a simple yet efﬁcient video encoder that considers spatio-temporal con-text to extract frame-wise representations. Inspired by the recent progress of self-supervised learning, we present a novel sequence contrastive loss (SCL) applied on two cor-related views obtained through a series of spatio-temporal data augmentations. SCL optimizes the embedding space by minimizing the KL-divergence between the sequence simi-larity of two augmented views and a prior Gaussian dis-tribution of timestamp distance. Experiments on FineGym,PennAction and Pouring datasets show that our method outperforms previous state-of-the-art by a large margin for downstream ﬁne-grained action classiﬁcation.Sur-prisingly, although without training on paired videos, our approach also shows outstanding performance on video alignment and ﬁne-grained frame retrieval tasks. Code and models are available at https://github.com/ minghchen/CARL_code. (a) Fine-grained frame retrieval on FineGym dataset. (b) Phase boundary detection on Pouring dataset. (c) Temporal video alignment on PennAction dataset.Figure 1. Multiple applications of our frame-wise representation learning on various datasets: (a) Fine-grained frame retrieval onFineGym [37]. (b) Phase boundary detection on Pouring [36]. (c)Temporal video alignment on PennAction [49]. As shown in theﬁgures, the representations obtained through our method (CARL) are invariant to the appearance, viewpoint and background. 