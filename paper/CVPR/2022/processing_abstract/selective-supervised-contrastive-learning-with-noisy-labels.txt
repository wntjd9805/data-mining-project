Deep networks have strong capacities of embedding data into latent representations and ﬁnishing following tasks.However, the capacities largely come from high-quality an-notated labels, which are expensive to collect. Noisy la-bels are more affordable, but result in corrupted represen-tations, leading to poor generalization performance. To learn robust representations and handle noisy labels, we propose selective-supervised contrastive learning (Sel-CL) in this paper. Speciﬁcally, Sel-CL extend supervised con-trastive learning (Sup-CL), which is powerful in represen-tation learning, but is degraded when there are noisy labels.Sel-CL tackles the direct cause of the problem of Sup-CL.That is, as Sup-CL works in a pair-wise manner, noisy pairs built by noisy labels mislead representation learning. To alleviate the issue, we select conﬁdent pairs out of noisy ones for Sup-CL without knowing noise rates. In the selec-tion process, by measuring the agreement between learned representations and given labels, we ﬁrst identify conﬁdent examples that are exploited to build conﬁdent pairs. Then, the representation similarity distribution in the built con-ﬁdent pairs is exploited to identify more conﬁdent pairs out of noisy pairs. All obtained conﬁdent pairs are ﬁnally used for Sup-CL to enhance representations. Experiments on multiple noisy datasets demonstrate the robustness of the learned representations by our method, following the state-of-the-art performance. Source codes are available at https://github.com/ShikunLi/Sel-CL. 