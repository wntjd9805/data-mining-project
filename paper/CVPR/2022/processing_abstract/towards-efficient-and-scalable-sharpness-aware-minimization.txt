Recently, Sharpness-Aware Minimization (SAM), which connects the geometry of the loss landscape and general-ization, has demonstrated a signiﬁcant performance boost on training large-scale models such as vision transform-ers. However, the update rule of SAM requires two se-quential (non-parallelizable) gradient computations at each step, which can double the computational overhead. In this paper, we propose a novel algorithm LookSAM - that only periodically calculates the inner gradient ascent, to signiﬁ-cantly reduce the additional training cost of SAM. The em-pirical results illustrate that LookSAM achieves similar ac-curacy gains to SAM while being tremendously faster - it en-joys comparable computational complexity with ﬁrst-order optimizers such as SGD or Adam. To further evaluate the performance and scalability of LookSAM, we incorporate a layer-wise modiﬁcation and perform experiments in the large-batch training scenario, which is more prone to con-verge to sharp local minima. Equipped with the proposed algorithms, we are the ﬁrst to successfully scale up the batch size when training Vision Transformers (ViTs). With a 64k batch size, we are able to train ViTs from scratch in min-utes while maintaining competitive performance. The code is available here: https://github.com/yong-6/LookSAM 