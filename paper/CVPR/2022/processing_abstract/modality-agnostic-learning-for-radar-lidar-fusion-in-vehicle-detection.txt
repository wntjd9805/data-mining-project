Fusion of multiple sensor modalities such as camera, Li-dar, and Radar, which are commonly found on autonomous vehicles, not only allows for accurate detection but also ro-bustifies perception against adverse weather conditions and individual sensor failures. Due to inherent sensor charac-teristics, Radar performs well under extreme weather con-ditions (snow, rain, fog) that significantly degrade camera and Lidar. Recently, a few works have developed vehicle de-tection methods fusing Lidar and Radar signals, i.e., MVD-Net. However, these models are typically developed under the assumption that the models always have access to two error-free sensor streams. If one of the sensors is unavail-able or missing, the model may fail catastrophically. To mitigate this problem, we propose the Self-Training Mul-timodal Vehicle Detection Network (ST-MVDNet) which leverages a Teacher-Student mutual learning framework and a simulated sensor noise model used in strong data aug-mentation for Lidar and Radar. We show that by (1) enforc-ing output consistency between a Teacher network and aStudent network and by (2) introducing missing modalities (strong augmentations) during training, our learned model breaks away from the error-free sensor assumption. This consistency enforcement enables the Student model to han-dle missing data properly and improve the Teacher model by updating it with the Student modelâ€™s exponential mov-ing average. Our experiments demonstrate that our pro-posed learning framework for multi-modal detection is able to better handle missing sensor data during inference. Fur-thermore, our method achieves new state-of-the-art perfor-mance (5% gain) on the Oxford Radar Robotcar dataset under various evaluation settings. 