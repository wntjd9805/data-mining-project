Neural image caption generation (NICG) models have received massive attention from the research community due to their excellent performance in visual understand-ing. Existing work focuses on improving NICG model ac-curacy while efﬁciency is less explored. However, many real-world applications require real-time feedback, which highly relies on the efﬁciency of NICG models. Recent re-search observed that the efﬁciency of NICG models could vary for different inputs. This observation brings in a new attack surface of NICG models, i.e., An adversary might be able to slightly change inputs to cause the NICG mod-els to consume more computational resources. To further understand such efﬁciency-oriented threats, we propose a new attack approach, NICGSlowDown, to evaluate the ef-ﬁciency robustness of NICG models. Our experimental re-sults show that NICGSlowDown can generate images with human-unnoticeable perturbations that will increase theNICG model latency up to 483.86%. We hope this research could raise the community’s concern about the efﬁciency robustness of NICG models. 