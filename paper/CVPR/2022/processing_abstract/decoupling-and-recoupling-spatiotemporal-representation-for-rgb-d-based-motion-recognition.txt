Decoupling spatiotemporal representation refers to features into decomposing the spatial and temporal dimension-independent factors. Although previous RGB-D-based motion recognition methods have achieved promis-ing performance through the tightly coupled multi-modal spatiotemporal representation, they still suffer from (i) op-timization difﬁculty under small data setting due to the tightly spatiotemporal-entangled modeling; (ii) informa-tion redundancy as it usually contains lots of marginal information that is weakly relevant to classiﬁcation; and (iii) low interaction between multi-modal spatiotemporal information caused by insufﬁcient late fusion. To allevi-ate these drawbacks, we propose to decouple and recou-ple spatiotemporal representation for RGB-D-based mo-tion recognition. Speciﬁcally, we disentangle the task of learning spatiotemporal representation into 3 sub-tasks: (1)Learning high-quality and dimension independent features through a decoupled spatial and temporal modeling net-work. (2) Recoupling the decoupled representation to es-tablish stronger space-time dependency. (3) Introducing a Cross-modal Adaptive Posterior Fusion (CAPF) mech-anism to capture cross-modal spatiotemporal information from RGB-D data. Seamless combination of these novel designs forms a robust spatiotemporal representation and achieves better performance than state-of-the-art methods on four public motion datasets. Our code is available at https://github.com/damo-cv/MotionRGBD. 