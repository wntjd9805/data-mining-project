In this paper we consider the problem of classifying fine-grained, multi-step activities (e.g., cooking different recipes, making disparate home improvements, creating various forms of arts and crafts) from long videos spanning up to several minutes. Accurately categorizing these activ-ities requires not only recognizing the individual steps that compose the task but also capturing their temporal depen-dencies. This problem is dramatically different from tradi-tional action classification, where models are typically op-timized on videos that span only a few seconds and that are manually trimmed to contain simple atomic actions.While step annotations could enable the training of mod-els to recognize the individual steps of procedural activi-ties, existing large-scale datasets in this area do not include such segment labels due to the prohibitive cost of manu-ally annotating temporal boundaries in long videos. To ad-dress this issue, we propose to automatically identify steps in instructional videos by leveraging the distant supervi-sion of a textual knowledge base (wikiHow) that includes detailed descriptions of the steps needed for the execution of a wide variety of complex activities. Our method uses a language model to match noisy, automatically-transcribed speech from the video to step descriptions in the knowl-edge base. We demonstrate that video models trained to recognize these automatically-labeled steps (without man-ual supervision) yield a representation that achieves supe-rior generalization performance on four downstream tasks: recognition of procedural activities, step classification, step forecasting and egocentric video classification. 