Visual representations underlie object recognition tasks, but they often contain both robust and non-robust features.Our main observation is that image classifiers may perform poorly on out-of-distribution samples because spurious cor-relations between non-robust features and labels can be changed in a new environment. By analyzing procedures for out-of-distribution generalization with a causal graph, we show that standard classifiers fail because the association between images and labels is not transportable across set-tings. However, we then show that the causal effect, which severs all sources of confounding, remains invariant across domains. This motivates us to develop an algorithm to es-timate the causal effect for image classification, which is transportable (i.e., invariant) across source and target envi-ronments. Without observing additional variables, we show that we can derive an estimand for the causal effect under empirical assumptions using representations in deep mod-els as proxies. Theoretical analysis, empirical results, and visualizations show that our approach captures causal in-variances and improves overall generalization. 