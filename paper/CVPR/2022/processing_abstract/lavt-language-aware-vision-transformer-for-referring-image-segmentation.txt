Referring image segmentation is a fundamental vision-language task that aims to segment out an object referred to by a natural language expression from an image. One of the key challenges behind this task is leveraging the re-ferring expression for highlighting relevant positions in the image. A paradigm for tackling this problem is to leverage a powerful vision-language (“cross-modal”) decoder to fuse features independently extracted from a vision encoder and a language encoder. Recent methods have made remarkable advancements in this paradigm by exploiting Transformers as cross-modal decoders, concurrent to the Transformer’s overwhelming success in many other vision-language tasks.Adopting a different approach in this work, we show that significantly better cross-modal alignments can be achieved through the early fusion of linguistic and visual features in intermediate layers of a vision Transformer encoder net-work. By conducting cross-modal feature fusion in the visual feature encoding stage, we can leverage the well-proven correlation modeling power of a Transformer en-coder for excavating helpful multi-modal context. This way, accurate segmentation results are readily harvested with a light-weight mask predictor. Without bells and whistles, our method surpasses the previous state-of-the-art methods onRefCOCO, RefCOCO+, and G-Ref by large margins.Figure 1. The task of referring image segmentation takes one im-age and one text description as inputs, and predicts a mask de-lineating the object specified in the description. (a) The previ-ous state-of-the-art method (i.e., VLT [12]) leverages a vision-language Transformer decoder for cross-modal feature fusion. (b)Conversely, we propose to directly integrate linguistic informa-tion into visual features at intermediate levels of a vision Trans-former network, where beneficial vision-language cues are jointly exploited. A light-weight mask predictor can thus readily replace the complicated cross-modal decoder in previous counterparts. 