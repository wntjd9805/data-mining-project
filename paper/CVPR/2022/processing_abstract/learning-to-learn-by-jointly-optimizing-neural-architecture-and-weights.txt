Meta-learning enables models to adapt to new envi-ronments rapidly with a few training examples. Current gradient-based meta-learning methods concentrate on find-ing good model-agnostic initialization (meta-weights) for learners. In this paper, we aim to obtain better meta-learners by co-optimizing the architecture and meta-weights simulta-neously. Existing NAS-based meta-learning methods apply a two-stage strategy, i.e., first searching architectures and then re-training meta-weights on the searched architecture. How-ever, this two-stage strategy would break the mutual impact of the architecture and meta-weights since they are optimized separately. Differently, we propose progressive connection consolidation, fixing the architecture layer by layer, in which the layer with the largest weight value would be fixed first.In this way, we can jointly search architectures and train the meta-weights on fixed layers. Besides, to improve the gener-alization performance of the searched meta-learner on all tasks, we propose a more effective rule for co-optimization, namely Connection-Adaptive Meta-learning (CAML). By searching only once, we can obtain both adaptive archi-tecture and meta-weights for meta-learning. Extensive ex-periments show that our method achieves state-of-the-art performance with 3x less computational cost, revealing our methodâ€™s effectiveness and efficiency. 