The diversity and complexity of degradations in real-world video super-resolution (VSR) pose non-trivial chal-lenges in inference and training. First, while long-term propagation leads to improved performance in cases of mild degradations, severe in-the-wild degradations could be ex-aggerated through propagation, impairing output quality.To balance the tradeoff between detail synthesis and arti-fact suppression, we found an image pre-cleaning stage in-dispensable to reduce noises and artifacts prior to propa-gation. Equipped with a carefully designed cleaning mod-ule, our RealBasicVSR outperforms existing methods in both quality and efficiency (Fig. 1). Second, real-worldVSR models are often trained with diverse degradations to improve generalizability, requiring increased batch size to produce a stable gradient. Inevitably, the increased com-putational burden results in various problems, including 1) speed-performance tradeoff and 2) batch-length trade-off. To alleviate the first tradeoff, we propose a stochas-tic degradation scheme that reduces up to 40% of training time without sacrificing performance. We then analyze dif-ferent training settings and suggest that employing longer sequences rather than larger batches during training al-lows more effective uses of temporal information, leading to more stable performance during inference. To facilitate fair comparisons, we propose the new VideoLQ dataset, which contains a large variety of real-world low-quality video se-quences containing rich textures and patterns. Our dataset can serve as a common ground for benchmarking. Code, models, and the dataset are publicly available at https://github.com/ckkelvinchan/RealBasicVSR. 1