Profiting from the advance of deep convolutional net-works, current state-of-the-art video action recognition models have achieved remarkable progress. Neverthe-less, most of existing models suffer from low interpretabil-ity of the predicted actions.Inspired by the observation that temporally-configured human-object interactions often serve as a key indicator of many actions, this work crafts an action reasoning framework that performs Markov LogicNetwork (MLN) based probabilistic logical inference. Cru-cially, we propose to encode an action by first-order logical rules that correspond to the temporal changes of visual re-lationships in videos. The main contributions of this work are two-fold: 1) Different from existing black-box models, the proposed model simultaneously implements the local-ization of temporal boundaries and the recognition of ac-tion categories by grounding the logical rules of MLN in videos. The weight associated with each such rule further provides an estimate of confidence. These collectively make our model more explainable and robust. 2) Instead of us-ing hand-crafted logical rules in conventional MLN, we de-velop a data-driven instantiation of the MLN. In specific, a hybrid learning scheme is proposed. It combines MLN’s weight learning and reinforcement learning, using the for-mer’s results as a self-critic for guiding the latter’s train-ing. Additionally, by treating actions as logical predicates, the proposed framework can also be integrated with deep models for further performance boost. Comprehensive ex-periments on two complex video action datasets (Charades& CAD-120) clearly demonstrate the effectiveness and ex-plainability of our proposed method. 