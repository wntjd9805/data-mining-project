Effectiveness and interpretability are two essential prop-erties for trustworthy AI systems. Most recent studies in visual reasoning are dedicated to improving the accuracy of predicted answers, and less attention is paid to explain-ing the rationales behind the decisions. As a result, they commonly take advantage of spurious biases instead of ac-tually reasoning on the visual-textual data, and have yet developed the capability to explain their decision making by considering key information from both modalities. This paper aims to close the gap from three distinct perspec-tives: first, we define a new type of multi-modal explana-tions that explain the decisions by progressively traversing the reasoning process and grounding keywords in the im-ages. We develop a functional program to sequentially ex-ecute different reasoning steps and construct a new dataset with 1,040,830 multi-modal explanations. Second, we iden-tify the critical need to tightly couple important compo-nents across the visual and textual modalities for explain-ing the decisions, and propose a novel explanation genera-tion method that explicitly models the pairwise correspon-dence between words and regions of interest. It improves the visual grounding capability by a considerable margin, resulting in enhanced interpretability and reasoning perfor-mance. Finally, with our new data and method, we perform extensive analyses to study the effectiveness of our explana-tion under different settings, including multi-task learning and transfer learning. Our code and data are available at https://github.com/szzexpoi/rex. 