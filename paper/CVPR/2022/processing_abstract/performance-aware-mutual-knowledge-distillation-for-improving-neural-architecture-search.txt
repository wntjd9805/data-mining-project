Knowledge distillation has shown great effectiveness for improving neural architecture search (NAS). Mutual knowledge distillation (MKD), where a group of mod-els mutually generate knowledge to train each other, has achieved promising results in many applications. In existingMKD methods, mutual knowledge distillation is performed between models without scrutiny: a worse-performing model is allowed to generate knowledge to train a better-performing model, which may lead to collective failures.To address this problem, we propose a performance-awareMKD (PAMKD) approach for NAS, where knowledge gen-erated by model A is allowed to train model B only if the performance of A is better than B. We propose a three-level optimization framework to formulate PAMKD, where three learning stages are performed end-to-end: 1) each model trains an initial model independently; 2) the initial mod-els are evaluated on a validation set and better-performing models generate knowledge to train worse-performing mod-els; 3) architectures are updated by minimizing a validation loss. Experimental results on a variety of datasets demon-strate that our method is effective. 