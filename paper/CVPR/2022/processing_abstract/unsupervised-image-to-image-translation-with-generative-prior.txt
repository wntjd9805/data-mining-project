Unsupervised image-to-image translation aims to learn the translation between two visual domains without paired data. Despite the recent progress in image translation mod-els, it remains challenging to build mappings between com-plex domains with drastic visual discrepancies.In this work, we present a novel framework, Generative Prior-guided UNsupervised Image-to-image Translation (GP-UNIT), to improve the overall quality and applicability of the translation algorithm. Our key insight is to lever-age the generative prior from pre-trained class-conditionalGANs (e.g., BigGAN) to learn rich content correspondences across various domains. We propose a novel coarse-to-fine scheme: we first distill the generative prior to capture a ro-bust coarse-level content representation that can link ob-jects at an abstract semantic level, based on which fine-level content features are adaptively learned for more accu-rate multi-level content correspondences. Extensive exper-iments demonstrate the superiority of our versatile frame-work over state-of-the-art methods in robust, high-quality and diversified translations, even for challenging and dis-tant domains. Code is available at https://github. com/williamyang1991/GP-UNIT. 