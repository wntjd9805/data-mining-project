Driving 3D characters to dance following a piece of mu-sic is highly challenging due to the spatial constraints ap-plied to poses by choreography norms.In addition, the generated dance sequence also needs to maintain tempo-ral coherency with different music genres. To tackle these challenges, we propose a novel music-to-dance framework,Bailando, with two powerful components: 1) a choreo-graphic memory that learns to summarize meaningful danc-ing units from 3D pose sequence to a quantized code-book, 2) an actor-critic Generative Pre-trained Transformer (GPT) that composes these units to a ﬂuent dance coher-ent to the music. With the learned choreographic mem-ory, dance generation is realized on the quantized units that meet high choreography standards, such that the gen-erated dancing sequences are conﬁned within the spatial constraints. To achieve synchronized alignment between diverse motion tempos and music beats, we introduce an actor-critic-based reinforcement learning scheme to theGPT with a newly-designed beat-align reward function.Extensive experiments on the standard benchmark demon-strate that our proposed framework achieves state-of-the-art performance both qualitatively and quantitatively. No-tably, the learned choreographic memory is shown to dis-cover human-interpretable dancing-style poses in an unsu-pervised manner. Code and video demo are available at https://github.com/lisiyao21/Bailando/. (cid:66) Corresponding author 