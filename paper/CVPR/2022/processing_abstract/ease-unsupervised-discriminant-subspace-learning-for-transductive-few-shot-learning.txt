Few-shot learning (FSL) has received a lot of attention due to its remarkable ability to adapt to novel classes. Al-though many techniques have been proposed for FSL, they mostly focus on improving FSL backbones. Some works also focus on learning on top of the features generated by these backbones to adapt them to novel classes. We present an unsupErvised discriminAnt Subspace lEarning (EASE) that improves transductive few-shot learning per-formance by learning a linear projection onto a subspace built from features of the support set and the unlabeled query set in the test time. Speciﬁcally, based on the support set and the unlabeled query set, we generate the similar-ity matrix and the dissimilarity matrix based on the struc-ture prior for the proposed EASE method, which is efﬁ-ciently solved with SVD. We also introduce conStraIned wAsserstein MEan Shift clustEring (SIAMESE) which ex-tends Sinkhorn K-means by incorporating labeled support samples. SIAMESE works on the features obtained fromEASE to estimate class centers and query predictions. On the mini-ImageNet, tiered-ImageNet, CIFAR-FS, CUB andOpenMIC benchmarks, both steps signiﬁcantly boost the performance in transductive FSL and semi-supervised FSL. 