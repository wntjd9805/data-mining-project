We present TubeFormer-DeepLab, the ﬁrst attempt to tackle multiple core video segmentation tasks in a uniﬁed manner. Different video segmentation tasks (e.g., video se-mantic/instance/panoptic segmentation) are usually consid-ered as distinct problems. State-of-the-art models adopted in the separate communities have diverged, and radically different approaches dominate in each task. By contrast, we make a crucial observation that video segmentation tasks could be generally formulated as the problem of assign-ing different predicted labels to video tubes (where a tube is obtained by linking segmentation masks along the time axis) and the labels may encode different values depend-ing on the target task. The observation motivates us to de-velop TubeFormer-DeepLab, a simple and effective video mask transformer model that is widely applicable to mul-tiple video segmentation tasks. TubeFormer-DeepLab di-rectly predicts video tubes with task-speciﬁc labels (either pure semantic categories, or both semantic categories and instance identities), which not only signiﬁcantly simpliﬁes video segmentation models, but also advances state-of-the-art results on multiple video segmentation benchmarks. 