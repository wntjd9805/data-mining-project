Generating a new font library is a very labor-intensive and time-consuming job for glyph-rich scripts. Few-shot font generation is thus required, as it requires only a few glyph references without ﬁne-tuning during test. Existing methods follow the style-content disentanglement paradigm and expect novel fonts to be produced by combining the style codes of the reference glyphs and the content representa-tions of the source. However, these few-shot font genera-tion methods either fail to capture content-independent style representations, or employ localized component-wise style representations, which is insufﬁcient to model many Chi-nese font styles that involve hyper-component features such as inter-component spacing and “connected-stroke”. To re-solve these drawbacks and make the style representations more reliable, we propose a self-supervised cross-modality pre-training strategy and a cross-modality transformer-based encoder that is conditioned jointly on the glyph image*These authors contributed equally and should be considered co-ﬁrst authors.†Corresponding author. and the corresponding stroke labels. The cross-modality encoder is pre-trained in a self-supervised manner to al-low effective capture of cross- and intra-modality corre-lations, which facilitates the content-style disentanglement and modeling style representations of all scales (stroke-level, component-level and character-level).The pre-trained encoder is then applied to the downstream font gen-eration task without ﬁne-tuning. Experimental comparisons of our method with state-of-the-art methods demonstrate our method successfully transfers styles of all scales.In addition, it only requires one reference glyph and achieves the lowest rate of bad cases in the few-shot font generation task (28% lower than the second best). 