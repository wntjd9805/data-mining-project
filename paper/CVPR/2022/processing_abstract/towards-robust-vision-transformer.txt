Recent advances on Vision Transformer (ViT) and its im-proved variants have shown that self-attention-based net-works surpass traditional Convolutional Neural Networks (CNNs) in most vision tasks. However, existing ViTs fo-cus on the standard accuracy and computation cost, lack-ing the investigation of the intrinsic influence on model ro-bustness and generalization. In this work, we conduct sys-tematic evaluation on components of ViTs in terms of their impact on robustness to adversarial examples, common cor-ruptions and distribution shifts. We find some components can be harmful to robustness. By leveraging robust compo-nents as building blocks of ViTs, we propose Robust VisionTransformer (RVT), which is a new vision transformer and has superior performance with strong robustness. Inspired by the findings during the evaluation, we further propose two new plug-and-play techniques called position-aware at-tention scaling and patch-wise augmentation to augment our RVT, which we abbreviate as RVT∗. The experimen-tal results of RVT on ImageNet and six robustness bench-marks demonstrate its advanced robustness and general-ization ability compared with previous ViTs and state-of-the-art CNNs. Furthermore, RVT-S∗ achieves Top-1 rank on multiple robustness leaderboards including ImageNet-C,ImageNet-Sketch and ImageNet-R. 