Vision Transformers (ViT) serve as powerful vision mod-els. Unlike convolutional neural networks, which domi-nated vision research in previous years, vision transform-ers enjoy the ability to capture long-range dependencies in the data. Nonetheless, an integral part of any trans-former architecture, the self-attention mechanism, suffers from high latency and inefficient memory utilization, mak-ing it less suitable for high-resolution input images. To alle-viate these shortcomings, hierarchical vision models locally employ self-attention on non-interleaving windows. This relaxation reduces the complexity to be linear in the input size; however, it limits the cross-window interaction, hurt-In this paper, we propose a ing the model performance. new shift-invariant local attention layer, called query and attend (QnA), that aggregates the input locally in an over-lapping manner, much like convolutions. The key idea be-hind QnA is to introduce learned queries, which allow fast and efficient implementation. We verify the effectiveness of our layer by incorporating it into a hierarchical vision transformer model. We show improvements in speed and memory complexity while achieving comparable accuracy with state-of-the-art models. Finally, our layer scales espe-cially well with window size, requiring up to x10 less mem-ory while being up to x5 faster than existing methods. The code is publicly available at https://github.com/ moabarar/qna. 