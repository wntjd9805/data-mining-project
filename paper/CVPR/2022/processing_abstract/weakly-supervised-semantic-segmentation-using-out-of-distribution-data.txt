Weakly supervised semantic segmentation (WSSS) meth-ods are often built on pixel-level localization maps obtained from a classiﬁer. However, training on class labels only, classiﬁers suffer from the spurious correlation between fore-ground and background cues (e.g. train and rail), fundamen-tally bounding the performance of WSSS. There have been previous endeavors to address this issue with additional su-pervision. We propose a novel source of information to distin-guish foreground from the background: Out-of-Distribution (OoD) data, or images devoid of foreground object classes.In particular, we utilize the hard OoDs that the classiﬁer is likely to make false-positive predictions. These samples typi-cally carry key visual features on the background (e.g. rail) that the classiﬁers often confuse as foreground (e.g. train), so these cues let classiﬁers correctly suppress spurious back-ground cues. Acquiring such hard OoDs does not require an extensive amount of annotation efforts; it only incurs a few additional image-level labeling costs on top of the original efforts to collect class labels. We propose a method, W-OoD, for utilizing the hard OoDs. W-OoD achieves state-of-the-art performance on Pascal VOC 2012. The code is available at: https://github.com/naver-ai/w-ood. 