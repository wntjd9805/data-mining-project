Supervised deep learning methods require a large repos-itory of annotated data; hence, label noise is inevitable.Training with such noisy data negatively impacts the gener-alization performance of deep neural networks. To combat label noise, recent state-of-the-art methods employ some sort of sample selection mechanism to select a possibly clean subset of data. Next, an off-the-shelf semi-supervised learning method is used for training where rejected samples are treated as unlabeled data. Our comprehensive analysis shows that current selection methods disproportionately se-lect samples from easy (fast learnable) classes while reject-ing those from relatively harder ones. This creates class imbalance in the selected clean set and in turn, deterio-rates performance under high label noise.In this work, we propose UNICON, a simple yet effective sample selec-tion method which is robust to high label noise. To address the disproportionate selection of easy and hard samples, we introduce a Jensen-Shannon divergence based uniform se-lection mechanism which does not require any probabilistic modeling and hyperparameter tuning. We complement our selection method with contrastive learning to further com-bat the memorization of noisy labels. Extensive experimen-tation on multiple benchmark datasets demonstrates the ef-fectiveness of UNICON; we obtain an 11.4% improvement over the current state-of-the-art on CIFAR100 dataset with a 90% noise rate. Our code is publicly available.1 