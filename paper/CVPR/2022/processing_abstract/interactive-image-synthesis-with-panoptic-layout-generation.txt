Interactive image synthesis from user-guided input is a challenging task when users wish to control the scene struc-ture of a generated image with ease. Although remarkable progress has been made on layout-based image synthesis approaches, existing methods require high-precision inputs such as accurately placed bounding boxes, which might be constantly violated in an interactive setting. When place-ment of bounding boxes is subject to perturbation, layout-based models suffer from “missing regions” in the con-structed semantic layouts and hence undesirable artifacts in the generated images. In this work, we propose Panop-tic Layout Generative Adversarial Network (PLGAN) to ad-dress this challenge. The PLGAN employs panoptic the-ory which distinguishes object categories between “stuff” with amorphous boundaries and “things” with well-deﬁned shapes, such that stuff and instance layouts are constructed through separate branches and later fused into panoptic layouts. In particular, the stuff layouts can take amorphous shapes and ﬁll up the missing regions left out by the in-stance layouts. We experimentally compare our PLGAN with state-of-the-art layout-based models on the COCO-Stuff, Visual Genome, and Landscape datasets. The ad-vantages of PLGAN are not only visually demonstrated but quantitatively veriﬁed in terms of inception score, Fr´echet inception distance, classiﬁcation accuracy score, and cov-erage. The code is available at https://github.com/wb-ﬁnalking/PLGAN. 