Given weak supervision from image- or video-caption pairs, we address the problem of grounding (localizing) each object word of a ground-truth or generated sen-tence describing a visual input. Recent weakly-supervised approaches leverage region proposals and ground words based on the region attention coefﬁcients of captioning models. To predict each next word in the sentence they at-tend over regions using a summary of the previous words as a query, and then ground the word by selecting the most attended regions. However, this leads to sub-optimal grounding, since attention coefﬁcients are computed with-out taking into account the word that needs to be local-ized. To address this shortcoming, we propose a novelGrounded Visual Description Conditional Variational Au-toencoder (GVD-CVAE) and leverage its latent variables for grounding. In particular, we introduce a discrete ran-dom variable that models each word-to-region alignment, and learn its approximate posterior distribution given the full sentence. Experiments on challenging image and video datasets (Flickr30k Entities, YouCook2, ActivityNet Enti-ties) validate the effectiveness of our conditional generative model, showing that it can substantially outperform soft-attention-based baselines in grounding. 