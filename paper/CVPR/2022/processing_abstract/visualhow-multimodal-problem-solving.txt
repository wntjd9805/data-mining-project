Recent progress in the interdisciplinary studies of com-puter vision (CV) and natural language processing (NLP) has enabled the development of intelligent systems that can describe what they see and answer questions accordingly.However, despite showing usefulness in performing these vision-language tasks, existing methods still struggle in un-derstanding real-life problems (i.e., how to do something) and suggesting step-by-step guidance to solve them. With an overarching goal of developing intelligent systems to assist humans in various daily activities, we propose Vi-sualHow, a free-form and open-ended research that fo-cuses on understanding a real-life problem and deriving its solution by incorporating key components across mul-tiple modalities. We develop a new dataset with 20,028 real-life problems and 102,933 steps that constitute their solutions, where each step consists of both a visual illustra-tion and a textual description that guide the problem solv-ing. To establish better understanding of problems and so-lutions, we also provide annotations of multimodal attention that localizes important components across modalities and solution graphs that encapsulate different steps in struc-tured representations. These data and annotations enable a family of new vision-language tasks that solve real-life problems. Through extensive experiments with represen-tative models, we demonstrate their effectiveness on train-ing and testing models for the new tasks, and there is sig-nificant scope for improvement by learning effective atten-tion mechanisms. Our dataset and models are available at https://github.com/formidify/VisualHow. 