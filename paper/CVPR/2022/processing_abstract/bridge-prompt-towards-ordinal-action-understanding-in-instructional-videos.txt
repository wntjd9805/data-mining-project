Action recognition models have shown a promising ca-pability to classify human actions in short video clips. In a real scenario, multiple correlated human actions commonly occur in particular orders, forming semantically meaning-ful human activities. Conventional action recognition ap-proaches focus on analyzing single actions. However, they fail to fully reason about the contextual relations between adjacent actions, which provide potential temporal logic for understanding long videos. In this paper, we propose a prompt-based framework, Bridge-Prompt (Br-Prompt), to model the semantics across adjacent actions, so that it si-multaneously exploits both out-of-context and contextual information from a series of ordinal actions in instruc-tional videos. More specifically, we reformulate the indi-vidual action labels as integrated text prompts for super-vision, which bridge the gap between individual action se-mantics. The generated text prompts are paired with cor-responding video clips, and together co-train the text en-coder and the video encoder via a contrastive approach.The learned vision encoder has a stronger capability for ordinal-action-related downstream tasks, e.g. action seg-mentation and human activity recognition. We evaluate the performances of our approach on several video datasets:Georgia Tech Egocentric Activities (GTEA), 50Salads, and the Breakfast dataset. Br-Prompt achieves state-of-the-art on multiple benchmarks. Code is available at: https://github.com/ttlmh/Bridge-Prompt. 