Differentiable rendering is an essential operation in modern vision, allowing inverse graphics approaches to 3D understanding to be utilized in modern machine learning frameworks. Explicit shape representations (voxels, point clouds, or meshes), while relatively easily rendered, often suffer from limited geometric ﬁdelity or topological con-straints. On the other hand, implicit representations (occu-pancy, distance, or radiance ﬁelds) preserve greater ﬁdelity, but suffer from complex or inefﬁcient rendering processes, limiting scalability. In this work, we endeavour to address both shortcomings with a novel shape representation that allows fast differentiable rendering within an implicit ar-chitecture. Building on implicit distance representations, we deﬁne Directed Distance Fields (DDFs), which map an oriented point (position and direction) to surface visibility and depth. Such a ﬁeld can render a depth map with a single forward pass per pixel, enable differential surface geometry extraction (e.g., surface normals and curvatures) via net-work derivatives, be easily composed, and permit extraction of classical unsigned distance ﬁelds. Using probabilisticDDFs (PDDFs), we show how to model inherent disconti-nuities in the underlying ﬁeld. Finally, we apply our method to ﬁtting single shapes, unpaired 3D-aware generative im-age modelling, and single-image 3D reconstruction tasks, showcasing strong performance with simple architectural components via the versatility of our representation. 