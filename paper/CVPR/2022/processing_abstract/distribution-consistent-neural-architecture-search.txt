Recent progress on neural architecture search (NAS) has demonstrated exciting results on automating deep network architecture designs. In order to overcome the unafford-able complexity of training each candidate architecture from scratch, the state-of-the-art one-shot NAS approaches adopt a weight-sharing strategy to improve training efficiency. Al-though the computational cost is greatly reduced, such one-shot process introduces a severe weight coupling problem that largely degrades the evaluation accuracy of each can-didate. The existing approaches often address the problem by shrinking the search space, model distillation, or few-shot training. Instead, in this paper, we propose a novel distribution consistent one-shot neural architecture search algorithm. We first theoretically investigate how the weight coupling problem affects the network searching performance from a parameter distribution perspective, and then pro-pose a novel supernet training strategy with a DistributionConsistent Constraint that can provide a good measurement for the extent to which two architectures can share weights.Our strategy optimizes the supernet through iteratively in-ferring network weights and corresponding local sharing states. Such joint optimization of supernetâ€™s weights and topologies can diminish the discrepancy between the weights inherited from the supernet and the ones that are trained with a stand-alone model. As a result, it enables a more accu-rate model evaluation phase and leads to a better searching performance. We conduct extensive experiments on bench-mark datasets with multiple searching spaces. The resulting architecture achieves superior performance over the cur-rent state-of-the-art NAS algorithms with comparable search costs, which demonstrates the efficacy of our approach. 