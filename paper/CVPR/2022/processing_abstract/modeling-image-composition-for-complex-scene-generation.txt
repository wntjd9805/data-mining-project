We present a method that achieves state-of-the-art re-sults on challenging (few-shot) layout-to-image genera-tion tasks by accurately modeling textures, structures and relationships contained in a complex scene. After compressing RGB images into patch tokens, we propose the Transformer with Focal Attention (TwFA) for explor-ing dependencies of object-to-object, object-to-patch and patch-to-patch. Compared to existing CNN-based andTransformer-based generation models that entangled mod-eling on pixel-level&patch-level and object-level&patch-level respectively, the proposed focal attention predicts the current patch token by only focusing on its highly-related tokens that speciﬁed by the spatial layout, thereby achieving disambiguation during training. Furthermore, the proposedTwFA largely increases the data efﬁciency during training, therefore we propose the ﬁrst few-shot complex scene gener-ation strategy based on the well-trained TwFA. Comprehen-sive experiments show the superiority of our method, which signiﬁcantly increases both quantitative metrics and qual-itative visual realism with respect to state-of-the-art CNN-based and transformer-based methods. Code is available at https://github.com/JohnDreamer/TwFA. 