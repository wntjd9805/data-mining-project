Unbiased SGG has achieved signiﬁcant progress over recent years. However, almost all existing SGG models have overlooked the ground-truth annotation qualities of prevail-ing SGG datasets, i.e., they always assume: 1) all the man-ually annotated positive samples are equally correct; 2) all the un-annotated negative samples are absolutely back-ground. In this paper, we argue that both assumptions are inapplicable to SGG: there are numerous “noisy” ground-truth predicate labels that break these two assumptions, and these noisy samples actually harm the training of unbi-ased SGG models. To this end, we propose a novel model-agnostic NoIsy label CorrEction strategy for SGG: NICE.NICE can not only detect noisy samples but also reassign more high-quality predicate labels to them. After the NICE training, we can obtain a cleaner version of SGG dataset for model training. Speciﬁcally, NICE consists of three compo-nents: negative Noisy Sample Detection (Neg-NSD), posi-tive NSD (Pos-NSD), and Noisy Sample Correction (NSC).Firstly, in Neg-NSD, we formulate this task as an out-of-distribution detection problem, and assign pseudo labels to all detected noisy negative samples. Then, in Pos-NSD, we use a clustering-based algorithm to divide all positive sam-ples into multiple sets, and treat the samples in the noisiest set as noisy positive samples. Lastly, in NSC, we use a sim-ple but effective weighted KNN to reassign new predicate labels to noisy positive samples. Extensive results on differ-ent backbones and tasks have attested to the effectiveness and generalization abilities of each component of NICE. 