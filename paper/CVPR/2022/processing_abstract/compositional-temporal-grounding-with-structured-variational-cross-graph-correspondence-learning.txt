Temporal grounding in videos aims to localize one tar-get video segment that semantically corresponds to a given query sentence. Thanks to the semantic diversity of natural language descriptions, temporal grounding allows activ-ity grounding beyond pre-defined classes and has received increasing attention in recent years. The semantic diver-sity is rooted in the principle of compositionality in lin-guistics, where novel semantics can be systematically de-scribed by combining known words in novel ways (composi-tional generalization). However, current temporal ground-ing datasets do not specifically test for the compositional generalizability. To systematically measure the composi-tional generalizability of temporal grounding models, we introduce a new Compositional Temporal Grounding task and construct two new dataset splits, i.e., Charades-CG and ActivityNet-CG. Evaluating the state-of-the-art meth-ods on our new dataset splits, we empirically find that they fail to generalize to queries with novel combinations of seen words. To tackle this challenge, we propose a varia-tional cross-graph reasoning framework that explicitly de-composes video and language into multiple structured hi-erarchies and learns fine-grained semantic correspondence among them. Experiments illustrate the superior compo-sitional generalizability of our approach. The repository of this work is at https://github.com/YYJMJC/Compositional-Temporal-Grounding. 