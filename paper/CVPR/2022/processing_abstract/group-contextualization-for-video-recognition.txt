Learning discriminative representation from the complex spatio-temporal dynamic space is essential for video recog-nition. On top of those stylized spatio-temporal compu-tational units, further refining the learnt feature with ax-ial contexts is demonstrated to be promising in achieving this goal. However, previous works generally focus on uti-lizing a single kind of contexts to calibrate entire feature channels and could hardly apply to deal with diverse video activities. The problem can be tackled by using pair-wise spatio-temporal attentions to recompute feature response with cross-axis contexts at the expense of heavy computa-tions. In this paper, we propose an efficient feature refine-ment method that decomposes the feature channels into sev-eral groups and separately refines them with different axial contexts in parallel. We refer this lightweight feature cal-ibration as group contextualization (GC). Specifically, we design a family of efficient element-wise calibrators, i.e.,ECal-G/S/T/L, where their axial contexts are information dynamics aggregated from other axes either globally or lo-cally, to contextualize feature channel groups. The GC mod-ule can be densely plugged into each residual layer of the off-the-shelf video networks. With little computational over-head, consistent improvement is observed when plugging inGC on different networks. By utilizing calibrators to em-bed feature with four different kinds of contexts in parallel, the learnt representation is expected to be more resilient to diverse types of activities. On videos with rich tempo-ral variations, empirically GC can boost the performance of 2D-CNN (e.g., TSN and TSM) to a level comparable to the state-of-the-art video networks. Code is available at https://github.com/haoyanbin918/Group-Contextualization.*Hao Zhang is the corresponding author.Figure 1. Perspective/axial preference of different video activities.The scene change caused by quick camera movement yearns for global context for recognizing the soccer highlight “Corner kick”.“Arm wrestling”, “Bee keeping” and “Ice skating” can be easily recognized even by a single keyframe. Whereas, the Something-Something activity examples (middle) rely much on temporal re-lations. The group activities, i.e., “Blocked shot” and “Layup”, require a model to localize sub-activities. 