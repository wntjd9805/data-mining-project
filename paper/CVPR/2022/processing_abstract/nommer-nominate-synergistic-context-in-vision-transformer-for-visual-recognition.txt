Recently, Vision Transformers (ViT), with the self-attention (SA) as the de facto ingredients, have demon-strated great potential in the computer vision community.For the sake of trade-off between efficiency and perfor-mance, a group of works merely perform SA operation within local patches, whereas the global contextual infor-mation is abandoned, which would be indispensable for visual recognition tasks. To solve the issue, the subse-quent global-local ViTs take a stab at marrying local SA with global one in parallel or alternative way in the model.Nevertheless, the exhaustively combined local and global context may exist redundancy for various visual data, and the receptive field within each layer is fixed. Alternatively, a more graceful way is that global and local context can adaptively contribute per se to accommodate different vi-sual data. To achieve this goal, we in this paper propose a novel ViT architecture, termed NomMer, which can dy-namically Nominate the synergistic global-local context in vision transforMer. By investigating the working pattern ofNomMer, we further explore what context information is fo-cused. Beneficial from this “dynamic nomination” mech-anism, without bells and whistles, the NomMer can not only achieve 84.5% Top-1 classification accuracy on Im-ageNet with only 73M parameters, but also show promis-ing performance on dense prediction tasks, i.e., object de-tection and semantic segmentation. The code and mod-els are publicly available at https://github.com/TencentYoutuResearch/VisualRecognition-NomMer. 