Audiovisual Event (AVE) localization requires the model to jointly localize an event by observing audio and visual information. However, in unconstrained videos, both in-formation types may be inconsistent or suffer from severe background noise. Hence this paper proposes a novel cross-modal background suppression network for AVE task, op-erating at the time- and event-level, aiming to improve lo-calization performance through suppressing asynchronous audiovisual background frames from the examined events and reducing redundant noise. Specifically, the time-level background suppression scheme forces the audio and visual modality to focus on the related information in the temporal dimension that the opposite modality considers essential, and reduces attention to the segments that the other modal considers as background. The event-level background sup-pression scheme uses the class activation sequences pre-dicted by audio and visual modalities to control the fi-nal event category prediction, which can effectively sup-press noise events occurring accidentally in a single modal-ity. Furthermore, we introduce a cross-modal gated atten-tion scheme to extract relevant visual regions from complex scenes exploiting both global visual and audio signals. Ex-tensive experiments show our method outperforms the state-of-the-art methods by a large margin in both supervised and weakly supervised AVE settings. 1 