The key to address clothes-changing person re-identiÔ¨Åcation (re-id) is to extract clothes-irrelevant features, e.g., face, hairstyle, body shape, and gait. Most current works mainly focus on modeling body shape from multi-modality information (e.g., silhouettes and sketches), but do not make full use of the clothes-irrelevant informationIn this paper, we propose in the original RGB images. a Clothes-based Adversarial Loss (CAL) to mine clothes-irrelevant features from the original RGB images by pe-nalizing the predictive power of re-id model w.r.t. clothes.Extensive experiments demonstrate that using RGB im-ages only, CAL outperforms all state-of-the-art methods on widely-used clothes-changing person re-id benchmarks.Besides, compared with images, videos contain richer ap-pearance and additional temporal information, which can be used to model proper spatiotemporal patterns to assist clothes-changing re-id. Since there is no publicly avail-able clothes-changing video re-id dataset, we contribute a new dataset named CCVID and show that there exists much room for improvement in modeling spatiotemporal informa-tion. The code and new dataset are available at: https://github.com/guxinqian/Simple-CCReID. 