The contextual information plays a core role in seman-tic segmentation. As for video semantic segmentation, the contexts include static contexts and motional contexts, corresponding to static content and moving content in a video clip, respectively. The static contexts are well ex-ploited in image semantic segmentation by learning multi-scale and global/long-range features. The motional con-texts are studied in previous video semantic segmentation.However, there is no research about how to simultane-ously learn static and motional contexts which are highly correlated and complementary to each other. To address this problem, we propose a Coarse-to-Fine Feature Min-ing (CFFM) technique to learn a uniﬁed presentation of static contexts and motional contexts. This technique con-sists of two parts: coarse-to-ﬁne feature assembling and cross-frame feature mining. The former operation prepares data for further processing, enabling the subsequent joint learning of static and motional contexts. The latter opera-tion mines useful information/contexts from the sequential frames to enhance the video contexts of the features of the target frame. The enhanced features can be directly ap-plied for the ﬁnal prediction. Experimental results on popu-lar benchmarks demonstrate that the proposed CFFM per-forms favorably against state-of-the-art methods for video semantic segmentation. Our implementation is available at https://github.com/GuoleiSun/VSS-CFFM. 