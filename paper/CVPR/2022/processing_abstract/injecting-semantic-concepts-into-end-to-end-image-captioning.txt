Tremendous progresses have been made in recent years in developing better image captioning models, yet most of them rely on a separate object detector to extract regional features. Recent vision-language studies are shifting towards the detector-free trend by leveraging grid representations for more ﬂexible model training and faster inference speed.However, such development is primarily focused on image understanding tasks, and remains less investigated for the caption generation task. In this paper, we are concerned with a better-performing detector-free image captioning model, and propose a pure vision transformer-based image caption-ing model, dubbed as ViTCAP, in which grid representations are used without extracting the regional features. For im-proved performance, we introduce a novel Concept TokenNetwork (CTN) to predict the semantic concepts and then incorporate them into the end-to-end captioning. In particu-lar, the CTN is built on the basis of a vision transformer, and is designed to predict the concept tokens through a classiﬁ-cation task, from which the rich semantic information con-tained greatly beneﬁts the captioning task. Compared with the previous detector-based models, ViTCAP drastically sim-pliﬁes the architectures and at the same time achieves com-petitive performance on various challenging image caption-ing datasets. In particular, ViTCAP reaches 138.1 CIDEr scores on COCO-caption Karpathy-split, 93.8 and 108.6CIDEr scores on nocaps and Google-CC captioning datasets, respectively. 