The vision community is witnessing a modeling shift fromCNNs to Transformers, where pure Transformer architec-tures have attained top accuracy on the major video recog-nition benchmarks. These video models are all built onTransformer layers that globally connect patches across the spatial and temporal dimensions. In this paper, we instead advocate an inductive bias of locality in video Transform-ers, which leads to a better speed-accuracy trade-off com-pared to previous approaches which compute self-attention globally even with spatial-temporal factorization. The lo-cality of the proposed video architecture is realized by adapting the Swin Transformer designed for the image do-main, while continuing to leverage the power of pre-trained image models. Our approach achieves state-of-the-art ac-curacy on a broad range of video recognition benchmarks, including on action recognition (84.9 top-1 accuracy onKinetics-400 and 85.9 top-1 accuracy on Kinetics-600 with∼20× less pre-training data and ∼3× smaller model size) and temporal modeling (69.6 top-1 accuracy on Something-Something v2). 