Video frame interpolation (VFI) is currently a very ac-tive research topic, with applications spanning computer vision, post production and video encoding. VFI can be extremely challenging, particularly in sequences contain-ing large motions, occlusions or dynamic textures, where existing approaches fail to offer perceptually robust inter-polation performance. In this context, we present a novel deep learning based VFI method, ST-MFNet, based on aSpatio-Temporal Multi-Flow architecture. ST-MFNet em-ploys a new multi-scale multi-ﬂow predictor to estimate many-to-one intermediate ﬂows, which are combined with conventional one-to-one optical ﬂows to capture both large and complex motions.In order to enhance interpolation performance for various textures, a 3D CNN is also em-ployed to model the content dynamics over an extended temporal window. Moreover, ST-MFNet has been trained within an ST-GAN framework, which was originally devel-oped for texture synthesis, with the aim of further improving perceptual interpolation quality. Our approach has been comprehensively evaluated – compared with fourteen state-of-the-art VFI algorithms – clearly demonstrating that ST-MFNet consistently outperforms these benchmarks on var-ied and representative test datasets, with signiﬁcant gains up to 1.09dB in PSNR for cases including large motions and dynamic textures. Our source code is available at https://github.com/danielism97/ST-MFNet. 