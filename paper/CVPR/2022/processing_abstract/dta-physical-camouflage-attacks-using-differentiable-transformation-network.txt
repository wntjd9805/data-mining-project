To perform adversarial attacks in the physical world, many studies have proposed adversarial camouflage, a method to hide a target object by applying camouflage pat-terns on 3D object surfaces. For obtaining optimal physi-cal adversarial camouflage, previous studies have utilized the so-called neural renderer, as it supports differentiabil-ity. However, existing neural renderers cannot fully rep-resent various real-world transformations due to a lack of control of scene parameters compared to the legacy photo-realistic renderers. In this paper, we propose the Differen-tiable Transformation Attack (DTA), a framework for gen-erating a robust physical adversarial pattern on a target ob-ject to camouflage it against object detection models with a wide range of transformations.It utilizes our novel Dif-ferentiable Transformation Network (DTN), which learns the expected transformation of a rendered object when the texture is changed while preserving the original properties of the target object. Using our attack framework, an ad-versary can gain both the advantages of the legacy photo-realistic renderers including various physical-world trans-formations and the benefit of white-box access by offering differentiability. Our experiments show that our camou-flaged 3D vehicles can successfully evade state-of-the-art object detection models in the photo-realistic environment (i.e., CARLA on Unreal Engine). Furthermore, our demon-stration on a scaled Tesla Model 3 proves the applicability and transferability of our method to the real world. 