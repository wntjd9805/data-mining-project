Existing text-to-image synthesis methods generally are only applicable to words in the training dataset. However, human faces are so variable to be described with limited words. So this paper proposes the ﬁrst free-style text-to-face method namely AnyFace enabling much wider open world applications such as metaverse, social media, cos-metics, forensics, etc. AnyFace has a novel two-stream framework for face image synthesis and manipulation given arbitrary descriptions of the human face. Speciﬁcally, one stream performs text-to-face generation and the other con-ducts face image reconstruction. Facial text and image fea-tures are extracted using the CLIP (Contrastive Language-Image Pre-training) encoders. And a collaborative CrossModal Distillation (CMD) module is designed to align the linguistic and visual features across these two streams. Fur-thermore, a Diverse Triplet Loss (DT loss) is developed to model ﬁne-grained features and improve facial diver-sity. Extensive experiments on Multi-modal CelebA-HQ and*Equal contribution†Corresponding authorCelebAText-HQ demonstrate signiﬁcant advantages of Any-Face over state-of-the-art methods. AnyFace can achieve high-quality, high-resolution, and high-diversity face syn-thesis and manipulation results without any constraints on the number and content of input captions. 