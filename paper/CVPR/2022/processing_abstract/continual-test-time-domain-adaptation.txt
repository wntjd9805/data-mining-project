Test-time domain adaptation aims to adapt a source pre-trained model to a target domain without using any source data. Existing works mainly consider the case where the target domain is static. However, real-world machine per-ception systems are running in non-stationary and contin-ually changing environments where the target domain dis-tribution can change over time. Existing methods, which are mostly based on self-training and entropy regulariza-tion, can suffer from these non-stationary environments.Due to the distribution shift over time in the target do-main, pseudo-labels become unreliable. The noisy pseudo-labels can further lead to error accumulation and catas-trophic forgetting. To tackle these issues, we propose a con-tinual test-time adaptation approach (CoTTA) which com-prises two parts. Firstly, we propose to reduce the error accumulation by using weight-averaged and augmentation-averaged predictions which are often more accurate. On the other hand, to avoid catastrophic forgetting, we pro-pose to stochastically restore a small part of the neurons to the source pre-trained weights during each iteration to help preserve source knowledge in the long-term. The proposed method enables the long-term adaptation for all parame-ters in the network. CoTTA is easy to implement and can be readily incorporated in off-the-shelf pre-trained models. We demonstrate the effectiveness of our approach on four clas-sification tasks and a segmentation task for continual test-time adaptation, on which we outperform existing methods.Our code is available at https://qin.ee/cotta. 