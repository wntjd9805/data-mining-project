Capturing and rendering life-like hair is particularly challenging due to its ﬁne geometric structure, the complex physical interaction and its non-trivial visual appearance.Yet, hair is a critical component for believable avatars. In this paper, we address the aforementioned problems: 1) we use a novel, volumetric hair representation that is com-posed of thousands of primitives. Each primitive can be rendered efﬁciently, yet realistically, by building on the lat-est advances in neural rendering. 2) To have a reliable con-trol signal, we present a novel way of tracking hair on the strand level. To keep the computational effort manageable, we use guide hairs and classic techniques to expand those into a dense hood of hair. 3) To better enforce temporal consistency and generalization ability of our model, we fur-ther optimize the 3D scene ﬂow of our representation with multiview optical ﬂow, using volumetric raymarching. Our method can not only create realistic renders of recorded multi-view sequences, but also create renderings for new hair conﬁgurations by providing new control signals. We compare our method with existing work on viewpoint syn-thesis and drivable animation and achieve state-of-the-art results. https://ziyanw1.github.io/hvh/ 