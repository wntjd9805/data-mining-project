It is well known that Winograd convolution algorithms speed up the widely used small-size convolutions. How-ever, the problem of quantization of Winograd convolutions is challenging – while quantization of slower Winograd al-gorithms does not cause problems, quantization of fasterWinograd algorithms often leads to a significant drop in the quality of models. We introduce a novel class of Winograd algorithms that balances the filter and input channels in theWinograd domain. Unlike traditional Winograd convolu-tions, the proposed convolution balances the ranges of in-put channels on the forward pass by scaling the input ten-sor using special balancing coefficients (the filter channels are balanced offline). As a result of balancing, the inputs and filters of the Winograd convolution are much easier to quantize. Thus, the proposed technique allows us to obtain models with quantized Winograd convolutions, the quality of which is significantly higher than the quality of models with traditional quantized Winograd convolutions. More-over, we propose a special direct algorithm for calculat-ing the balancing coefficients, which does not require ad-ditional model training. This algorithm makes it easy to obtain the post-training quantized balanced Winograd con-volutions – one should just feed a few data samples to the model without training to calibrate special parameters. In addition, it is possible to initialize the balancing coefficients using this algorithm and further train them as trainable variables during Winograd quantization-aware training for greater quality improvement. 