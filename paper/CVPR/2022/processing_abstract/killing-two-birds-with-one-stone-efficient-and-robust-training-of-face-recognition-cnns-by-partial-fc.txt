Learning discriminative deep feature embeddings by us-ing million-scale in-the-wild datasets and margin-based softmax loss is the current state-of-the-art approach for face recognition. However, the memory and computing cost of the Fully Connected (FC) layer linearly scales up to the number of identities in the training set. Besides, the large-scale training data inevitably suffers from inter-class con-flict and long-tailed distribution.In this paper, we pro-pose a sparsely updating variant of the FC layer, namedPartial FC (PFC). In each iteration, positive class centers and a random subset of negative class centers are selected to compute the margin-based softmax loss. All class cen-ters are still maintained throughout the whole training pro-cess, but only a subset is selected and updated in each it-eration. Therefore, the computing requirement, the prob-ability of inter-class conflict, and the frequency of passive update on tail class centers, are dramatically reduced. Ex-tensive experiments across different training data and back-bones (e.g. CNN and ViT) confirm the effectiveness, robust-ness and efficiency of the proposed PFC. The source code is available at https://github.com/deepinsight/ insightface/tree/master/recognition. 