Hierarchical multi-granularity classification (HMC) as-signs hierarchical multi-granularity labels to each object and focuses on encoding the label hierarchy, e.g., [“Al-batross”, “Laysan Albatross”] from coarse-to-fine levels.However, the definition of what is fine-grained is subjec-tive, and the image quality may affect the identification.Thus, samples could be observed at any level of the hier-archy, e.g., [“Albatross”] or [“Albatross”, “Laysan Alba-tross”], and examples discerned at coarse categories are often neglected in the conventional setting of HMC. In this paper, we study the HMC problem in which objects are la-beled at any level of the hierarchy. The essential designs of the proposed method are derived from two motivations: (1) learning with objects labeled at various levels should trans-fer hierarchical knowledge between levels; (2) lower-level classes should inherit attributes related to upper-level su-perclasses. The proposed combinatorial loss maximizes the marginal probability of the observed ground truth label by aggregating information from related labels defined in the tree hierarchy. If the observed label is at the leaf level, the combinatorial loss further imposes the multi-class cross-entropy loss to increase the weight of fine-grained classi-fication loss. Considering the hierarchical feature interac-tion, we propose a hierarchical residual network (HRN), in which granularity-specific features from parent levels act-ing as residual connections are added to features of chil-dren levels. Experiments on three commonly used datasets demonstrate the effectiveness of our approach compared to the state-of-the-art HMC approaches. The code will be available at https://github.com/MonsterZhZh/HRN. 