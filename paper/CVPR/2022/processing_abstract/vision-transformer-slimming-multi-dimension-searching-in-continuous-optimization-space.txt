This paper explores the feasibility of ﬁnding an opti-mal sub-model from a vision transformer and introduces a pure vision transformer slimming (ViT-Slim) framework.It can search a sub-structure from the original model end-to-end across multiple dimensions, including the input to-kens, MHSA and MLP modules with state-of-the-art perfor-mance. Our method is based on a learnable and uniﬁed (cid:96)1 sparsity constraint with pre-deﬁned factors to reﬂect the global importance in the continuous searching space of dif-ferent dimensions. The searching process is highly efﬁcient through a single-shot training scheme. For instance, onDeiT-S, ViT-Slim only takes ∼43 GPU hours for the search-ing process, and the searched structure is ﬂexible with di-verse dimensionalities in different modules. Then, a bud-get threshold is employed according to the requirements of accuracy-FLOPs trade-off on running devices, and a re-training process is performed to obtain the ﬁnal model. The extensive experiments show that our ViT-Slim can compress up to 40% of parameters and 40% FLOPs on various vi-sion transformers while increasing the accuracy by ∼0.6% on ImageNet. We also demonstrate the advantage of our searched models on several downstream datasets. Our code is available at https://github.com/Arnav0400/ViT-Slim. 