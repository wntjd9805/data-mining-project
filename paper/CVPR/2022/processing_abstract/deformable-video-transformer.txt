Video transformers have recently emerged as an effec-tive alternative to convolutional networks for action clas-sification. However, most prior video transformers adopt either global space-time attention or hand-defined strate-gies to compare patches within and across frames. These fixed attention schemes not only have high computational cost but, by comparing patches at predetermined loca-tions, they neglect the motion dynamics in the video.In this paper, we introduce the Deformable Video Transformer (DVT), which dynamically predicts a small subset of video patches to attend for each query location based on mo-tion information, thus allowing the model to decide where to look in the video based on correspondences across frames. Crucially, these motion-based correspondences are obtained at zero-cost from information stored in the com-pressed format of the video. Our deformable attention mechanism is optimized directly with respect to classifica-tion performance, thus eliminating the need for suboptimal hand-design of attention strategies. Experiments on four large-scale video benchmarks (Kinetics-400, Something-Something-V2, EPIC-KITCHENS and Diving-48) demon-strate that, compared to existing video transformers, our model achieves higher accuracy at the same or lower com-putational cost, and it attains state-of-the-art results on these four datasets. 