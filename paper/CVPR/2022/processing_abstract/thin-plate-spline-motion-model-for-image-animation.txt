Image animation brings life to the static object in the source image according to the driving video.Recent works attempt to perform motion transfer on arbitrary ob-jects through unsupervised methods without using a priori knowledge. However, it remains a significant challenge for current unsupervised methods when there is a large pose gap between the objects in the source and driving images.In this paper, a new end-to-end unsupervised motion trans-fer framework is proposed to overcome such issues. Firstly, we propose thin-plate spline motion estimation to produce a more flexible optical flow, which warps the feature maps of the source image to the feature domain of the driving image. Secondly, in order to restore the missing regions more realistically, we leverage multi-resolution occlusion masks to achieve more effective feature fusion. Finally, ad-ditional auxiliary loss functions are designed to ensure that there is a clear division of labor in the network modules, encouraging the network to generate high-quality images.Our method1 can animate a variety of objects, including talking faces, human bodies, and pixel animations. Experi-ments demonstrate that our method performs better on most benchmarks than the state of the art with visible improve-ments in motion-related metrics. 