Despite plenty of efforts focusing on improving the do-main adaptation ability (DA) under unsupervised or few-shot semi-supervised settings, recently the solution of ac-tive learning started to attract more attention due to its suitability in transferring model in a more practical way with limited annotation resource on target data. Neverthe-less, most active learning methods are not inherently de-signed to handle domain gap between data distribution, on the other hand, some active domain adaptation meth-ods (ADA) usually requires complicated query functions, which is vulnerable to overfitting.In this work, we pro-pose a concise but effective ADA method called Select-by-Distinctive-Margin (SDM), which consists of a maximum margin loss and a margin sampling algorithm for data se-lection. We provide theoretical analysis to show that SDM works like a Support Vector Machine, storing hard exam-ples around decision boundaries and exploiting them to find informative and transferable data.In addition, we propose two variants of our method, one is designed to adaptively adjust the gradient from margin loss, the other boosts the selectivity of margin sampling by taking the gra-dient direction into account. We benchmark SDM with standard active learning setting, demonstrating our algo-rithm achieves competitive results with good data scala-bility. Code is available at https://github.com/TencentYoutuResearch/ActiveLearning-SDM 