As RGB-D sensors become more affordable, using RGB-D images to obtain high-accuracy 6D pose estimation re-sults becomes a better option. State-of-the-art approaches typically use different backbones to extract features for RGB and depth images. They use a 2D CNN for RGB images and a per-pixel point cloud network for depth data, as well as a fusion network for feature fusion. We find that the essential reason for using two independent backbones is the “projec-tion breakdown” problem. In the depth image plane, the projected 3D structure of the physical world is preserved by the 1D depth value and its built-in 2D pixel coordinate (UV). Any spatial transformation that modifies UV, such as resize, flip, crop, or pooling operations in the CNN pipeline, breaks the binding between the pixel value and UV coordi-nate. As a consequence, the 3D structure is no longer pre-served by a modified depth image or feature. To address this issue, we propose a simple yet effective method denoted as Uni6D that explicitly takes the extra UV data along withRGB-D images as input. Our method has a Unified CNN framework for 6D pose estimation with a single CNN back-bone. In particular, the architecture of our method is based on Mask R-CNN with two extra heads, one named RT head for directly predicting 6D pose and the other named abc head for guiding the network to map the visible points to their coordinates in the 3D model as an auxiliary module.This end-to-end approach balances simplicity and accu-racy, achieving comparable accuracy with state of the arts and 7.2× faster inference speed on the YCB-Video dataset. (a) Projection breakdown caused by the RoI transformation, including crop, resize and RoI-Align [8]: The red dotted line connects a 3D object and its projected RoI in the depth image. Any pixel (d, u, v) in the RoI and its corresponding point (a, b, c) on the 3D object follow the projection equation. The equation no longer holds if the built-in coordinate (UV) is modified by RoI-Align, as though the RoI was moved to the top left corner of the image. (b) Introducing UV data fixes the problem of projection breakdown and improve the 6D pose estimation performance with various spatial transfor-mations. x axis is the training epoch, and y axis is the testing accuracy.Figure 1. Visualization and experiment results of projection break-down problem. 