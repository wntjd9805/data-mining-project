Training quantised neural networks (QNNs) is a non-differentiable optimisation problem since weights and fea-tures are output by piecewise constant functions. The stan-dard solution is to apply the straight-through estimator (STE), using different functions during the inference and gradient computation steps. Several STE variants have been proposed in the literature aiming to maximise the task accuracy of the trained network.In this paper, we anal-yse STE variants and study their impact on QNN training.We ﬁrst observe that most such variants can be modelled as stochastic regularisations of stair functions; although this intuitive interpretation is not new, our rigorous dis-cussion generalises to further variants. Then, we analyseQNNs mixing different regularisations, ﬁnding that some suitably synchronised smoothing of each layer map is re-quired to guarantee pointwise compositional convergence to the target discontinuous function. Based on these theo-retical insights, we propose additive noise annealing (ANA), a new algorithm to train QNNs encompassing standard STE and its variants as special cases. When testing ANA on theCIFAR-10 image classiﬁcation benchmark, we ﬁnd that the major impact on task accuracy is not due to the qualitative shape of the regularisations but to the proper synchroni-sation of the different STE variants used in a network, in accordance with the theoretical results. of DNNs, which need millions or even billions of parame-ters and operations to deliver their performance.Research in tiny machine learning (TinyML) has made considerable steps towards enabling the deployment ofDNNs on resource-constrained devices. A ﬁrst class of techniques aims at making DNNs more efﬁcient in terms of accuracy-per-parameter or accuracy-per-operation [22, 23].We refer to these techniques as topological optimisations since they revolve around achieving higher model efﬁciency by changing the structure of DNNs. A second class of ap-proaches has instead focussed on deriving models that lever-age the properties of the target deployment platform. These hardware-related optimisations include hardware-friendly activation functions [18], weight clustering and weight ten-sor decomposition [7, 13, 28], and QNNs [10].QNNs use reduced-precision integer operands to meet the storage requirements and exploit the optimised sup-port for integer arithmetic of embedded and edge platforms.With respect to their ﬂoating-point counterparts, QNNs typ-ically introduce drops in task accuracy [11]. Several strate-gies have been proposed to counteract this shortcoming, ranging from changes to the target QNN’s topology [16,30], through mixing different data representations and preci-sions inside the same network [17,24], to learning the shape of the stair functions used [4, 6, 12]. However, understand-ing how to propagate gradients through the discontinuous functions that model quantised operands remains a critical problem in QNN training. 