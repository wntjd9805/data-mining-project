Quantitative descriptions of confidence intervals and un-certainties of the predictions of a model are needed in many applications in vision and machine learning. Mechanisms that enable this for deep neural network (DNN) models are slowly becoming available, and occasionally, being in-tegrated within production systems. But the literature is sparse in terms of how to perform statistical tests with the uncertainties produced by these overparameterized models.For two models with a similar accuracy profile, is the former model’s uncertainty behavior better in a statistically signifi-cant sense compared to the second model? For high resolu-tion images, performing hypothesis tests to generate mean-ingful actionable information (say, at a user specified sig-nificance level α = 0.05) is difficult but needed in both mis-sion critical settings and elsewhere. In this paper, specif-ically for uncertainties defined on images, we show how revisiting results from Random Field theory (RFT) when paired with DNN tools (to get around computational hur-dles) leads to efficient frameworks that can provide a hy-pothesis test capabilities, not otherwise available, for un-certainty maps from models used in many vision tasks. We show via many different experiments the viability of this framework. 