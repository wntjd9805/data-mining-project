Deep neural networks achieve outstanding results in a large variety of tasks, often outperforming human experts.However, a known limitation of current neural architec-tures is the poor accessibility to understand and interpret the network response to a given input. This is directly re-lated to the huge number of variables and the associated non-linearities of neural models, which are often used as black boxes. When it comes to critical applications as au-tonomous driving, security and safety, medicine and health, the lack of interpretability of the network behavior tends to induce skepticism and limited trustworthiness, despite the accurate performance of such systems in the given task.Furthermore, a single metric, such as the classiÔ¨Åcation ac-curacy, provides a non-exhaustive evaluation of most real-world scenarios. In this paper, we want to make a step for-ward towards interpretability in neural networks, providing new tools to interpret their behavior. We present Agglomer-ator, a framework capable of providing a representation of part-whole hierarchies from visual cues and organizing the input distribution matching the conceptual-semantic hierar-chical structure between classes. We evaluate our method on common datasets, such as SmallNORB, MNIST, Fash-ionMNIST, CIFAR-10, and CIFAR-100, providing a more interpretable model than other state-of-the-art approaches. 