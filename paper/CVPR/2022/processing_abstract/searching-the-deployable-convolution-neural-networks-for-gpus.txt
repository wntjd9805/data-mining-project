Customizing Convolution Neural Networks (CNN) for production use has been a challenging task for DL practi-tioners. This paper intends to expedite the model customiza-tion with a model hub that contains the optimized models tiered by their inference latency using Neural ArchitectureSearch (NAS). To achieve this goal, we build a distributedNAS system to search on a novel search space that con-sists of prominent factors to impact latency and accuracy.Since we target GPU, we name the NAS optimized models as GPUNet, which establishes a new SOTA Pareto frontier in inference latency and accuracy. Within 1ms, GPUNet is 2x faster than EfficientNet-X and FBNetV3 with even better accuracy. We also validate GPUNet on detection tasks, andGPUNet consistently outperforms EfficientNet-X and FB-NetV3 on COCO detection tasks in both latency and ac-curacy. All of these data validate that our NAS system is effective and generic to handle different design tasks. With this NAS system, we expand GPUNet to cover a wide range of latency targets such that DL practitioners can deploy our models directly in different scenarios. 