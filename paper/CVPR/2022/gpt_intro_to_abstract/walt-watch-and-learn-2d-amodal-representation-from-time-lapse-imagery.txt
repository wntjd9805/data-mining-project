This paper addresses the problem of object detection and segmentation in severely occluded scenarios. While data-driven methods have made progress in less occluded environments, they underperform when faced with severe occlusions commonly found in busy intersections and crowded places. This is due to occlusions being treated as noise and the difficulty of labeling occluded objects in datasets. To overcome these challenges, the paper proposes a novel approach that combines real and synthetic data. The authors mine a large dataset of unoccluded objects from time-lapse imagery captured by stationary cameras observing street intersections. These unoccluded objects are then used to synthesize occlusion scenarios for training a deep network. The paper introduces a new dataset called Watch and Learn Time-lapse (WALT), consisting of 12 cameras capturing urban environments over a year. The proposed method improves pedestrian and vehicle detection and segmentation in occluded scenarios, outperforming existing approaches. The paper presents simple yet effective baselines for future work on exploiting longitudinal supervision for computer vision under strong occlusions.