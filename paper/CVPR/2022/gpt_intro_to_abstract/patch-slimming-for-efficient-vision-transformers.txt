Transformer models have gained popularity in computer vision tasks, but their high computational cost hinders their deployment on resource-limited devices. To address this, various model compression techniques have been proposed, including quantization, knowledge distillation, and network pruning. While network pruning has been extensively explored for convolutional neural networks (CNNs), there are limited works on accelerating vision transformers. Vision transformers differ from CNNs in how they process input images, as they split the image into patches and calculate their features in parallel using attention mechanisms. This paper introduces patch slimming, a novel pruning approach specifically designed for vision transformers. The proposed method aims to identify and remove redundant patches, ensuring the retention of high-level discriminative features. Patch pruning is performed in a top-down manner, with importance scores determining which patches to preserve or discard. The pruning scheme is carefully controlled to maintain the model's original performance while significantly reducing computational cost. Experimental results demonstrate the effectiveness of the proposed patch slimming method, achieving substantial FLOPs reduction with minimal accuracy loss on the ImageNet dataset.