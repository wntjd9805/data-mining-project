This paper introduces the concept of test-time domain adaptation, which involves adapting a pre-trained model to unlabeled test data during inference time. The goal is to address the domain shift between the source training data and the target test data in order to achieve good performance. Existing methods often suffer from error accumulation and forgetting, resulting in performance deterioration over time. The paper focuses on the practical problem of online continual test-time adaptation, where the target test data is continuously changing and the model needs to adapt in real-time without access to the full test or source data. The proposed approach, called CoTTA (Continual Test-Time Adaptation), aims to tackle these problems by improving pseudo-label quality and preserving source knowledge. The first component of CoTTA improves pseudo-label quality by using a weight-averaged teacher model and augmentation-averaged predictions. This helps to reduce error accumulation. The second component helps preserve source knowledge by stochastically restoring a small part of neurons in the network back to the pre-trained source model, thus alleviating forgetting. CoTTA can be easily implemented and incorporated into any off-the-shelf pre-trained model without the need for re-training on source data. The effectiveness of the approach is demonstrated on four classification tasks and a segmentation task, where it significantly improves performance compared to existing methods. In summary, this paper proposes a continual test-time adaptation approach that effectively adapts off-the-shelf pre-trained models to continually changing target data. It reduces error accumulation, preserves source knowledge, and improves performance in real-world machine perception applications under domain shift.