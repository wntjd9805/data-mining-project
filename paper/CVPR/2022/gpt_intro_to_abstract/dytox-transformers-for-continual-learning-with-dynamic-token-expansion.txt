This paper introduces a novel approach for tackling the challenge of catastrophic forgetting in deep learning models. Existing methods either require knowledge of the task at test time or suffer from high memory overhead in handling a large number of tasks. To address these issues, the authors leverage the computer vision transformer ViT and propose a dynamic network architecture called DyTox. The encoder layers are shared among all tasks, while the decoder layer is specialized using a task-specific learned token to produce task-specific embeddings. This approach significantly limits memory growth and requires no hyperparameter tuning. The authors validate the effectiveness of DyTox on CIFAR100, ImageNet100, and ImageNet1000 datasets, achieving state-of-the-art results with minimal overhead. Overall, this work presents a robust and scalable solution for continual learning in computer vision tasks.