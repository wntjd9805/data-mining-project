Multi-label classification is a task that aims to identify multiple objects or attributes in a single image, making it relevant for real-world scenarios where scenes often contain multiple objects. However, creating datasets for multi-label classification can be challenging and costly due to the need for annotators to label all categories for each image. To address this issue, researchers have explored weakly supervised learning approaches, where only a small number of categories are annotated per image. These approaches align with the availability of large-scale multi-label datasets that provide partial labels. Two naive approaches to training models with partial labels have been proposed. One approach trains the model using only observed labels, ignoring unobserved labels. The other approach assumes that unobserved labels are negative and incorporates them into training, considering the prevalence of negative labels in multi-label settings. However, the latter approach introduces noise into the labels, which hinders model learning. Previous works have mostly followed the first approach and attempted to leverage the cues from unobserved labels through techniques like bootstrapping or regularization, but these methods involve heavy computation or complex optimization pipelines. This paper takes a new approach by considering the problem of weakly supervised multi-label classification from the perspective of noisy label learning. The authors observe a memorization effect in which models trained with noisy labels first fit clean labels and then memorize noisy labels. This effect has been previously observed in noisy multi-class classification scenarios, but this paper finds that it also occurs in noisy multi-label classification scenarios. Drawing inspiration from the literature on noisy multi-class learning, the authors propose a method that selectively trains the model using samples with small losses to prevent the memorization of false negative labels. The proposed method is simple and computationally efficient, yet it outperforms state-of-the-art weakly supervised multi-label classification methods on various datasets. Unlike existing methods that are effective only in specific partial label settings, the proposed method is broadly applicable to both artificially created and real partial label datasets. The paper concludes with an analysis of why the proposed method performs well from different perspectives. In summary, the contributions of this paper are threefold: 1) The empirical demonstration of the memorization effect in noisy multi-label classification for the first time. 2) The proposal of a novel scheme for weakly supervised multi-label classification that explicitly utilizes noisy labels. 3) The achievement of state-of-the-art classification performance on various partial label datasets with a simple and efficient method.