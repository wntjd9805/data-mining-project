This paper introduces a novel framework called audio-visual (AV) speech codecs for enhancing speech quality in telecommunication applications, particularly in AR/VR environments. Existing methods for speech enhancement often focus on speech intelligibility but fall short in terms of output quality and realism. AV speech codecs address these limitations by explicitly modeling the speech distribution and synthesizing clean speech conditioned on audio-visual cues. The approach leverages a neural speech codec trained on a discrete codebook of natural speech to compress and decode clean speech signals. An auto-regressive probabilistic model is then trained on the codes, conditioned on noisy audio and visual inputs, to generate speech codes that are synthesized into clean speech using the decoder module. Unlike speaker-agnostic methods, personalized AV speech codecs are developed to capture speaker-specific cues for higher-fidelity speech enhancement. To facilitate this, a high-quality audio-visual dataset called Facestar2 is introduced, containing 10 hours of speech data from two speakers. Experimental results demonstrate that personalized AV speech codecs outperform baselines in terms of quantitative metrics and human evaluation studies. Additionally, scalability is addressed by proposing a strategy for personalizing AV speech codecs to new individuals with minimal data, enabling high-fidelity telecommunications for a larger volume of users. The strategy involves a multi-speaker extension of AV speech codecs with a speaker identity encoder that can be pre-trained on a multi-speaker dataset and fine-tuned with a small sample of data from the new speaker. This approach not only enables personalized speech enhancement but also allows for voice conversion between different speakers, opening up creative applications in AR/VR environments. Overall, this work presents a significant advancement in audio-visual speech enhancement, enabling high-quality telecommunications in challenging environments.