Self-supervised learning (SSL) has gained popularity in various domains, including NLP and visual representation, in recent years. Contrastive learning (CL) frameworks have particularly attracted attention due to their intuitive motivation. CL aims to bring anchor samples close to positive samples and repel them from negative samples. However, CL frameworks require a large number of negative samples, posing challenges for training with a large mini-batch size. In this paper, we investigate the hardness-aware property of the InfoNCE loss used in CL. We propose a decomposed loss that reflects intra-anchor and inter-anchor hardness-aware properties, enabling a fine-grained analysis of CL frameworks. We demonstrate that a small dictionary is sufficient for the vector component of the loss, while the scalar component requires a large dictionary. We also find that increasing the dictionary size and temperature helps alleviate the inter-anchor hardness-aware sensitivity. Our findings simplify CL frameworks by removing the dictionary and momentum, leading to a dictionary-free and momentum-free approach, SimCo, that achieves comparable or superior performance. Additionally, our investigation helps bridge the gap between CL and non-CL frameworks, contributing to a unified perspective on SSL frameworks.