This paper introduces Mobile-Former, a new network that combines MobileNet and transformer architectures in parallel with a bidirectional bridge. The aim is to efficiently encode both local processing and global interaction. Previous works have shown the benefits of combining convolution and transformer architectures in series, but this paper proposes a shift to a parallel design. The authors demonstrate the effectiveness of Mobile-Former across various tasks, including image classification and object detection. In object detection, Mobile-Former outperforms MobileNetV3 as a backbone in RetinaNet with less computational cost. It also achieves higher performance than DETR while using fewer FLOPs and having a smaller model size. The experimental results highlight the efficacy of Mobile-Former in leveraging transformer capabilities in low FLOP regimes where efficient convolutional neural networks dominate.