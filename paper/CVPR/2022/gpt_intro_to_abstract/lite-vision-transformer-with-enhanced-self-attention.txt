Transformer-based architectures have achieved remarkable success in various vision tasks. Inspired by the success of self-attention in the NLP community, Dosovitskiy proposed a transformer-based network for computer vision. However, to reduce computational complexity, Swin-Transformer upgraded the architecture by introducing local non-overlapping windows and hierarchical feature representations. Similarly, PVT proposed spatial-reduction attention and removed positional embedding. While these models are effective for vision tasks, there is a significant performance degradation when scaling down the models for mobile applications. In this work, we introduce a Lite Vision Transformer (LVT) backbone, designed specifically for mobile applications. LVT follows a four-stage structure and has a similar parameter size to existing mobile network architectures. We propose two novel self-attention layers for LVT. The first improvement is Convolutional Self-Attention (CSA), which incorporates local self-attention into the convolutional layer. This enriches the low-level features and improves generalization ability. The second improvement is Recursive Atrous Self-Attention (RASA), which increases the representation capacity of lite transformers by utilizing multi-scale context and a recursion pipeline. Experiments on ImageNet classification, ADE20K semantic segmentation, and COCO panoptic segmentation demonstrate the superior performance of LVT as a generalized vision model backbone. The main contributions of this work are the introduction of CSA and RASA layers, which enhance the capabilities of lite transformers, and the development of LVT as a lightweight vision transformer backbone.