This paper introduces SLidR (Superpixel-driven Lidar Representations), a novel self-supervised 2D-to-3D representation distillation approach for autonomous driving data. The goal is to leverage synchronized and calibrated arrays of cameras and Lidar sensors to pre-train 3D networks without the need for annotations. SLidR utilizes superpixels to pool visually similar regions in both images and point clouds. A contrastive loss is then used to align corresponding pooled features. This pooling strategy mitigates noise and balances asymmetries in the data. The proposed method is extensively evaluated on semantic segmentation and object detection tasks, outperforming state-of-the-art self-supervised pre-training methods. The results demonstrate the effectiveness of the image-to-Lidar pre-training strategy for autonomous driving scenes.