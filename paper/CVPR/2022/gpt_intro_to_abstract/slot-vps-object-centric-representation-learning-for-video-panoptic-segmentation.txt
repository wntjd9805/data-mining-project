Video panoptic segmentation (VPS) is a task that involves classifying foreground instances and background semantics in a video. Existing methods model these objects separately using multiple sub-networks, resulting in complex post-processing for fusion of predictions. However, this decomposed pipeline has limitations including time-consuming post-processing, error propagation, and lack of end-to-end training. To address these issues, this paper proposes Slot-VPS, a unified end-to-end framework that represents all panoptic objects in the video using panoptic slots. These slots are learnable parameters that encode spatial and temporal information and allow direct predictions of class, mask, and object ID. The framework incorporates a Video Panoptic Retriever (VPR) to retrieve and encode spatio-temporal coherent information from the video. Experimental results on Cityscapes-VPS and VIPER datasets demonstrate the effectiveness of the proposed method, outperforming state-of-the-art approaches while maintaining better efficiency. The contributions of this paper include the introduction of panoptic slots for unified representation, the development of VPR for object localization and association, and the validation of the method's performance through experiments.