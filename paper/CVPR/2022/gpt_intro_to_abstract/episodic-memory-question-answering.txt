This paper introduces the concept of egocentric AI assistants, which are AI-powered, augmented reality glasses capable of passively capturing visual data from the wearer's perspective and utilizing this information to answer questions about the environment. Unlike traditional embodied question answering agents, egocentric assistants passively observe visual frames rather than taking actions in the environment, and they must build scene-specific memory representations that persist across different questions. The paper proposes a novel task called Episodic Memory Question Answering (EMQA), where the AI assistant is taken on a guided tour of an indoor environment and asked to localize answers to post-hoc questions about the tour. The paper presents a model for the EMQA task that constructs an episodic scene memory and utilizes it to ground answers to multiple text questions. The model is compared against various baselines and outperforms them in terms of accuracy. Additionally, the paper tests the model's robustness in real-world conditions and investigates the impact of noise in the agent's pose. Overall, this paper contributes to the development of egocentric AI assistants and provides insights into the effectiveness and resilience of their memory-based approach.