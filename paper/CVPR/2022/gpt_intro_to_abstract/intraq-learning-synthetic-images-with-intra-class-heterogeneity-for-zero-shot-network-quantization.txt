The increasing demands in computing power and memory footprint of deep neural networks (DNNs) pose a challenge for edge computing devices with limited hardware resources. Network quantization, which represents parameters and activations within networks using low-bit integers, has emerged as an effective method for reducing memory usage and improving efficiency. However, most existing methods rely on access to original training data, which may not be feasible due to privacy and security concerns. To address this issue, zero-shot quantization (ZSQ) has been proposed to quantize models without accessing real data. Existing ZSQ methods either calibrate parameters without using data or use synthetic fake images for training. However, these methods have limitations in terms of performance degradation or lack of intra-class heterogeneity in the synthetic images. In this paper, we propose a novel ZSQ method called IntraQ that addresses these limitations. IntraQ incorporates local object reinforcement to introduce heterogeneity in the synthetic images and uses a soft inception loss to preserve complex scenes. Experimental results show that IntraQ achieves significant performance improvements compared to existing methods, demonstrating the effectiveness of our proposed solutions. Specifically, when quantizing MobileNetV1 to 4-bit on ImageNet, IntraQ achieves a top-1 accuracy of 51.36%, a 9.17% increase compared to the advanced DSG method equipped with the traditional inception loss.