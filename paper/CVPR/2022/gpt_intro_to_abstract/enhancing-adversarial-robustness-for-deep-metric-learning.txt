Deep Metric Learning (DML) is a task in computer vision that aims to learn a metric between two inputs, such as images, using deep neural networks. DML has wide-ranging applications in image retrieval and face recognition and has also influenced areas such as self-supervised learning. However, recent studies have found that DML models are vulnerable to adversarial attacks, where imperceptible perturbations can cause unexpected retrieval results or change rankings. This vulnerability raises concerns about security, safety, and fairness in DML applications. Existing defense methods for DML are based on adversarial training, but they suffer from low efficiency and weak robustness. In this paper, we propose a new defense method called Hardness Manipulation (HM) that efficiently and effectively creates adversarial example triplets for subsequent training. We also introduce Linear Gradual Adversary (LGA) as a pseudo-hardness function for HM, which balances the training objectives during the training process. Additionally, we propose an Intra-Class Structure (ICS) loss term to improve model robustness and training efficiency. Our experiments on three commonly used DML datasets demonstrate that our proposed method outperforms the state-of-the-art defense in terms of robustness, training efficiency, and performance on benign examples. Overall, our contributions include the development of HM as a flexible and efficient tool for adversarial training, the introduction of LGA as a pseudo-hardness function, and the incorporation of ICS loss for improved model robustness and training efficiency.