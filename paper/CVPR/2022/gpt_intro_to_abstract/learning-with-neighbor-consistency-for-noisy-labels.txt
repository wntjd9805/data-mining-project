Deep learning has shown remarkable accuracy in image classification tasks, but it heavily relies on large supervised datasets which are often expensive to obtain. Unsupervised and semi-supervised learning methods have been developed to address this issue by incorporating unlabelled examples. However, these approaches do not take advantage of the noisy labels that are abundant in modern sources like social media or web pages. The current dominant approach in dealing with noisy labels is to use model predictions to modify or reject training examples. However, this approach is risky, as deep networks can fit arbitrary labels and it requires significant measures against overfitting. This approach also leads to complicated training procedures.In this paper, we propose a novel approach called Neighbor Consistency Regularization (NCR) for learning with noisy labels. Instead of using model predictions as pseudo-labels, NCR introduces an additional consistency loss that encourages each example to have similar predictions to its neighbors. This is achieved by penalizing the divergence of each example's prediction from a weighted combination of its neighbors' predictions. The similarity in feature space determines the weights.The motivation behind NCR is to allow incorrect labels to be improved or at least attenuated by the labels of their neighbors. We assume that the noise in the labels is weak or unstructured enough to not overwhelm the correct labels. Compared to the popular approach of bootstrapping model predictions, NCR can be seen as bootstrapping the learned feature representation, making it less susceptible to overfitting and improving stability at random initialization.NCR draws inspiration from label propagation algorithms for semi-supervised learning. However, while label propagation is typically performed in a batch setting, our method performs label propagation online within mini-batches during stochastic gradient descent. This results in a simple, single-stage training procedure. Additionally, NCR is an inductive form of label propagation, producing a model that can classify unseen examples.The key contributions of this paper are the proposal of Neighbor Consistency Regularization, empirical verification of its superior accuracy compared to baselines at various noise levels, and its competitive or state-of-the-art performance on datasets with both synthetic and realistic noise scenarios.