This paper focuses on improving the interpretability of visual reasoning models in artificial intelligence by providing explanations for their decision-making process. Existing models often lack the ability to explain the rationales behind their answers, leading to difficulty in understanding their decision-making process.To address this issue, the authors propose an integrated framework that incorporates reasoning-aware and visually grounded explanations. These explanations are derived by traversing the reasoning process and connecting key components across different modalities. The authors introduce a functional program to automatically construct these explanations and collect a new dataset with a large number of multi-modal explanations.Furthermore, the authors propose a novel explanation generation method that explicitly models the correspondence between important words and regions of interest in the visual scene. This method significantly improves the visual grounding, resulting in enhanced interpretability and reasoning performance.The contributions of this paper include the introduction of reasoning-aware and visually grounded explanations, the development of a functional program for automatic explanation construction, and the proposal of a novel explanation generation method. The effectiveness of these approaches is demonstrated through extensive experiments, including multi-task learning and transfer learning, and the correlation between different visual skills and reasoning performance is analyzed.Overall, this research aims to enhance the interpretability of visual reasoning models by providing explanations that are closely tied to the reasoning process and grounded in visual scenes.