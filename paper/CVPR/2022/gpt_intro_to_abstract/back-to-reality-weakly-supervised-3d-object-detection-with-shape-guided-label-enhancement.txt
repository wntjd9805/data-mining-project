3D object detection is an important problem in computer vision, but existing 2D methods are not directly applicable to 3D object detection due to the irregular form of point clouds. Recent works have utilized deep learning techniques on point cloud understanding to achieve favorable performance in 3D object detection. However, these methods require large amounts of labeled bounding boxes for training, which is time-consuming and limits their practicality. To address this issue, weakly-supervised 3D object detection methods have been proposed, where only scene-level or position-level annotations are required. While scene-level annotation is more time-saving, the performance is not satisfactory due to the lack of position information. Position-level annotation is a more practical solution, but current methods still require a number of precisely labeled boxes and can only handle sparse outdoor scenes. In this paper, we propose a shape-guided label enhancement approach called Back to Reality (BR) for weakly-supervised 3D object detection. Our approach converts weak labels into virtual scenes that contain lost information and utilizes them to additionally supervise real-scene training. We utilize the large-scale datasets of synthetic shapes as strong priors and leverage the coarse layout provided by position-level annotations to assemble fully-annotated virtual scenes and apply physical constraints to remedy information loss. We also propose a virtual-to-real domain adaptation method to align features between real and virtual scenes. Experimental results show the effectiveness of our proposed BR method on the ScanNet dataset.