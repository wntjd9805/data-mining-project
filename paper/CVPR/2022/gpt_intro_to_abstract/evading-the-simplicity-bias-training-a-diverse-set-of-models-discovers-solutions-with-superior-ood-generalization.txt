This paper introduces the concept of inductive biases in deep learning and their role in OOD (Out of Distribution) generalization. Inductive biases are a set of assumptions that define the learned function outside of training examples and enable extrapolation to novel test points. Deep neural networks, known for their effectiveness, exhibit a simplicity bias. They preferentially represent simple, approximately piecewise linear functions. However, this bias can hinder the learning of complex patterns that are crucial for the task at hand, leading to the reliance on spurious correlations or statistical shortcuts. OOD generalization, which involves accurate predictions under arbitrary covariate shifts, requires models to reflect the intrinsic mechanisms of the task rather than rely solely on training examples. Existing methods integrate extra information during training, but this study proposes deferring this process to a model selection stage. The authors train a collection of similar models, each optimized for empirical risk minimization, while a regularizer encourages diversity across the models. This approach allows the models to rely on different patterns in the data, including complex ones that would typically be ignored due to the simplicity bias. The paper demonstrates improvements in OOD generalization through experiments on image recognition datasets and suggests potential applications in addressing adversarial vulnerabilities, model biases, and cross-domain transfer. The contributions of this paper include a rationale for addressing generalization during model selection, a method to overcome the simplicity bias, and the demonstration of benefits on existing benchmarks. However, the paper acknowledges that more research is needed to fully address the challenges of OOD generalization in deep learning.