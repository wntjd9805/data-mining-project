This paper introduces the problem of anomaly detection (AD) and its significance in various applications such as industrial defect detection, medical out-of-distribution detection, and video surveillance. In the context of unsupervised AD, where no prior information on anomalies is available, previous approaches have focused on self-supervision tasks on normal samples. This study tackles unsupervised anomaly detection from a knowledge distillation-based perspective. The paper proposes a new paradigm called Reverse Distillation, which addresses the limitations of conventional knowledge distillation methods in precisely detecting and localizing anomalies. The proposed method incorporates an encoder-decoder structure and a reverse knowledge distillation strategy, which enhance the discrimination capability of the teacher-student model on anomalies. Additionally, a one-class bottleneck embedding module is introduced to project high-dimensional features into a compact one-class embedding space, facilitating the restoration of anomaly-free representations at the student. The paper presents experimental results on public benchmarks, demonstrating the effectiveness of the proposed approach and achieving state-of-the-art performance in anomaly detection.