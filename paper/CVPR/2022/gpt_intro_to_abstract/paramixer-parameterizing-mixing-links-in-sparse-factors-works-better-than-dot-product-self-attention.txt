Transformer models, powered by the self-attention mechanism, have been widely used in various tasks such as text classification, text summarization, promoter region prediction, and image classification. However, the original self-attention approach is not scalable due to the quadratic cost it incurs for long sequences. Various methods have been proposed to alleviate this issue, but most Transformer variants still rely on dot-product self-attention, which is limited by the low-rank bottleneck. This paper introduces Paramixer, a novel attention building block that overcomes the scalability and expressive power limitations of dot-product self-attention. Paramixer directly parameterizes the mixing links in sparse factors to form an attention matrix with full-rank factorizing matrices. The paper presents two approaches to specify the non-zero positions in each sparse factor, resulting in an economical approximation of the full attention matrix with a low computational cost. Experimental results demonstrate that Paramixer outperforms dot-product-based self-attention networks on long sequence tasks, including synthetic data inference, Genome classification, and character-level long document classification. Furthermore, Paramixer achieves state-of-the-art accuracy on the public Long Range Arena benchmark tasks. The remaining sections of the paper discuss dot-product self-attention and related work, present the development clue and model architecture of Paramixer, and provide experimental settings and results.