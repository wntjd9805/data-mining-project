This paper introduces SWINBERT, an end-to-end fully Transformer-based model for video captioning. Unlike previous approaches that rely on offline-extracted 2D/3D features, SWINBERT employs a video Transformer as the video encoder to capture the spatial-temporal dynamics in videos. The model generates a sequence of output words by learning the relationships between visual and textual elements. The authors propose a sparse attention mask that adaptively learns attention patterns to improve long-range video sequence modeling. This mask allows the model to focus more on video frame patches with more spatial-temporal movements, enhancing the performance of the video captioning task. Experimental results on five video captioning datasets demonstrate that SWINBERT outperforms previous state-of-the-art methods by a significant margin. The proposed model achieves an absolute CIDEr improvement of +64.8 on MSVD, +55.4 on YouCook2, +3.0 on MSRVTT, +5.9 on TVC, and +14.9 on VATEX. Furthermore, the learnable sparse attention mask effectively regularizes model training and reduces redundancy in video inputs, opening new possibilities for video-and-language modeling. In summary, the contributions of this work are: 1. Introduction of SWINBERT, the first end-to-end pure Transformer-based model for video captioning.2. Introduction of a learnable Sparse Attention Mask that improves long-range video sequence modeling.3. Demonstration of superior performance over previous state-of-the-art methods on five popular video captioning benchmarks.