In recent years, high-capacity neural classifiers have achieved state-of-the-art performance in computer vision tasks. However, researchers have started to examine the issue of unfairness in these models, specifically in relation to accuracy differences across protected subgroups defined by human-sensitive attributes such as gender and race. Various strategies have been developed to train models that address these accuracy differences and promote fairness. These strategies typically quantify unfairness by comparing accuracy-related rates between different groups. Previous fairness studies in computer vision have focused on a trade-off between fairness and accuracy, where higher accuracy in one group comes at the cost of lower accuracy in another group. However, this trade-off does not hold when using high-capacity neural classifiers. Instead, fairness methods tend to degrade the overall accuracy of the classifier, with a greater decrease in accuracy for the better performing groups. This phenomenon, known as "leveling down," has received criticism in law and philosophy. In this paper, we argue that if fairness methods decrease performance for all groups, they are Pareto Inefficient and should not be used in contexts where group accuracy is a primary concern. We provide evidence that existing fairness methods result in decreased performance across all groups, including the worst performing group. We attribute this problem to the fact that high-capacity classifiers fit training data nearly perfectly, making fairness constraints on the training set inappropriate. Furthermore, the evaluation of fairness methods often masks a systematic deterioration of classifiers, where accuracy decreases across all groups. To address these limitations, we propose three contributions: 1) a reconsideration of the bias-variance trade-off in fairness, highlighting the importance of generalization error in high-capacity classifiers; 2) an extensive evaluation of existing fairness methods, demonstrating their trade-off between fairness metrics and performance across all groups; and 3) the exploration of data augmentation and adaptive sampling as a means to improve fairness by improving accuracy on the most disadvantaged groups.