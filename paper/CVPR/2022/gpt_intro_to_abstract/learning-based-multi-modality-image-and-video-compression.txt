This paper introduces a learning-based multi-modality compression framework for image pairs from different modalities, specifically infrared and visible images. The framework aims to exploit cross-modality redundancy in the feature space by leveraging affine transformation and attention mechanisms for channel-wise and spatial-wise feature alignment, respectively. The proposed framework offers flexibility by allowing one modality to be used as a reference for compressing images from another modality and can be extended for multi-modality video compression. Experimental results show that the proposed method outperforms single-modality image and video compression approaches on various benchmark datasets. The contributions of this framework include the introduction of channel-wise and spatial-wise alignment modules and the development of a flexible and effective multi-modality image/video compression framework.