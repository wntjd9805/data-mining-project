Self-supervised learning (SSL) has gained significant attention in the computer vision community for its potential in utilizing unlabeled data. Among SSL approaches, contrastive learning has achieved high performance on various tasks, surpassing its supervised counterpart. The Siamese structure, commonly used in state-of-the-art unsupervised methods, learns visual features by minimizing the distance between augmented views of an image. However, an important challenge in contrastive learning is the selection of positive views. Previous methods primarily use RandomCrop to generate diverse views but overlook the semantic information and variance of the paired views. This can lead to missed objects and trivial views with high similarity. To address these issues, we propose ContrastiveCrop, a method that incorporates semantic-aware localization and center-suppressed sampling. By considering the content of an image, our approach selects crops that avoid false positives and captures diverse parts of the object. Our method can be easily applied to Siamese structures and is agnostic to contrastive frameworks. Experimental results demonstrate that ContrastiveCrop consistently improves classification accuracy and performs better on downstream tasks such as detection and segmentation. The contributions of this paper include introducing the problem with RandomCrop in contrastive learning, proposing ContrastiveCrop to generate improved views, and demonstrating its effectiveness and generality in Siamese representation learning.