Facial expression recognition (FER) has significant importance in various applications, such as human-computer interaction and lie detection. While there are several large-scale datasets available for static FER, there are limited options for video-based FER. The existing video datasets often lack complexity and varied scene context, making them impractical for real-world applications. To address this issue, we introduce FERV39k, a large-scale, multi-scene dataset containing 38,935 video clips labeled with 7 classic expressions across 22 fine-grained scenes in 4 isolated scenarios. FERV39k is designed to cover realistic challenges and incorporates a four-stage candidate clip generation strategy to automatically generate massive video clips. The dataset demonstrates three main characteristics: multi-scene division, large-scale volume, and high-quality labels. We benchmark four deep learning-based architectures on FERV39k and perform several baseline evaluations to identify the challenges associated with multi-scene expression representation in videos. Additionally, we conduct ablation studies to further understand key components in modeling dynamic FER. The contributions of our work include the construction of a novel large-scale multi-scene dataset, the development of a candidate clip generation and annotation workflow, and the identification of key challenges in video-based FER through extensive studies on FERV39k.