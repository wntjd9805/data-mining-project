This paper introduces a performance-aware mutual knowledge distillation (PAMKD) method for improving neural architecture search (NAS), which aims to automatically search for high-performance neural architectures. Many NAS works have proposed to use knowledge distillation (KD) to improve the quality of searched architectures by transferring knowledge from human-designed architectures to auto-searched architectures. Mutual KD, where a group of models mutually perform KD, has shown promising results in achieving better generalization to test data and learning multi-scale representations. However, existing mutual KD methods suffer from the problem of collective failure, where the performance of all models is degraded due to the use of low-quality knowledge from poorly-performing models. To address this problem, the proposed PAMKD approach performs performance scrutiny before transferring knowledge. A learner is allowed to generate knowledge to train another learner only if its performance is better. This significantly reduces the risk of collective failure. Additionally, existing mutual KD methods are not amenable for performance scrutiny as they use the same model weights for measuring performance and training models simultaneously, leading to a degenerated solution. The proposed approach overcomes this limitation by learning two sets of model weights sequentially for each learner, enabling performance measurement and knowledge generation in one set and training in the other set.The proposed method is formulated as a three-level optimization problem, consisting of three learning stages performed end-to-end. In the first stage, each learner independently trains a predictive model. In the second stage, models are evaluated on a validation dataset, and if the performance of a model is better than another model, it generates knowledge to train the latter. In the third stage, models trained in the second stage are further validated, and their architectures are updated by minimizing validation losses. Furthermore, the proposed approach addresses the limitation of existing KD methods in capturing complex dataset structures. A new group-wise relative similarity (GRS) based approach is proposed to transfer knowledge, which considers the higher-order relationships between data instances. This allows for the capture of high-order nonlinear manifold structures in the dataset, facilitating more effective knowledge transfer between learners.The major contributions of this paper include the PAMKD method, which addresses the collective failure problem in mutual KD for NAS, and the group-wise relative similarity based knowledge transfer approach. Experimental results on several datasets demonstrate the effectiveness of the proposed method.