Humans have the unique ability to continually acquire new skills and knowledge while still being able to perform tasks learned in childhood. Continual Learning (CL) aims to replicate this process, allowing deep neural networks to learn new tasks sequentially without storing all associated data. However, deep neural networks often suffer from performance degradation on previously learned tasks when learning new ones, known as catastrophic forgetting. This issue has motivated the development of more effective CL algorithms to mitigate catastrophic forgetting as the number of tasks and associated knowledge increases over time. Existing approaches modify network learning strategies through regularization methods, memory-based experience replay, and dynamic modular approaches. This work proposes Sparse neural Networks for Continual Learning (SNCL), which enforces the sparsity of network neurons to reserve more parameters for future tasks. SNCL prevents interference between learned and new tasks, reducing the risk of forgetting. The proposed approach utilizes variational Bayesian sparsity priors on activations to induce a sparse network, allowing for greater control over the learning and forgetting processes. A small replay buffer is used to exploit the commonality between new and previous tasks, increasing the network's total learning capacity and reducing the risk of forgetting. The variational Bayesian framework enables learning sparse networks through backpropagation, and a Full Experience Replay (FER) strategy is introduced to store and replay old samples' intermediate layer features, providing effective supervision on learning sparse activation. Additionally, a Loss-aware Reservoir Sampling (LRS) strategy is proposed to maintain the memory by selecting samples based on performance improvement. The proposed approaches improve the performance of experience replay-based methods and can achieve state-of-the-art results under the same setting. Furthermore, the proposed method is agnostic to network structures and task boundaries, making it a general approach for continual learning.