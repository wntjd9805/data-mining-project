Convolutional Neural Networks (CNNs) have become the standard architecture for computer vision tasks such as image classification, object detection, and segmentation. As more complex vision tasks are tackled, there is a need to scale deep convolutional networks to massive sizes. However, this poses challenges for edge device applications due to storage and computation limitations. To address this, research has focused on compressing deep convolutional networks for efficient storage and computation. Various approaches, such as pruning, quantization, low-rank factorization, and knowledge distillation, have been proposed.Pruning, a popular compression technique, aims to discard parameters in the model while preserving performance. However, effectively sparsifying a multitask network, where multiple related tasks are solved using a single model, remains unexplored. Multitask learning offers advantages in terms of training and inference time reduction and improved generalization performance. Yet, compressing and sparsifying multitask networks is challenging due to the entangled nature of shared features.In this paper, we propose a pruning scheme called DiSparse that enforces sparsity in multitask networks by considering the entangled features. We argue that correctly identifying saliency scores for each task in the shared space is key to compressing multitask models effectively. DiSparse makes unanimous selection decisions among all tasks, removing parameters only if they are not critical for any task. This prevents extreme degradation in performance for certain tasks and leads to a more balanced network.Extensive experiments are conducted to validate the efficiency of DiSparse. Compression performance is demonstrated on models of different structures and datasets of various sizes. Results show that DiSparse outperforms popular pruning and sparse training methods in terms of training loss, evaluation metrics, and multitask learning approaches. Additionally, interesting observations are made, including the similarity in sparse network architecture among tasks even before training starts and the existence of a "watershed" layer where task relatedness sharply drops.Overall, our contributions include proposing an effective pruning and sparse training scheme for multitask networks, demonstrating the superiority of DiSparse over existing methods, and providing valuable insights into task relatedness and multitask model architecture design. This work is beneficial for both the compression and multitask learning communities.