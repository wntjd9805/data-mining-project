Person re-identification (ReID) is a challenging task in computer vision that involves identifying individuals across different cameras, locations, and time periods. Existing approaches have addressed the geometric misalignment among person images caused by variations in human poses, camera viewpoints, and style/scales. However, these methods often assume that query and gallery images of the same person have the same clothing, resulting in performance degradation when tested on long-term ReID datasets with large clothing variations. To tackle the cloth-changing ReID (CC-ReID) problem, recent studies have introduced new datasets and algorithms that learn cloth-agnostic representations. However, these methods are prone to estimation errors and neglect the rich dynamic motion information, such as gait. In this paper, we propose a Gait-assisted Image-based ReID framework (GI-ReID) that leverages gait features as a unique biometric cue to learn cloth-agnostic and discriminative ReID representations. We introduce a Gait Sequence Prediction (GSP) module to forecast gait frames from a single image, enriching the learned gait information. Additionally, we enforce a Semantics Consistency (SC) constraint to encourage efficient learning and enhance the distinguishing power of our method. Experimental results show that GI-ReID achieves state-of-the-art performance on image-based CC-ReID, demonstrating its effectiveness and compatibility with existing ReID-specific networks.