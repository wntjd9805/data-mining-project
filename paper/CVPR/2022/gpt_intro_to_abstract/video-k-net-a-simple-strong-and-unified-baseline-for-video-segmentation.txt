Video Panoptic Segmentation (VPS) has gained attention in recent years for its applications in various vision systems, such as autonomous driving and robot navigation. VPS aims to segment and track every pixel of input video clips, and it extends Panoptic Segmentation (PS) into the video domain by unifying Video Semantic Segmentation (VSS) and Video Instance Segmentation (VIS) into a single task. Existing studies for video panoptic segmentation can be divided into top-down and bottom-up approaches. Top-down methods solve VPS as a multi-task learning problem, while bottom-up methods first perform semantic segmentation and then localize and track instances. Inspired by the design of object queries in Detection Transformer (DETR), new efforts have emerged to explore unified solutions to image panoptic segmentation. In this paper, we propose Video K-Net, a fully end-to-end framework for video panoptic segmentation. Video K-Net simplifies the pipeline of VPS by adopting learnable kernels from K-Net, which encode complexity without extra tracking queries or RoI features. The tracking is performed in an online manner, and our approach achieves improved association performance compared to previous methods. Video K-Net outperforms the strong baseline K-Net on KITTI-STEP and Cityscapes-VPS datasets, as well as achieves state-of-the-art results on these datasets. Additionally, Video K-Net achieves the best trade-off between accuracy and computational complexity. We further validate the effectiveness of kernel fusion on the VSS task and extend Video K-Net into clip-level processing for the VIS task. The experimental results demonstrate that Video K-Net can serve as a new baseline for future research on unified video segmentation tasks.