This paper presents a new video synthesis model that supports diverse, multimodal conditioning signals for improved control and flexibility in video generation. The model consists of two phases, with the first phase using an autoencoder to obtain discrete representations from images. The second phase employs a bidirectional transformer trained with a masked sequence modeling task to generate video representations conditioned on input modalities. The proposed framework, named MMVID, tackles challenges related to video consistency, accurate learning of textual information, and computational demands of training a transformer model. Several techniques, including video token training, text augmentation, and long sequence synthesis, are introduced to improve video generation quality and diversity. The framework enables various applications, including independent and dependent multimodal generation, utilizing visual modalities and language instructions. Experimental results on four datasets, including a new dataset called Multimodal VoxCeleb, validate the effectiveness of the proposed approach.