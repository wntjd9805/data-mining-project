Implicit neural representation (INR) has recently gained attention for representing visual data, particularly 2D images, using multi-layer perceptron (MLP) and positional encoding techniques. While existing works have explored video-based INR as an extension of image-based INR, this paper presents a novel approach that considers a video as a sequence of images evolving over time. The authors propose leveraging the phase information in Fourier-based positional encoding to generate temporally varying video content. They develop a frame generation module and a phase-shift generation module within the INR model to achieve this. The study aims to answer two key questions: 1) Can the model learn to fit a video with changing spatial coordinates? and 2) Does the learned phase space have meaningful structures that can be manipulated to create different temporal dynamics? Experimental results demonstrate the model's ability to fit videos and manipulate the phase-shift vectors to generate various temporal effects. The paper introduces a motion-adjustable neural implicit video representation, contributes to the understanding of Fourier-based positional encoding, and explores applications in video processing.