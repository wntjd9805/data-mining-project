When visual models are trained with limited or biased data, they may learn unintended correlations. This paper focuses on the problem of training a classifier to distinguish between two categories of birds ("landbird" and "waterbird") using background features from their respective habitats. A baseline model may mistakenly learn the "location" task instead of the actual task, leading to poor performance on birds outside their usual habitat. Previous approaches have incorporated language specifications to guide visual models, but these approaches rely on class-discriminative language, which is not always available. Additionally, conditioning on language embeddings may not prevent models from attending to spurious correlations in biased datasets. Furthermore, these models may struggle with rare or fine-grained classes due to insufficient data. To address these limitations, the authors propose a framework called Guiding visual Attention with Language Specification (GALS), which translates available language specifications into spatial attention to supervise a CNN's attention during training. The authors leverage an off-the-shelf pretrained vision-language model to obtain a saliency map for each image and guide the classifier's attention based on this map. The visual classifier retains flexibility to attend to relevant features beyond the given guidance. The paper demonstrates the effectiveness of GALS on datasets with explicit and implicit biases and shows improvements in accuracy, fairness metrics, and classifier explanations. The code and datasets are available on GitHub.