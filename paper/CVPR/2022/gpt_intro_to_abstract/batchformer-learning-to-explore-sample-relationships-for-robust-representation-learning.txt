Exploring sample relationships and addressing data scarcity in deep neural networks is a challenge for robust representation learning. Previous methods have focused on regularization or knowledge transfer, but they fail to enable deep neural networks to explore sample relationships directly. In this paper, we propose a Batch Transformer (BatchFormer) module to empower deep neural networks with structural advances for sample relationship learning. By capturing and modeling sample relationships within mini-batches, BatchFormer allows all samples to contribute to learning on any object categories. The module is only required during training, ensuring no changes to the inference structures of deep neural networks. BatchFormer enables comprehensive and invariant representations, improving performance in long-tailed recognition, zero-shot learning, domain generalization, and self-supervised representation learning tasks. Experimental results demonstrate the effectiveness of BatchFormer in these visual recognition tasks.