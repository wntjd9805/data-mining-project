Federated Learning (FL) is a privacy-preserving and efficient distributed training approach that generates a global model through collaboration with isolated clients. FL algorithms, such as FedAvg, aggregate local parameters using coordinate-based averaging. However, traditional FL algorithms have limitations in terms of permutation invariance and non-i.i.d. data distribution, leading to misaligned models and weight divergence. Existing methods for alignment, such as Bayesian nonparametric learning and optimal transport, are complex and post-processing strategies. This paper proposes a more straightforward and general technique called Position-Aware Neurons (PANs) for pre-aligning neurons during local training in FL. PANs introduce position encodings to neurons' outputs, allowing the control of permutation invariance. PANs can effectively reduce misalignment and weight divergence, facilitating coordinate-based parameter averaging in FL. This technique is easy to implement and computationally friendly, making it applicable to various FL algorithms. The contributions of the paper include the proposal of PANs to address permutation invariance in deep networks and the application of PANs in FL to improve parameter averaging.