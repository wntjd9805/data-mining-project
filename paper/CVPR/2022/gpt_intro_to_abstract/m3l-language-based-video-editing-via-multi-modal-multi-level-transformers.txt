The paper introduces the concept of language-based video editing (LBVE), a task where a target video is controlled directly through a language instruction. This approach aims to make video editing more accessible to novices by allowing them to use natural language instructions instead of complex editing tools. The main challenge of LBVE is linking video perception with language understanding to manipulate video semantics under a similar scenario. To address this challenge, the paper proposes a multi-modal multi-level transformer (M3L) that combines video perception and language understanding for video editing. The M3L utilizes multi-level fusion to integrate visual understanding at both global and local levels, resulting in better video synthesis. The paper also presents three new datasets for evaluating LBVE, including content replacing and semantic manipulation tasks. Experimental results demonstrate that the M3L architecture is effective for LBVE and that the multi-level fusion enhances video perception and language understanding. The contributions of the paper include introducing the LBVE task, proposing the M3L approach, providing evaluation datasets, and showcasing the potential of LBVE for vision-and-language research.