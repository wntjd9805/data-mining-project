Event-based cameras offer unique sensing capabilities, such as high temporal resolution, high dynamic range, low power consumption, and reduced motion blur. These sensors have shown promise in object tracking tasks, especially in adverse conditions. However, existing computer vision approaches designed for conventional frame-based cameras cannot directly work with event-based cameras due to their asynchronous nature. Previous works have attempted to convert asynchronous events to conventional frames but have struggled with the sparsity and lack of texture information in the accumulated event frames. Other works focused on learning-based object tracking approaches tailored to event frames but either required both traditional and event frames or overlooked the rich temporal information encoded in the event domain. In this paper, we propose a spiking transformer network, STNet, for single object tracking using only events as input. STNet directly extracts spatial and temporal information from the event positions, leveraging global spatial cues for temporal feature extraction. The proposed architecture combines a spiking neural network for temporal feature extraction and a reduced Swin-transformer for spatial information extraction. We demonstrate the effectiveness of STNet through extensive experiments on multiple datasets, outperforming state-of-the-art methods in terms of success rate, precision rate, and processing speed. Ablation experiments highlight the importance of each key component of the proposed approach. Our work also emphasizes the significance of a dynamic spiking threshold in object tracking tasks. Overall, this paper contributes a novel spiking transformer architecture for event-based single object tracking, demonstrating the importance of dynamic threshold adjustment and providing empirical evidence of the proposed approach's superior performance.