Automated understanding of remote sensing imagery has been a long-standing goal in computer vision. This field has contributed to various applications such as construction phase detection, infrastructure mapping, land use monitoring, post-natural disaster damage assessment, urban 3D reconstruction, population migration prediction, and climate change tracking. However, most existing methods require manual annotation efforts, which can be expensive and time-consuming. While satellite imagery has become more accessible, annotating land cover and change labels in remote sensing imagery is challenging due to the large number of smaller objects and unfamiliar viewpoints.Recently, self-supervised learning methods have emerged as a solution to reduce the reliance on labeled data. These methods leverage self-applied transformations or implicit metadata information to learn representations. Contrastive learning, in particular, has been successful in minimizing the distance between feature representations of original and transformed images. Nonetheless, these approaches have not been thoroughly investigated in the remote sensing domain, and even existing techniques that use contrastive approaches still show limitations in transferability to downstream task learning.In this paper, we propose a novel self-supervised material and texture representation learning method for remote sensing tasks. We argue that material and texture have a strong inductive bias that can improve performance and convergence speeds for these tasks. We provide evidence of this by demonstrating the effectiveness of our self-supervised pre-trained features in change detection, land cover segmentation, and land cover classification tasks.Our method draws inspiration from classical and modern texton filter banks, which describe micro-structures and material consistency in images. We define material as elements corresponding to multi-spectral signatures, structures as gradients in intensity, texture as the spatial distribution of structures, and surface as the combination of material and texture. To learn surface representations, we contrastively learn the similarity between residuals of unchanged regions in multi-temporal and spatially aligned imagery. This pre-training stage acts as a foundation for downstream remote sensing tasks.In summary, our contributions include: 1) a novel material and texture-based approach for self-supervised pre-training, which generates features with a high inductive bias for remote sensing tasks, 2) state-of-the-art performance in unsupervised and supervised change detection, semantic segmentation, and land cover classification using our pre-trained network, and 3) a curated dataset of multi-temporal, spatially aligned, and atmospherically corrected remote sensing imagery for self-supervised learning.