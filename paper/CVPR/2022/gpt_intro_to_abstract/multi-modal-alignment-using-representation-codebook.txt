This paper introduces a vision language (V&L) representation learning problem, which involves learning a unified feature embedding using both image and text signals. The goal of V&L pretraining is to align the feature spaces of different modalities and capture the interaction across modalities. Existing approaches have focused on either the alignment or fusion aspect of V&L pretraining. In this work, the authors propose a hybrid approach that aligns image and text features using a learnable codebook. The codebook serves as a bridge between the two modalities, enabling contrastive reasoning at the cluster level. To optimize the alignment, an optimal transport problem is solved to calculate the distance between each modality and the prototypes in the codebook. The codebook is learned alongside the feature encoders and is updated using a momentum distillation technique. Experimental results show that the proposed approach achieves competitive performance compared to state-of-the-art methods, even with smaller amounts of training data. The contributions of this work include a codebook-based approach for efficient vision-language alignment learning and a novel distillation algorithm that aids in both unimodal and crossmodal contrastive optimization.