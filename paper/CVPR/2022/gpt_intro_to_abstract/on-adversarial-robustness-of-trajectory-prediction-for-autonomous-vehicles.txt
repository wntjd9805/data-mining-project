Abstract:Autonomous vehicles (AVs) are revolutionizing transportation systems, relying on complex systems that include modules for perceiving obstacles, planning driving behaviors, and controlling the vehicle. Trajectory prediction, which forecasts future paths of nearby objects, is crucial for safe AV driving and impacts the planning module. While many studies propose trajectory prediction models based on deep neural networks, few evaluate the models from a security perspective or analyze their robustness against adversarial examples.To address this gap, we propose white/black box adversarial attacks on trajectory prediction. These attacks add minor perturbations to normal trajectories to maximize prediction errors. Unlike attacks on image/video classification, attacking trajectory prediction requires naturalness and obeys physical rules to avoid easy detection as anomalous by AVs. We also define multiple attractive attack dimensions for trajectory prediction, such as lateral and longitudinal deviations.By evaluating the proposed attacks on different prediction models and trajectory datasets, we demonstrate that adversarial perturbations can significantly increase prediction errors by around 150%. Many attacks cause predictions to deviate by more than half of the lane width, potentially leading to significant changes in AV navigation decisions. We analyze various factors affecting attack results and provide recommendations for improving trajectory prediction, including leveraging map information and driving rules. Additionally, we explore mitigation mechanisms through data augmentation and trajectory smoothing, reducing prediction errors under attacks by 28%.Our work highlights the need to evaluate the worst-case performance of trajectory prediction and raises safety concerns associated with natural yet adversarial trajectories. Our contributions include proposing the first adversarial attacks and robustness analysis specifically for trajectory prediction in AVs considering real-world constraints and impacts. We provide a comprehensive evaluation of adversarial attacks on different prediction models and trajectory datasets, along with exploring mitigation methods.