Under-display cameras (UDCs) are essential for achieving full-screen smartphones. However, the quality of UDC images is significantly diminished due to light loss and diffraction caused by the display panel in front of the camera. To address this issue, increasing the transparency of the panel has been suggested by reducing pixel density and modifying the layout of RGB sub-pixels. Nevertheless, UDC distortion remains unavoidable, particularly with screens of acceptable resolution. Furthermore, the presence of wires needed to drive the pixels exacerbates this distortion. Restoring the images received by UDCs to resemble images without the display in front is necessary. Recent methods have proposed the use of paired image datasets and image restoration with a neural network learning approach.Image restoration methods utilizing deep neural networks (DNNs) have made significant progress for various restoration tasks. However, most existing methods for UDC image restoration employ networks developed for other restoration tasks. Certain methods consider the physical processes impacting UDC images, taking the angle of incident light into account during inference. Nevertheless, errors in predicting the angle of incidence hinder the efficacy of this approach. While previous works have modeled UDC degradation in terms of diffraction and reduced intensity caused by display pixels, these models only capture the reduction in high-spatial-frequency components by diffraction and overlook other degradation processes like color shifts, spatial attenuation, transmission of wavelengths by thin-film layers, and long-range degradation caused by non-uniformity of those layers.In this paper, we propose a physical model of UDC image degradation that incorporates low-spatial-frequency degradation processes and introduce a DNN architecture to reverse the predicted changes in the image. Our proposed branched network effectively removes high-spatial-frequency noise and low-spatial-frequency degradation. We also present an affine transform connection to preserve image structure while reducing over-smoothing caused during restoration. Additionally, we propose an inverse color filtering technique to improve color fidelity in images with severe color shifts. Our contributions include a new model specific to OLED's optical properties, a DNN architecture with separate branches for high- and low-frequency restoration, and state-of-the-art performance on public datasets. Our network outperforms existing methods conditioned by a point-spread function and does not require it as a prior.