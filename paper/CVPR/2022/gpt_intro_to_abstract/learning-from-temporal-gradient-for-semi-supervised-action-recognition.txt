Video action recognition is a fundamental task in video understanding, but it is prone to overfitting due to the complexity of the tasks. Pre-training on large-scale datasets and fine-tuning on smaller datasets is a common practice. However, annotating large-scale video datasets is time-consuming and expensive, limiting the utilization of these datasets. Semi-supervised learning methods have been explored to leverage unlabeled data, but most methods treat video clips as 'images' without considering the video properties. This paper proposes a semi-supervised action recognition model that incorporates temporal gradient information to better utilize the temporal and multimodal properties of videos. Experimental results on UCF-101, HMDB-51, and Kinetics-400 datasets show that the proposed model outperforms state-of-the-art methods by a large margin. The model requires no additional computation or parameters for inference and achieves superior performance with a simple yet effective design.