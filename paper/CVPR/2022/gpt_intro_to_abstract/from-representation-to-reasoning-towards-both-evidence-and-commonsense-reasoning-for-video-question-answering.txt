Videos contain richer temporal and causal relations than simple images, presenting a challenge for computational models in reasoning from video clips. While recognizing independent actions or segmenting specific instances in videos has become relatively easy, performing reasoning tasks remains difficult. Human beings can easily answer reasoning questions from video clips, such as explaining why something is happening, predicting future events, and imagining scenarios under different conditions. However, current models struggle with these reasoning tasks. This paper aims to explore video reasoning by introducing the task of Causal-VidQA. The task requires models to answer questions related to scene description, evidence reasoning, and commonsense reasoning. The dataset, Causal-VidQA, contains 26,900 unique video clips and 107,600 question-answer pairs, making it the first large-scale dataset in this realm. The dataset focuses on evidence and commonsense reasoning in real-world actions, catering to the need for deeper video understanding. The paper evaluates different state-of-the-art VideoQA methods on the Causal-VidQA dataset and identifies areas where these models fall short in understanding causal relations and commonsense phenomena. The contributions of this paper are the exploration of evidence and commonsense reasoning in VideoQA, the introduction of the Causal-VidQA benchmark, and the evaluation and analysis of VideoQA methods on the dataset.