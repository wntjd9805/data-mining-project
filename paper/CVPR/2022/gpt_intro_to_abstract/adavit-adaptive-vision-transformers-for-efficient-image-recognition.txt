Transformers, the dominant architectures in natural language processing, have gained interest in the computer vision community with the success of the Vision Transformer (ViT). These architectures, built on self-attention mechanisms, effectively capture long-range dependencies among pixels or patches in input images. While self-attention-based vision transformers have outperformed convolutional neural networks (CNNs) in various vision tasks, their computational cost can be large.This paper investigates the necessity of attending to all patches throughout the network for accurate image classification. It questions whether all self-attention blocks with multiple heads are needed to model the underlying dependencies in different images, considering the variations in object shape, size, occlusion, and background complexity. The authors propose an adaptive computation framework called Adaptive Vision Transformer (AdaViT), which learns to determine the usage of patches, self-attention heads, and backbone layers on a per-input basis.AdaViT incorporates a decision network into each transformer block of the backbone network, which predicts binary decisions on the usage of patch embeddings, self-attention heads, and blocks. Gumbel-Softmax is used during training to make the framework trainable end-to-end. The decision network is jointly optimized with the transformer backbone using a usage loss and a cross-entropy loss, incentivizing the network to produce policies that reduce computational cost while maintaining classification accuracy. The overall computational cost can be controlled by a hyperparameter.Extensive experiments on ImageNet demonstrate that AdaViT improves the inference efficiency of vision transformers by more than 2x with only a 0.8% drop in classification accuracy. The method achieves good trade-offs between efficiency and accuracy compared to other standard vision transformers and CNNs. The paper also provides quantitative and qualitative analyses on the learned usage policies, offering insights into redundancy in vision transformers. Visualizations demonstrate that AdaViT allocates more computation to harder samples with complex scenes, and less computation to easier object-centric samples.