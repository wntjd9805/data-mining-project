This paper introduces a novel approach for the collaborative analysis of top-view and side-view videos captured by unmanned aerial vehicles (UAVs) and wearable cameras, respectively. The goal is to connect these two complementary views and enable comprehensive scene understanding and activity analysis. The paper addresses three challenging tasks: camera location identification, view direction estimation, and cross-view multiple human detection and association. The authors propose a spatial-aware deep framework that leverages the spatial position layout of the subjects in both views. They develop a camera identification module and a subject association module to infer the side-view camera location, view direction, and match subjects across the two views. The proposed method is evaluated on a large-scale synthetic dataset with rich annotations, and the experimental results demonstrate its effectiveness in handling the three tasks. The contributions of this paper include the first deep model to jointly handle these tasks, a spatial-aware deep framework, and a new dataset for training and evaluation purposes.