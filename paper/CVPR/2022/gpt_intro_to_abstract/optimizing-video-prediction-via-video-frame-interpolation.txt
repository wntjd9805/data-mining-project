Video prediction is a challenging task that involves predicting future video frames based on past frames. It has wide-ranging applications in robotics planning, autonomous driving, and video manipulations. Traditional approaches to video prediction often rely on RGB frames as input and struggle with the complexity and uncertainty of future states. To simplify the problem, some methods introduce additional assumptions and constraints, such as semantic maps or depth maps, but these assumptions limit their generality and applicability in diverse real-world scenarios. Furthermore, models trained with these assumptions typically lack the ability to generalize to other scenarios, making them less flexible.To address these limitations, we propose an optimization-based video prediction method that does not require external training or any specific assumptions about the scene. Our approach leverages the concept of video frame interpolation (VFI) to tackle the video prediction problem in a new way. By connecting these two problems, we can achieve state-of-the-art results without relying on external datasets or prior knowledge about the scene.Our method offers several advantages. First, it is highly flexible and can be applied to video prediction in any scene at any resolution, without the need for semantic or instance maps. Second, it outperforms existing video prediction approaches that rely on additional information, surpassing them by a large margin. Lastly, our method achieves outstanding performance across multiple datasets, demonstrating its effectiveness and versatility.In summary, our work presents the first optimization framework for video prediction, casting the problem as a VFI-based optimization task. Our approach eliminates the need for external training or specific assumptions about the scene, offering a flexible and powerful solution for video prediction in diverse real-world scenarios.