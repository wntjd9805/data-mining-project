Text-based video segmentation is a challenging task that requires fine-grained multi-modal and temporal understanding to locate and segment objects described in a language sentence within a video sequence. Previous works have focused on reasoning between visual and linguistic modalities and leveraging temporal information for segmentation. However, they fail to explore explicit motion information between frames, which can be crucial for this task. In this paper, we propose a multi-modal fusion and alignment network that effectively incorporates motion information from optical flow maps with appearance and linguistic features. Our network consists of a multi-modal video transformer (MMVT) that models the interaction between appearance, motion, and linguistic features, eliminating the need for computationally expensive 3D CNNs. We also introduce a language-guided feature fusion (LGFF) module to progressively fuse multi-modal features and a multi-modal alignment loss to better align features from different modalities. Experimental results show that our approach outperforms existing state-of-the-art methods on A2D Sentences and J-HMDB Sentences datasets while reducing computational overhead. This work represents the first attempt to incorporate motion information with appearance and linguistic features for text-based video segmentation.