The paper introduces the concept of dynamic re-parameterization (DyRep) as a method to enhance the performance of deep convolution neural networks (CNNs) without incurring expensive computational overhead and high inference complexity. Previous re-parameterization techniques, such as RepVGG and DBB, have shown success in expanding network structures during training and then transforming them back to the original model for inference. However, these methods often utilize the same branches in all layers, leading to suboptimal structures and increased memory and computation cost. To address these issues, the authors propose DyRep, which dynamically evolves network structures during training and recovers them to the original network in inference. The key concept behind DyRep is adaptively seeking the operations with the biggest contributions to performance, rather than universal re-parameterization. This is accomplished by calculating saliency scores based on the gradients w.r.t. the loss. Furthermore, the authors extend the Rep technique to enable stable training with multiple branches and introduce a de-parameterization method to identify and discard redundant operations. By initializing the additional branches with small scale factors in batch normalization layers, the authors ensure a smooth structure evolution. If a branch has a zero scale factor, its operations are discarded. The main contributions of the paper are outlined as follows: 1. Introducing DyRep, a dynamic re-parameterization method that achieves significant efficiency and performance improvement by identifying important operations dynamically during training.2. Highlighting the flexibility of DyRep in downstream tasks, such as object detection, without the need for pre-training on an image classification task.3. Conducting extensive experiments that demonstrate the superior performance of DyRep compared to other re-parameterization methods in terms of both accuracy and runtime cost.