This paper aims to develop an interactive understanding of objects by learning state-sensitive features and identifying object affordances. The authors propose using egocentric videos to observe human hands interacting with objects, as this provides natural and informative data for learning. To achieve this, the authors design techniques to extract an understanding of objects from the observation of hands using off-the-shelf models. For learning state-sensitive features, the authors use contrastive learning based on hand appearance and motion to derive supervision for the object state. For inferring object affordances, the authors incorporate hand grasp-type predictions and train a model to focus on the object rather than the hand using a context prediction task. By deriving supervision from hands, the authors avoid the need for semantic supervision and achieve better recall and more specific predictions for interaction sites and hand-grasps. Overall, this research provides valuable insights and techniques for acquiring an interactive understanding of objects.