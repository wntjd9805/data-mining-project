Mobile robots need to be able to understand and carry out tasks given in human instructions rather than machine-readable scripts. One challenging task is navigating from one location to another based on semantic instructions, such as "go to the kitchen sink". Previous approaches to vision-and-language navigation (VLN) tasks have used end-to-end pipelines that learn to map images and instructions to actions. However, this approach requires large amounts of training data and does not align with how humans navigate spatially. Research has shown that humans build cognitive maps and spatial representations of their environment for wayfinding. Additionally, studies have shown that humans navigate more effectively when given landmark-based instructions rather than full paths. In this paper, we propose a novel navigation system called Cross-modal Map Learning (CM2) for the VLN task in continuous environments. CM2 uses cross-modal attention networks to semantically and spatially ground the instruction on egocentric maps. Through a two-stage process, CM2 learns to hallucinate information outside the agent's field-of-view and predict the path on the egocentric map. The attended representations in CM2 focus on instruction-relevant objects and locations on the map. Unlike existing approaches that generate actions based on image-language attention, CM2 builds a cognitive map that encodes environmental priors and follows instructions based on this map. When given a local ground-truth map of the environment, CM2 outperforms other approaches in the VLN task. CM2 also learns layout priors through cross-modal attention, leveraging spatial and semantic descriptions from natural language. In contrast to previous work, CM2 predicts the entire trajectory rather than a single waypoint. The waypoints are determined by the alignment between language and egocentric maps. The contributions of this paper include a novel system for the VLN task that learns explicit maps as an intermediate representation, semantic grounding of language to these maps through cross-modal attention, spatial grounding of instructions through cross-modal attention on semantic maps and language, an analysis of the learned representation, and competitive results on the VLN-CE dataset compared to existing state-of-the-art methods.