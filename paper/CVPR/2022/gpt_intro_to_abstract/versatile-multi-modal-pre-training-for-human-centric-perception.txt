This paper addresses the problem of human-centric perception, which involves tasks such as action recognition, keypoints detection, and pose estimation. The authors argue that a versatile pre-training model is needed to support these various tasks. They propose a multi-modal approach that leverages different modalities, such as RGB, depth, and infrared, to create effective representations that can transfer well to different downstream tasks. The authors explore two groups of modalities: dense representations, which provide rich texture and geometry information, and sparse representations, which are semantic but lack detail. They highlight two main challenges in integrating these modalities: learning representations suitable for dense prediction tasks and effectively using weak priors from sparse representations. To address these challenges, the authors propose a dense intra-sample contrastive learning objective for dense targets and a sparse structure-aware contrastive learning target for sparse priors. The proposed framework, called HCMoCo, achieves better performance than training from scratch or pre-training on ImageNet in several human-centric tasks, including DensePose estimation, human parsing, and 3D pose estimation. Additionally, the authors evaluate the modal-invariance of the learned latent space and provide an RGB-D human parsing dataset, NTURGBD-Parsing-4K. The contributions of this work include an in-depth analysis of human-centric pre-training, the proposed HCMoCo framework, superior performance compared to existing methods, and the new dataset. Overall, this paper presents a comprehensive approach to multi-modal pre-training for human-centric perception tasks.