Object pose estimation in 6 degrees of freedom (DoF) is crucial for various applications in computer vision and robotics, such as autonomous driving, robotic navigation, manipulation, and augmented reality. Different methods have been proposed, some utilizing RGB input while others incorporate depth information. Single-view and multi-view approaches have also been explored, with multi-view methods further categorized into offline structure from motion (SfM) and online SLAM styles.This paper focuses on image-based 6DoF pose estimation for multiple objects within an online monocular SLAM system. A typical multi-view pose estimation method consists of a single-view estimation stage and a multi-view enhancement stage. While fusion of pose estimates from multiple views can enhance performance, handling extreme inconsistency caused by objects with rotational symmetry remains challenging. Manually tuning thresholds for outlier rejection and assigning residual weights for optimization are unreliable.To address these challenges, this paper proposes a symmetry and uncertainty-aware 6DoF object pose estimation method that combines semantic keypoint measurements within a SLAM framework. The main contributions of this work are:1. Design of a keypoint-based object SLAM system that estimates globally-consistent object and camera poses in real-time, even in the presence of incorrect detections and symmetric objects.2. Development of a method to consistently predict and track 2D semantic keypoints for symmetric objects by leveraging the projection of existing 3D keypoints as informative prior input to the keypoint network.3. Introduction of a method to train the keypoint network to estimate uncertainty, enhancing the object pose estimation in the SLAM system.The rest of the paper is organized as follows: a brief review of related literature is presented in Section 2, followed by a detailed explanation of the proposed method in Section 3, including the keypoint detector and its integration into the system. The framework is thoroughly evaluated in Section 4, and the paper concludes in Section 5.