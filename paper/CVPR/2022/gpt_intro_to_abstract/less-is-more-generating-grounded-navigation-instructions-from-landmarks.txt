This paper focuses on the automatic generation of navigation instructions for wayfinding in indoor environments. While template-based language generators are commonly used in outdoor mapping applications, generating instructions for indoor navigation requires egocentric movement guidance and reference to the visual environment. Previous systems for generating indoor wayfinding instructions relied on pre-existing floorplans and landmark databases, but recent work aims to generate instructions directly from visual inputs. The goal is to generate accurate and fluent navigation instructions in multiple languages from visual representations and actions taken to traverse a path. This research seeks to address the limitations of existing instruction generation models, which struggle to provide instructions that are easy for humans to follow. The paper introduces a two-stage approach that involves landmark detection and instruction generation based on these landmarks. The system, called MARKY-MT5, is trained using a dataset that includes silver landmark annotations and over one million generated navigation instructions. Experimental results show that MARKY-MT5 trained with silver landmarks achieves a success rate of 71%, significantly reducing the gap between model-generated and human-written instructions. This research has implications for the development of conversational navigation aids and human-robot communication.