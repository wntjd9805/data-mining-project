In this paper, we address the problem of incrementally learning novel classes in deep classifiers, particularly in scenarios where there is a large number of base classes. We highlight the drawbacks of training classifiers with both old and novel data, as the imbalance between these datasets can lead to catastrophic forgetting for the old classes. We discuss existing class-incremental learning (CIL) methods that typically start with a small number of base classes and add new classes incrementally. However, in practical scenarios, starting with a larger number of base classes can be more beneficial. We also examine the limitations of current CIL approaches, including the use of static models and the risk of performance degradation when modifying well-trained network weights. Additionally, we discuss dynamic models that learn separate parameters for novel tasks, but note that these methods are primarily focused on task-incremental learning (TIL) and do not address the distinction between base and novel classes. To overcome these limitations, we propose a 2-stage training scheme for CIL. In the first stage, we duplicate a portion of the backbone network as an adaptation module and fine-tune it on the novel data, instead of fine-tuning the entire backbone. In the second stage, we combine the independently trained base and novel classifiers into a unified classifier at each incremental step using a score fusion network. This network enables knowledge transfer between base and novel classes by combining their logits. We demonstrate the effectiveness of our approach through experiments, showing that fine-tuning fewer layer blocks outperforms full fine-tuning when starting with a strong pre-trained model. Furthermore, we examine a more general setting where some novel classes can overlap with base classes, potentially with different distributions. We show how our score fusion algorithm handles overlapping classes by using a knowledge pooler to combine the base and novel logits. In summary, our contributions include the proposal of a 2-stage CIL training strategy, the introduction of a score fusion algorithm for unifying classifiers, and the generalization of CIL to a broader scenario with overlapping classes and changed distributions.