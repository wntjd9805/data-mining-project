Depth-aware panoptic segmentation (DPS) is a challenging task in scene understanding, aimed at creating a 3D scene with instance-level semantic understanding from a single image. This involves assigning depth values, semantic labels, and instance IDs to each pixel. Previous approaches to DPS have used independent branches to handle monocular depth estimation and panoptic segmentation separately, but they fail to leverage the mutually-beneficial relations between the two tasks. We propose PanopticDepth, a unified model that predicts depth and instance masks in the same instance-wise manner. Instead of predicting depth values for all pixels at once, our approach estimates depth for each thing/stuff instance using dynamic convolution. We generate instance-specific mask and depth kernels concurrently, apply them to the corresponding embeddings, and produce the mask and depth map for each instance. We then merge individual instance masks into a panoptic segmentation map and aggregate each instance's depth into a whole depth map. Our method improves performance on both depth estimation and panoptic segmentation tasks by leveraging instance-specific convolution kernels, which incorporate global and local information into depth prediction, especially at instance boundaries. To ease depth estimation, we represent each instance depth map as a triplet, including depth values, a depth range, and a depth shift, and normalize the values to [0, 1]. We also propose instance-level depth statistics and a corresponding depth loss to enhance depth supervision. Experimental results on Cityscapes-DPS and SemKITTI-DPS datasets demonstrate the effectiveness of our unified approach for depth-aware panoptic segmentation, with the potential to lead a new paradigm in this challenging task.