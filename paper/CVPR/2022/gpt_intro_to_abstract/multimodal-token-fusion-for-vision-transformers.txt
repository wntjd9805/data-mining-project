In this paper, we address the challenge of extending vision transformers to handle multimodal data with complex alignment relations. While existing methods for vision-language fusion simply concatenate vision and language tokens, they do not explicitly utilize inter-modal alignments and often result in suboptimal performance. Additionally, methods for fusing multiple vision modalities neglect to incorporate inter-modal alignments into transformers. To overcome these limitations, we propose TokenFusion, a fusion scheme that adaptively and effectively fuses multiple single-modal transformers. TokenFusion prunes individual units in each single-modal transformer and replaces them with projected alignment features from other modalities. This approach retains the relative attention relations of important units and allows multimodal transformers to inherit parameters from single-modal pretraining. We evaluate TokenFusion on multimodal image translation, RGB-depth semantic segmentation, and 3D object detection tasks, achieving state-of-the-art performance on various benchmarks. Specifically, TokenFusion achieves 64.9% and 70.8% mAP@0.25 for 3D object detection on the SUN RGB-D and ScanNetV2 benchmarks, respectively. Our results demonstrate the effectiveness and generality of TokenFusion in handling multimodal data with inter-modal alignments.