Estimating 3D scenes from a single image has long been a challenge in computer vision. While humans can effortlessly understand the geometry and semantics of a scene from a single glance, developing algorithms to do the same has proven to be complex. While depth sensors like Lidar or depth cameras have been used to aid in 3D estimation, they are often expensive and not as readily available as cameras found in everyday devices like smartphones and drones. This motivates the need for algorithms that can estimate 3D scenes from just an RGB image, opening up new possibilities for applications.In this paper, we propose MonoScene, a novel framework that can infer dense 3D semantic scenes from a single RGB image, regardless of whether it is an indoor or outdoor scene. Unlike existing methods that rely on depth data, MonoScene projects 2D features along their line of sight, inspired by optics, thereby bridging the gap between 2D and 3D networks. This approach allows the 3D network to discover relevant 2D features on its own. Furthermore, we introduce new losses that optimize the semantic distribution of groups of voxels, both globally and in local frustums, aiming to improve context awareness in the network.To enhance context understanding, we design a 3D context layer that provides the network with a global receptive field and insights into the semantic relations between voxels. Extensive testing of MonoScene on indoor and outdoor scenes demonstrates its superiority over comparable baselines and even some 3D input baselines.The main contributions of this work include MonoScene, the first SSC method capable of handling both indoor and outdoor scenes from a single RGB image, as well as the mechanisms of 2D Features Line of Sight Projection (FLoSP) and the 3D Context Relation Prior (3D CRP) layer. Additionally, we introduce new SSC losses that optimize scene-class affinity and local frustum proportions.Overall, our work showcases a promising approach for inferring dense 3D semantic scenes from a single RGB image, with potential applications in a variety of fields such as robotics, augmented reality, and autonomous driving.