Large-scale pre-training of vision and language transformers has resulted in significant improvements in various downstream tasks. Contrastive methods such as CLIP and ALIGN have demonstrated that natural language supervision can lead to high-quality visual models for transfer learning. However, these methods have limitations when it comes to multimodal problems that require dealing with multiple modalities simultaneously. They also rely on large corpora that are not accessible to the research community. In contrast, there are transformer models explicitly designed for the multimodal vision-and-language domain, but they often neglect the performance of the unimodal vision-only or language-only aspects. To address these limitations, we propose FLAVA, a language and vision alignment model that learns strong representations from multimodal and unimodal data. FLAVA employs a common transformer model architecture and targets visual recognition, language understanding, and multimodal reasoning tasks. By combining information from different modalities into a universal architecture, FLAVA aims to improve sample efficiency and generate richer representations. We validate FLAVA by applying it to 35 tasks across vision, NLP, and multimodal domains, showcasing impressive performance. Importantly, our model is trained on a smaller corpus of openly available datasets compared to other similar models. The FLAVA models and code are publicly available for further exploration.