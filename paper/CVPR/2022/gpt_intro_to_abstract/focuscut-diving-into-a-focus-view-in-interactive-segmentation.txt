Interactive image segmentation is a vital tool in pixel-level data annotation and image editing. This paper focuses on improving user interaction efficiency and effectively utilizing user-provided interaction. Various interactive modes, such as bounding boxes, polygons, clicks, and scribbles, have been explored. Click-based methods have become mainstream due to their simplicity. Researchers have also investigated interaction ambiguity, input information, and backpropagation to improve segmentation results without modifying user input. In high-precision interactive segmentation, refining object details often requires more interactive clicks and time. However, current methods consider all previous clicks together, which can result in disagreeable results when users focus on specific regions for repair. To address this, the paper proposes a concise pipeline called FocusCut that introduces the concept of the focus view. This pipeline allows the segmentation network to not only segment the target object but also repair local details. The progressive cropping scopes in the focus view are adjusted dynamically based on prediction variation in the global view. The paper conducts comprehensive experiments on various datasets to validate the effectiveness of FocusCut. The contributions of this research include introducing the focus view to capture user intentions, proposing the FocusCut pipeline for local refinement without additional parameters, and achieving state-of-the-art performance in fine segmentation.