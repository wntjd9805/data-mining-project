Recent mobile devices, such as the Apple iPhone and Samsung Galaxy series, are equipped with multiple asymmetric multi-cameras with different focal lengths. This configuration offers advantages in terms of capturing higher-resolution frames and finer details. However, it has not been explored in the field of reference-based video super-resolution (RefVSR). This paper introduces the concept of RefVSR, which utilizes a reference video to reconstruct a high-resolution video from a low-resolution video in an asymmetric multi-camera setting. The unique relationship between low-resolution and reference frames in multi-camera videos is considered, where the frames share similar content in their overlapped field of view (FoV). The proposed RefVSR framework combines the objectives of RefSR and VSR and leverages temporal reference frames to reconstruct regions both inside and outside the overlapped FoV. An end-to-end learning-based RefVSR network is presented, which adopts a bidirectional recurrent pipeline to align and propagate reference features with low-resolution frames. A propagative temporal fusion module is introduced to effectively fuse and propagate well-matched reference features. The proposed framework is trained and evaluated on the RealMCVSR dataset, consisting of video triplets captured with triple cameras of a smartphone. A two-stage training strategy is proposed to fully utilize the video triplets in the dataset. Experimental results demonstrate the effectiveness of the proposed RefVSR framework in achieving high-fidelity video super-resolution. The contributions of this work include the introduction of the RefVSR framework for asymmetric multi-camera settings, the propagative temporal fusion module, the RealMCVSR dataset, and a two-stage training strategy for real-world video super-resolution.