Deep neural networks (DNNs), particularly deep convolutional neural networks, have led to significant advances in computer vision. While DNNs excel at tasks like image recognition and segmentation, their confidence in predictions is often poorly calibrated. Overconfident predictions can lead to incorrect classifications. For various real-world applications, it is crucial for a classifier to not only provide accurate predictions but also sound estimates of confidence. This is known as calibration. Calibrated classifiers are able to provide accurate posterior probabilities for each class, enabling better decision making. The calibration of DNNs has gained attention in the computer vision and machine learning communities.In this paper, we propose a novel approach called calibration by pairwise constraints (CPC) to improve the calibration of DNNs. We show that by providing calibration supervision to all class pairs, the degree of supervision is significantly increased over the traditional cross-entropy training. We establish that the multiclass posterior probability estimators are calibrated if and only if all the derived binary posterior probability estimators are calibrated, providing a theoretical grounding for CPC.We demonstrate that the efficiency of cross-entropy training can be improved by calibrating the binary posterior estimations using two types of losses. For class pairs that involve the true class, the binary cross-entropy loss is used to encourage high probability for the true class and low probability for the opposite class. For the remaining class pairs, an alternative loss is used to encourage uncertain predictions, outputting the same posterior probability for both classes.CPC can be implemented with high computational simplicity, as the additional losses can be computed by adding sigmoid functions at the top of the network. Empirical evaluations show that CPC achieves state-of-the-art calibration performance across multiple datasets and DNN architectures, with minimal increase in training complexity and no increase in memory or time complexity during testing.This work contributes by highlighting the limited supervision provided by cross-entropy loss as a reason for poor calibration in DNNs. It also proposes and provides theoretical support for the CPC approach. Furthermore, it demonstrates that CPC significantly improves calibration performance, complementing existing approaches and achieving state-of-the-art results.