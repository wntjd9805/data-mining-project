This paper introduces a novel architecture called GroupViT, which combines visual grouping and recognition in deep networks for semantic segmentation tasks. Unlike traditional approaches that rely on per-pixel human labels, GroupViT leverages only text supervision and learns to automatically group visual concepts into irregular-shaped segments. The model is trained on large-scale paired image-text data using contrastive losses, enabling zero-shot transfer to different semantic segmentation vocabularies without further annotation or fine-tuning. The GroupViT model consists of hierarchical Transformer layers and a grouping module that merges smaller segments into larger ones. During inference, GroupViT extracts visual groups from input images and assigns category labels to image segments based on their similarity in the embedding space. Experimental results demonstrate that GroupViT achieves competitive performance on semantic segmentation tasks without the need for pixel-wise labels, establishing a strong baseline for zero-shot transfer from text supervision alone. The contributions of this work include the introduction of the GroupViT architecture, the successful application of text supervision for visual grouping and semantic segmentation, and the establishment of a baseline for this new task.