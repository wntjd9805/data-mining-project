Vision architectures based on convolutional neural networks (CNNs) and transformers have shown significant advancements in computer vision tasks. The analysis of input signals at multiple resolutions has been a recurring idea in classical methods and deep learning approaches. Multiscale processing in the image domain typically involves pyramid structures to capture isotropic statistics and shift invariance. Previous approaches for modeling multiscale temporal information in videos have utilized multiple streams or graph neural networks. However, pyramidal structures and subsampling operations lead to the loss of spatio-temporal information. In this paper, we propose a simple transformer-based model that leverages multiple input representations or "views" of the video to capture multi-resolution temporal context. Our proposed multiview transformer consists of separate transformer encoders specialized for each view, with lateral connections to fuse information. We find that using smaller encoders for broader views and larger encoders for finer details provides better accuracy and computation trade-offs compared to pyramid-based approaches. Our method generalizes readily to a variable number of views and achieves superior accuracy across a spectrum of model sizes. We demonstrate the advantages of processing more views in parallel rather than increasing network depth. Through comprehensive ablation studies, we validate the efficacy of our design choices and achieve state-of-the-art results on six video classification datasets. Furthermore, we show that our results can be further improved with large-scale pretraining.