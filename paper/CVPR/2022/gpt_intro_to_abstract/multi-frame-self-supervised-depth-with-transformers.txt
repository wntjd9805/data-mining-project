This paper focuses on the task of self-supervised feature matching, specifically in the context of monocular depth estimation. Self-supervised methods enable learning without explicit ground-truth by using view synthesis losses obtained through the warping of information from one image onto another, obtained from multiple cameras or a single moving camera. These methods can leverage unlabeled data, achieving performance comparable to supervised methods and enabling new applications. Single-frame self-supervised methods use multi-view information only at training time, while multi-frame methods utilize multi-view information at inference time. Multi-frame methods outperform single-frame methods by learning both appearance-based and geometric features. However, feature matching in these methods relies solely on image information, resulting in noisy and inaccurate correspondences due to various factors such as lack of texture or dynamic objects.To address this issue, the proposed architecture, called DepthFormer, improves self-supervised feature matching for monocular depth estimation. The architecture includes a cost volume built between target and context image features using differentiable depth-discretized epipolar sampling. Additionally, an attention-based mechanism is introduced to refine per-pixel matching probabilities, resulting in sharper and more representative probabilities of the underlying 3D structure. The multi-frame cost volume is then converted into depth estimates using high-response window filtering, combined with single-frame features from a separate network to account for failure cases. Extensive experiments demonstrate that the feature matching refinement module in DepthFormer achieves state-of-the-art results in self-supervised depth estimation, outperforming existing multi-frame methods and even surpassing supervised single-frame architectures. Furthermore, the learned attention-based matching function is transferable across datasets, leading to improved convergence speed and decreased memory usage. The main contributions of this paper are the introduction of the DepthFormer architecture, its superior depth estimation performance, and the transferability of the attention-based matching function across datasets.