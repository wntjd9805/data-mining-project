Deep neural networks have achieved impressive performance on computer vision tasks by utilizing large-scale datasets. One important result of pre-training these networks is the ability to reuse features through fine-tuning the learned weights for downstream tasks. However, real-world deep learning pipelines often involve sequential fine-tuning of large models on independent tasks, leading to issues such as catastrophic forgetting and drift. Continual learning provides a framework for modeling this scenario, but there is limited research on applying it to video data. This paper introduces vCLIMB, a novel benchmark for continual learning in video, specifically focusing on activity recognition. Existing video continual learning methods suffer from variability in experimental protocols, lack of publicly available datasets, and unrealistic setup with trimmed videos. vCLIMB addresses these limitations by providing a fixed task split on three popular video datasets – UCF101, ActivityNet, and Kinetics – and including modifications to better fit the nature of video data. It introduces the concept of memory frame capacity to account for storing frames rather than video instances and explores the use of trimmed and untrimmed video data in continual learning.The paper establishes baselines by adapting image-based continual learning methods to the video domain using vCLIMB's data splits. It also proposes a novel strategy based on consistency regularization to improve memory consumption and performance in continual action recognition.Overall, vCLIMB provides a standardized approach to prototyping and evaluating video continual learning, paving the way for future research in this area. The contributions of this work include the benchmark itself, the evaluation of baseline methods, and the introduction of a novel strategy for video continual learning.