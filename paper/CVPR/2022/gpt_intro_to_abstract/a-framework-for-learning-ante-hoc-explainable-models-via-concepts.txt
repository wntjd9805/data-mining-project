Recent years have witnessed a growing interest in the explainability of Deep Neural Network (DNN) models across various domains. Existing efforts in computer vision have focused on post hoc analysis, which separates the explanation from the prediction of the DNN model. However, this separation can make it difficult to determine the reliability of the explanation. In response to this challenge, ante hoc methods have been proposed to jointly learn to explain and predict, leading to inherently interpretable models. This paper proposes a new method for ante hoc explanations via concepts. The proposed method can be easily integrated with existing backbone classification architectures with minimal additional parameters. It can provide explanations for model decisions in terms of concepts for individual input images or groups of images. Importantly, the method can work with different levels of supervision, including no concept-level supervision at all. The proposed model is evaluated through comprehensive experiments on multiple benchmark datasets, showcasing its accuracy and explainability. The method outperforms existing approaches in terms of accuracy and explainability metrics, with negligible computational overhead. The key contributions of this work include the introduction of a simple and effective method for ante hoc explanations through concepts, the ability to learn concepts with different levels of supervision, and the introduction of a concept intervention metric for evaluating ante hoc explainable models.