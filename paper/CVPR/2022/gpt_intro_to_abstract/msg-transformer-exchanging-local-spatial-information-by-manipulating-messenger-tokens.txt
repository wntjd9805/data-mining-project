In the past decade, convolutional neural networks (CNNs) have dominated the computer vision community as popular models in deep learning. However, CNNs hinder communication between distant features. To address this limitation, researchers propose replacing convolutions with the Transformer module, which has been successful in natural language processing. Transformers have demonstrated the ability to learn visual representations and achieve remarkable success in various visual recognition problems. The Transformer module uses tokens to formulate features at each spatial position and allows tokens to exchange information with others at every layer.While the Transformer module facilitates faster information exchange, it also increases computational complexity. To reduce computational costs, researchers propose computing attention in local windows of 2D visual features. However, constructing local attention within overlapped regions leads to memory waste and computation cost, while computing attention within non-overlapped regions impedes information communication. Existing local-attention vision Transformer methods, such as HaloNet and Swin Transformer, achieve competitive performance but suffer from drawbacks.To address these issues, this paper presents a new methodology that involves constructing a messenger (MSG) token in each local window. The MSG token summarizes information in the corresponding window and facilitates information exchange with other MSG tokens. This design is implementation-friendly and flexible, allowing the construction of various architectures for different purposes. The proposed architecture, named MSG-Transformer, integrates the Transformer with MSG tokens in a multi-scale design, combining multi-level feature extraction with computational efficiency.The paper instantiates MSG-Transformer by shuffling and reconstructing features of MSG tokens from different locations, effectively exchanging information between local regions. The model is evaluated on both image classification and object detection tasks and achieves promising performance. The contributions of this work include:- Proposing a local-attention based vision Transformer with hierarchical resolutions that computes attention in non-overlapped windows and achieves communication between windows through the proposed MSG tokens.- Introducing the shuffle operation, which enables efficient information exchange between MSG tokens.- Demonstrating the promising results of MSG-Transformers in image classification and object detection tasks, surpassing the state-of-the-art Swin Transformer in performance and showcasing speed advantages, particularly on CPU devices.- Proposing the use of lightweight MSG tokens for information exchange, which has the potential to be applied in other scenarios.Overall, the proposed methodology offers an efficient exchange of information in vision Transformers, alleviating computational burdens and demonstrating superior performance in visual recognition tasks. The work opens avenues for future explorations and applications of vision Transformers.