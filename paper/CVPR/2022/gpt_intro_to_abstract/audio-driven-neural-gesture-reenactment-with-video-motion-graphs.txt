Gesture is an essential visual component of human speech communication. It enhances expressiveness and aids in better comprehension of speech content. As technology advances in talk-ing head generation, synthesizing realistic gesture videos has become increasingly important for applications like digital voice assistants and photorealistic virtual avatars. This paper proposes an audio-driven gesture reenactment system that synthesizes speaker-specific human speech videos using a target audio clip and a single reference speech video. While previous methods focused on specific mappings between phonemes and lip motions or low-frequency sentimental signals and facial expressions, gestures have complex relationships with acoustics and audio semantics. Bridging the gap between audio and video synthesis is challenging, especially for the same speaker. Previous approaches have predicted body pose as an intermediate representation, but they often result in noticeable artifacts and distorted body parts.To address these challenges, this paper introduces a video reenactment method that directly synthesizes high-resolution, high-quality speech gesture videos in the video domain. The process involves cutting, reassembling, and blending clips from a single reference video. A novel video motion graph, inspired by 3D motion graphs used in character animation, is used to drive this synthesis. The graph's nodes represent frames in the reference video, and edges encode possible transitions between them. Valid transitions and paths in the graph are discovered to ensure coherence and consistency with the target audio's rhythms and speech content.However, directly playing back discovered paths can lead to temporal inconsistencies at frame boundaries. To address this issue, a human pose-aware video blending network is proposed to smoothly blend frames around these boundaries, resulting in natural-looking video transitions. This approach transforms the audio-driven gesture reenactment problem into a search for valid paths that best match the given audio.The path discovery algorithm is motivated by psychological studies on co-speech gesture analysis, categorizing gestures into rhythmic and referential types. Rhythmic gestures are synchronized with audio onsets, while referential gestures co-occur with certain phrases. By analyzing the reference video's speech and detecting audio onset peaks and keywords, the optimal paths that match audio features are used to drive video synthesis.The contributions of this paper include a system that generates high-quality human speech videos with realistic gestures driven by audio only, a novel video motion graph that preserves realism and subtleties, a pose-aware video blending neural network for smooth transitions, and an audio-based search algorithm that aligns synthesized gesture frames with audio rhythms and speech content.