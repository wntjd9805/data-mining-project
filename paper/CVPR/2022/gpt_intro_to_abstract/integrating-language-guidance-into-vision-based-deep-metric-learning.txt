Visual similarity learning with deep networks has become increasingly important in various applications such as image retrieval, face verification, clustering, and representation learning. Deep Metric Learning (DML) is commonly used to contextualize visual similarities by learning metric representation spaces that have a strong connection to the semantic similarity of two samples. However, most DML methods rely solely on class labels in the training data, which limits their ability to capture high-level semantic connections between different classes. To address this limitation, we propose leveraging pretrained natural language models to provide contextualization for class labels and improve the consistency of visual representation spaces. This approach, referred to as language guidance (LGC), involves computing language embeddings and similarities based on natural language class names, which are then used to correct and rearrange the visual embedding relations learned by DML methods. To avoid the need for additional supervision, we also introduce pseudolabel guidance (PLG), which utilizes ImageNet pseudolabels obtained from the commonly used ImageNet pretraining in DML pipelines. By re-embedding these pseudolabels into pretrained language models, we can access generic pseudolabel similarities that can be used for language guidance. Extensive experiments demonstrate that our proposed approach significantly improves the generalization performance of DML models while maintaining negligible overhead in training time. This work sets a new state-of-the-art in visual similarity learning and highlights the benefits of incorporating language context.