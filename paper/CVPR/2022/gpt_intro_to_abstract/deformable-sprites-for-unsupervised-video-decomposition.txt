This paper introduces a new approach for decomposing videos of complex dynamic scenes into sets of persistent motion groups. Inspired by classic methods, the approach utilizes Deformable Sprites as a representation of motion groups across the entire video. The Deformable Sprites consist of a canonical texture image, masks locating the group in each frame, and a non-rigid geometric transformation. The decomposition is derived solely from image and motion cues in the video, without the need for user input or training on a dataset. The approach can handle videos with moving cameras and articulated or deformable objects. It models non-rigid motion using a composition of a homography with 2D spatial splines. The sprites and masks are optimized through a convolutional neural network. Compared to recent approaches, the method recovers persistent layer appearances and does not require user inputs or segmentation masks. It achieves competitive results on video object segmentation benchmarks and demonstrates the discovery of meaningful groupings in Internet clips without user supervision. This work is the first to demonstrate video decomposition with a global texture model on in-the-wild videos.