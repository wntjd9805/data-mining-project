Deep Neural Networks (DNNs) have achieved remarkable success in computer vision tasks. However, they are vulnerable to adversarial examples, posing safety risks for their practical deployment. In the black-box setting, where only the input-output feedback of the victim model is accessible, attacking DNNs becomes challenging. This paper introduces a triple-player framework to train a surrogate model for black-box adversarial attacks. The framework incorporates a discriminator to enhance the training efficiency by limiting the searching space of the generator. The paper also highlights the importance of enlarging inter-class similarity rather than diversity in synthesizing effective data. A novel loss function is proposed to achieve this goal. Additionally, a loss function is specifically designed to boost intra-class diversity. The proposed method demonstrates competitiveness even in scenarios with little semantic overlap between scarce proxy data and training data. Experimental results show the effectiveness of the method in terms of attack success rate and boundary loss.