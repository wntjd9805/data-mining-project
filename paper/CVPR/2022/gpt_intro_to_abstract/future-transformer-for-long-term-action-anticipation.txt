Long-term action anticipation from videos has become a crucial task for advanced intelligent systems. Unlike most existing research that focuses on predicting a single action in a short period of time, long-term action anticipation aims to predict a sequence of multiple actions lasting several minutes. This task is challenging as it requires capturing long-range dependencies between past and future actions. While current methods encode observed video frames and use recurrent neural networks (RNNs) to predict future actions, they have limitations in preserving fine-grained temporal relations, modeling long-term dependencies, and considering global relations between past and future actions. To address these limitations, we propose a new method called Future Transformer (FUTR), an end-to-end attention neural network that effectively captures long-term relations over the entire sequence of actions. FUTR employs an encoder-decoder structure, where the encoder captures long-range temporal relations and the decoder captures global relations between upcoming actions. Unlike previous autoregressive models, FUTR anticipates future actions in parallel decoding for more accurate and faster inference. We also introduce an action segmentation loss to learn distinctive feature representations in the encoder. Experimental results on standard benchmarks demonstrate that FUTR achieves state-of-the-art performance in long-term action anticipation. This paper's contributions lie in the introduction of FUTR, a neural network that leverages fine-grained features and global interactions, the proposal of parallel decoding for accurate and fast inference, the integration of action segmentation in the encoder and action anticipation in the decoder, and the achievement of new state-of-the-art results on standard benchmarks.