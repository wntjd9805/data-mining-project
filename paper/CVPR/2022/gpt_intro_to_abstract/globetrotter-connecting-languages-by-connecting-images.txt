Machine translation has made significant advancements in recent years, particularly with the adoption of sequence-to-sequence deep learning models. However, these models heavily rely on large parallel text datasets, which are difficult and expensive to collect, especially for uncommon language pairs. In this paper, we propose a novel approach that eliminates the need for parallel corpora by leveraging the visual consistency observed across different languages. By utilizing the natural correspondence between visual observations and textual descriptions, we establish a transitive relationship to estimate the similarity between sentences. Our multimodal contrastive approach utilizes vision as a bridge to connect unrelated languages and learns machine translation in a self-supervised manner. In extensive experiments, we demonstrate that our approach outperforms existing methods on both sentence and word translation tasks, achieving robust translation across 52 different languages. Notably, our approach does not require vision during the inference phase, making it practical for real-world translation scenarios. Moreover, we release a federated multimodal dataset to enable further evaluation and analysis of our approach. Our work highlights the effectiveness of grounding language in vision and demonstrates the potential for building language translation models without the need for parallel corpora. Code, data, and pre-trained models will be made publicly available.