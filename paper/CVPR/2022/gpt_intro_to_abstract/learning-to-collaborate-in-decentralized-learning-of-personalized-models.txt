Training large models for computer-vision tasks, such as deep neural networks (DNNs) and vision transformers, is often hindered by the lack of sufficient data. In many applications, data is distributed over millions or even billions of personal and IoT devices, making it infeasible to transmit this data due to privacy concerns and limited communication bandwidth. To address this challenge, two distributed learning schemes, federated learning (FL) and decentralized learning (DL), have been studied extensively. FL aims to train a global model by sharing local models and gradients across devices, while DL allows devices to communicate only with their neighboring nodes on a network topology to update their local models without global synchronization. However, FL and DL do not account for the fact that devices and their users may have different tasks and data distributions, leading to suboptimal performance of the global model on individual devices. Personalization techniques have been proposed to address this data/task heterogeneity, but they often involve trade-offs between global consensus and local model personalization. In this paper, we propose a novel approach called "learning to collaborate" (L2C) for decentralized learning, which learns to weigh and mix neighbors' messages to update each local model for improved performance on its local task and data. L2C optimizes the mixing weights solely based on the validation loss, ensuring privacy protection and better personalization. We also introduce meta-L2C, which learns an attention mechanism to assign mixing weights given the models of a node and its neighbors. This allows for adaptation to new nodes, tasks, and data distributions without re-training or fine-tuning. Empirical results on benchmark datasets demonstrate that L2C and meta-L2C outperform FL and DL methods, showing improved efficiency and performance in personalization and handling data heterogeneity. Additionally, case studies and empirical analysis showcase the effectiveness of L2C and meta-L2C in capturing task correlation among nodes and enabling efficient communication through a sparse network topology. Overall, this work provides automated and adaptive solutions for decentralized learning of personalized local models in large-scale distributed systems.