Video inpainting is a technique used to fill in corrupted regions in a video with meaningful details, resulting in a completed video that is consistent both spatially and temporally. This technique has various industrial applications, including video restoration, unwanted object removal, and video retargeting. However, existing methods in this field still face issues such as producing artifacts, blurry results, and misplacement artifacts in the completed video.To address these challenges, we propose a novel approach called the Discrete Latent Transformer (DLFormer) for video inpainting. Unlike previous methods that formulate the task in a continuous feature space, we model the problem as a code inference problem in a discrete latent space. By leveraging the Vector Quantized Variational AutoEncoder (VQ-VAE), we quantize the continuous representation of an image into limited discrete codes in a codebook, allowing for the reconstruction of the original image. To capture fine-grained details, we learn a video-specific and discriminative codebook and corresponding autoencoder in the discrete latent space. This codebook captures global discriminative features in the video sequence, even for unknown regions. Inpainting unknown regions with plausible content is achieved by inferring the proper discrete code indices with the codebook through a self-supervised training strategy.To ensure spatial-temporal consistency and avoid visual jitters, we enforce short-term consistency with a residual aggregation block. We evaluate our DLFormer method on video restoration and object removal tasks using the YouTube-VOS and DAVIS datasets, and our experimental results show significant improvements over state-of-the-art methods. Our method is capable of generating visually plausible and spatial-temporal coherent content with fine-grained details in unknown regions.In summary, our contributions include formulating the video inpainting task as a discrete code inference problem, proposing a discrete latent transformer to model the global code distribution, and developing a residual temporal aggregation block to alleviate visual jitters. These advancements enable our method to generate more realistic and detailed inpainting results compared to previous methods.