This paper addresses the challenge of enabling machines to accurately read the time from analog clocks in unconstrained images. While digital clocks can be easily handled by existing text spotting methods, reading analog clocks poses significant difficulties due to appearance variations, camera viewpoint distortions, shadows, and reflections. Current solutions are limited in their ability to robustly read the time from clocks in real-world scenarios. Surprisingly, this problem has received little attention in the computer vision literature, and there are no reliable benchmarks for evaluation. Drawing parallels between analog clock reading and text spotting, the paper explores the use of synthetic datasets and spatial transformer networks to tackle this task. The authors propose a synthetic dataset generator, a two-stage framework involving detection and recognition stages, and leverage the uniformity of time to generate pseudo-labels on unlabeled clock videos. Additionally, three new benchmark datasets are introduced, and the proposed model achieves high accuracy in reading analog clocks in unconstrained images, allowing for applications such as image correction, video forensics, and image and video retrieval.