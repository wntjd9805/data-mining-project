Making immersive AR/VR experiences requires the ability to create personalized avatars effortlessly. This is particularly important for telepres-ence applications, where remote participants need to scan themselves and have their avatars realistically placed in new environments and poses. However, existing methods face several challenges. They often rely on parametric meshes and linear blend skinning (LBS) to represent human models, limiting their ability to capture various clothing types and dynamics. Some methods use neural implicit functions to model static clothed humans, but they still require full-body scans or complex multi-view setups. In this paper, we introduce a novel approach called Personalized Implicit Neural Avatars (PINA) that addresses these limitations. Our method learns animatable avatars from monocular RGB-D video sequences, without the need for extensive prior knowledge or technical expertise. We fuse partial depth maps into a consistent representation and learn articulation-driven deformations using an implicit signed distance field (SDF). To handle the challenge of incomplete point clouds, we propose a point-based supervision scheme that enables learning of articulated non-rigid shapes. Our approach achieves state-of-the-art results in avatar reconstruction and animation tasks, and we demonstrate its effectiveness in capturing and animating different humans in various clothing styles. The code and video for our method will be made available on our project page.