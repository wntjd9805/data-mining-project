This paper introduces a novel method, 3DCrowdNet, for robustly estimating a single person's 3D human mesh from in-the-wild crowded scenes. Previous works have focused on scenes without inter-person occlusion and have provided inaccurate results on crowded scenes. The paper investigates the limitations of the current literature and proposes a solution to overcome these challenges. The proposed method addresses two main issues: the domain gap between training and testing data, and the spatial information collapse caused by global average pooling. To resolve the domain gap, the paper guides a deep CNN to extract crowded scene-robust image features using a 2D pose estimator trained on in-the-wild datasets. To preserve spatial information, the paper introduces a joint-based regressor that samples image features from a small area around the target person, excluding features of non-target people. Experimental results demonstrate that 3DCrowdNet outperforms previous methods on in-the-wild crowded scenes and achieves state-of-the-art accuracy on multiple 3D benchmarks. The contributions of this work include the introduction of 3DCrowdNet, the extraction of crowded scene-robust image features, and the distinguishing of a target person's image features from others.