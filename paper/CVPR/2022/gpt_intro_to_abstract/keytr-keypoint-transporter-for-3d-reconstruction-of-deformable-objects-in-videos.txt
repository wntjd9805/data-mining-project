This paper addresses the problem of reconstructing 3D dynamic scenes from casually recorded videos. The goal is to reconstruct the depth of moving objects in the scene, which is a challenging task in comparison to reconstruction in controlled capture setups. Previous approaches have utilized deep networks and multi-view geometry principles to estimate depth in casual videos, but they often rely on assumptions that do not explicitly model non-rigid deformations. To overcome this limitation, the authors propose a new method called Keypoint Transporter (KeyTr) that tracks deformations across the entire video. KeyTr maintains a set of 3D keypoints that can be deformed within a low-rank subspace and enforces constraints to ensure the coverage of the object region in each frame and the compatibility with measured optical flow. The authors demonstrate that globally modeling and constraining object deformations significantly reduce reconstruction ambiguity and achieve superior or comparable reconstruction quality without relying on pre-trained depth estimation models. Additionally, they introduce new datasets for evaluating dynamic video depth algorithms. The proposed method and datasets are evaluated quantitatively on synthetic datasets and qualitatively on real videos of pets. Overall, this work contributes to the reconstruction of 3D dynamic scenes from casually recorded videos and provides a novel approach that explicitly considers non-rigid deformations in the reconstruction process.