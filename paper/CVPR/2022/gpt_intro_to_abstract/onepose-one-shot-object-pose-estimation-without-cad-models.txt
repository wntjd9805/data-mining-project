Object pose estimation is essential in augmented reality (AR), particularly for using everyday objects as "virtual anchors" of AR effects. Existing approaches in object pose estimation rely on known CAD models of objects, which can be inaccessible for everyday objects. Recent methods have explored category-level pose estimation as an alternative, but this requires a large number of labeled training samples and may struggle with generalization to new instances. To address these challenges, this paper proposes a one-shot object pose estimation approach that does not rely on CAD models or category-specific training. Instead, it leverages a sparse SfM model of the object generated from a simple video scan. The proposed approach utilizes a generic 3D-2D feature matching network to estimate the object's pose. The network learns a category-level representation of object appearances and shapes, enabling it to generalize to new instances in the same category. Unlike previous works, this approach does not require instance- or category-specific network training. The paper introduces the problem setting of one-shot object pose estimation and presents a novel architecture using graph attention networks (GATs) for accurate and robust 2D-3D feature matching. A large-scale dataset for one-shot pose estimation is collected and used to evaluate the proposed method, showcasing improved precision compared to existing instance-level and category-level methods. The proposed approach achieves real-time performance without requiring training for specific object instances or categories.