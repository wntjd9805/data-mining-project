Image synthesis is a rapidly advancing field within computer vision, but it also poses significant computational challenges, particularly in generating high-resolution, complex scenes. Likelihood-based models have been successful in scaling up image synthesis, but they require billions of parameters in autoregressive transformers. On the other hand, generative adversarial networks (GANs) have shown promising results but struggle to handle complex, multi-modal distributions. Diffusion models, built from hierarchies of denoising autoencoders, have demonstrated impressive image synthesis capabilities and define the state-of-the-art in various tasks, including class-conditional image synthesis and super-resolution. These models are computationally demanding, requiring massive resources for training and evaluation. This accessibility issue motivates the need to reduce the computational complexity of diffusion models without compromising their performance.This paper introduces Latent Diffusion Models (LDMs), a novel approach to high-resolution image synthesis that reduces the computational demands of diffusion models while maintaining their effectiveness. The approach involves training an autoencoder to provide a lower-dimensional, perceptually equivalent space. Unlike previous work, LDMs do not rely on excessive spatial compression but rather train diffusion models in the learned latent space, which exhibits better scaling properties. This reduced complexity enables efficient image generation using a single network pass. The universal autoencoding stage needs to be trained only once and can be reused for multiple diffusion model trainings or different tasks.The contributions of this work include (i) scalability to higher-dimensional data, leading to more faithful and detailed reconstructions compared to previous approaches, (ii) competitive performance in multiple image synthesis tasks with significantly lower computational costs, (iii) elimination of the need for delicate weighting of reconstruction and generative abilities, resulting in faithful reconstructions and reduced regularization of the latent space, (iv) the ability to handle densely conditioned tasks such as super-resolution, inpainting, and semantic synthesis, producing large, consistent images, (v) the design of a general-purpose conditioning mechanism based on cross-attention for multi-modal training, and (vi) the release of pretrained models that can be reused for various tasks.Overall, LDMs provide an effective solution for high-resolution image synthesis with reduced computational complexity, making this powerful model class more accessible to the research community and users in general.