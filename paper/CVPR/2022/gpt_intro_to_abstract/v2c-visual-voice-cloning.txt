Voice Cloning (VC) is a technique that converts text to speech using a desired voice from a reference audio. However, there are real-world applications, such as movie dubbing, that require generated speeches to have rich emotions in addition to a template voice. This paper introduces a new task called Visual Voice Cloning (V2C), which aims to generate speech with the voice of a reference audio and the emotion derived from a reference video. The V2C task takes a triplet of input (text/subtitle, reference audio, reference video) and generates speech that covers the content of the text while incorporating the voice and visual emotion from the reference audio and video, respectively. This task presents several challenges, including generating human-like speech with emotion and disentangling voice and emotion from different sources. To facilitate research in V2C, the authors create the first V2C-Animation dataset, which consists of video clips with audio, subtitles, and emotion annotations from 26 animated movies. They propose a new method called Visual Voice Cloning Network (V2C-Net) based on the Text-to-Speech framework and introduce an evaluation metric, MCD-DTW-SL, to assess the quality of the generated speech. The paper's contributions include the introduction of the V2C task, the creation of the V2C-Animation dataset, the development of V2C-Net, and the introduction of the MCD-DTW-SL evaluation metric.