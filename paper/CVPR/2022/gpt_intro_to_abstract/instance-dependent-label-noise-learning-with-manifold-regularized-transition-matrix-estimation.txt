Label-noise learning has become a crucial topic in the deep learning community due to the challenges and costs associated with accurately annotating large-scale datasets. This problem arises when collecting data from crowd-sourcing platforms or online queries, which often results in low-quality and noisy data. In this paper, the focus is on instance-dependent label noise (IDN), where the probability that an instance is mislabeled depends on both its class and features. Existing label-noise learning methods can be divided into two categories: algorithms with statistically inconsistent classifiers and algorithms with statistically consistent classifiers. The former employ heuristics to reduce the negative effects of label noise without explicitly modeling the noise distribution, while the latter estimate the transition matrix to explicitly model the generation process of noisy labels. However, obtaining the instance-dependent transition matrix (IDTM) is challenging under IDN without any constraint. Previous methods have addressed this problem by simplifying the estimation problem or making strong assumptions. In this paper, a novel approach is proposed that leverages the assumption that instances of similar appearances are more likely to be mislabeled to correlated or same noisy classes. This assumption is formulated as a manifold embedding, which reduces the estimation error without sacrificing too much approximation error. Extensive experiments on various datasets demonstrate the superior classification performance of the proposed method compared to state-of-the-art methods for label-noise learning under IDN. The main contributions of this paper are the practical assumption on the geometry of the transition matrices, the formulation of this assumption into a manifold-regularized method, and the extensive empirical evaluation demonstrating the superiority of the proposed method.