Denoising diffusion models and score-based models have gained significant attention in the computer science community due to their state-of-the-art performance in generative modeling. Although these models are inspired differently, they share similar aspects and can be seen as variants of each other. In the forward diffusion process, a data point is perturbed gradually with Gaussian noise, resulting in a spherical Gaussian distribution. In the reverse diffusion process, the trained score function is used to denoise the data, resulting in a high-quality sample. Diffusion models have been applied to various conditional image generation tasks, such as super-resolution, in-painting, MRI reconstruction, and image translation. Some approaches redesign the diffusion model for specific tasks, while others modify the inference procedure to sample from a conditional distribution. However, diffusion models suffer from slow sampling speed. In this work, we propose an acceleration method called Come-Closer-Diffuse-Faster (CCDF), which utilizes the contraction property of the reverse diffusion path. CCDF perturbs the initial estimate via forward diffusion before starting reverse diffusion, reducing the total number of reverse diffusion steps. With better initialization, the number of reverse sampling can be further reduced. We demonstrate through extensive experiments that CCDF significantly accelerates diffusion-based models for inverse problems, such as super-resolution, inpainting, and MRI reconstruction.