Abstract:The quality degradation of scene text images, such as low resolution and blurry structures, poses challenges for downstream high-level recognition tasks. In recent years, scene text image super-resolution (STISR) methods based on deep learning have shown promising results in improving image quality. However, existing CNN-based methods still struggle with spatially-deformed text images with rotation and curved shapes, producing blurry texts with incorrect characters. To address this issue, we propose a novel architecture called Text ATTention network (TATT) for spatial deformation-robust text super resolution. The TATT model incorporates a text recognition module to extract the character semantics as text prior, and a transformer-based module to capture long-range correlations between the text prior and image feature through cross attention. Additionally, a text structure consistency loss is introduced to refine the text appearance under spatial deformation. Experimental results demonstrate that our method achieves better visual quality and correct semantics compared to existing approaches. Our proposed model also achieves state-of-the-art performance on the TextZoom dataset and exhibits outstanding generalization performance for orientation-distorted and curve-shaped low-resolution text images.