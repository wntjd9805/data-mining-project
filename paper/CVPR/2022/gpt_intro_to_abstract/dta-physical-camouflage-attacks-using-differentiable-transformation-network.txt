Deep neural networks (DNNs) have shown remarkable success in computer vision tasks. However, they are vulnerable to adversarial examples, where carefully crafted inputs can cause misrepresentations and incorrect predictions. Adversarial attacks, deliberate attempts by adversaries to exploit this weakness, have gained attention in both digital and physical domains. Physical adversarial attacks are particularly challenging due to the need to account for various physical constraints. Previous studies have relied on simulations using photo-realistic rendering software, but these approaches are non-differentiable and yield lower attack performance. More recent methods have used neural renderers for generating adversarial camouflage, but they still struggle with accurately blending foreground and background elements. In this paper, we propose a Differentiable Transformation Attack (DTA) framework that leverages our novel Differentiable Transformation Network (DTN) to generate robust physical adversarial camouflage. Our DTN learns the transformations of an object while preserving its original parameters, enabling realistic output and differentiability. We demonstrate the effectiveness and robustness of DTA through experiments in both simulations and the real world, outperforming previous works in terms of target object detection and transferability to other models. Our framework has the potential to significantly enhance the security of deep neural networks in computer vision applications.