Pose Guided Person Image Generation (PGPIG) is a challenging task that aims to generate person images with arbitrary poses. It has numerous applications in e-commerce, film special effects, person re-identification, etc. Early works on PGPIG were built on vanilla Convolutional Neural Networks (CNNs), which lacked the ability to handle complex geometry transformations. To address this, attention mechanisms and optical flow were introduced to improve spatial transformation abilities. However, existing methods solely focused on training the generator on the Source-to-Target Task, resulting in unrealistic images, especially for large pose changes. In this paper, we propose a novel Dual-task Pose Transformer Network (DPTN) for PGPIG. DPTN incorporates two branches, a self-reconstruction branch for the Source-to-Source Task and a transformation branch for the Source-to-Target Task. These branches share weights and are trained simultaneously with different loss functions. We also introduce a Pose Transformer Module (PTM) to explore the correlation between the dual tasks, improving the fine-grained mapping between the sources and targets and guiding source texture transmission. Experimental results on benchmark datasets demonstrate the superior performance of our method in terms of PSNR and LPIPS. Additionally, our model has significantly fewer parameters compared to the state-of-the-art method.