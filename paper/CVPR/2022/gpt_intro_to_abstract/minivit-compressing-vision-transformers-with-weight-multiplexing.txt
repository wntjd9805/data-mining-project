Large-scale pre-trained vision transformers have gained significant attention in computer vision due to their high model capabilities and superior performance on downstream tasks. However, these models often have large model sizes and require extensive pre-training data. This poses a challenge for applications with limited computational resources or real-time predictions. In this paper, we explore the use of weight sharing, a technique commonly used in natural language processing, to reduce the size and computational overhead of pre-trained vision transformers. We apply weight sharing to DeiT-S and Swin-B transformers and observe two main issues: training instability and performance degradation. Through analysis, we identify that strictly identical weights across different layers cause these issues. To address this, we propose a new technique called weight multiplexing, which consists of weight transformation and weight distillation. Weight transformation introduces slight differences in weights across layers, promoting parameter diversity and improving training stability. Weight distillation transfers information from the original pre-trained models to the weight-shared small models, ensuring performance is not compromised. Experimental results demonstrate that our weight multiplexing method achieves improved accuracy compared to baselines and compresses pre-trained vision transformers up to 2 times while maintaining good performance on downstream tasks. For example, the Mini-Swin-B model with 12-layer parameters achieves higher accuracy than the 24-layer Swin-B model. Our approach also achieves high accuracy on ImageNet with significantly reduced model sizes compared to the original models. Additionally, our compressed models perform well in object detection tasks. Overall, our contributions include investigating the efficacy of weight sharing in vision transformers, proposing the MiniViT compression framework, and demonstrating its effectiveness in achieving high compression ratios without sacrificing accuracy.