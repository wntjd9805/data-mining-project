This paper introduces a deep-learning framework for the task of de-rendering sRGB images to raw-RGB images using metadata sampled from the raw-RGB at capture time. The motivation for this task arises from the desire to access the camera's raw-RGB sensor image, which has a linear relationship with scene radiance and enables more accurate photo-editing operations. However, most images are stored in the standard sRGB format, which has undergone nonlinear processing by the camera's image signal processor (ISP).To recover the raw-RGB values, the paper proposes a de-rendering approach that embeds samples from the original raw-RGB image into the sRGB image as specialized metadata. Previous methods rely on uniform sampling and simple mapping functions, but this work introduces an end-to-end framework that learns both the sampling and reconstruction processes. The sampling is performed in a content-aware manner using superpixel-based max-pooling, and the reconstruction network incorporates online fine-tuning to improve performance at inference time.The effectiveness of the proposed method is demonstrated through experiments on raw reconstruction tasks, achieving state-of-the-art performance using only 1.5% of raw-RGB pixels saved in the metadata. Furthermore, the applicability of the method is extended to other image recovery tasks, such as bit-depth recovery, by applying the sampling/reconstruction framework. The paper provides an overview of the proposed deep-learning framework and presents a visual example to illustrate the process.