Recently, contrastive learning has emerged as a successful method for self-supervised representation learning in computer science. This approach involves learning representations that contain shared information between different views of the data. These representations have been shown to be useful for various downstream tasks such as classification, detection, and segmentation. In contrastive learning, the representation that captures all shared information is referred to as the sufficient representation, while the representation that eliminates non-shared information is called the minimal sufficient representation. However, it has been observed that the optimal views for contrastive learning may vary depending on the specific downstream task. This suggests that non-shared task-relevant information cannot be ignored and may lead to performance degradation. In this work, we theoretically prove that the minimal sufficient representation contains less task-relevant information than other representations and has a non-ignorable gap with the optimal representation. We also demonstrate that contrastive learning models may over-fit to the shared information between views, highlighting the need to introduce more non-shared task-relevant information to the representations. To address this, we propose an objective term that increases the mutual information between the representation and input, effectively introducing more task-relevant information. We validate the effectiveness of our method on various contrastive learning models for classification, detection, and segmentation tasks, and provide extensive experimental analysis to support our theoretical findings. This work contributes to a deeper understanding of the limitations and potential improvements of contrastive learning in self-supervised representation learning.