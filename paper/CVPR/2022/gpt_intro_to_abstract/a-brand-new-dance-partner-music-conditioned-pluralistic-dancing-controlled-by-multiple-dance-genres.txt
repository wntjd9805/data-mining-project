Dance is widely recognized as a powerful form of expression that transcends language barriers. With the rise of short-form video platforms like TikTok and Youtube Shorts, more and more people are sharing their dance performances online. However, creating a visually appealing and rhythmically complex dance video often requires professional training. In recent years, deep autoregressive networks have been employed to generate dance motions based on music inputs, but existing methods still struggle with capturing diverse and realistic movements.This paper addresses the challenge of music-conditioned pluralistic dance generation, where the goal is to synthesize dance motions across multiple genres that align with musical beats. To achieve this, we propose a novel approach that combines music-conditional sequence-to-sequence learning with Transformer Conditional Generative Adversarial Networks (GANs). Our model leverages the generative capabilities of the transformer decoder, incorporating both conditional and stochastic representations through a self-attention module.To ensure diversity and consistency in the generated dance motions, we inject a latent code and query a specific duration of music and initial pose sequence. Our proposed model demonstrates the ability to synthesize diverse dance sequences within a single genre as well as various dance genres throughout the duration of the music.However, the process of injecting latent codes with conditions becomes inefficient when dealing with multiple dance genres. To address this scalability issue, we introduce two additional modules: a mapping network and a multi-task discriminator. The mapping network transforms random Gaussian noise into style codes specific to each dance genre, while the multi-task discriminator performs per-style classification. These modules enable our generator to successfully synthesize diverse dance motions across multiple domains within a single model.The main contributions of this work are threefold. Firstly, we introduce MNET, a novel Transformer-based conditional GAN framework, which enables the generation of pluralistic dance motions by sampling from latent representations of multiple dance genres. Secondly, we demonstrate that our approach can generate realistic and diverse dance motions that scale to multiple genres in terms of visual quality and empirical metrics. Lastly, we conduct comprehensive ablation studies of the architecture and loss components, showcasing state-of-the-art performance on the AIST++ dataset, which contains 3D motions reconstructed from real dancers paired with music and multiple dance genres. The code for this project will be made available for research purposes.