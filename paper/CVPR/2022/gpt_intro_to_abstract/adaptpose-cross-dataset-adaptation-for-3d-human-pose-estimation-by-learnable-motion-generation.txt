Monocular 3D human pose estimation is a challenging task that involves reconstructing the 3D skeleton of a human body from 2D images. Deep learning models have achieved impressive results by learning 2D to 3D correspondences from similar datasets. However, these models often struggle when applied to images from different datasets due to differences in camera orientation and body poses. This problem, known as the domain gap, leads to unreliable predictions for the target domain. In this paper, we propose a method called AdaptPose to address this issue. Our approach involves generating synthetic 3D data that aligns with the distribution of the target domain and fine-tuning the pose estimation network using this synthetic data. Importantly, our method does not require 3D labels or camera information from the target domain, as it is trained solely on sample videos. We compare our approach to existing methods that focus on data augmentation within the source domain and lack consideration of the target domain distribution. Furthermore, we extend the camera viewpoint generation from a deterministic approach to a probabilistic one. We experimentally demonstrate the effectiveness of our approach in achieving cross-dataset generalization in 3D human pose estimation. Our contributions include the introduction of a kinematics-aware domain discriminator and the learning of a camera viewpoint distribution. Overall, our work offers a novel solution for generating human motions specifically for cross-dataset generalization in 3D human pose estimation, with potential applications in other tasks such as human action recognition.