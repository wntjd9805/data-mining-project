Deep neural networks (DNNs) have demonstrated superior performance across various artificial intelligence applications. However, the large size of these models poses challenges in terms of training, transferring, storing, and evaluating. Pruning, a technique that sets certain parts of the network's weights to zero, has emerged as a solution to reduce model complexity and memory requirements, accelerate inference, and potentially enhance generalization ability. In recent years, sparse training methods have gained interest, offering benefits in terms of reduced memory requirements and runtime for both inference and training. This work focuses on methods that prune individual parameters before training, while keeping the number of zeroed coefficients fixed. By employing unstructured pruning, the memory footprint of the network can be reduced. Additionally, specialized soft and hardware are required to further decrease runtime. The paper distinguishes between pruning at initialization, finding the sparse architecture through iterative train-prune-reset cycles (referred to as the lottery ticket method), and dynamic sparse training which allows for changes in the pruning mask during training. The paper introduces the concept of interspace pruning (IP) as a general method to improve the performance of CNNs with sparse coefficients. By representing and training convolutional filters in an interspace formed by a trainable filter basis, IP overcomes the limitations of spatial pruning and leads to improved information flow and trainability. The paper provides theoretical proof of IP's advantages and empirical evidence showcasing its superiority over standard pruning methods in terms of runtime and memory costs. The authors emphasize that while IP can be a useful tool to reduce costs in pruning-based applications, they distance themselves from any negative ethical or societal impacts that may arise from its use. Overall, this work contributes to the advancement of pruning techniques, benefiting the broader community by reducing costs and improving generalization ability in deep learning.