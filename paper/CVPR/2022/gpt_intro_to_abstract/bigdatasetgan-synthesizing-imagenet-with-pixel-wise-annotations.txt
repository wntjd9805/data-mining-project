The ImageNet dataset has played a crucial role in advancing computer vision and deep learning. It has been widely used for large-scale classification and has also served as a valuable resource for pre-training backbone models. In this paper, we propose enhancing ImageNet by incorporating pixel-wise labels, enabling large-scale multi-class segmentation challenges and opening up new possibilities for pre-training strategies in dense prediction tasks. Instead of manually labeling masks for the 1 million images in ImageNet, we utilize a dataset synthesis method to generate high-quality labeled data at a fraction of the cost. Building upon DatasetGAN, which introduced the idea of annotating GAN-generated images with pixel-wise labels, we demonstrate that the generator's feature maps are highly powerful and semantically meaningful. By utilizing BigGAN as the generative model, we scale up DatasetGAN to the ImageNet scale with minimal human labeling effort. Additionally, we show that VQGAN can also serve as a dataset generator without the need for additional annotation. We refer to this dataset generator for ImageNet as BigDatasetGAN. Experimental results demonstrate the benefits of leveraging synthesized datasets in various dense prediction tasks and datasets, including PASCAL-VOC, MS-COCO, Cityscapes, and chest X-ray. By comparing supervised and self-supervised methods, we show that performance is significantly improved when utilizing our synthetic datasets. Furthermore, we annotate a subset of real ImageNet with pixel-wise labels and introduce a new semantic segmentation benchmark, providing a platform for evaluating existing methods in this area. The annotated data and benchmark will be made available online, supporting a leaderboard for segmentation challenges.