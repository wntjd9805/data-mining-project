Learning an adequate representation of concepts from raw data without strong supervision is a challenging task, particularly in research areas where prior knowledge is lacking. However, advancements in machine learning models have improved the potential for achieving this goal. It is important, however, to ensure that these models do not learn incorrect concepts or confounding features. Concept learning is also complicated by its dynamic and subjective nature, where different tasks may require varying levels of granularity in concept representation. Additionally, in online learning settings, the knowledge and hierarchy of concepts should be constantly re-evaluated and updated.One approach to teaching concept information to machine learning models is through supervised training using symbolic representations. However, this method requires extensive prior knowledge and is impractical given the subjective and dynamic nature of concept learning. Another approach focuses on learning disentangled latent distribution models, which initially focused on unsupervised learning but have shown promising results with weak supervision. These models aim to learn latent representations that correspond to human-interpretable factors but often rely on continuous representations that are difficult for humans to understand without additional techniques for interpretation.Inspired by the concept of prototypes in psychology and cognitive science, this paper investigates the advantages of prototype representations in learning human-understandable and revisable concept representations for neural concept learners. The authors propose the Interactive Concept Swapping Network (iCSN), a framework that learns to bind semantic concepts to latent prototype representations through weak supervision. This binding is enforced through a discretized distance estimation and swapping of shared concept representations between paired data samples. The iCSN allows for querying and revising learned concepts and integrating knowledge about unseen concepts.To evaluate their approach, the authors develop a novel dataset called Elementary Concept Reasoning (ECR) that focuses on learning object-centric visual concepts. They conduct multiple experiments to demonstrate the advantages of their approach, including learning a consistent and human-understandable latent space through weak supervision, revising concept representations through human interactions, and updating concepts in an online learning fashion.Overall, this paper highlights the benefits of prototype representations in concept learning, providing a framework and experimental evidence for learning concept representations that are consistent, human-understandable, and revisable.