This paper addresses the challenge of effectively compressing the attributes of unstructured point clouds, which are commonly used in various applications such as mixed reality, self-driving vehicles, and high-resolution mapping. While the compression of raw 3D coordinates can be achieved, the compression of attributes (e.g., color, reflectance) is non-trivial. Previous methods have focused on initial coding algorithms but have largely overlooked entropy coding, which losslessly encodes coefficients to a final bitstream. Only a few traditional entropy coders have been proposed, which do not incorporate geometry or context information. In this paper, a learning-based compression framework called 3DAC is proposed. It utilizes a Region Adaptive Hierarchical Transform for initial coding and an attribute-oriented deep entropy model to estimate the probability distribution of transform coefficients, considering context information and inter-channel correlations. Experimental results show that the proposed method achieves higher reconstruction quality with a lower bitrate, demonstrating excellent attribute compression performance. The contributions of this paper include the introduction of a learning-based framework for attribute compression, the proposal of an attribute-oriented deep entropy model, and the demonstration of state-of-the-art compression performance on indoor and outdoor point cloud datasets.