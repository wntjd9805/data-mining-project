Deep learning algorithms heavily rely on large amounts of labeled data, making it challenging to leverage the exponentially growing unlabelled data from open sources. Self-supervised learning (SSL) has emerged as a solution by formulating supervision without manual annotation, achieving significant progress in natural language processing (NLP) and computer vision (CV). However, SSL requires enormous amounts of data and training budgets to unleash its true power, making it expensive to train models of various architectures to cover different downstream needs. Neural architecture search (NAS) has been proposed to address these challenges, but it seems impossible to apply NAS in SSL due to the lack of clue for model selection in the absence of labels or metrics. This paper presents a supernet training mechanism for siamese-based SSL, where the architecture of the teacher branch in the supernet remains fixed, while the architectures of the query branch vary. By stabilizing the behavior of the teacher, a steady knowledge source is provided for the different student architectures, enabling efficient convergence and improved feature representation. Additionally, the distance between subnets and the maximum network in the supernet serves as a self-supervised metric for evaluating network quality. Moreover, the proposed approach extends the searching process to consider the types of downstream tasks, minimizing the gap between student and teacher architectures while maintaining a plug-and-play searching strategy. The approach allows for training models of various sizes and searching for specialized models for specific downstream tasks, computation constraints, and data domains. The pipeline requires no labels for training or model selection and is validated using MoCo v2 and evaluated on standard self-supervised benchmarks. The generalizability of the approach is demonstrated by combining it with other existing SSL methods.