Pretraining has become a popular approach in computer vision tasks, where models are trained on large datasets before being fine-tuned for specific tasks. Self-supervised pretraining, which breaks the dominance of supervised pretraining, has shown promising results in various downstream tasks such as image classification, object detection, and semantic segmentation. Existing self-supervised methods mainly employ instance discrimination as a pretext task to learn transferable representations. However, these methods are suboptimal for dense prediction tasks like object detection and semantic segmentation. Recent research has explored dense self-supervised learning by leveraging spatial information and geometric correspondences. However, pixel-wise correspondence based on similarity or geometry information tends to be noisy and suffer from misleading features and spatial semantic inconsistency. In this paper, we propose a novel approach, called SetSim, that explores set similarity across views for dense self-supervised representation learning. By considering sets of pixel-wise features instead of individual counterparts, we generalize pixel-wise similarity learning and improve robustness. Our method constructs a corresponding set based on attention maps, which effectively maintains the coherence of the input image by capturing semantic information. We also enhance the structured neighborhood information by searching for cross-view nearest neighbors. Experimental results demonstrate that our proposed method outperforms or achieves comparable performance to state-of-the-art methods on various dense prediction tasks, including object detection, keypoint detection, instance segmentation, and semantic segmentation. Compared to the baseline, our method significantly improves localization and classification abilities on these tasks, achieving higher average precision, mean average precision, and mean intersection over union scores.