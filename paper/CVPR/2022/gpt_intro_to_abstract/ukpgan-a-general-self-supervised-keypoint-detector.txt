Recently, there has been an increasing interest in 3D object analysis and scene understanding. While various methods have been proposed for object analysis, there is still a lack of capability in processing and understanding objects under unsupervised settings. 3D keypoints, which provide sparse but meaningful representations of objects, have been widely utilized in tasks such as object matching, tracking, and shape retrieval. However, traditional hand-crafted methods for detecting keypoints in 3D domains have limitations in performance compared to learning-based approaches.In this paper, we propose a novel method called Unsupervised Key Point GANeration (UKPGAN) to obtain 3D keypoints. Our approach involves using a detector network to generate a saliency distribution of keypoints, which is then controlled by an adversarial GAN loss to ensure sparsity. To ensure the informative nature of the keypoints, we employ a salient information distillation process to reconstruct the original point cloud from the sparse keypoints, creating an encoder-decoder architecture. The main idea behind our method is to compress a large amount of object information into a small set of keypoints. Our model demonstrates the ability to output stable and informative keypoints for unseen objects and performs well in real-world scenarios.Compared to previous methods, UKPGAN offers several advantages. Firstly, our detector has proven to be rotation invariant without the need for data augmentation, as it estimates a Local Reference Frame (LRF). This disentangles the local keypoint representation from rotations. Secondly, the keypoints detected by our model are consistent and stable for both rigid and non-rigid objects, with high repeatability. Lastly, our model, trained on clean object collections, generalizes well to real-world point clouds without relying on real-world training data.We evaluate our method on ShapeNet models with keypoint labels, achieving remarkable results in consistency with human-labeled parts and keypoints. Additionally, UKPGAN can be applied to both rigid and non-rigid objects, maintaining consistency on deformable human body meshes. We also evaluate our model on real-world geometric registration benchmarks (3DMatch and ETH datasets), showing that our model, when trained on clean objects, improves the registration performance of current state-of-the-art methods. Finally, we conduct extensive experiments to demonstrate high rotation repeatability, a desired property of keypoints, achieved by UKPGAN.