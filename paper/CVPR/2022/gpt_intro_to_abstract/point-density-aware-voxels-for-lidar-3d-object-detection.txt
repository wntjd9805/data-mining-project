3D object detection is a crucial problem in autonomous driving, as accurately estimating object pose is essential for downstream perception tasks. LiDAR has become a popular sensor for 3D object detection due to its accurate 3D point cloud generation. However, the density of LiDAR points varies with distance, resulting in fewer points for objects located farther away. Voxel-based methods, which rely on quantized representation of the point cloud, often suffer from spatial misalignment and performance degradation. Farthest point sampling (FPS) methods attempt to address point density variations but come with increased computation time and limitations in the number of sampled points. Additionally, existing methods primarily focus on detecting vehicles and ignore smaller objects like pedestrians and cyclists. To overcome these challenges, we propose the Point Density-Aware Voxel network (PDV) for multi-class 3D object detection. PDV leverages voxel point centroid localization, density-aware region of interest (RoI) grid pooling, and density confidence prediction. By calculating voxel point centroids and encoding density information, PDV retains fine-grained position details without the need for expensive point cloud sampling methods like FPS. Additionally, PDV incorporates density information during RoI grid pooling and refines bounding box confidence predictions using the number of LiDAR points within the box. Experimental results show that PDV outperforms current state-of-the-art methods on the Waymo Open Dataset and achieves competitive performance on the KITTI dataset.