Video Instance Segmentation (VIS) is a challenging task in the field of computer vision. It aims to predict segmentation masks with class labels for each object instance in a video. Existing methods solve the VIS problem either at the frame-level or clip-level. Frame-level methods perform image instance segmentation followed by data association to link masks with history tracklets. However, this approach is limited by complex data association algorithms and is sensitive to object occlusions. Clip-level methods jointly perform segmentation and tracking within each clip, resulting in stronger performance due to larger temporal receptive fields. However, most clip-level methods are not end-to-end and have slow inference speeds.In this paper, we propose a new clip-level VIS framework called EfﬁcientVIS that addresses the limitations of existing methods. EfﬁcientVIS delivers a fully end-to-end framework with fast convergence and strong performance. Inspired by the query-based R-CNN architecture in image object detection, EfﬁcientVIS uses tracklet queries paired with tracklet proposals to model space-time interactions in videos. Tracklet queries encode appearance information for a target instance, while tracklet proposals locate the target in the video. Our framework enriches temporal object context through clip-by-clip interactions and uses factorized temporo-spatial self-attention to exchange information over space and time. This enables EfﬁcientVIS to generate target tracklet masks in a single end-to-end pass.Compared to prior works, EfﬁcientVIS has several advantages. First, it achieves fast convergence by interacting with video features only in the region defined by tracklet proposals, reducing redundancies. Second, it is fully end-to-end learnable over the entire video, seamlessly associating instance tracklets between clips. Third, EfﬁcientVIS is robust to dramatic object movements and can track instances in low frame rate videos. Our experimental results demonstrate the superiority of EfﬁcientVIS in terms of both speed and performance.In summary, our major contributions include the development of EfﬁcientVIS, the first RoI-wise clip-level VIS framework that runs in real-time. We also propose a fully end-to-end neural network for VIS, eliminating the need for data association or post-processing. Our diagnostic experiments show that the fully end-to-end paradigm of EfﬁcientVIS is simpler and more effective than previous approaches.