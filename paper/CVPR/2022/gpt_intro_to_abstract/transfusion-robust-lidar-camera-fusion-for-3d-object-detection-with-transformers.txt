The introduction of this computer science paper discusses the challenges and limitations of 3D object detection in self-driving vehicles using LiDAR-only methods. It highlights the need for multi-modal detectors that combine LiDAR and camera data for more accurate and robust 3D detection. The paper categorizes existing LiDAR-camera fusion methods into result-level, proposal-level, and point-level approaches, pointing out the drawbacks of current point-level fusion methods. To address these shortcomings, the paper proposes a new multi-modal detection framework that focuses on soft-association instead of hard-association between LiDAR points and image pixels. The framework utilizes transformer decoder layers for detection and incorporates object queries that are enriched with position and category information. It also includes an image-guided query initialization module to handle objects that are difficult to detect in point clouds. The proposed framework significantly improves the effectiveness and robustness of LiDAR-camera 3D detection and achieves state-of-the-art performance on nuScenes and competitive results on Waymo datasets. It also performs well in the 3D tracking task, ranking first in the nuScenes tracking challenge leaderboard.