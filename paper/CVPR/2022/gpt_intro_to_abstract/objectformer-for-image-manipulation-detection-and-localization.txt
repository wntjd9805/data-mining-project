With the widespread availability of deep generative models such as GANs and VAEs, image editing applications have become easily accessible to the public. While these tools have made it simple to produce photo-realistic images and videos, there is growing concern about the misuse of editing techniques for malicious purposes. Therefore, it is essential to develop effective methods for detecting image manipulation.Image manipulation techniques can be broadly classified into three types: splicing, copy-move, and removal. These techniques often manipulate images at the object-level, such as adding or removing objects, to produce visually convincing results. While previous studies have focused on image manipulation detection using CNNs, they do not explicitly model object-level representations.In this paper, we propose ObjectFormer, a multimodal transformer framework for image manipulation detection and localization. The framework leverages transformers due to their impressive performance on various vision tasks and their ability to model consistencies between different spatial locations using self-attention. ObjectFormer introduces learnable object prototypes as mid-level object representations to capture object-level consistencies and refine patch embeddings for patch-level consistencies modeling.To account for visual artifacts that may not be perceptible in the RGB domain but are noticeable in the frequency domain, ObjectFormer jointly models both domains. It first transforms an image from RGB to the frequency domain using the Discrete Cosine Transform and extracts multimodal patch embeddings. These RGB and frequency patch embeddings are then concatenated to complement each other.By iteratively refining patch embeddings with object prototypes using cross-attention, ObjectFormer derives global feature representations that explicitly encode mid-level object features. These features are then utilized to detect manipulation artifacts and predict whether images have been modified, along with the corresponding manipulation mask.The proposed framework is evaluated on several widely used image tampering datasets, and the results demonstrate its superiority over state-of-the-art tampering detection and localization methods.In summary, this work introduces ObjectFormer, a multimodal framework for image manipulation detection and localization that combines RGB and frequency features. It leverages learnable object prototypes to model object-level consistencies and refine patch embeddings for patch-level consistencies. Experimental results confirm the state-of-the-art performance of ObjectFormer in detecting and localizing image manipulation.