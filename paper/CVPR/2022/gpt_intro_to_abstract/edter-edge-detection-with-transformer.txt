Edge detection is a fundamental problem in computer vision with various applications, such as image segmentation, object detection, and video object segmentation. Extracting accurate object boundaries and visually salient edges is challenging due to complex backgrounds and inconsistent annotations. Traditional methods mainly rely on low-level local cues, while recent approaches leverage convolutional neural networks (CNNs) to capture global and semantic-aware visual concepts. However, fine details are often lost in the process, and shallow features can result in noisy edges. Inspired by the success of vision transformers in modeling long-range contextual information, this paper introduces a transformer-based framework, named Edge Detection TransformER (EDTER), for edge detection. To address the challenges of working with fine-grained patches and extracting precise edges from intersected objects, EDTER adopts a two-stage approach. In the first stage, a global transformer encoder captures long-range global context, followed by a Bi-directional Multi-Level Aggregation (BiMLA) decoder to generate high-resolution representations. In the second stage, a local transformer explores short-range local cues on fine-grained patches using a sliding window approach, and the cues are then integrated and decoded using a local BiMLA decoder. The information from both stages is fused using a Feature Fusion Module (FFM) and fed into a decision head to predict the final edge map. Experimental results on benchmark datasets demonstrate the superiority of EDTER over state-of-the-art methods in edge detection. This work contributes a novel transformer-based edge detection model that effectively explores global and local information, incorporates a BiMLA decoder for improved feature flow, and achieves high-quality edge maps.