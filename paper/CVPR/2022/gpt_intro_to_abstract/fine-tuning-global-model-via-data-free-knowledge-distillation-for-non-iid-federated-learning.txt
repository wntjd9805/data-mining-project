The explosive growth of data and the need for strict privacy protection have made the transmission and aggregation of reckless data unacceptable. Federated Learning (FL) has emerged as an alternative to traditional centralized learning, offering improved data privacy. However, one of the main challenges in FL is the heterogeneity of data, where the data in clients are non-identically and independently distributed (Non-IID). Existing FL algorithms, such as FedAvg, struggle to handle this data heterogeneity, leading to drifted local models and degraded performance. To address this challenge, previous methods have focused on constraining the direction of local model updates or introducing lightweight generators to regulate local training. However, these methods ignore the incompatibility of local knowledge and the potential for utilizing the server's computing resources. In this paper, we propose FedFTG, a novel approach that fine-tunes the global model via data-free knowledge distillation. FedFTG models the input space of local models through an auxiliary generator, generates pseudo data, and transfers the knowledge from local models to the global model. We also introduce customized label sampling and class-level ensemble techniques to exploit the knowledge correlations among clients. FedFTG can be seamlessly embedded into existing FL optimizers, enhancing their performance. Experimental results demonstrate that FedFTG outperforms state-of-the-art FL methods on various benchmarks. The contributions of this work include the proposal of FedFTG, the development of hard sample mining, customized label sampling, and class-level ensemble techniques, and the demonstration of FedFTG's compatibility with existing local optimizers.