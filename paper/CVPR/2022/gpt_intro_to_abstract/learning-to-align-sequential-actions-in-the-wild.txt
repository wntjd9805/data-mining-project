This paper aims to address the limitations of current approaches in understanding human activities in video sequences. While previous research has focused on supervised, coarse-scale action understanding by predicting explicit classes for clips, these techniques do not provide a fine-grained analysis of human action. Additionally, the dependence on per-frame labels requires a large amount of human effort that is not scalable across different types of subjects, environments, and scenarios. This paper proposes a novel approach to overcome these limitations and improve the understanding of human behavior in videos. The proposed method leverages temporal variations in videos to extract more detailed information about human actions. Experimental results show that the proposed approach outperforms existing methods in terms of accuracy and scalability. The findings from this study have implications for applications such as human-computer interaction, video analysis, robot learning, and surveillance.