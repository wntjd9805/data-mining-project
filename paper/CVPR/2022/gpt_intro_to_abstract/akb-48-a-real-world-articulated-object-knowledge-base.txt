Articulated objects, consisting of multiple rigid parts connected by joints, are widely present in our daily lives and are of interest to various research communities such as computer vision, robotics, and embodied AI. Although there have been several datasets proposed for articulated objects, they tend to focus more on structural information and lack detailed annotations of appearance, physics properties, and semantics. This limitation hinders the generalization of learning models for important tasks like object detection, 3D reconstruction, and object manipulation. To address this gap, we introduce AKB-48, a large-scale real-world Articulated Knowledge Base comprising 48 categories and 2,037 instances. Each instance is scanned from its real counterpart and manually refined. The object knowledge is organized into an Articulation Knowledge Graph (ArtiKG), which includes detailed annotations of different object attributes and properties. To enable efficient scanning and annotation, we present a Fast Articulation Knowledge Modeling (FArM) pipeline that incorporates a 3D sensor-based object recording system, a GUI for annotations, and real-world experiments for physical property annotation. This pipeline significantly reduces the time and cost required for modeling articulated objects.Additionally, we propose AKBNet, an integral pipeline for Category-level Visual Articulation Manipulation (C-VAM) task. AKBNet consists of three perception sub-modules: the Pose Module for estimating per-part 6D pose of unseen articulated objects, the Shape Module for reconstructing the shape of each part, and the Manipulation Module for learning interaction policies. We evaluate AKBNet both individually and systematically, reporting results based on ground truth inputs and outputs from previous modules.Our contributions include the introduction of AKB-48, the first large-scale articulated object dataset with rich annotations collected from the real world. We also present the FArM pipeline, which simplifies the process of collecting real-world articulated objects for creating 3D model datasets. Lastly, we propose AKBNet as an efficient pipeline for integral category-level visual articulation manipulation, demonstrating its effectiveness in real-world experiments. Overall, this work aims to bridge the gap between current vision and embodied AI research in the computer vision and robotics communities.