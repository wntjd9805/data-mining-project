This paper addresses the problem of training image retrieval models using deep metric learning and Euclidean search in the learned image embedding space. The goal is to rank all database examples based on their relevance to a given query. The standard evaluation metrics for image retrieval are precision, recall, and mean Average Precision (mAP), with recall at top-k retrieved results (recall@k) being the primary focus of this work. However, optimizing non-differentiable evaluation metrics like recall@k is challenging.Existing deep learning methods for non-differentiable losses, such as actor-critic and learning surrogates, are not directly applicable to recall@k. Therefore, this paper proposes a new loss function that acts as a surrogate for recall@k and is differentiable, allowing for gradient-based optimization. Experimental results show that this proposed loss consistently outperforms existing competitors.In addition to the new loss function, this paper introduces two other important elements for achieving state-of-the-art results: the use of a large batch size (several thousand large resolution images on a single GPU) and the use of the mixup regularization technique, which efficiently enlarges the batch. These elements are demonstrated to improve the performance of the retrieval models on four fine-grained retrieval datasets and two instance-level retrieval datasets.The proposed loss function is used for training both ResNet architectures and vision transformers (ViT), with the superiority of the loss function demonstrated for both architectures. Furthermore, with ViT-B/16, the proposed loss achieves top results at a lower throughput compared to ResNet. Overall, this paper presents a comprehensive approach to training image retrieval models using deep metric learning, utilizing a novel loss function, large batch sizes, and mixup regularization technique.