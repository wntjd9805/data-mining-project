Deep neural networks (DNNs) have shown significant success in various applications, assuming independent and identically distributed (i.i.d.) training and test data. However, real-world problems often involve training and testing datasets collected under different scenarios, leading to poor performance of DNNs trained on the source data when applied to out-of-distribution target data. This performance degradation, known as domain shift, hampers the generalization ability of DNNs. To address this issue, domain generalization (DG) aims to exploit the diversity of source domains to improve model generalization. Unlike domain adaptation, DG assumes access only to source domains during training. Most prior works in DG focus on learning domain-invariant representations by aligning different source domains. In this paper, we explore the use of contrastive learning as a potential solution to this problem. By aligning a projection head on both sample embeddings and proxy weights, we propose a novel proxy-based contrastive learning technique for domain generalization. We empirically demonstrate the degradation of model generalization caused by the positive alignment problem in contrastive learning. Furthermore, our proposed technique achieves state-of-the-art accuracy on multiple benchmark datasets and consistently improves model performance in a more complex scenario without using ImageNet pre-trained models. Overall, our contributions include empirical insights on the positive alignment problem, the introduction of a simple yet effective proxy-based contrastive learning technique, and superior results on multiple benchmarks.