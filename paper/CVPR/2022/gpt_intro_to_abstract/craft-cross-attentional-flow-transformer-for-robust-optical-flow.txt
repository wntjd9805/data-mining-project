Optical flow estimation is a crucial task in computer vision, which involves calculating the 2D motion of pixels between consecutive video frames. The accurate estimation of optical flow has wide-ranging applications in various fields such as action recognition, video segmentation, medical image registration, and autonomous driving. Deep learning-based methods have significantly advanced optical flow estimation in recent years, but there are still challenges in cases involving large displacements with motion blur, which can lead to significant flow errors. Most existing optical flow estimation methods rely on a correlation volume that stores the similarity between pixels in the two frames. However, the current paradigm suffers from limitations due to the locality and rigid weights of convolutional operations, resulting in limited contextual information and high levels of randomness in computed correlations. Noises in the correlations, caused by input image variations, can lead to incorrect image matching and inaccurate flow estimation, particularly in the presence of large displacements. Therefore, reducing noisy correlations is crucial for improving flow estimation accuracy.The introduction of Vision Transformers (ViTs) has brought significant advancements in computer vision tasks by encoding global context through dynamic weight assignments. ViTs excel in spatial smoothing and feature denoising, as they perform weighted sums of similar feature vectors after attention-based operations. Motivated by the feature denoising capabilities of ViTs, this paper proposes a novel architecture called "CRoss-Attentional Flow Transformer" (CRAFT) for optical flow estimation. CRAFT incorporates two key components: a semantic smoothing transformer layer that enhances global and semantically smoother features, and a cross-frame attention layer that replaces the traditional dot-product operation for correlation computation. The cross-frame attention layer improves feature filtering through query and key projections, leading to more accurate computed correlations.Extensive evaluations of CRAFT on standard optical flow benchmarks demonstrate its superior performance compared to state-of-the-art methods. CRAFT achieves new state-of-the-art results on the Sintel (Final) and KITTI (foreground) benchmarks. Additionally, an image shifting attack is designed to test the robustness of different models to large artificial motions. CRAFT demonstrates robust performance even with increasing motion magnitudes, while two representative methods, RAFT and GMA, suffer severe deterioration.In conclusion, this paper introduces CRAFT, a novel architecture that leverages the semantic smoothing and feature denoising capabilities of Vision Transformers for accurate optical flow estimation. The extensive evaluations demonstrate the superiority of CRAFT over existing methods and its robustness against large displacements.