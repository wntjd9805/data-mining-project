The development of neural architectures for analyzing 3D data has led to significant advancements in automated scene analysis. Most methods rely on colorized point clouds, which require specialized sensors or a closed-source colorization step. However, images combined with 2D architectures are better suited for learning textural and contextual cues. This study aims to leverage the complementary nature of 3D point clouds and images by projecting 2D features onto 3D points. This involves addressing challenges such as occlusion recovery, computing point-pixel mapping, and merging 2D features from multiple images. While multi-view aggregation has been extensively studied for shape recognition, this paper proposes a novel attention-based scheme for merging features from multiple images. The proposed approach can efficiently perform point-pixel mapping without the need for depth maps, meshing, or colorization. It achieves state-of-the-art results on the S3DIS and KITTI-360 datasets without requiring point cloud colorization or depth sensors. The method is modular and can handle large-scale scenes with arbitrary numbers of 2D views and 3D points. Overall, this study presents a novel multi-view aggregation method for semantizing hybrid 2D/3D data based on the viewing conditions of 3D points in images.