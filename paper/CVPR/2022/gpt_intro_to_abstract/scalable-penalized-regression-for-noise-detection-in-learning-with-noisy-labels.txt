Deep learning has achieved remarkable success in supervised learning tasks, but its performance heavily relies on the quality of label annotation. Neural networks can be susceptible to noisy labels, leading to the degradation of generalization and robustness. Obtaining precise labels can be expensive and difficult in many real-world scenarios, posing a challenge for supervised deep models. There has been a large literature addressing this challenge, including modifying network architectures, loss functions, or dynamically selecting clean data during training. Dynamic sample selection methods aim to provide only clean data for training, forming a "virtuous" cycle between noisy data elimination and network training. In this paper, we propose a unified approach that considers both the label and feature spaces, assuming a linear relationship between the feature-label pair of data. We propose a statistical framework called Scalable Penalized Regression (SPR) that consistently identifies noisy data and efficiently learns with noisy labels. We utilize a sparse penalty on the residuals to encourage sparsity and focus on noisy instances. We incorporate SPR into the end-to-end training pipeline of deep architectures using a split algorithm to make it scalable to large datasets. Additionally, we propose using a sparse penalty on the fully-connected output and utilize SPR in a semi-supervised manner. Experimental results on benchmark datasets and real-world noisy datasets validate the effectiveness of SPR. Our contributions include presenting a statistical approach for identifying noisy data, proposing a scalable framework, introducing a sparse penalty to encourage linearity, and designing a semi-supervised training framework.