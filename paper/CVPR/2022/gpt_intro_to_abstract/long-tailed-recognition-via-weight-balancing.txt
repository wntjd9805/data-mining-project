In this paper, we address the problem of long-tailed recognition (LTR) in computer vision, where the class distribution of data is heavily imbalanced. While existing methods have proposed various techniques to balance the per-class data distributions or losses during training, we explore an orthogonal direction by focusing on weight balancing across classes. We introduce three simple techniques - L2-normalization, weight decay, and the MaxNorm constraint - to balance the network weights in norm. Through experimentation, we demonstrate that these regularizers effectively learn balanced weights and improve LTR performance. Our approach follows a two-stage training paradigm and achieves competitive results without the need for complex architectures or aggressive data augmentation techniques. We discuss the significance of our findings and highlight how our simple approach challenges existing LTR models, serving as a strong future baseline for the field.