Despite the success of deep neural networks in various applications, the design of optimal network architecture remains challenging. Handcrafted models have shown impressive performance, but efforts to identify the optimal architecture automatically have been limited. Current approaches either result in suboptimal architectures or require a significant amount of training. While powerful and efficient operations have been investigated for deep neural networks, pooling operations have not received much attention. The size and shape of a receptive field are critical for effective recognition of different objects. Existing approaches rely on human-engineered hyperparameters or time-consuming neural architecture search. To address these limitations, we propose DynOPool, a learnable resizing module that dynamically optimizes the receptive field. DynOPool finds the optimal scale factor for receptive field size and shapes intermediate feature maps accordingly. This relieves the need for delicate hyperparameter design. Our contributions include tackling the limitations of current scaling operators and presenting DynOPool as a solution. We demonstrate the effectiveness of DynOPool in improving performance on multiple datasets and network architectures in classification and segmentation tasks. Our paper is organized as follows: related works in Section 2, motivation in Section 3, technical details of DynOPool in Section 4, experimental results in Section 5, and concluding remarks and future work in Section 6.