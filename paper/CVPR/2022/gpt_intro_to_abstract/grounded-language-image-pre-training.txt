In this paper, we introduce Grounded Language-Image Pre-training (GLIP), a method for learning object-level, language-aware, and semantic-rich visual representations. We propose the unification of object detection and phrase grounding tasks, where object detection is reformulated as context-free phrase grounding and phrase grounding is seen as contextualized object detection. We showcase the effectiveness of this approach by demonstrating that any object detection model can be converted to a grounding model by replacing the object classification logits with word-region alignment scores. We also highlight the scalability of GLIP by augmenting pre-training data with automatically generated grounding boxes for massive image-text pairs. We present empirical results showing that GLIP achieves superior performance in transfer learning tasks, surpassing previous state-of-the-art models in object detection and demonstrating excellent data efficiency. Additionally, we show that GLIP allows for domain transfer with few or no additional human annotations and can be fine-tuned for specific tasks without modifying the entire model. Overall, GLIP offers a holistic and efficient approach to visual recognition models, improving their usability and performance in real-world applications.