This paper addresses the problem of few-shot class incremental learning (FSCIL), where a model needs to incrementally learn new classes with only a few training examples while avoiding catastrophic forgetting of old classes. The authors propose a fully learned solution based on meta-learning to directly optimize the model for forgetting alleviation and adaptation. They introduce a sequential task sampling scheme to mimic the incremental learning process and a bi-directional guided modulation mechanism to strengthen the back-propagation and preserve old knowledge while learning new classes. The proposed method achieves state-of-the-art performance on standard benchmarks, including CIFAR100, MiniImageNet, and CUB200. The experimental results demonstrate the effectiveness of the proposed approach in tackling the challenges of FSCIL.