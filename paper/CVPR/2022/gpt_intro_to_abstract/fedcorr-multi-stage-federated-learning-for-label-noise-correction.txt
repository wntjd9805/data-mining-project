Federated learning (FL) is a promising solution for large-scale collaborative learning, where clients jointly train a machine learning model while maintaining local data privacy. However, in real-world FL implementations, there may be differences in the characteristics of different clients due to diverse annotators' skill, bias, and hardware reliability. This introduces challenges in two aspects: the discrepancy in local data statistics and the quality of local labels.Although recent works have explored these challenges individually in FL, there is currently no unified approach for addressing both challenges simultaneously. Existing works assume that the given labels of local data are completely correct, which is rarely the case in real-world datasets. Additionally, methods for dealing with label noise in centralized learning are inadequate in the FL setting due to the limited sizes of local datasets and privacy requirements.In this paper, we propose a multi-stage FL framework called FedCorr to simultaneously tackle both challenges. To ensure privacy, we introduce a dimensionality-based filter to identify noisy clients by measuring the local intrinsic dimensionality of model prediction subspaces. We also propose a method to filter noisy samples and relabel them based on per-sample training losses. Furthermore, we introduce a weighted proximal regularization term based on estimated local noise levels to improve training stability and mitigate the negative impact caused by noisy clients.Our main contributions include the proposal of the FedCorr framework, a framework for generating synthetic label noise and non-IID client data partitions, and the identification of noisy clients and labels. We demonstrate through experiments that FedCorr outperforms state-of-the-art FL methods on multiple datasets with different noise levels and data partitions.Overall, FedCorr provides a comprehensive approach to address the challenges of data heterogeneity in FL, incorporating privacy-preserving label correction to improve robustness.