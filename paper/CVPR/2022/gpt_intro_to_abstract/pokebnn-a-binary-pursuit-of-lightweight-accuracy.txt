The introduction of this computer science paper highlights the need for Pareto optimization in deep learning research. In the past, a single metric per benchmark was sufficient, but now there is a need to account for various model architectures, sizes, and computational costs. The choice of the optimization metric is discussed, with the paper proposing a new proxy metric called arithmetic computation effort (ACE) that aims to estimate inference cost abstracting of concrete ML hardware. The impact of quantization and binarization is also explored, showing that reducing the quantization bits can result in significant inference cost reduction. Binarization, in particular, has the potential to greatly improve performance in binary neural networks (BNNs). However, optimizing BNNs is challenging due to their chaotic and discontinuous loss landscape. The paper's main contributions include the proposal of PokeConv, a binary convolutional block that improves BNN accuracy, and PokeInit, a block that reduces the network's cost. The optimization of a clipping bound hyper-parameter in BNNs and the introduction of the ACE cost metric are also presented. The paper empirically shows that PokeBNN establishes the Pareto state-of-the-art in top-1 accuracy together with CPU64, ACE, and network size cost metrics on ImageNet.