Spatial-temporal video super-resolution (ST-VSR) has gained significant attention due to its applications in high-resolution slow-motion generation, movie production, and high-definition television upgrades. Current approaches can be categorized into two-stage and one-stage methods, but they have limitations in exploring the mutually coupled correlations between spatial video super-resolution (S-VSR) and temporal video super-resolution (T-VSR). One-stage methods either fail to consider these correlations or only focus on unilateral relationships. This leads to reconstruction errors and aliasing effects in super-resolved results. To address these limitations and make full use of spatial and temporal information, we propose a novel one-stage method called CycMu-Net. CycMu-Net utilizes mutual learning to achieve spatial-temporal fusion and eliminate cross-space errors. It incorporates iterative up-and-down projection units to aggregate temporal relations for accurate spatial representation, and to refine temporal information using updated spatial predictions. Experiments on ST-VSR, S-VSR, and T-VSR tasks demonstrate that CycMu-Net outperforms state-of-the-art methods. Our contributions include the proposal of CycMu-Net, a one-stage cycle-projected mutual learning network for spatial-temporal video super-resolution, and the introduction of iterative up-and-down projection units to exploit mutual information for improved spatial-temporal fusion and inference accuracy.