The goal of our work is to develop an interactive system that can generate novel views of a scene based on a few RGB images, without requiring expensive per-scene processing. This system has potential applications in virtual exploration of urban spaces, mapping, visualization, and augmented/virtual reality. The main challenge lies in inferring a scene representation that contains sufficient 3D information to render novel views with accurate parallax and occlusions.Traditional methods for scene representation involve explicit 3D representations such as colored point clouds, meshes, voxels, octrees, and multi-plane images. While these representations enable interactive rendering, they often require costly and fragile reconstruction processes. More recent work explores the use of purely implicit representations, such as NeRF, which represents the scene as a 3D volume parameterized by a multi-layer perceptron (MLP). However, NeRF requires a large number of MLP evaluations for volumetric rendering, relies on precise camera poses, and necessitates an expensive training procedure for new scenes.To address these limitations, we propose a novel approach that utilizes an encoder-decoder model based on transformers, allowing for scalable implicit representation learning. We replace explicit geometric operations with learned attention mechanisms. Unlike previous methods that rely on auto-decoder optimization or locally-conditioned geometry, our model instantaneously infers novel scenes using an encoder architecture while reasoning globally. This approach offers stronger generalization abilities and improved efficiency. Instead of processing global information hundreds of times per pixel, as in previous methods, our model processes it only once per scene.We evaluate our model on various datasets of increasing complexity, comparing it to relevant prior methods. Our evaluation demonstrates that our model achieves a unique balance of scalability, robustness to noisy camera poses (or no poses at all), and efficiency in interactive applications. Additionally, we provide a proof-of-concept for semantic segmentation using the learned representation on a challenging real-world dataset.Overall, our work presents a promising approach to interactive novel view synthesis that overcomes the limitations of previous methods and demonstrates superior performance in diverse scenarios.