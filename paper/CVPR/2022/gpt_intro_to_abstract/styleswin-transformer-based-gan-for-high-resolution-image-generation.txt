Image generative modeling has made significant advances in recent years, with generative adversarial networks (GANs) leading the way in synthesizing high-resolution images. While early efforts focused on training stability and loss designs, recent advancements have centered around architectural modifications like self-attention, model scaling, and style-based generators. Despite these improvements, using transformers in generative networks for high-resolution image generation remains challenging. This paper aims to explore the use of transformers in competitive GANs for high-resolution image generation. The authors propose leveraging Swin transformers as the basic building block to balance computational efficiency and modeling capacity. They introduce local attention to characterize all image scales and avoid the need for relearning image regularities. The proposed network, called StyleSwin, incorporates instrumental architectural adaptations like style injection, double attention, and sinusoidal positional encoding to enhance generative model capacity. The authors also address blocking artifacts in synthesizing high-resolution images and propose a wavelet discriminator approach to suppress these artifacts. StyleSwin achieves state-of-the-art results on multiple benchmarks, outperforming prior ConvNet-based methods in terms of quality and FID scores.