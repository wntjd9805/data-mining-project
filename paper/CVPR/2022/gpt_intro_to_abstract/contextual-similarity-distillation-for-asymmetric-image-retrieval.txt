Most existing image retrieval methods use the same model to map both query and gallery images to feature vectors, resulting in symmetrical retrieval. However, this approach often requires a large model for feature extraction, which is inefficient and impractical in scenarios with limited computing and memory resources. To address this issue, we propose a Contextual Similarity Distillation (CSD) framework for transferring knowledge from large gallery models to lightweight query models while ensuring feature compatibility. Unlike previous methods, our framework does not rely on specific labels or training datasets, making it suitable for real retrieval scenarios. We introduce a contextual similarity consistency constraint that takes into account both first-order feature preserving and second-order neighbor relationships between images. This constraint guides the learning of the lightweight model, which can be trained using a large amount of unlabeled data. Experimental results on the Revisited Oxford and Paris datasets demonstrate the effectiveness and generalizability of our approach, outperforming state-of-the-art methods by a significant margin.