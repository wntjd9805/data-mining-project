Deep Neural Networks (DNNs) have become essential in machine learning, with applications in various domains such as face recognition, image generation, autonomous driving, and medical diagnosis. However, the lack of transparency and interpretability in DNNs makes them susceptible to manipulation, leading to serious security threats. Adversarial examples, where deliberate distortions are added to images during inference, can cause misclassification in neural network classifiers. Backdoor attacks, on the other hand, exploit the opacity and overfitting of DNNs to create maliciously trained networks that perform well on normal samples but behave poorly on specific attacker-chosen inputs. Backdoor attacks are more challenging to detect, especially in black-box models, and pose a greater risk due to the increased availability of cloud training and third-party models. Various defense methods have been proposed, but they require a significant amount of clean data and may not accurately locate the attacked neurons. To address this, we introduce the concept of Shapley value from game theory and propose the Shap-Pruning framework for detecting and mitigating backdoor attacks. Shapley value is used to attribute the backdoor behavior to each neuron, allowing the identification of neurons with the highest Shapley value as the most responsible for the backdoor behavior. Our Shap-Pruning method requires minimal data and prunes only a small percentage of neurons while maintaining good classification accuracy. We also propose techniques to accelerate Shapley value estimation and a data-free backdoor cleanse method using batch normalization layer information. Our contributions include the introduction of Shapley value in backdoor mitigation, accurate identification and pruning of poisoned neurons, and robustness in different scenarios.