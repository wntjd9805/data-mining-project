The parametric face/head model is a popular research topic in computer vision and computer graphics, with applications in identity recognition, face analysis, and film/game production. Early models focused on 3D faces, ignoring non-face parts like hair and teeth. However, with the advancement of deep learning, 2D generative adversarial networks (GANs) have emerged, allowing for the direct rendering of photo-realistic face images. Some methods introduce constraints to render face images in a user-controlled way but do not explicitly encode or model 3D geometry. Recently, neural radiance fields (NeRF) have been proposed as a way to represent 3D scenes and synthesize photorealistic images. Compared to previous methods, NeRF implicitly encodes the 3D geometry of the scene, resulting in excellent multi-view consistency. Building on this, we propose HeadNeRF, a novel NeRF-based parametric head model. HeadNeRF inherits NeRF's properties, generating high-fidelity head images with multi-view consistency and supporting pose editing of rendered images. HeadNeRF also supports differentiable rendering, unlike traditional 3D representations, and only requires 2D images as input. We collect and process three large-scale human head image datasets and design novel loss terms to disentangle the parametric representation. By integrating volume rendering with 2D neural rendering, HeadNeRF achieves real-time rendering speeds exceeding 40fps without sacrificing quality. We apply HeadNeRF to various applications, including novel view synthesis, face attribute editing, and facial reenactment. Our contributions include proposing the first NeRF-based parametric human head model, presenting an effective training strategy, and demonstrating the effectiveness of HeadNeRF through various applications.