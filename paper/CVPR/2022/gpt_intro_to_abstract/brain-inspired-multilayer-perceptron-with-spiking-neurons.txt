In computer vision tasks, convolution neural networks (CNNs) have gained popularity due to their ease of training and fewer parameters compared to MLPs, but their learning ability is limited by inductive bias. Recent advancements in vision backbones, such as Transformers and MLPs, have presented challenges to CNNs. Transformers, initially proposed for NLP, have shown potential for computer vision tasks with their self-attention mechanism. MLPs, on the other hand, have demonstrated better learning ability on larger datasets without inductive bias. The key approach in utilizing Transformers and MLPs is to divide images into patches and perform calculations on each patch. Communication between patches is achieved through permutation and shift operations. However, spiking neural networks (SNNs) have a mature mechanism for information communication, especially for sparse data. SNNs, inspired by the brain, are frequently used for tasks like dynamic vision sensing. While SNNs excel in energy efficiency, they suffer from accuracy loss compared to CNNs. Research on SNNs has focused on transforming CNNs to SNNs more efficiently and losslessly, as well as training SNNs directly to achieve comparable accuracy with CNNs. The state-of-the-art conversion methods can adapt classic CNN models with minimal accuracy drop, but still require a larger number of time steps, resulting in higher latency. In this paper, we introduce the use of spiking neurons (specifically the LIF module) inspired by the brain to communicate information between patches in MLP models. Different variations of LIF neurons, including horizontal LIF, vertical LIF, and group LIF, are utilized to preserve information and extract better local features. Our experiments on classification, segmentation, and detection demonstrate that the proposed SNN-MLP models achieve state-of-the-art performance among existing MLPs. Notably, our model achieves top-1 accuracies of 81.9%, 83.3%, and 83.5% on the ImageNet dataset with only 4.4G, 8.5G, and 15.2G FLOPs, respectively.