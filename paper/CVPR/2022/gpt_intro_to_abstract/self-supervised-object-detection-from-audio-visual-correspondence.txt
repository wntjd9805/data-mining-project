This paper addresses the problem of learning interpretable and actionable concepts automatically in image and video representations, without the need for manual labels. Specifically, the focus is on learning to simultaneously detect and classify objects without any manual supervision. The authors propose a method that utilizes the multi-modal aspect of videos, using sound as a weak cue to learn about objects in the visual component of the data. They suggest treating the sound component as a useful cue for learning an object detector, but not as a necessary requirement for detection. The proposed method involves using a sound source localization network to learn possible object locations in videos, extracting bounding box pseudo-annotations, and training a standard object detector using only the visual modality. Additionally, a joint formulation is presented to simultaneously learn to localize sound sources and classify them without any supervision. The results demonstrate the effectiveness of the proposed method in learning an object detector without any manual annotation.