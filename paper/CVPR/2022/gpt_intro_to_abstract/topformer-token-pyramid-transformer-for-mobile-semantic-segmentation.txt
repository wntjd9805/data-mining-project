Vision Transformers (ViTs) have shown remarkable results in various vision tasks. However, their full-attention mechanism requires substantial computational resources beyond the capabilities of many mobile and embedded devices. This paper introduces a mobile-friendly Vision Transformer designed specifically for dense prediction tasks like semantic segmentation. The proposed approach combines the strengths of traditional Convolutional Neural Networks (CNNs) and ViTs. It uses a Token Pyramid Module based on light-weight MobileNetV2 blocks to quickly process high-resolution images and generate local feature pyramids. To obtain rich semantics and a large receptive field, a ViT-based module called Semantics Extractor is adopted. The computational cost is further reduced by using average pooling to reduce the number of tokens. Scale-aware global semantics are obtained by pooling and concatenating tokens from different scales. These semantics are fused with corresponding tokens to enhance representation for dense prediction tasks. Experimental results demonstrate the effectiveness of the proposed approach on segmentation and object detection tasks, achieving better results with lower latency compared to MobileNets on various datasets. The contributions of this paper include the TopFormer module for obtaining scale-aware semantics with light computation cost, the Semantics Injection Module for building powerful hierarchical features, and the base model achieving superior accuracy and real-time segmentation on mobile devices.