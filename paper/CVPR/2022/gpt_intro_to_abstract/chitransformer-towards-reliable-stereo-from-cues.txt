In this paper, we address the limitations of stereo matching and monocular depth estimation (MDE) methods in computer vision by proposing a novel approach called ChiTransformer. Stereo matching involves finding correspondences between pixels in two rectified images, while MDE estimates depth from a single view. We introduce ChiTransformer, a self-supervised binocular depth estimation network inspired by the optic chiasm. Our approach combines the benefits of both stereo matching and MDE by injecting stereo information into the MDE process to improve depth estimation. ChiTransformer uses a vision transformer as the backbone and employs a retrieval cross-attention layer to retrieve depth cues from the other view. We design a self-adjoint operator to condition the initial state and enable feature-coincident retrieval. This design facilitates reliable retrieval and improves feature-consistent details while maintaining global coherence. The model can also be extended to handle non-rectilinear images using a gated positional embedding. We train the model using a self-supervised learning strategy. Unlike traditional stereo methods, ChiTransformer does not rely on pixel-level matching optimization but leverages the context-infused depth cues for overall depth prediction. The model performs well on depth estimation tasks with stereo pairs, outperforming top-performing self-supervised stereo methods by more than 11%. ChiTransformer is also tested on stereo tasks and demonstrates the benefits of stereo cues and the reliability of instilled stereo information. Additionally, the model achieves visually satisfactory results on non-rectilinear images. Overall, ChiTransformer provides reliable depth prediction with guided cues, making it suitable for complex and dynamic environments.