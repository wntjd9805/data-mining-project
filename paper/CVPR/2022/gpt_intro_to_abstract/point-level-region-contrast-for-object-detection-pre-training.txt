Un-/self-supervised learning, particularly contrastive learning, has emerged as a powerful tool for obtaining visual representations using unlabeled data. Currently, state-of-the-art object detectors still rely on weights from supervised pre-training, such as classification on ImageNet-22K. The full potential of unsupervised pre-training for object detection is yet to be realized. Object detection requires accurate localization and semantic recognition of objects, which are tightly connected and mutually reinforce each other. We propose that a useful representation for object detection should balance recognition and localization by leveraging information at various levels during pre-training. To achieve this, we present a self-supervised pre-training approach that contrasts at the region-level while operating at the point-level. We divide each image into non-overlapping regions and define an intra-image discrimination task on top of the inter-image discrimination task. Unlike existing methods that aggregate features, we directly operate at the point-level by sampling multiple points from each region and contrasting point pairs individually across regions. Operating at the point-level has two advantages: robustness to imperfect regions and the ability to bootstrap for better regions during training. We demonstrate the effectiveness of our approach on standard pre-training datasets and transfer the representation to multiple downstream datasets, showing strong results compared to state-of-the-art pre-training methods. We also provide extensive ablation studies and visualize the learned point affinities through knowledge distillation. Although improvements on larger models and longer training schedules are yet to be explored, our work inspires further research on pre-training designs that better balance recognition and localization in object detection.