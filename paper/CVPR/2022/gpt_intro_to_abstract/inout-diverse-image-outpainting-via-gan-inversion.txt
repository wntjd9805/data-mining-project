Given an input image, the ability to mentally extrapolate and visualize adjacent scenery is a valuable skill. This skill, known as image outpainting, has numerous applications in content creation, such as image editing, panorama generation, and virtual reality experiences. While recent advancements in image inpainting have addressed the problem of filling in missing pixels, the outpainting problem presents unique challenges due to limited context near the boundary. Existing image-to-image translation methods have limitations in terms of texture and structure leakage and deterministic output. In this paper, we propose an approach to tackle the outpainting problem using generative adversarial networks (GANs). We extend the StyleGAN2-based generator to perform coordinate conditional generation and independently generate spatially consistent micro-patches. We formulate outpainting as finding optimal latent codes for input micro-patches and generating desired regions with proper coordinate conditioning. To enhance diversity in the output, we employ a GAN inversion process to find multiple latent codes. Additionally, we introduce a categorical generation schema for flexible user control. We evaluate the proposed method qualitatively and quantitatively using datasets and metrics such as Fr√©chet Inception Distance (FID) and Learned Perceptual Image Patch Similarity (LPIPS). Finally, we demonstrate the effectiveness of our method in categorical generation and panorama generation scenarios.