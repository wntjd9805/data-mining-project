This paper focuses on the generation of accurate representations of 3D human shape and appearance in image or video generation tasks. UV coordinates, which establish correspondences between 2D images and 3D surface-based representations of the human body, are crucial in encoding human pose and shape for various applications. Unlike previous works that use large networks to capture motion and appearance, this paper proposes a model that generates temporally coherent UV coordinates, specifically targeting loose clothing. The paper compares indirect and direct methods of deriving UV coordinates from 3D shape models and emphasizes the limitations of existing methods in capturing loose clothing and maintaining temporal consistency in video sequences. The proposed approach aims to improve spatial coverage and temporal coherence of UV coordinates in video generation, regardless of the UV source used. A challenge in learning a model for UV coordinates is the lack of direct supervision, so the paper presents a novel learning scheme that combines supervised and unsupervised components. The results show that using loss terms in both UV and image space are crucial for generating high-quality UV coordinates with temporal coherence. The trained model can directly pair the generated UV coordinates with different texture maps for virtual try-on videos, offering a device-independent and efficient solution. The main contributions of this work include a model-agnostic method for capturing the complete appearance of the human body, an approach for training neural networks to generate completed and temporally coherent UV coordinates without ground truth, and an efficient method for generating virtual try-on videos with arbitrary clothing styles and textures.