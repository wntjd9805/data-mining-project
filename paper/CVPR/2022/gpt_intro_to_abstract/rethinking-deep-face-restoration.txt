Face images are crucial in various applications such as face identification and portrait taking. However, low-quality face images are common in real-world scenarios due to factors like low resolution, motion blur, and encoding artifacts. Deep generative adversarial networks (GANs) have been utilized for face restoration, where a degraded face is transformed into a high-fidelity one. Existing models, although capable of generating realistic faces, often fail to preserve delicate facial features and may change the subject's eye color, skin texture, and facial component shapes. This can impact downstream applications such as face identification and perceptual quality of photos. We argue that the balance between face generation and face reconstruction is key to addressing these issues. We propose a new model that focuses on improving both generation and reconstruction. To enhance face generation, we incorporate adaptive conditional noise inspired by successful image generation models. For face reconstruction, we improve the latent features in skip connections by quantizing features using a learned codebook and introducing a global feature fusion module. We also explore the model architecture to optimize the balance between generation and reconstruction. Existing evaluation metrics for face restoration are biased towards either generation or reconstruction. Therefore, we propose a new metric that measures both image quality and content preservation (identity preservation). Our empirical results demonstrate that the proposed model outperforms state-of-the-art methods on blind face restoration and super-resolution benchmarks. Additionally, our proposed metric better correlates with human perceptual quality. A user study confirms that our model is preferred by human raters compared to existing face restoration models.