In the field of computer science, the training and evaluation of image super-resolution (SR) models have been traditionally carried out using certain benchmark datasets and performance metrics such as peak-signal-to-noise ratio (PSNR) and structural similarity index (SSIM). However, this standard approach has several limitations. Firstly, it represents the model's performance using a single value, which fails to capture the variability within images or datasets. Secondly, without a clear description of the datasets, the performance results may be misleading. Additionally, during inference, SR networks often change the semantic meaning of images, which can have significant implications in safety-critical domains. To address these concerns, this paper proposes a texture-based error analysis framework for image SR. The framework aims to analyze the sources of SR errors from different perspectives, including texture distributions, SR performance vs. texture classes, and texture alignment between high-resolution and SR images. The paper presents insights and contributions, such as providing a comprehensive semantic profile of datasets, highlighting biases in the datasets, proposing alternative evaluation methods, demonstrating the impact of deep learning on different semantic labels, and introducing a metric to curate a strategic training set. The paper also uncovers how SR models can alter the semantic meaning of patches. Overall, this research aims to improve the understanding and evaluation of image SR models.