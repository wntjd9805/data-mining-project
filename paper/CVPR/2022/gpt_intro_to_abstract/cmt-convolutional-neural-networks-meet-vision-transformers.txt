The introduction of this computer science paper discusses the contributions of Convolutional Neural Networks (CNNs) and self-attention based transformers in the field of computer vision and natural language processing (NLP) tasks, respectively. While transformer-based models have shown promise in vision tasks, they still lag behind convolutional neural network counterparts in terms of performance. The authors propose a novel architecture called CMT (CNNs meet transformers) that combines the advantages of both CNNs and transformers. The CMT architecture utilizes CNNs for fine-grained feature extraction and transformers for modeling global relationships and capturing long-range dependencies. The authors address the limitations of existing transformer-based models, such as the simplification of images into patches that ignore spatial information, the difficulty in extracting low-resolution and multi-scale features, and the high computational and memory cost of self-attention modules. The proposed CMT architecture achieves superior performance in terms of accuracy and computational efficiency compared to existing models. Additionally, the CMT architecture can be easily applied to other vision tasks as a versatile backbone.