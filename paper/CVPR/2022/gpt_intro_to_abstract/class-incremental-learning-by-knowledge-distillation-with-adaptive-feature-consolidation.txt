Deep neural networks have shown remarkable performance in a variety of applications. However, training models for a sequence of tasks in online settings remains challenging due to limited availability of examples from previous tasks. Fine-tuning strategies are ineffective for streaming tasks due to catastrophic forgetting, where the model performs well on the current task but fails on previous ones. Incremental learning, which enables online learning without forgetting, has been actively studied to mitigate this problem. Different algorithms, such as architectural approaches, rehearsal methods, parameter regularization, and knowledge distillation approaches, have been proposed. In this paper, we present a knowledge distillation approach for class incremental learning. Unlike existing methods, our approach considers the importance of feature maps in maintaining previously acquired knowledge. We estimate the impact of model updates on loss and show that proper weighting of feature maps enhances knowledge distillation. Our optimization strategy aims to minimize the upper bound of expected loss increases over previous tasks, reducing the catastrophic forgetting problem. Experimental results demonstrate the superiority of our technique compared to existing methods. The paper is organized as follows: related works are discussed in Section 2, our method is described in Section 3, experimental results are presented in Section 4, and conclusions are provided in Section 5.