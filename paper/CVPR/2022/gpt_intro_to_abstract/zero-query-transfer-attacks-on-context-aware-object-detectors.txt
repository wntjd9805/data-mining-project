Deep neural networks (DNNs) have achieved significant performance gains in vision and language tasks. However, they are vulnerable to adversarial attacks. One popular type of attack is the perturbation-bounded evasion attack, where an attacker adds imperceptible perturbations to an input image to manipulate the model's classification results. Most existing work in this area focuses on attacking classifiers trained on datasets like ImageNet and CIFAR, which recognize a dominant object in an image. In contrast, our work focuses on object detectors that localize and recognize multiple objects in an image, which is more representative of natural scenes. Object detectors often consider the context of an image, using the relationships between objects to aid in recognition. Recent work has proposed context-aware defense mechanisms and attacks that consider the context of object co-occurrence. However, these attacks typically require multiple queries to the victim system. In this paper, we present a zero-query attack algorithm that changes multiple objects simultaneously in a context-consistent manner, creating a holistic adversarial scene. We introduce the concept of a perturbation success probability matrix (PSPM) to guide our attack plans and improve the fooling rate. Our experiments show that our zero-query attacks achieve higher fooling rates compared to context-agnostic black-box attacks and are comparable to few-query attacks.