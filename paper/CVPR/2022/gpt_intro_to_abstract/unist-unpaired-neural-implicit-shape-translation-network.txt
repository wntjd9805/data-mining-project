Unpaired image-to-image translation has garnered significant attention in computer vision research since the introduction of CycleGAN, DualGAN, and UNIT in 2017. However, there has been a lack of research on unpaired shape-to-shape translation, which involves more significant structural alterations. Existing methods, such as LOGAN, are limited by low-resolution point clouds and the lack of positional information in the encoding. In this paper, we propose a method for unpaired shape-to-shape translation using autoencoding neural implicit fields instead of point clouds. Our approach combines the advantages of neural implicit models with position awareness, resulting in better preservation of spatial features and details. Our model, called UNIST, consists of two separately trained networks: an autoencoding network and a translation network. The autoencoding network learns to encode and decode shapes using latent grids, while the translation network performs the actual shape translation using the latent grid features produced by the autoencoder. We demonstrate the effectiveness of our model in both shape reconstruction and shape translation tasks and compare it to existing methods. Our work represents the first deep implicit model for general-purpose, unpaired shape-to-shape translation and shows significant improvements in quality by leveraging autoencoding implicit fields and position awareness.