This paper introduces the concept of zero-shot image captioning using the CLIP image-text transformer model and the GPT-2 language model. Unlike traditional image captioning methods that require training on curated datasets, our approach leverages the CLIP model trained on the web-scale dataset WIT. This difference in training data leads to more diverse and visually accurate captions, as demonstrated in our results. Additionally, our method allows for semantic analysis in the image space and can perform visual-semantic arithmetic, providing a novel computer vision capability. We also showcase the ability to compare and analyze two images semantically, further highlighting the power of zero-shot learning in computer vision.