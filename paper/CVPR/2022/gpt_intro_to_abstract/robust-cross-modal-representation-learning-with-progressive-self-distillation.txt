This paper introduces the problem of data and compute inefficiency in multimodal pre-training methods, specifically focusing on the CLIP model. These methods rely on large-scale weakly correlated multimodal data to learn cross-modal representations through contrastive learning techniques. However, the success of CLIP comes with the requirement of vast amounts of training data and computational resources. This limitation prevents widespread adoption and sustainability of the approach.The inefficiency of CLIP can be partly attributed to the assumptions it makes about the web-harvested data used for training. Mainstream vision-language datasets often contain captions that may not accurately correspond to the image content. Additionally, when using larger batch sizes, the model is more likely to encounter negatives with high semantic similarity, degrading the learned representations.To address these challenges, the proposed approach models the relationships between web-harvested images and captions more accurately using soft probabilities instead of hard pairing labels. This framework incorporates progressive self-distillation and soft image-text alignment targets, allowing for more efficient learning from noisy data. Instead of explicitly correcting or pruning noisy correspondences, the joint student-teacher model dynamically generates soft alignments for a random subset of images and captions in each mini-batch.During training, the network generates soft alignments for larger subsets of the mini-batch, effectively becoming its own teacher. Key elements are identified to ensure the student network predicts targets without representation collapse or reinforcing mistakes.The approach is extensively compared to CLIP on 14 benchmark datasets using multiple pretraining datasets. The results consistently show that the proposed approach outperforms CLIP under various settings. Analysis using an ImageNet-based test-bed demonstrates better effective robustness to natural distribution shifts compared to ImageNet-trained models and CLIP.The simplicity of the proposed approach allows for easy incorporation into existing and future methods. Overall, the paper addresses the data and compute inefficiencies in multimodal pre-training and presents a more efficient framework for contrastive language-image pretraining.