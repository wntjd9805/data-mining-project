Autonomous driving is a critical technology that aims to reduce traffic accidents and improve transportation efficiency. Current perceptual systems in autonomous vehicles mainly rely on LiDAR or camera sensors. However, these systems have limitations in terms of observing surroundings for extended periods and dealing with occlusion. In contrast, roadside cameras offer advantages such as robustness to occlusion and the ability to predict long-term events. The importance of roadside perception lies in its cooperative role with autonomous driving, global perception capabilities, cost efficiency, and contribution to intelligent traffic control. Existing research on roadside perception has focused mostly on 2D tasks, while the ability of 3D localization is still underexplored. This work aims to address this gap by focusing on monocular 3D detection from single images captured by roadside cameras. To facilitate research in this area, the authors present a large-scale and diverse dataset called "Rope3D" specifically designed for roadside 3D perception tasks. The dataset includes images and 3D objects collected across various lighting and weather conditions, as well as different road scenes with varying camera specifications. The authors also adapt existing frontal-view monocular 3D detection methods to the roadside application and propose new metrics for evaluating roadside 3D detection performance. The contributions of this work include the Rope3D dataset and a comprehensive study to facilitate the development of monocular 3D perception in roadside scenarios.