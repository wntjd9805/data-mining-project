Image processing tasks, such as restoration and enhancement, play a critical role in computer vision by aiming to produce high-quality outputs from degraded input images. Different types of image degradations require specific image enhancement treatments, such as denoising, deblurring, resolution enhancement, and dehazing. Recent advancements in convolutional neural network (CNN) based approaches have achieved state-of-the-art (SOTA) performance on various image processing tasks due to improved architectural designs. These designs incorporate modules and building blocks such as residual learning, dense connections, hierarchical structures, multi-stage frameworks, and attention mechanisms.While CNN models have been the go-to choice for image processing tasks, recent research has explored the potential of Vision Transformers (ViT) as an alternative. ViT, along with its variants MLP-Mixer, gMLP, GFNet, and FNet, have shown promising performance on high-level vision tasks. However, their efficacy on low-level enhancement and restoration problems has been limited. Existing Transformer-based approaches for low-level vision either utilize full self-attention, which suffers from patch boundary artifacts, or employ local-attention that limits the receptive field or loses non-locality.To address these limitations, we propose MAXIM, a generic image processing network for low-level vision tasks. MAXIM incorporates a multi-axis approach that captures both local and global interactions in parallel. By employing an MLP-based operator that mixes information on a single axis for each branch, MAXIM becomes fully-convolutional and scales linearly with respect to image size. Additionally, we introduce a pure MLP-based cross-gating module that adaptively gates the skip-connections in the neck of MAXIM, further improving its performance.Inspired by recent restoration models, we develop a multi-stage, multi-scale architecture by stacking MAXIM backbones. This architecture achieves strong performance on a range of image processing tasks while requiring few parameters and floating-point operations (FLOPs). Our contributions include the novel MAXIM architecture, the multi-axis gated MLP module tailored for low-level vision tasks, and the cross-gating block. Experimental results demonstrate that MAXIM achieves SOTA results on more than 10 datasets, including denoising, deblurring, deraining, dehazing, and enhancement tasks.