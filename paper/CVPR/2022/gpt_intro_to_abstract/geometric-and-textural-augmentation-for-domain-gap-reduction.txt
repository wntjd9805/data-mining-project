Recognizing objects in different depictions is a challenging problem in visual computing. While high performance rates have been achieved with photographic inputs, performance significantly drops with artwork inputs. This paper addresses the issue of "cross depiction" problem, where classifiers trained on photographs show decreased performance on artwork. It is hypothesized that the differences in low-level statistics and biased datasets contribute to this performance degradation. The paper explores the similarities and differences between neural networks and human perception, highlighting the vulnerability of neural networks to depiction shifts. Previous research has approached the problem as domain generalization, but the distance between images of the same object class but different styles poses a challenge for existing methods. Recent literature has utilized style transfer to mitigate depiction bias, resulting in improved performance. However, artists also introduce geometric style variations in addition to texture changes, which can impact subjective judgments of style similarity. The contribution of this paper is to bridge the depiction-domain gap by considering both geometric and texture style, addressing the limitations of previous approaches. The research aims to support applications such as indexing digital art collections, curating multimedia databases, building artist interfaces, and intellectual property protection.