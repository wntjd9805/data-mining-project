Video visual relation detection (VidVRD) is a task that aims to detect relation instances in videos, involving subjects, objects, and their relationships. It connects basic vision tasks like object detection and tracking with more complex video semantic understanding tasks like captioning and VideoQA. Existing methods for VidVRD employ a multi-stage pipeline approach, utilizing object detectors to detect and track objects in a video, enumerating object tracklet pairs, and then predicting relation types for them. However, these approaches have limitations in terms of limited spatio-temporal contexts, excessive tracklet pair enumeration, and independently optimized modules. This work proposes a unified transformer-based video visual relation detection framework called VRDFormer, which addresses these limitations. The framework consists of an encoding module and a query-based relation instance generation module. It adopts a query-based approach for object pair detection and tracking, utilizing static queries for spatial context aggregation and recurrent queries for temporal context aggregation. A transformer-based model is used for relation classification, taking into account long-term spatio-temporal history. VRDFormer is end-to-end trained for both object pair detection and relation classification tasks. Extensive experiments show that VRDFormer achieves state-of-the-art performance in relation detection and tagging on benchmark datasets. In summary, the contributions of this work include the proposal of VRDFormer as a unified model for VidVRD, the design of static and recurrent queries for context aggregation, and achieving state-of-the-art performance on benchmark datasets through extensive experiments and analysis.