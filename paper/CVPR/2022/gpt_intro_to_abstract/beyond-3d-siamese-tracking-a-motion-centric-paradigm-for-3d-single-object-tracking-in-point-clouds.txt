Single Object Tracking (SOT) is a vital problem in computer vision, with applications in autonomous driving and surveillance systems. Existing SOT methods based on LiDAR technology follow the Siamese paradigm, which relies on appearance matching techniques for tracking. However, we observe that these methods are not suitable for natural scenes with significant appearance changes and distractors. We propose a new motion-centric paradigm for 3D SOT that explicitly models the target's motion between consecutive frames. We introduce a two-stage tracker called M2-Track, which predicts the inter-frame relative target motion in the first stage and refines the bounding box using aggregated target point cloud information in the second stage. We evaluate our model on KITTI, NuScenes, and Waymo Open Dataset, and it outperforms existing methods in terms of tracking performance while running faster. Our contributions include the motion-centric paradigm, the M2-Track pipeline, and improved online tracking performance on multiple datasets.