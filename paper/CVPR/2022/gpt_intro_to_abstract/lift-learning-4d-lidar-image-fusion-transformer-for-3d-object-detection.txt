This paper focuses on the problem of 3D object detection for autonomous driving, where cameras and LiDAR sensors are commonly used to perceive the environment. The authors observe that the information provided by these sensors may be misaligned over time, due to asynchronous timelines and different coordinate systems. Existing 3D object detection algorithms typically perform information fusion either over time or across sensors, but not simultaneously. The authors propose a novel approach called LiDAR Image Fusion Transformer (LIFT) to explicitly model the mutual correlations between cross-sensor data over time. LIFT combines a grid feature encoder with a sensor-time 4D attention network to effectively capture the spatiotemporal information fusion. The authors also introduce a data augmentation technique to preserve the consistency of the cross-sensor and cross-time information. Experimental results on large-scale datasets demonstrate the effectiveness of LIFT compared to state-of-the-art methods. This work contributes to the field by proposing a Transformer-based end-to-end 3D detection framework that integrates sequential multi-modal data and aligns the spatiotemporal cross-sensor information.