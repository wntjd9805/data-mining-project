Recent consistency-based semi-supervised learning (SSL) methods have shown significant progress in achieving competitive performance to supervised learning. These methods generate pseudo-labels on unlabeled samples using a model trained on labeled samples and enforce prediction consistency. However, these methods assume that the labeled and unlabeled data share the same class distribution, which is not always the case in real-world scenarios. This distribution mismatch can invalidate most SSL methods. To illustrate this problem, we compare the performance under matched and mismatched distribution scenarios using state-of-the-art SSL methods on CIFAR-10. We observe a significant drop in test accuracy in the presence of a distribution mismatch. Inspired by distribution alignment, we propose a method called Distribution Consistency SSL (DC-SSL) to rectify the biased pseudo-labels from a distribution perspective. DC-SSL estimates a reference class distribution from the unlabeled data and modifies the pseudo-labels using two updating strategies. These strategies, the training-free and training-based strategies, improve SSL performance by scaling the pseudo-labels and minimizing a distribution consistency loss, respectively. Our method consistently outperforms existing SSL methods, particularly in scenarios with distribution mismatches. Our contributions include revisiting the EMA model in SSL, proposing the DC-SSL method, and achieving state-of-the-art performance in SSL image classification benchmarks under both matched and mismatched distribution scenarios.