Self-supervised learning (SSL) techniques have shown promising results in producing visual representations comparable to those generated by fully supervised networks for various downstream tasks. This is significant as it enables computer vision to overcome limitations in obtaining large amounts of labeled data and addresses challenges in domains where supervision is difficult or costly to obtain. However, most state-of-the-art approaches in SSL are evaluated on standard datasets like ImageNet, limiting our understanding of their effectiveness on other datasets.This paper aims to answer important questions regarding the conditions under which self-supervised contrastive representation learning methods produce "good" visual representations. By exploring these questions, it contributes to the understanding of SSL and presents opportunities for new methods. Additionally, it addresses the needs of domain experts with limited resources who are interested in applying SSL to real-world problems.The paper investigates the impact of various factors on self-supervised pretraining, such as data quantity, pretraining domain, data quality, and task granularity. The questions addressed include the impact of data quantity on pretraining, the transferability of self-supervised representations across different domains, the robustness of self-supervised methods to data corruption, and the effectiveness of self-supervised features for different levels of classification tasks.Through extensive quantitative evaluation across diverse large-scale visual datasets, several interesting observations and recommendations are made. Notably, decreasing the amount of unlabeled training data has minimal impact on downstream classification performance, suggesting that current self-supervised methods may not effectively utilize very large pretraining sets. The effectiveness of self-supervised representations is highly dependent on the similarity between the pretraining and test domains, indicating limitations in generalization. Combining datasets or self-supervised features does not yield significant performance improvements, implying further research is needed for highly generalizable representations. Pretraining on corrupted images affects supervised and self-supervised learning differently, with self-supervised representations being sensitive to factors like image resolution. Finally, while self-supervised methods excel at coarse-grained concepts, they lag behind supervised baselines for fine-grained tasks, suggesting the need for improvements in handling fine-grained visual concepts.Overall, this paper contributes to the understanding of SSL and provides insights into the effectiveness and limitations of self-supervised representation learning in various domains. The findings have implications for researchers and practitioners interested in leveraging SSL techniques for real-world computer vision problems beyond traditional datasets like ImageNet.