Generative Adversarial Networks (GANs) have been highly successful in various applications, including text-to-image synthesis. This paper focuses on the generation of realistic and text-consistent images from natural language descriptions, which is an important area of research. The major challenges in text-to-image synthesis are ensuring the authenticity of the generated image and maintaining semantic consistency with the given text. Existing models address these challenges by employing stacked architectures, cross-modal attention, and extra networks. However, these approaches have limitations, such as entanglements between different generators, susceptibility to adversarial features, and computational constraints. To overcome these issues, this paper proposes a novel text-to-image generation method called Deep Fusion Generative Adversarial Network (DF-GAN). The proposed DF-GAN utilizes a one-stage backbone instead of stacked architectures to generate high-resolution images directly, avoiding entanglements. It employs a Target-Aware Discriminator to enhance semantic consistency by using Matching-Aware Gradient Penalty (MA-GP) and One-Way Output. Additionally, a Deep text-image Fusion Block (DFBlock) is introduced to effectively fuse text and visual features by employing Affine Transformations. Experimental results on challenging datasets demonstrate that DF-GAN outperforms existing state-of-the-art models in terms of both qualitative and quantitative evaluations. The contributions of this work include the development of DFBlock for effective text-image fusion and the superior performance of DF-GAN compared to existing models.