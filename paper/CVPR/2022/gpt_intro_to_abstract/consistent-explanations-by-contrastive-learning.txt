Deep neural networks have become popular for various computer vision tasks due to their high performance. However, these networks are often considered black box systems, as they lack transparency in their decision-making process. This lack of interpretability is undesirable, especially in critical applications like medical image analysis. To address this issue, researchers have proposed post-hoc explanation methods, such as CAM, Grad-CAM, and Full-Grad, which generate heatmaps to highlight the regions of the image that contribute the most to the network's decision. However, these methods do not always provide consistent interpretations when the images undergo spatial transformations. In this paper, we propose a novel method called Contrastive Grad-CAM (CGC) that aims to improve the consistency of Grad-CAM explanations. Inspired by self-supervised learning, we introduce a loss function that encourages the Grad-CAM of an image to be close to the Grad-CAM of an augmented version of the same image, while being far from the Grad-CAM of other random images. We evaluate our method using classification accuracy and a metric called Content Heatmap (CH), which measures the consistency of the explanation heatmap with respect to human annotations. Our experiments show that CGC improves both the consistency of interpretations and the classification accuracy, especially in fine-grained tasks and with limited labeled data. Additionally, since our method does not require labeled data, it can benefit from unlabeled data during training. Overall, our approach enhances the interpretability of deep neural networks, making them more reliable and trustworthy in real-world applications.