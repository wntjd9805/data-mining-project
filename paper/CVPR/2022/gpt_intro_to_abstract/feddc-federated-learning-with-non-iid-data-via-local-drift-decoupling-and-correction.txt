Federated learning (FL) is a distributed machine learning paradigm that uses decentralized data from multiple clients to train a shared global model. FL overcomes privacy risks associated with traditional parallel optimization by not sharing raw data. The widely used FL aggregation algorithm, FedAvg, faces challenges in converging well with heterogeneous data (non-iid). Client data distribution in FL can vary greatly, leading to inconsistency in local objective functions and optimization directions. Data heterogeneity introduces drift in clients' local updates, slowing down convergence speed. The contradiction between minimizing local empirical loss and reducing global empirical loss hinders convergence in highly heterogeneous environments. Existing methods attempt to reduce the variance of local updates or correct client drift, but these methods only partially address the issue and do not eliminate it. Most previous FL methods force consistency between local and global models, neglecting the inconsistency between local and global objectives. In this paper, we propose a new FL algorithm called FedDC, which handles inconsistent objectives by introducing auxiliary drift variables to track the local parameter drift. FedDC dynamically updates the local objective function of each client and decouples the local and global models using drift variables. This reduces the impact of local drift on the global objective, leading to faster convergence and better performance. Experimental results on various datasets demonstrate that FedDC outperforms competing FL methods in both iid and non-iid client settings.