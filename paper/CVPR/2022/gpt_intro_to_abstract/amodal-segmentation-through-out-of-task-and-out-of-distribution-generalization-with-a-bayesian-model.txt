This paper introduces a novel approach for amodal instance segmentation in computer vision, which is the ability to perceive and estimate the complete structure of an object even when it is partially occluded. The main limitation of current methods is the requirement for detailed supervision of amodal object masks, either through human annotation or artificially occluded images. Additionally, these methods assume prior knowledge of the occluder's object class, which is not practical in real-world scenarios such as autonomous driving. To address these limitations, the authors propose a Bayesian generative model trained from non-occluded objects with bounding boxes and class annotations only. The model is able to generalize to amodal segmentation of partially occluded objects, without explicit modeling of nuisances such as deformations or illumination changes. The authors extend the network architecture with a generative model of the object's background context and a prior of the object shape to make these notions explicit. The proposed Bayesian model, combined with an outlier process, allows for robust amodal segmentation by formulating it as an out-of-distribution task. The model is learned using maximum likelihood estimation with an EM-type algorithm and can be further improved through joint end-to-end fine-tuning. Experimental results on common amodal segmentation datasets demonstrate that the Bayesian approach outperforms alternative weakly-supervised methods and even performs well compared to fully supervised methods, particularly in scenarios with significant occlusion. In summary, this paper contributes to the field of amodal instance segmentation by formulating it as a generalization problem and proposing a Bayesian generative model that improves performance and generalizes to previously unseen occluders. This work has potential applications in various domains, such as autonomous driving.