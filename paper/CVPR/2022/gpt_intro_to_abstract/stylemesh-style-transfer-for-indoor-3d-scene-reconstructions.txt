Creating stylized 3D content from RGB-D scans is an intriguing topic in computer vision. While Neural Style Transfer (NST) has shown promising results for stylizing images and videos, its application to 3D content like meshes has been underexplored. In this paper, we propose a novel use case in this area, focusing on the stylization of reconstructed meshes with explicit RGB textures. We compare our approach to prior texture mapping methods and address the challenge of extending style transfer losses to 3D meshes. To achieve 3D-consistent stylization, we formulate an energy minimization problem that combines texture mapping with style transfer. We incorporate depth and surface normals to avoid view-dependent artifacts and leverage different resolutions and angles to create equally-sized stylization patterns in the world-space of the mesh. Our experiments demonstrate improved 3D-consistent stylization compared to state-of-the-art methods, both qualitatively and quantitatively. Furthermore, our explicit texture representation allows for direct integration with traditional rendering pipelines. Overall, our contributions enhance the field of 3D style transfer by introducing depth and angle awareness to achieve more realistic and consistent stylization of room-scale indoor scene meshes.