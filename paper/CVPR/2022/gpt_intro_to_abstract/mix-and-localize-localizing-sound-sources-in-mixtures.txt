Humans have the ability to localize multiple sounds at once, while existing audio-visual sound localization methods are designed for single sound sources. This paper addresses the issue of grouping a scene into multiple audio-visual events by proposing a model that combines contrastive learning and cycle consistency. The model produces embeddings for each sound source and image patch, and learns cross-modal correspondences to bring co-occurring sounds and images close together. The proposed model extracts sound sources from mixtures and groups them with distinct image content. Experimental results demonstrate the model's accuracy in localizing sounds in multi-source mixtures, surpassing other self-supervised localization methods.