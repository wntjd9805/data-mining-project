This paper introduces OBJECTFOLDER 2.0, a large dataset of implicitly represented multisensory replicas of real-world objects. The goal is to build a dataset of realistic and multisensory 3D object models that can be used for learning and generalization to real-world tasks. The existing methods for modeling real-world objects are limited and unrealistic, focusing mostly on 2D representations and lacking the full spectrum of physical object properties. OBJECTFOLDER 2.0 addresses these limitations by leveraging high-quality scans of real-world objects and simulating visual, acoustic, and tactile data for each object based on their intrinsic properties. This is achieved using an implicit neural representation network called Object File. The authors improve the rendering and simulation pipelines to generate more realistic multisensory data. The dataset contains 1,000 objects and is 10 times larger than existing work. The paper demonstrates the successful transfer of models learned on virtualized objects to three real-world tasks: object scale estimation, contact localization, and shape reconstruction. OBJECTFOLDER 2.0 enables applications in multisensory learning, robot grasping, and on-the-fly multisensory data generation. The main contributions of this paper include the introduction of a large-scale multisensory dataset, significant improvements in rendering quality and speed, and the demonstration of successful transfer learning to real-world tasks. This work opens up new possibilities for multisensory learning in computer vision and robotics.