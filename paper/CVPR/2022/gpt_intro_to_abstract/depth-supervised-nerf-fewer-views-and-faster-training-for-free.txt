Neural rendering using implicit representations has become a popular technique in computer vision and graphics, spanning a wide range of applications such as view synthesis, relighting, pose and shape estimation, 3D-aware image synthesis, and modeling dynamic scenes. The groundbreaking work of Neural Radiance Fields (NeRF) demonstrated impressive results in view synthesis by utilizing implicit functions to encode volume density and color observations.However, NeRF has its limitations. When provided with only a few training views, NeRF is prone to overfitting and fails to generate accurate novel views. Additionally, even with a large number of input views, NeRF's training process can be time-consuming and resource-intensive.In this paper, we propose the use of depth as an additional source of supervision to enhance the geometry learned by NeRF. Traditional NeRF pipelines rely on images and camera poses estimated from structure-from-motion (SFM) solvers, which also provide sparse 3D point clouds and their reprojection errors. By incorporating depth supervision in the form of reprojection error, we anchor NeRF's search for implicit correspondences with explicit sparse keypoints derived from SFM.Experimental results demonstrate that our depth-supervised NeRF approach significantly improves training efficiency and reduces the amount of required training data. We observe a 2-3x acceleration in model training without sacrificing output quality. In sparse view settings, our method outperforms both the original NeRF and recent sparse-views NeRF models on NeRF Real and Redwood-3dscan datasets. Furthermore, we show that our depth supervision loss can be applied to depth derived from other sources, such as depth cameras.The code and additional results of our work can be found at https://www.cs.cmu.edu/Ëœdsnerf/. For a more comprehensive understanding, please refer to the full version of the paper at https://arxiv.org/abs/2107.02791.