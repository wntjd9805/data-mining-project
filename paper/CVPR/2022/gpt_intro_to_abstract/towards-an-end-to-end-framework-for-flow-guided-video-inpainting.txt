Video inpainting is a popular technique used to fill in corrupted regions in video clips with coherent content. While significant progress has been made in image inpainting, video inpainting presents unique challenges due to the complexity of video scenarios and deteriorated frames. Deep learning has been explored as a solution, and flow-based methods have emerged as a promising approach for preserving temporal coherence. However, these methods require multiple stages and hand-crafted operations, leading to error accumulation and slow processing times. In this paper, we propose an end-to-end framework for flow-guided video inpainting (E2FGVI) that addresses these flaws. Our framework consists of three trainable modules: flow completion, feature propagation, and content hallucination. These modules work collaboratively and efficiently to improve the accuracy and efficiency of video inpainting. Experimental results demonstrate that our framework achieves state-of-the-art accuracy on various metrics and greatly reduces processing time compared to previous flow-based methods. We hope that our proposed framework can serve as a strong baseline for future research in the video inpainting community.