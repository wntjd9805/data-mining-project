Deep learning has achieved significant progress in various fields such as computer vision and natural language processing. However, for deep neural networks (DNNs) to become pervasive, they must be deployed on embedded or edge devices. These devices are characterized by limited storage, memory, and processing capabilities, posing challenges to the deployment of DNNs. This paper aims to address these constraints by proposing novel techniques and methodologies for efficient running of DNNs on embedded or edge devices, enabling the widespread adoption of deep learning in resource-constrained environments.