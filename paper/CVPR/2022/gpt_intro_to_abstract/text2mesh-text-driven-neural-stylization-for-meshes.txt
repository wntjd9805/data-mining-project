Editing visual data to conform to a desired style has been a long-standing objective in computer graphics and vision. This paper proposes a method called Text2Mesh that uses natural language cues to modify the style of 3D objects. The desired style is expressed through text prompts, similar to how an artist is provided with a verbal or textual description of the desired work. The method utilizes a neural network called a neural style field (NSF) to synthesize color and local geometric details over the 3D input shape. The weights of the NSF network are optimized to adhere to the style described by the text prompt. The optimization is guided by multiple 2D views of the stylized mesh matching the target text. The results show that Text2Mesh can produce different colors and local deformations for the same 3D mesh content, matching the specified text. The method also demonstrates an understanding of global structure, accurately stylizing human body parts in accordance with their semantic role. The proposed approach combines explicit mesh surfaces and the generality of neural fields to provide intuitive control for stylizing 3D shapes. It can handle low-quality meshes and can be extended to other target modalities such as images or cross-modal combinations.