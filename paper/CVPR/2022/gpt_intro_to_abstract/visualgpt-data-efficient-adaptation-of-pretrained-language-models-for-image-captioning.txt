Recent advancements in image captioning have been achieved through the use of large-scale datasets and manual annotation of captions. However, this process is time-consuming and requires significant effort. Semi-automatic collection of image-caption pairs from the Internet can lead to incorrect or undesirable training data. Additionally, specialized domains, such as medical report generation and low-resource language captioning, face challenges in obtaining sufficient data. This paper investigates the data efficiency problem in image captioning and proposes a solution using pre-trained language models (PLMs). PLMs, such as BERT, XLNet, and GPT, have demonstrated effective knowledge transfer in downstream natural language processing tasks. However, their adaptation for multimodal tasks like image captioning remains underexplored. This paper presents a novel encoder-decoder attention mechanism with self-resurrecting activation units (SRAUs) to balance features from visual and textual modalities. This approach improves data efficiency by leveraging the knowledge acquired by pre-trained language models. To the best of our knowledge, this work is the first to focus on efficiently adapting large pre-trained language models for image captioning.