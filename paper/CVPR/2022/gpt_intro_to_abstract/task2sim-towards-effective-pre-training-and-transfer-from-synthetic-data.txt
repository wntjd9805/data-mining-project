Large-scale labeled or weakly-labeled datasets collected from the web have been commonly used for pre-training classifiers in computer vision tasks. However, these datasets come with various challenges such as the need for label curation and limited availability for reproducibility and fair comparison of algorithms. Synthetic data generated through graphics engines offers a compelling alternative to address these concerns. By varying scene and image-capture parameters, infinite images can be generated, providing a rich source of training data. While previous research has explored the use of synthetic data for transfer learning in specialized tasks, there has been limited investigation into its transferability across different recognition tasks from various domains. In this paper, we present a comprehensive study on the effectiveness of synthetic pre-training for different downstream tasks, aiming to understand its potential as a universal pre-trained model. We evaluate the performance of models trained on synthetic data on various recognition tasks from different domains and compare it to models trained on real-world data. The results of this study shed light on the applicability and limitations of synthetic pre-training, providing insights for further advancements in the field of computer vision.