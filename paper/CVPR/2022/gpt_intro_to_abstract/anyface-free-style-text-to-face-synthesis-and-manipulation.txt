This paper introduces the problem of generating variable face images from arbitrary text descriptions, referred to as the free-style Text-to-Face task. Current technology only supports limited facial captions in the training dataset, making the generation of faces according to specific requirements a challenging problem. To address this, the paper explores text-to-image synthesis methods and proposes a two-stream framework called AnyFace for open-world applications. Leveraging the Contrastive Language-Image Pre-training (CLIP) model, AnyFace utilizes face image synthesis and faces image reconstruction to generate target images consistent with given texts. The framework is trained independently and utilizes a cross-modal transfer loss to align linguistic and visual features. A novel Diverse Triplet Loss is also introduced to encourage diverse text embedding along with correct visual semantics. The paper's key contributions include the definition, solution, and application of the free-style text-to-face problem, the proposed two-stream framework, and extensive experiments demonstrating the advantages of the method in synthesizing and manipulating high-fidelity face images. This work opens up possibilities for more creative and wider applications of text-to-face technology.