Vision-and-language navigation (VLN) is a challenging problem in cross-domain research, where an agent needs to interpret human instructions and navigate through unseen environments. Previous works have proposed two scenarios for VLN research: navigation in discrete environments and navigation in continuous environments. These scenarios have been studied independently due to the large domain gap, preventing the direct application of advancements from one scenario to the other. The main difference between navigation in discrete and continuous environments lies in the reliance on connectivity graphs. In discrete environments, agents can navigate using panoramic high-level actions by teleporting to adjacent waypoints on the graph. On the other hand, navigation in continuous environments relies on a limited field of view and low-level controls. Previous works in VLN with high-level actions primarily focus on visual-textual matching. Despite the efficiency of learning in discrete environments, navigation in continuous spaces is more realistic. This paper aims to bridge the learning gap between discrete and continuous domains by adapting agents designed for discrete VLN to continuous environments. The paper introduces a candidate waypoints predictor that estimates navigable locations in continuous spaces, allowing agents to effectively navigate and reduce the gap between discrete and continuous success rates. Experimental results show that agents trained with the predicted waypoints achieve state-of-the-art performance on benchmarking test sets, demonstrating the potential of the proposed predictor in enabling effective discrete-to-continuous transfer and benefiting other navigation problems.