Gaze estimation is a crucial aspect of numerous applications, including virtual/augmented reality, human-computer interaction, and medical analysis. Appearance-based gaze estimation, which requires minimal hardware, has shown promising results. However, existing methods often struggle to adapt to new domains due to differences in subject appearance, image quality, shooting angle, and illumination. Unsupervised domain adaptation approaches aim to address this challenge by finding gaze-relevant constraints that generalize the model to target domains without requiring labels. In this paper, we propose the Rotation-enhanced Unsupervised Domain Adaptation (RUDA) framework for gaze estimation. We leverage the rotation-consistency property of gaze, which implies that rotating a face image results in the same rotation angle of the gaze direction. We define the relative rotation angle as a sub-label and use it as a desired gaze-relevant constraint for adaptation. Our approach generates sub-labels between original and randomly rotated images, allowing the model to be adapted to the target domain without target domain labels and with low computational cost. Experimental results demonstrate that the RUDA framework consistently outperforms the baseline model on four cross-domain gaze estimation tasks, achieving improvements ranging from 12.2% to 30.5% and even outperforming some state-of-the-art methods trained on the target domain with labels.