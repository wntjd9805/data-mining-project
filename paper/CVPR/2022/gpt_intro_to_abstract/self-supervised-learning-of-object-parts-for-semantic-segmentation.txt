This paper addresses the challenge of defining objects in the context of self-supervised or unsupervised semantic segmentation. It highlights the diverse definitions of objects and the importance of learning self-supervised dense representations for efficient data labeling and scalability. The paper introduces a novel approach called Leopart, which learns object parts through a dense image patch clustering pretext task. The use of Vision Transformers (ViTs) is explored, leveraging their ability to localize objects and combining it with a dense loss to train spatial tokens for unsupervised segmentation. The method is validated through transferability studies and experiments in fully unsupervised semantic segmentation. The results demonstrate that the proposed approach outperforms self-supervised ViTs and Resnets, achieving state-of-the-art performance in semantic segmentation and learning transferable dense representations. The contributions of the paper include the proposal of a dense clustering pretext task, the development of a cluster-based foreground extraction technique, and the surpassing of state-of-the-art results in various semantic segmentation benchmarks.