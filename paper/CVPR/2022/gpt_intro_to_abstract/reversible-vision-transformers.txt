The deep learning revolution in computer vision has been greatly supported by high-performance hardware accelerators. However, while compute requirements have been exponentially growing, the memory bandwidth bottleneck has proven difficult to scale proportionally. This bottleneck is especially prominent in transformers, where both model performance and training speed are tightly bound by memory limitations. In this paper, we propose Reversible Vision Transformers, a family of visual recognition architectures that offer more favorable activation memory footprints compared to their non-reversible counterparts. By trading off GPU activation caching with on-the-fly activation re-computation, these reversible models effectively decouple activation memory growth from model depth. We address convergence instabilities by reconfiguring the residual paths in Vision Transformers and Multiscale Vision Transformers. Our experiments demonstrate that reversible vision transformers have competitive performance across various image recognition tasks with minimal to no performance decay. Additionally, these models significantly reduce per-image memory footprint, saving up to 15.5× on the ViT-Large model and 2.3× on the MViT-Base model during reversible training. Our contributions include the proposal of Reversible Vision Transformers and Reversible Multiscale Vision Transformers, the observation of stronger inherent regularization in reversible transformers, and extensive benchmarking across image classification, object detection, and action recognition tasks. Overall, reversible networks achieve higher throughput and lighter memory footprints compared to non-reversible counterparts.