Abstract:The adversarial vulnerability of machine learning models has been extensively studied due to its unpredictable behavior and potential impact on safety-critical tasks. While many defenses have been proposed, most of them can be circumvented by adaptive attacks. Adversarial training (AT) has been recognized as an effective defense approach, but current state-of-the-art AT methods still struggle to achieve high robust test accuracy on CIFAR-10. To address this limitation, we propose the incorporation of a rejection or detection module along with the adversarially trained classifier to enable the model to refuse to make predictions for abnormal inputs. We introduce the concept of true confidence (T-Con), which reflects the model's generalization ability, and use it as a certainty oracle for rejection. We show that executing rejection based on T-Con can significantly increase the test accuracy under a given true positive rate. Additionally, we propose rectified confidence (R-Con) as a means to learn T-Con, which allows for provable separability in the adversarial setting. Our rejection module, the rectified rejection (RR) module, is compatible with existing defense methods and consistently improves the robust accuracy of returned predictions under various attacks and threat models, with minimal computational burden. Experimental results on CIFAR-10, CIFAR-10-C, and CIFAR-100 datasets demonstrate the effectiveness and practicality of our approach.