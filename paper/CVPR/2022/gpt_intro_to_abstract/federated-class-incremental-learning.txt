Federated learning (FL) is a collaborative learning approach that allows multiple local clients to train a global model while preserving the privacy of the local data. FL has gained significant attention in academia and has been successfully applied in various industrial domains. However, most existing FL methods are designed for static application scenarios where the data classes are fixed and known in advance. In real-world dynamic scenarios, where new classes of data arrive continuously, existing FL methods suffer from high storage and computation overheads or catastrophic forgetting on old classes.To address these challenges, we propose a novel approach called Federated Class-Incremental Learning (FCIL). FCIL aims to enable local clients to continuously learn new classes while collaborating on training a global model in a privacy-preserving and memory-efficient manner. We use the example of COVID-19 diagnosis among hospitals to illustrate the FCIL problem, where new COVID-19 variants emerge and new hospitals join the FL training with little data on the old infectious diseases.Integrating FL with class-incremental learning (CIL) seems like an intuitive solution for continuous learning of new classes. However, this approach violates privacy preservation requirements and exacerbates local and global catastrophic forgetting. To overcome these challenges, we propose the Global-Local Forgetting Compensation (GLFC) model. GLFC addresses both the local catastrophic forgetting on local clients and the global catastrophic forgetting across clients. It includes a class-aware gradient compensation loss to balance the forgetting of different old classes and a class-semantic relation distillation loss to capture consistent inter-class relations. To compensate for global forgetting caused by non-i.i.d. class imbalance across clients, a proxy server selects the best old model for class-semantic relation distillation and uses a privacy-preserving prototype gradient-based communication mechanism.Experimental results on benchmark datasets show that our GLFC model outperforms various baseline methods in terms of average accuracy, achieving improvements ranging from 4.4% to 15.1%. The major contributions of this paper are the introduction of FCIL as a practical FL problem, the development of the GLFC model to address local and global catastrophic forgetting, and the design of a proxy server to protect privacy while selecting the best old model for distillation.