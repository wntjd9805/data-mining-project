Deep learning has revolutionized action recognition, but it requires large amounts of annotated data, which is difficult and costly to collect. To overcome this limitation, Few-Shot Learning (FSL) has been proposed, which aims to classify unlabeled samples into new unseen classes with only a few labeled examples. While FSL has been extensively studied in the domain of image classification, extending it to video classification is challenging due to the additional temporal dimension. Recent methods have utilized temporal alignment techniques, such as Dynamic Time Warping and attention mechanisms, to match video frames or segments in the temporal dimension. However, there are two crucial aspects for building a robust Few-Shot Action Recognition (FSAR) model: task-specific motion pattern mining and multi-level temporal fragment alignment. This paper proposes an end-to-end Motion-modulated Temporal Fragment Alignment Network (MT-FAN) that jointly explores task-specific motion modulation and multi-level temporal fragment alignment for FSAR. The proposed model aggregates temporal differences between consecutive frames to induce task-specific motion patterns and uses a motion modulator to enhance informative motion information. It also introduces a Transformer-inspired Segment Attention Layer to generate higher-level segments for multi-level temporal fragment alignment. The model is formulated as an Optimal Transport problem and solved using the Sinkhorn algorithm. Experimental results on four benchmarks demonstrate the superiority of the proposed method compared to state-of-the-art FSAR methods. This work makes three key contributions: the proposal of MT-FAN, the utilization of task-specific motion modulation, and the introduction of multi-level temporal fragment alignment.