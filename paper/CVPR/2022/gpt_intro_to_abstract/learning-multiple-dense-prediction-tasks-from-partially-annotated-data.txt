Abstract:This paper addresses the problem of learning multiple dense prediction computer vision tasks jointly through multi-task learning (MTL) in the context of partially annotated data. Unlike previous works that assume all training images are labeled for all tasks, we consider the practical scenario where not all task labels are available for each image. To tackle this challenge, we propose a novel MTL model that utilizes a shared feature extractor and learns to relate each task pair in a joint pairwise task-space. By penalizing inconsistencies between ground-truth labels and predictions, our model is able to extract task-specific information from partially annotated data. Additionally, we dynamically estimate the weights for each task pair relation to address the computational cost of modelling each relation. Experimental results demonstrate the effectiveness of our approach, outperforming baseline methods on standard multi-task benchmarks.