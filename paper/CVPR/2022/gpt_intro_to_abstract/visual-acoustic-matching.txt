This paper introduces the concept of visual acoustic matching, which aims to transform sounds from one space to another space by altering their scene-driven acoustic signatures. The goal is to create audio signals that are consistent with a given environment, delivering a realistic and immersive experience in augmented reality (AR) and virtual reality (VR) applications. The paper addresses the challenges of modeling complex cross-modal interactions and obtaining scalable training data. A cross-modal transformer model is proposed, along with a self-supervised training objective that utilizes in-the-wild Web videos with unknown room acoustics. The model incorporates fine-grained audio-visual reasoning by attending to different regions of the image and how they affect the acoustics. To capture reverberation effects, 1D convolutions are used to generate time-domain signals, and a multi-resolution generative adversarial audio loss is applied. Despite the limitation of paired training data, a self-supervised objective is introduced to create acoustically mismatched audio for training. The proposed model is evaluated on real-world sounds and environments, as well as controlled experiments with realistic acoustic simulations. Quantitative results and subjective evaluations demonstrate that the model generates audio that matches the target environment with high perceptual quality, outperforming existing models. This research has potential applications in smart video editing, film dubbing, audio enhancement for video conference calls, and audio synthesis for AR/VR.