Video captioning is a challenging task in computer vision, with applications in assisting visually-impaired individuals, human-computer interaction, and video retrieval. Existing methods for generating captions from videos often focus on designing complex video encoders or narrowing the semantic gap between video representations and linguistic captions. However, these methods do not fully consider the relevance and fine-grained details between video content and linguistic captions. In this paper, we propose a hierarchical modular network that learns video representations at three different levels: the entity level, the predicate level, and the sentence level. At the entity level, our model identifies and highlights the objects most likely to be mentioned in the caption. The predicate level learns the actions conditioned on the highlighted objects, and the sentence level learns the global representation of the video. This multi-level approach captures the fine-grained details and global relevance between video content and linguistic captions.To address the issue of selecting principal objects, we propose a novel entity module based on the transformer architecture. This module selects principal objects from the pre-extracted objects of a video, guided by the video content and supervised by entities in the captions. Our method outperforms state-of-the-art models on two widely-used benchmarks, MSVD and MSR-VTT.In summary, our hierarchical modular framework improves video captioning by associating multi-level visual representations with their linguistic counterparts and selectively highlighting principal objects. The proposed transformer-based entity module enhances object selection, resulting in more accurate and meaningful captions.