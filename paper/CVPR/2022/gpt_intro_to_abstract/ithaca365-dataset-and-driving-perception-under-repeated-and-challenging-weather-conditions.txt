The self-driving car research community has made significant progress in computer vision and perception using real-world sensory datasets. However, most existing datasets were collected under sunny conditions, limiting the generalizability of perception approaches to adverse weather conditions. This paper addresses the need for large-scale datasets with multi-modal sensor data in such challenging conditions. While some diverse datasets with rain, low light, and bright conditions have been published, none of them include snowy conditions. Additionally, there is a lack of amodal segmentation datasets with varying weather conditions. Amodal perception, which aims to understand occluded objects and scenes, is critical for autonomous driving. Existing datasets with amodal masks are limited to sunny conditions. Thus, this paper presents a large-scale, diverse, and amodal dataset collected over a 1.5-year period, including weather diversity (snow, rain, sunny, cloudy, night) and environmental diversity (urban, highway, rural, university campus). The dataset, consisting of over 680k frames with LiDAR, image, and GPS data, enables amodal labeling of background classes across varied conditions. New metrics for evaluating amodal segmentation are also proposed, along with baseline model architectures for key self-driving car perception tasks, such as amodal background segmentation, instance segmentation, 3D object detection, and depth estimation across different weather conditions and route types.