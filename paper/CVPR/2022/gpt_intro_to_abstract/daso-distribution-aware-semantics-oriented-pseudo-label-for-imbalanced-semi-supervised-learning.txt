Semi-supervised learning (SSL) has shown promise in leveraging unlabeled data to reduce the cost of labeling and improve performance. However, most SSL algorithms assume class-balanced data, while many real-world datasets exhibit long-tailed distributions. When dealing with class-imbalanced data, the class distribution of pseudo-labels from unlabeled data becomes biased towards the majority classes, which can further bias the model during training. While there have been methods proposed for handling class-imbalanced labels in supervised learning, little attention has been given to re-balancing pseudo-labels in SSL. This is particularly challenging because the actual class distribution of unlabeled data is unknown without labels. In this paper, we propose a new imbalanced SSL method, DASO, specifically designed to alleviate bias in pseudo-labels under class-imbalanced data. DASO blends two types of pseudo-labels in different proportions, taking into account the current class distribution of pseudo-labels. We also introduce a semantic alignment loss to establish balanced feature representation. We evaluate DASO on various benchmarks, including CIFAR-10/100 and STL-10, as well as a large-scale long-tailed dataset with open-set classes. The results demonstrate that DASO effectively reduces bias and improves performance in different imbalanced SSL scenarios. Our contributions include the proposal of DASO, a debiasing pseudo-labeling framework, the introduction of a semantic alignment loss for improved feature representation, and the integration of DASO with other frameworks for significant performance improvements in imbalanced SSL scenarios.