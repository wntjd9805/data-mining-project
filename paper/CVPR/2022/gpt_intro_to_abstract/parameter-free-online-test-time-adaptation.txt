Training state-of-the-art machine learning models has become computationally intensive, leading to significant environmental impact. Therefore, there is growing interest in designing efficient procedures for adapting pre-trained models to new scenarios without accessing the training data or procedure. In this study, we focus on developing online adaptation methods that can handle non-i.i.d. data and are not specific to a particular model. We aim to address realistic scenarios where adaptation is performed with limited data and where there may be domain shifts between training and test data. We propose a test-time adaptation system that is unsupervised, operates online, assumes no knowledge of training data, and can be applied to different models. We compare our approach with existing methods and show that existing methods demonstrate sensitivity to variables such as the model and type of domain shift. We find that selecting hyperparameters using scenario-specific information yields better results, but this is impractical when test-time conditions are unknown. To address this, we propose adapting the output of pre-trained models rather than their parameters by optimizing a manifold-regularized likelihood of the data. Our approach significantly outperforms existing methods across various datasets, shifts, training strategies, and network architectures. Additionally, our method reduces inference time and memory footprint compared to existing methods due to its output correction approach.