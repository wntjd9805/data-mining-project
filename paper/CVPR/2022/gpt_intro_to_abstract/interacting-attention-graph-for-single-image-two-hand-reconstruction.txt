Recently, there has been significant progress in monocular single hand pose and shape recovery using deep neural networks and large-scale datasets. However, the reconstruction of two hands remains a challenging problem in computer vision due to mutual occlusions and difficulties in effectively formulating the interaction context between the hands. While monocular depth-based methods have shown promise, their energy demand and algorithm complexity restrict their widespread application. Existing learning-based methods for two-hand reconstruction either use heatmaps to estimate hand joint positions or extract sparse image features. However, these methods struggle with modeling hand surface occlusions and extracting dense interaction context. This paper proposes IntagHand, a novel graph convolutional network (GCN) based method for single image two-hand reconstruction. IntagHand utilizes GCN to regress mesh vertices of each hand in a coarse-to-fine manner. To address the challenges in two-hand reconstruction, IntagHand incorporates two novel attention modules: the pyramid image feature attention (PIFA) module, which updates vertex features using a transformer encoder and image features, and the cross hand attention (CHA) module, which encodes interaction context into hand vertex features. Experimental results demonstrate that IntagHand outperforms existing methods on the InterHand2.6M dataset and is efficient for real-time applications. The contributions of this work include proposing the first GCN based two-hand reconstruction method, introducing the PIFA module to improve alignment between hand vertices and image features, introducing the CHA module to model two-hand interaction context, and achieving state-of-the-art results on the InterHand2.6M benchmark.