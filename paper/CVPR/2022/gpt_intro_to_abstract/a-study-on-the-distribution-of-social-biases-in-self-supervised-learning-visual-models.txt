Supervised deep learning models have dominated computer vision and natural language processing, but recent developments in self-supervised learning (SSL) are closing the performance gap. SSL methods aim to learn descriptive feature embeddings through solving pretext tasks with automatically generated labels. However, it has been observed that deep learning models can replicate social biases present in the labeled training data, which can lead to biased outcomes. Initiatives have been developed to address these biases and regulate the use of machine learning with human implications. While previous studies have shown that supervised learning models can learn biases from the datasets, it is unclear whether SSL models are also prone to implicit biases. This paper aims to investigate the origin and impact of biases in SSL models. The authors study the association biases acquired by 11 SSL models with varying pretext tasks and analyze biases at different layers of the models. The results suggest that the nature of the pretext task influences the biases acquired, and models using contrastive methods are more prone to biased associations. Additionally, the analysis reveals that the number and strength of biases vary across different layers of the models. Understanding biases in SSL models can help improve the trade-off between bias and accuracy in transfer learning applications. This study is the first attempt to explore a wide range of SSL models and their biases.