The Metaverse is a visionary concept in which people can explore a virtual world that mimics reality. Achieving this vision requires advancements in various domains, including real-time, high-quality, memory-efficient novel view synthesis (NVS). NVS involves re-rendering a scene from different viewpoints in a realistic manner. Researchers have explored different methods for reproducing the visual world, such as using multi-planar imagery, point clouds, meshes, and image-based rendering. While these approaches can generate high-quality images efficiently, they often require significant memory and proxy geometry. Alternatively, neural radiance fields have shown promise in synthesizing realistic images with a low memory footprint. However, these methods struggle with accurately capturing scene geometry, leading to artifacts during view extrapolation. In this paper, we propose an alternative 3D scene representation using planes, which are simple geometric primitives that can effectively represent complex scenes. Our representation allows for greater flexibility in approximating scene geometry compared to existing approaches. We validate our method using standard benchmarks for novel-view synthesis and demonstrate its superiority in terms of rendering speed, quality, and memory reduction compared to volume-based and surface-based neural rendering methods. Our approach also outperforms other state-of-the-art methods in view extrapolation tasks and can be readily integrated into modern graphics engines.