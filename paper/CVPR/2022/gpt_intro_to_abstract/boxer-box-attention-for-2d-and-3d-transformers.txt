This paper introduces a novel approach to vision representation learning using transformers, specifically for object detection, instance segmentation, and image classification tasks. The authors highlight the limitations of existing transformers in explicitly considering the inherent regularities of vision and the loss of local connectivity among pixels. To address these issues, they propose a Box-Attention mechanism that enriches image features with positional encoding. This approach differs from previous methods that only augment image features with position information, as it incorporates spatial awareness into the network architecture. The authors present the BoxeR-2D network, which utilizes the Box-Attention mechanism for improved object detection and instance segmentation. They further extend this approach to tackle 3D object detection with the BoxeR-3D model. Experimental results on the COCO dataset demonstrate the effectiveness of their contributions, achieving leading results in end-to-end object detection. The proposed method also outperforms established architectures with fewer parameters in end-to-end instance segmentation on the challenging COCO instance segmentation dataset. Additionally, the authors showcase the applicability of their method for end-to-end 3D object detection on the Waymo Open dataset, leveraging data-independent prior information.