Convolutional neural networks (CNNs) have traditionally been the leading architectures in computer vision. However, recent research has shown that standard Transformer models can also achieve good performance on various computer vision tasks. This presents a challenge to the dominance of CNNs in the field. In response, simpler architectures that involve only multi-layer perceptrons (MLPs) have gained attention. MLP architectures offer less inductive bias and have the potential to be applied to a wider range of tasks. This paper introduces a new MLP architecture called Wave-MLP and compares its performance with existing architectures. The Wave-MLP model consists of two blocks: the channel-mixing MLP and the token-mixing MLP. These blocks, composed of fully-connected layers and activation functions, transform features of individual tokens and aggregate information from different tokens respectively. By alternating these two blocks, Wave-MLP demonstrates strong feature extraction capability and achieves good performance on vision tasks. However, MLP architectures still lag behind state-of-the-art Transformer and CNN architectures in terms of performance. The paper highlights the bottleneck in MLP architecture lies in its fixed weights for aggregating tokens, compared to the dynamic adjustment of weights in Transformer. To address this limitation, the proposed Wave-MLP architecture dynamically adjusts the weights for token aggregation. Experimental results demonstrate that Wave-MLP outperforms existing architectures, such as Swin-T, on tasks like image classification, object detection, and semantic segmentation. The paper is organized as follows: Section 2 provides a review of existing model architectures, while Section 3 presents the Wave-MLP architecture in detail. In Section 4, the effectiveness of the proposed method is empirically investigated on multiple vision tasks. Finally, conclusions are drawn in Section 5.