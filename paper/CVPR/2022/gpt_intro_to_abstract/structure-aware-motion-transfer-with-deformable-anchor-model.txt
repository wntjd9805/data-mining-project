Motion transfer has emerged as a topic of interest in computer vision due to its various applications in video re-enactment, fashion design, and face swapping. The objective of motion transfer is to generate a video that preserves the appearance of a source image while depicting the motion pattern from a driving video. The key challenge lies in establishing correspondence between the source image and the driving video. Existing methods either utilize pre-trained models to extract structural information or treat motion keypoints as unknown variables and optimize image reconstruction loss. However, these methods often suffer from false correspondences and result in artifacts in the generated videos. To address these challenges, we propose a new motion transfer approach called the deformable anchor model (DAM). DAM combines the strengths of both model-based and model-free methods. It represents motion keypoints as unknown variables, allowing motion transfer on arbitrary objects without prior structural information. Additionally, to eliminate false correspondences, structural information is encoded to constrain the motion anchors. Unlike model-based methods, DAM does not rely on pre-trained models but instead introduces a latent root anchor inspired by the deformable part model (DPM). This latent anchor regularizes motion anchors and models the object structure, improving the correspondence between the source image and driving video. Furthermore, DAM can be extended to a hierarchical version by introducing additional latent anchors to effectively model complex object structures. DAM can be learned in an end-to-end manner similar to previous model-free methods. We evaluate our method on four benchmark datasets and demonstrate its superior quantitative performance and ability to capture the motion structure of different objects, including human bodies, faces, and animals.