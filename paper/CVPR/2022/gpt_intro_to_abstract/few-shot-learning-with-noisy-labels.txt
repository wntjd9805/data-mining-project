Modern few-shot learning (FSL) methods aim to learn classifiers for novel classes from only a handful of examples. However, these methods assume carefully selected support set samples for training, which is rarely guaranteed in real-world settings. Even curated datasets can contain mislabeled samples. While there are methods for learning with noise in many-shot supervised settings, noise in few-shot settings remains largely unexplored. This is surprising considering the utility of FSL methods in settings where human supervision is difficult. FSL methods are vulnerable to label noise, and even a single noisy example can significantly degrade the model's accuracy. In this paper, we address this vulnerability by proposing technical innovations. We explore alternative designs to the popular ProtoNet method and introduce our Transformer model for Noisy Few-Shot Learning (TraNFS), which leverages a modified self-attention mechanism to achieve robustness to label noise. We extensively test our methods on MiniImageNet and TieredImageNet datasets with various types of label noise and demonstrate their superiority over popular FSL methods. Our contributions include proposing alternative prototypes, introducing TraNFS, and benchmarking popular FSL methods on different types of label noise.