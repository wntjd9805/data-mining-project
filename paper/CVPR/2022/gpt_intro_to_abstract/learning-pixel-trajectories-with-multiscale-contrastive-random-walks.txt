Temporal correspondence is a key aspect of various video understanding tasks, including optical flow and object tracking. While these tasks have different practical requirements, there is a need to unify them under the concept of temporal correspondence. Recent advancements in self-supervised learning have shown that pretraining generic representations on unlabeled images and videos can lead to strong performance in tracking tasks. By formulating tracking as label propagation on a space-time graph, similarity between nodes becomes crucial. The contrastive random walk formulation has demonstrated the efficacy of learning such similarity measures for temporal correspondence problems. However, extending this approach to pixel-level space-time graphs presents challenges, such as the computational cost of computing frame similarity and handling ambiguous cases like occlusion. On the other hand, the unsupervised optical flow community has employed effective methods for dense matching, but these rely on hand-crafted distance functions and may lack robustness in long-term dynamics. This work aims to bridge the gap between tracking and optical flow by applying the contrastive random walk formulation to denser, pixel-level space-time graphs. The main contribution is the introduction of a hierarchical multiscale contrastive random walk, enabling efficient consideration of pixel-level trajectories. Experimental results across optical flow and video label propagation benchmarks demonstrate the following: a unified technique for self-supervised optical flow, pose tracking, and video object segmentation; competitive optical flow performance, despite using a novel loss function without hand-crafted features; superior pose tracking performance compared to existing self-supervised approaches; competitive results in video object segmentation; the complementary learning signal of cycle-consistency; and improved performance through multi-frame training.