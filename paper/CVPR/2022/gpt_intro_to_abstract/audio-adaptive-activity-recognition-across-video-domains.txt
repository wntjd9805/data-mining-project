This paper aims to address the challenge of recognizing activities in videos under domain shift, which can be caused by changes in scenery, camera viewpoint, or actor. Existing solutions for aligning distribution-shifted domains in visual video networks often suffer from decreased ability to distinguish between classes in the target domain. In this study, the authors propose a video model that leverages activity sounds as domain-invariant cues to adapt to video distribution shifts. By exploiting the rich information carried by sounds and introducing audio-adaptive learning methods, the model learns more discriminative features in the target domain while dealing with imbalanced semantic distributions between domains. Additionally, an audio-infused recognizer is introduced to eliminate domain-specific features and facilitate effective cross-modal interaction across domains. The paper also introduces a new task of actor shift and a corresponding audio-visual video dataset to evaluate the proposed approach. Experimental results on various video distribution shifts demonstrate the advantages of the proposed approach for both audible and silent activities.