Services like TensorFlow Hub and PyTorch Hub offer a variety of pre-trained models that achieve state-of-the-art performance in computer vision tasks. The common approach is to choose a pre-trained model and fine-tune it for a specific task, even if the pre-training task is significantly different. Fine-tuning often involves adding more layers to the pre-trained network and updating all parameters, which is computationally expensive. Alternatively, training a cheap classifier on top of the learned representation is efficient but may result in a performance gap. The challenge is to select the best model for fine-tuning. Existing approaches can be categorized into task-agnostic and task-aware model search strategies. However, previous work mainly focused on homogeneous sets of models, not reflecting the current model landscape. Therefore, this paper aims to address the open questions regarding the performance of existing methods in the presence of both generalist and expert models across diverse datasets, as well as finding a good balance between computational cost and performance. The contributions include defining and studying the model search problem using a notion of regret, examining 19 downstream tasks with 46 models grouped into 5 sets, highlighting the performance dependence on the constraints of the model pool, and proposing a hybrid approach that generalizes across model pools.