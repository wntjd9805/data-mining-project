In this paper, the authors address the challenge of learning effective representations from unstructured and sparse 3D geometric data, specifically point clouds, for object detection. They compare two existing approaches: processing point clouds directly using PointNet++ networks and converting point clouds into voxelizations and applying 3D sparse convolutional neural networks (Sparse CNNs) for feature extraction. However, they identify limitations in both approaches, such as time-consuming operations and reduced sparsity. To overcome these limitations, the authors propose two new modules: focal sparse convolution (Focals Conv) and its improved version with fusion (Focals Conv-F). Focals Conv predicts cubic importance maps for the output pattern of convolutions, dynamically determining which input features deserve dilation and dynamic output shapes based on their predicted importance. Focals Conv-F enhances importance prediction by fusing RGB features with LIDAR-only Focals Conv.The proposed modules improve the representation capacity of Sparse CNNs for 3D object detection. Focals Conv enables the learning process to focus on valuable foreground data, increasing the ratio of valuable information and removing background voxels. Both modules are lightweight and can be easily integrated into existing Sparse CNNs.The authors evaluate their approach on the KITTI and nuScenes benchmarks for 3D object detection and demonstrate its effectiveness compared to state-of-the-art methods. They emphasize the importance of learnable sparsity with focal points in achieving superior performance. The proposed approach utilizes the intrinsic sparsity of the data and promotes feature learning on more valuable information. The authors also discuss the relations and differences to previous literature.Overall, the proposed approach shows promise in addressing the challenge of learning effective representations from sparse 3D geometric data for object detection, achieving high representation capability and efficiency.