Visual Emotion Analysis (VEA) is gaining increasing research attention as social network users express their opinions and emotions through visual content. VEA has practical applications in various domains such as opinion mining, business intelligence, entertainment assistance, and personalized emotion prediction. However, one of the biggest challenges in VEA is bridging the affective gap between pixel-level information and high-level emotion semantics. To address this challenge, this paper proposes a novel approach that disentangles the affective gap into smaller steps through level-wise discrimination. The approach introduces an extra top-down branch with local classifiers at each semantic level, focusing on learning the discrimination among emotions at specific affective levels. The proposed approach leverages multi-head channel attention and cross attention mechanisms to model pixel interdependencies and attribute dependencies between feature maps at adjacent levels. Experimental results on six VEA benchmarks demonstrate that the proposed framework achieves state-of-the-art performance, outperforming existing methods in terms of classification accuracy. This work provides a new perspective for VEA and contributes to the advancement of emotion analysis in visual content.