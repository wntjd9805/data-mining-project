This paper explores the use of synthetic data in fueling emerging AI applications, particularly in training computer vision models and teaching robots physical tasks. As AI models become larger and more data-hungry, virtual platforms and synthetic datasets offer a cost-effective source of training data. However, the distribution mismatch between real and virtual worlds hinders model generalization. One potential solution is digitizing physical objects and creating interactive digital twins in virtual environments. While research in 3D vision and SLAM has made progress in capturing realistic static 3D models, there is a need for interactive digital twins of articulated objects. This is challenging as it requires not only understanding the overall geometry but also the part compositions and kinematic relations. Prior research has incorporated interactive objects in simulated environments but relies heavily on manual curation, limiting scalability. This paper aims to automate the estimation and reconstruction of articulated objects using vision-based methods. While previous work has focused on individual components of the problem, this paper aims to construct a full-fledged model. The authors propose a method to jointly reconstruct part-level geometry and articulation models of objects, enabling the creation of interactive digital twins for robot simulation and AR/VR applications.