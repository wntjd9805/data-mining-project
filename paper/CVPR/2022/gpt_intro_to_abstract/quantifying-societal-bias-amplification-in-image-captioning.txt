Undesirable biases in computer vision applications have become a growing concern, with evidence showing major imbalances in how different subgroups of the population are represented in large-scale datasets and the models trained on them. Detecting and addressing these biases, known as societal biases, has become an active research area. It is important to note that biases are not only present in the datasets but can also be perpetuated by model choices and training methods. Additionally, relying solely on accuracy as a metric for optimization has made other aspects such as fairness and efficiency less of a priority. Societal bias affects various computer vision tasks, such as facial recognition, object classification, and pedestrian detection, and requires specific solutions for each task. This paper focuses on examining and quantifying societal bias in image captioning, which has previously been shown to reproduce gender and racial biases. The paper proposes a metric to measure and evaluate societal bias in image captioning models, taking into account both the bias inherent in the training datasets and the bias introduced by the model itself. The metric allows for a comprehensive analysis of gender and racial bias in image captioning models and highlights the need for a standard, unified metric to measure and address bias in these systems. The paper concludes with an analysis of the limitations of the proposed metric and a summary of the main findings.