This paper introduces a novel hierarchical self-supervised pretraining strategy for learning video representations in the context of movie understanding. While previous research in self-supervised video representation learning has mainly focused on action recognition, we aim to understand movies at multiple levels of complexity, including low-level actions and high-level semantic narratives. To overcome the challenges of annotating large-scale video datasets for movie tasks, we propose a hierarchical learning approach that separately pretrains each level of our model. We leverage contrastive learning objectives to pretrain the low-level video backbone encoder on YouTube-style action clips and use mask prediction on movies to pretrain the higher-level transformer contextualizer. Our pretraining strategies do not require annotations and allow for the use of different data sources. We evaluate our approach on the VidSitu and LVU datasets, demonstrating improved performance on various tasks and metrics, such as Semantic Role Prediction. The results show the effectiveness of our self-supervised pretraining strategies in enhancing movie understanding. Additionally, we conduct ablation studies to analyze the impact of different design choices in our pretraining recipes.