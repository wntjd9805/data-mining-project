Realistic rendering of people and objects has seen significant advancements in recent years, with potential applications in telepresence, virtual reality (VR), and augmented reality (AR). However, most existing methods focus on static scenes and synthetic data, and the computational time required is still prohibitive. This paper aims to bridge this gap by creating a dataset that emulates smartphone users in real-world conditions. The SmartPortrait dataset is designed to facilitate reconstruction and rendering applications, such as 3D portrait reconstruction and view synthesis, by providing camera pose state estimation. The dataset includes handheld movements captured by smartphones and incorporates a high-quality depth sensor for added robustness. It offers recorded smartphone video images, IMU data, and external depth camera data. The evaluation of the dataset involves comparing different methods against a reference trajectory obtained from motion capture systems and using a non-reference metric in environments where motion capture systems cannot be deployed. State-of-the-art methods for visual SLAM, SfM, and visual-inertial methods are also benchmarked. The paper also explores the connection between camera pose estimation and downstream tasks such as 3D reconstruction and view synthesis using various algorithms. Ethical considerations are addressed, ensuring that all participants in the dataset have provided signed consent for the academic use of their data and have the option to request removal at any time.