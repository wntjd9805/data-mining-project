Dataset distillation is a technique proposed to distill the knowledge from a large training dataset into a small set of synthetic training images. While prior methods have focused on toy datasets, this work presents a new approach that outperforms previous methods and is applicable to large-scale datasets. The goal of dataset distillation is to compress the information in the dataset while retaining enough task-related information for models trained on it to generalize well on unseen test data. To achieve this, the distilling algorithm must strike a balance between compressing information and preserving essential knowledge. The proposed method involves training a set of models from scratch on the real dataset and recording their expert training trajectories. A new model is then initialized with a random time step from a randomly chosen expert trajectory and trained on the synthetic dataset. The distilled data is penalized based on how far the synthetically trained network deviates from the expert trajectory, allowing for knowledge transfer from multiple expert training trajectories to the distilled images. Extensive experiments demonstrate the superiority of the proposed method compared to existing dataset distillation and coreset selection methods on standard datasets. For example, on CIFAR-10, the method achieves 46.3% accuracy with a single image per class and 71.5% accuracy with 50 images per class, outperforming the previous state-of-the-art methods. Moreover, the method also generalizes well to larger datasets, allowing for high-resolution images distilled from ImageNet for the first time. Additional ablation studies and visualizations are conducted to analyze the method further. Code and models are also made available for access.