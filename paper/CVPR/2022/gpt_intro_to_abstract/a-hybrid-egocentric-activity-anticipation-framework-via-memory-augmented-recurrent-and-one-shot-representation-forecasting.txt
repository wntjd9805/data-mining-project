This paper introduces the concept of egocentric activity anticipation (EAA) in the context of wearable cameras. EAA involves predicting future activities in egocentric videos, which has numerous real-world applications such as human-robot interaction and autonomous driving. However, anticipating unseen egocentric activities is challenging due to the gap between past and future events, limited clues in incomplete observations, and the presence of ego-motion and cluttered backgrounds in the videos. The paper argues that current recursive-model-based EAA methods struggle to achieve satisfactory performance because of accumulated prediction errors and the lack of contextual cues. To address these issues, the paper proposes a hybrid framework called "HRO" that combines memory-augmented recurrent and one-shot representation forecasting. The framework includes a memory bank to store long-term activity semantics and a one-shot transferring strategy to incorporate contextual clues into the forecasted representations. Experimental results on two challenging datasets demonstrate the superior performance of the proposed framework compared to other state-of-the-art methods. Overall, this paper presents a novel approach to improving egocentric activity anticipation, which has important implications for various applications in computer vision and robotics.