This paper presents a novel approach for training models to recognize procedural steps in instructional videos without manual annotation. The approach leverages a textual knowledge base, wikiHow, to automatically identify segments corresponding to different procedural steps in the videos. Language models are used to compare transcribed segments with textual descriptions in wikiHow, serving as distant supervision to train a video understanding model. The resulting video representation is shown to transfer effectively to four different downstream tasks, achieving state-of-the-art results. The approach outperforms fully-supervised video features and produces better results than directly matching video to the automatic speech recognition narration. The system is evaluated on various datasets, demonstrating its effectiveness in understanding complex procedural activities in videos. The code and automatic annotations provided by the distant supervision framework are released.