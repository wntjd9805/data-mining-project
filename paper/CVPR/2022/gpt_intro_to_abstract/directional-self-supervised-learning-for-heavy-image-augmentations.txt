Unsupervised visual representation learning, which aims to learn image features without manual annotations, has recently experienced great success using self-supervised learning techniques. These techniques involve instance discrimination tasks or Siamese architecture, resulting in high-quality visual features and narrowing the performance gap with supervised pretraining. The visual embedding space is constructed by minimizing dissimilarity between representations of variations from the same image, or by increasing the distance between representations from different images. Image transformation plays a crucial role in self-supervised visual representation learning. However, current methods have a fundamental weakness - only a few carefully selected augmentation policies are beneficial for model training. The combination of random crop, color distortion, Gaussian blur, and grayscale, known as the standard augmentations, has been widely used. Recently, other augmentations such as RandAugment and JigSaw have shown potential for further improving the performance of self-supervised learning methods. However, for negative pair-free methods like SimSiam, introducing these heavy augmentations often results in unstable or even collapsing performance. Inspired by previous works, we hypothesize a gold standard feature cluster for augmented views of the same image instance and propose Directional Self-supervised Learning (DSSL) to improve the performance of instance-wise self-supervised learning. DSSL introduces heavily augmented views and applies an asymmetric loss to tighten the feature cluster and gather each heavily augmented view to its relevant standard view. This prevents the adverse effects of missing information and expands the embedding space while keeping instances well-separated. DSSL is easy to implement and applicable to various standard self-supervised learning frameworks without the need for additional hyperparameters. Experimental results on benchmark datasets show that DSSL consistently improves performance, even when using heavy image transformations that previously had adverse effects. Overall, DSSL provides a novel paradigm for unsupervised visual representation learning that stabilizes and enhances self-supervised learning methods.