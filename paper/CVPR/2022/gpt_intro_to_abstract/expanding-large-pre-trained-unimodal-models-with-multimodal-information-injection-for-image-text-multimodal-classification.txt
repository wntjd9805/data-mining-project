Abstract:Multimodal classiﬁcation, which leverages both visual and language modalities, has gained attention for social media analysis. Current methods for image-text multimodal classiﬁcation fall into two categories: those that maintain separate backbones for each modality and perform multimodal fusion on the resulting features, and those that perform fusion on the mid-level features of each modality. However, existing approaches primarily focus on homogeneous settings with similar modalities. This work proposes a method to expand large pre-trained unimodal models for image-text multimodal recognition. The proposed approach, called Multi-modal Information Injection Plug-in (MI2P), augments the mid-level features of unimodal models by integrating features from other modalities. MI2P performs cross-modal feature transformation using ﬁne-grained cross-modal attentions. By attaching MI2P units to multiple layers of the unimodal networks, our approach models the cross-modal interactions of different abstraction levels, while maintaining the strong intra-modal processing ability of the unimodal models. Experimental results on various image-text multimodal classiﬁcation benchmarks demonstrate state-of-the-art performance.