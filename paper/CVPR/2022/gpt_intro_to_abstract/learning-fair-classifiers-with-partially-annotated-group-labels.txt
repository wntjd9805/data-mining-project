Recent advancements in machine learning (ML) models have shown promising results in various societal applications, including credit estimation, crime assessment systems, automatic job interviews, face recognition, and law enforcement. However, these ML models have been found to be discriminatory towards certain demographic groups, leading to the emergence of fairness-aware learning to mitigate these issues. Many existing approaches for achieving group fairness rely on both target labels (task-oriented) and group labels (based on sensitive attributes) to train fair classifiers. However, in realistic applications such as computer vision, assuming that all images have sensitive group labels can be unrealistic and impractical. Additionally, sensitive attributes are often personal information protected by laws, making it difficult to collect group labels for all data points in real-world applications. In this work, we propose and investigate a practical problem called Algorithmic Group Fairness with Partially Annotated Group Labels (Fair-PG). We show that existing fair-training methods that operate only on group-labeled samples perform worse than the "scratch" training that uses all training samples when the number of group-labeled samples is small. To address this issue, we propose a strategy called Confidence-based Group Label assignment (CGL), which assigns pseudo group labels to group-unlabeled samples based on the predictions of an auxiliary group classifier. We provide theoretical support and experimental results demonstrating that the combination of CGL with state-of-the-art fair-training methods improves target accuracies and fairness parities, even under the low group label regime, on both facial image and tabular datasets. This work is promising as it improves the accuracy and fairness of baseline methods by augmenting the training data with target label-only datasets, which are relatively easier to obtain than requiring group labels.