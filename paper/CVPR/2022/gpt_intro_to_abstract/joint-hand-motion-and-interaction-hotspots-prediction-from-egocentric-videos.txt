Predicting a person's intent, preference, and future activities is a fundamental goal for AI systems, especially in applications such as augmented reality (AR) and robotics. In the context of egocentric video data, the ability to anticipate human actions can provide useful guidance and collaboration opportunities. While previous approaches have focused on predicting discrete action categories or pixel-level future frames, our work is inspired by human motion trajectory prediction. In this paper, we propose a joint prediction model for the future hand motion trajectory and interaction hotspots on the next-active object in egocentric videos. We tackle the uncertainty of future predictions by utilizing a probabilistic approach and leverage a Transformer-based model for joint predictions. To train our model, we introduce an automatic method for generating a large-scale dataset without expensive human labor. We utilize off-the-shelf hand detectors and homography to collect data for hand trajectories and interaction hotspots. Additionally, we propose an Object-Centric Transformer (OCT) model that captures hand-object relations for prediction. Evaluation on multiple datasets demonstrates that our OCT model outperforms baselines in both hand trajectory and interaction hotspot prediction tasks. Furthermore, we find that trajectory estimation improves interaction hotspot prediction and that fine-tuning our model on the action anticipation task yields promising results. Our contributions include the introduction of the joint prediction approach, the OCT model, and the new dataset annotations. We achieve state-of-the-art performance on prediction tasks and show the potential of our model in improving action classification.