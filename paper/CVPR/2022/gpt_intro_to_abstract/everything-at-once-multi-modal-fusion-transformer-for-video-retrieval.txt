This paper introduces a novel approach for self-supervised learning of multi-modal embedding spaces. The proposed approach leverages self-attention in a fusion transformer to jointly process any number of input modalities and allow them to attend to each other. The fusion transformer is designed to handle input of any length and can learn multi-modal correlations. A combinatorial loss function is proposed to consider contrastive loss between all possible input combinations. The model is trained on the HowTo100M dataset and evaluated on several downstream datasets, showing improved performance and achieving state-of-the-art results in tasks such as text-to-video retrieval and step action localization. The contributions of the paper include the proposal of a multi-modal fusion transformer, a combinatorial contrastive loss, and the demonstration of improved performance in multi-modal embedding space learning.