The paper introduces the challenge of measuring image similarity in multifarious image generation tasks. Various metrics are needed to measure inter-image similarity across different properties such as image structures, semantics, and perceptual realism. However, defining generic metrics is challenging due to the subjective nature of visual similarity and the entanglement of visual properties in pixels. The paper focuses on contrastive learning as a versatile metric for image translation tasks. Contrastive learning aims to pull positive samples towards an anchor and push negative samples far away from it. However, existing studies use a vanilla contrastive objective that pushes away negative samples indiscriminately, regardless of their similarity to the anchor. To address this limitation, the paper proposes different weighting strategies for negative samples based on their similarity to the anchor. Hard negative samples (with high similarity to the anchor) are assigned higher weights in unpaired image translation tasks, but an inverse weighting strategy is found to be more effective for paired image translation tasks. Additionally, the paper introduces a Modulated Noise Contrastive Estimation (MoNCE) loss that uses optimal transport to collaboratively modulate the re-weighting of negative samples across multiple contrastive objectives. The contributions of the paper include formulating contrastive learning as a versatile metric, investigating the effect of negative pair weighting, and proposing a modulated contrast approach using optimal transport.