With the advancement of automatic video generation technology, the task of audio-driven talking head generation has gained attention for its real-world applications. Existing methods have focused on achieving accurate lip-sync but have not fully explored the control of facial expressions. Some methods have generated rhythmic or changeable head poses but lack the ability to change detailed expressions. Other methods that generate emotional dynamics are person-specific and rely on labeled emotional data. In this paper, we propose the Granularly Controlled Audio-Visual Talking Heads (GC-AVT) system, which allows for independent control of speech content, head pose, and emotional expression. Our method avoids using intermediate representations and is purely learning-based without specific emotion labels. We divide the driving information into granular parts through delicate pre-processing designs and extract the necessary information from visual and audio sources. Experiments show that our method can generate expressive talking heads with precise control over mouth shape, head pose, and emotional expression. The contributions of this work include the GC-AVT system, three pre-processing procedures for different control sources, and the integration of audio-visual synchronization for accurate mouth movements.