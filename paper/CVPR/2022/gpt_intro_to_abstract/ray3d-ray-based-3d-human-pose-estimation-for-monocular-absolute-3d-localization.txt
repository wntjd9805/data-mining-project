Accurate monocular 3D human pose estimation is important for various applications in augmented reality, human-object interaction, and video action recognition. However, it is a challenging and ill-posed problem with limited generalization capability. Absolute 3D human pose estimation in a metric space is even more difficult but more desirable in real-world applications. This is because knowing the exact location of human joints in the World Coordinate System is crucial, especially for tasks like accurate hand localization in a store. The problem arises from the ambiguities introduced by factors like body size, camera parameters, and 3D position. For instance, scaling up the body size or the distance to the camera does not change the projected 2D keypoint locations. Similarly, doubling the focal length and 3D distance also keeps the 2D keypoints the same. Since these ambiguities make it challenging to learn a model for mapping from 2D pixel locations to 3D world locations, several approaches have been proposed. These include lifting methods and image-based methods, each with their own limitations and issues. To address these challenges, we propose the Ray3D method, which converts 2D keypoints to 3D rays in a normalized space to achieve intrinsic-parameter-invariant representation. We also fuse 3D rays from consecutive frames using temporal convolution to resolve ambiguities introduced by occlusion and improve accuracy. Additionally, we incorporate camera extrinsic parameters into the network to resolve the ambiguity of human body part size. We conduct extensive experiments to benchmark the robustness and generalization capability of existing 3D pose estimation approaches. The results demonstrate the advantages of our Ray3D approach on real and synthetic benchmark datasets.