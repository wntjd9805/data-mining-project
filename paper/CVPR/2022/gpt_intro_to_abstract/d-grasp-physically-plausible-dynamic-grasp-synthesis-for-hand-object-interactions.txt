A key problem in computer vision is understanding how humans interact with their surroundings, particularly through hand-object pose estimation and grasp synthesis. However, human grasping involves continuous interaction and maintaining a stable grasp, which adds complexity to the task. A generative model that can synthesize realistic and physically plausible object manipulation sequences would have various applications in AR/VR, robotics, and HCI. In this paper, we propose the task of dynamic grasp synthesis, where the goal is to generate a grasping motion and move an object to a target pose in a natural and physically plausible way. This task presents challenges such as considering the object geometry and spatial configuration, maintaining stability of grasps through contact between the hand and object, and generating smooth and plausible hand motion without dense supervision. To address these challenges, we introduce D-Grasp, a hierarchical framework that consists of a grasping policy and a motion synthesis module. The grasping policy aims to establish and maintain a stable grasp, while the motion synthesis module generates a motion to move the object to the target position. Our approach leverages reinforcement learning and a physics simulation to ensure physical plausibility and compensate for data scarcity. We propose a reward function parameterized by a grasp label to incentivize the fingers to reach contact points on the object and produce human-like grasps. Our experiments demonstrate that samples from motion capture, static grasp synthesis, or image-based pose estimates often do not lead to stable grasps. However, our method can learn to produce physically plausible and stable grasps when guided by such labels. We also show the importance of the hierarchical approach and the reward formulation for dynamic grasp synthesis. In summary, our contributions include introducing the task of dynamic grasp synthesis, proposing D-Grasp as an RL-based method for synthesizing natural hand-object interactions, and demonstrating the feasibility of generating grasp motions with static grasp references.