Federated learning (FL) has become popular in collaborative machine learning for leveraging inter-user similarities without sharing private data. However, FL can face challenges when dealing with non-IID datasets, where sharing a global model for all clients may lead to slow convergence or poor inference performance. To address this, personalized federated learning (pFL) mechanisms have been proposed, allowing each client to train a customized model for their own data distribution. Existing pFL methods use a distance metric among the whole model parameters or loss values of different clients, but this approach fails to exploit the layer-level differences in neural networks. In this paper, we propose a new pFL framework that performs layer-level aggregation for FL personalization, accurately recognizing the utility of each layer for adequate personalization and improving training performance on non-IID datasets. We present a toy example that illustrates the limitations of traditional model-level aggregation and demonstrate the potential of layer-wise aggregation for achieving higher performance. Motivated by these observations, we introduce a novel federated training framework called pFedLA, which facilitates collaboration between clients in a layer-wise manner. We use a dedicated hypernetwork at the server side to learn the weights of cross-clients' layers during the pFL training procedure. Extensive experiments on image classification tasks demonstrate that pFedLA outperforms state-of-the-art baselines on widely used models and datasets. This paper is the first to explicitly reveal the benefits of layer-wise aggregation in pFL among FL clients and contributes a framework that effectively exploits inter-user similarities and produces accurate personalized models.