Continual learning, or lifelong learning, is the ability to continuously learn and adapt to new environments by leveraging knowledge gained from past experiences. However, current learning models are often limited to specific tasks and fail to effectively incorporate lifelong learning methods. The phenomenon of catastrophic forgetting occurs when training a model on a continuous stream of tasks, causing the model to forget previous knowledge as it focuses on learning new information. This paper addresses the limitations of continual learning by proposing a framework that models representation drift on both a semantic and feature level. By integrating this framework into the learning process, the paper demonstrates improved performance in preserving knowledge and mitigating catastrophic forgetting. Experimental results show that the proposed methods outperform other approaches on various benchmarks, highlighting the importance of maintaining accurate representations in continual learning scenarios.