In this paper, we introduce an improved algorithm based on temporal optimization for boosting the training of neural ordinary differential equations (ODEs) in the context of continuous normalizing flows. We demonstrate that temporal optimization achieves competitive performance compared to original models but with significantly less training time. Additionally, we propose the use of temporal regularization and clipping functions, which effectively stabilize the training process without resulting in a degradation of model performance. Furthermore, we optimize the stopping time T alternately with the parameters θ of the movement speed f, resulting in more compatible T and θ values that lead to a decrease in the number of function evaluations (NFE) and a reduction in training loss. Our contributions include the development of an improved algorithm based on temporal optimization, the introduction of temporal regularization and clipping functions, and the optimization of T in conjunction with θ for improved training efficiency.