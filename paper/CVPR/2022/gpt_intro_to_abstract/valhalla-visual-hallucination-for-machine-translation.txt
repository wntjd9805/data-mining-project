Machine Translation (MT) is a fundamental task in natural language processing that has evolved over the years, from rules-based systems to statistical approaches, and most recently to neural network-based models. While these advancements have shown impressive results on standard benchmarks, they primarily rely on text-only information and lack explicit grounding in the real world. There is a growing interest in developing multimodal MT systems that incorporate visual context to enhance translation.The idea of utilizing visual context in machine translation is based on the observation that there is common grounding information between sentences that describe the same visual scene. By incorporating images into the translation process, MT systems can improve data efficiency and perform better in low resource scenarios. However, existing methods in this area require manually annotated sentence-image pairs during inference, which limits their scalability.This paper addresses the question of whether a translation system trained with access to images can generalize to settings where images are not available during inference. The authors propose the concept of visual hallucination, which is the ability to imagine visual scenes, as a means to improve machine translation systems. They introduce the VALHALLA framework, which incorporates images at training time to create a more effective text-only model for machine translation.VALHALLA comprises two transformers: a visual hallucination transformer that maps the source sentence to a discrete image representation, and an MMT transformer that maps the source sentence paired with its discrete image representation to the target sentence. The models are trained end-to-end with a combination of hallucination, translation, and consistency losses. The authors leverage a Gumbel-Softmax relaxation to train the hallucination transformer, as the sampling of discrete image representations is non-differentiable.The experimental evaluation conducted on three standard MT datasets demonstrates the superiority of VALHALLA over strong translation baselines. The framework achieves an average BLEU improvement of 2-3 compared to text-only translation baselines and outperforms state-of-the-art MMT methods that use continuous image representations. VALHALLA shows significant gains, up to +3.1 BLEU, in under-resourced translation settings, confirming the practical value of visual hallucinations in these scenarios. Additional analysis suggests that VALHALLA models effectively leverage visual hallucination to generate better translations, particularly under limited textual context. This work presents the first successful application of an autoregressive image transformer in conjunction with a translation transformer for hallucinating discrete visual representations in machine translation.