Conventional visual classification tasks are limited to object categories that are present in both the training and testing sets. However, in many real-world applications, unseen categories are common due to the finite nature of training datasets. Zero-shot learning (ZSL) aims to address this by leveraging shared visual and semantic representations to recognize unseen or novel instances. Generalized zero-shot learning (GZSL) is an even more challenging task that deals with visual samples from both seen and unseen domains.Early ZSL methods focused on embedding visual and semantic representations into a shared space or generating synthetic features for unseen categories. However, in GZSL, accurately distinguishing between seen and unseen visual features is a significant challenge. This is problematic because similar samples in seen domains can lead to misclassifications of unseen categories. Therefore, the separation of seen and unseen samples is crucial for improving GZSL performance.This paper focuses on accurately separating seen and unseen categories in GZSL. The existing approaches for this task can be categorized into embedding methods, generative methods, and domain-aware methods. The embedding methods aim to map semantic descriptions into visual space, while generative methods synthesize samples for unseen categories. Domain-aware methods explicitly distinguish seen and unseen domains.The proposed method falls into the domain-aware group and employs two Variational Autoencoder (VAE) models to align visual and semantic representations in a latent space. A two-stage training strategy is utilized, and a classifier is learned to distinguish seen and unseen representations. The method introduces a fictitious class to accurately distinguish similar seen and unseen visual features, and an unseen expert with attention mechanism is proposed for classifying unseen samples. The contributions of this paper include: (1) a novel method for accurately distinguishing seen and unseen domains in GZSL, leveraging both seen and unseen semantic attributes, (2) the introduction of a fictitious class to separate similar visual representations, and (3) extensive experiments on five benchmarks that demonstrate the significant improvement over previous state-of-the-art approaches.