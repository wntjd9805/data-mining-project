Deep neural networks have achieved great success in computer vision tasks, but they often struggle with imbalanced datasets. Early methods focused on re-sampling and re-weighting techniques, but contrastive learning approaches have been less explored. This paper introduces a balanced contrastive learning (BCL) method that addresses the imbalance problem by forming a regular simplex configuration for long-tailed data. The BCL method includes two modifications: class-complement and class-averaging. Additionally, a two-branch framework is proposed, combining the BCL module with a classification module using logit compensated cross-entropy. The paper presents a theoretical analysis that reveals the undesired geometric configuration of supervised contrastive learning for long-tailed data and demonstrates the effectiveness of BCL in achieving competitive results on popular long-tailed datasets.