Inverse rendering is a challenging task in computer vision and computer graphics that involves recovering geometry, material, and illumination from a single image. This task is crucial for applications such as scene editing and virtual object insertion in mixed reality, which require realistic and physically accurate properties. However, inverse rendering is an ill-posed problem due to its complexity, including intricate geometry, diverse material types, and varying local illumination. These factors result in complex lighting effects such as specular reflectance, inter-reflection, and cast shadows.There are three main challenges in solving this task. First, modeling complex material is difficult, as most existing methods assume Lambertian surfaces and only produce diffuse reflectance. Some approaches handle specular reflectance in an unphysical manner. Second, representing changeable local illumination is challenging, as indoor scenes have spatially-varying illumination due to occlusion and non-uniform light distribution. Ensuring coherence and avoiding flickering results for dynamic object insertion has been a problem for many approaches. Third, the lack of high-quality datasets with comprehensive labels poses a challenge, as collecting ground truth labels from real-world images is difficult, and synthetic datasets often lack necessary properties.To address these challenges, we propose PhyIR, an end-to-end neural inverse rendering framework. PhyIR incorporates a more comprehensive spatially-varying bidirectional reflectance distribution function (SVBRDF) representation and a physics-based in-network rendering layer. Our framework tackles the challenges by presenting a more physical inverse rendering model that can handle complex material, proposing a novel spatially-coherent loss to ensure consistency in per-pixel illumination, and generating a large-scale photorealistic indoor panorama dataset with high-quality depth, normal, SVBRDFs, and spatially-varying illumination.The main contributions of our method are: 1) a physics-based inverse rendering framework capable of handling complex material, including metal and mirror material, 2) a spatially-coherent loss to guarantee consistency in neighboring per-pixel illumination, and 3) a large-scale photorealistic indoor panorama dataset with high-quality depth, normal, SVBRDFs, and spatially-varying illumination. By addressing these challenges, our method aims to advance the field of inverse rendering and enable realistic scene editing and virtual object insertion in mixed reality applications.