Large Transformer models have become popular in Computer Vision, but they store world knowledge implicitly in their parameters, making modification and interpretability challenging. Additionally, storing every visual cue in real-world data is impractical. To address these issues, we propose augmenting classification pipelines with an explicit external memory. We focus on the problem of Long-Tail visual recognition, which involves highly skewed class distributions. Learning in this scenario is challenging due to limited information for tail classes and the dominance of head classes. Existing approaches typically adjust the learner or ensemble models, while our approach, Retrieval Augmented Classification (RAC), stores tail knowledge using a retrieval-based augmentation. RAC's retrieval module uses image representations and associated text for retrieval. We demonstrate that RAC improves performance on long-tail classification benchmarks, even outperforming consistent approaches. RAC also allows the use of large pretrained models, broadening their applicability. Our contributions include the effective use of external memory in long-tail visual recognition, a novel method for long-tail classification, and insights into the proposed method with reliable baselines.