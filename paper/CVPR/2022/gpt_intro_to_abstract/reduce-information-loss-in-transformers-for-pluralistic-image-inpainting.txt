Image inpainting, the task of filling missing regions in damaged images, is a widely studied topic in computer vision. Traditional texture matching-based methods perform well in simple cases but struggle with complex natural images. Recently, convolutional neural networks (CNNs) have achieved success in image inpainting by learning on large-scale datasets. However, CNNs still have limitations in understanding global structure and inpainting large masked regions due to their local inductive bias and spatial-invariant kernels.Transformers, known for their ability to model long-term relationships, have shown promising results in various vision tasks. Some recent works have applied transformers to pluralistic image inpainting and have achieved notable success in diverse and high-quality inpainting of large regions. These methods typically downsample the input image, quantize the pixels, and use transformers to predict missing patches using un-quantized feature vectors. This design enhances prediction accuracy by avoiding information loss.To highlight the superiority of our proposed method, we conducted extensive experiments on FFHQ, Places2, and ImageNet datasets. The results demonstrate that our method outperforms CNN-based pluralistic inpainting methods on different evaluation metrics. Our approach also achieves higher fidelity than existing transformer-based solutions, particularly in the inpainting of large regions and complex large-scale datasets.