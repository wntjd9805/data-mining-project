Predicting network generalization performance using complexity metrics based on training data is an important area of research in machine learning. Understanding the relationship between these metrics and generalization accuracy is crucial for selecting optimal network topologies and tuning hyper-parameters. While the field of metric development is growing, this work focuses on leveraging explainability metrics to enhance the training of deep neural networks (DNNs). New metrics, including stable rank, condition number, and a quality measure, are introduced to assess the knowledge representation capabilities of DNN layers. These metrics are used to augment stochastic gradient descent (SGD) by dynamically adjusting the learning rate. Additionally, this work sheds light on commonly used hyper-parameter tuning techniques and provides reasonable explanations for their effectiveness. The main contributions of this work involve the introduction of new explainability metrics, their use in explaining neural network training mechanisms, and their exploitation in augmenting SGD with the new RMSGD optimizer, resulting in improved performance and generalization.