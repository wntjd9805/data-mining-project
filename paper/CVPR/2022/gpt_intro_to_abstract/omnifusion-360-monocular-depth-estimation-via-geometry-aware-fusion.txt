This paper introduces OmniFusion, a framework for 360 monocular depth estimation in order to address the geometric distortions introduced by commonly used representation formats for 360 images. Previous studies have proposed various approaches such as distortion-aware convolutions, spherical CNNs, and using less-distorted formats like cubemap or perspective projections. However, these approaches have limitations in terms of depth scale inconsistency, loss of holistic scene understanding, and lack of information exchange between tangent images. To overcome these limitations, OmniFusion incorporates three key components. First, a geometric embedding module compensates for discrepancies between patch-wise features by encoding 3D points on the spherical surface. Second, a self-attention-based transformer globally aggregates patch-wise information to enhance depth estimation and improve consistency. Third, an iterative refinement mechanism utilizes predicted depth maps to further improve depth quality. Experimental results on benchmark datasets demonstrate that OmniFusion outperforms state-of-the-art methods in terms of depth estimation accuracy. The contributions of this work include the development of a distortion-aware 360 monocular depth prediction pipeline, the introduction of a geometric embedding network to mitigate feature discrepancies, the incorporation of a self-attention-based transformer to enhance depth estimation, and the proposal of an iterative mechanism for improved depth estimation with structural details.