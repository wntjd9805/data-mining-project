Recent research has demonstrated that Deep Neural Networks (DNNs) can be easily manipulated by adversarial examples in both the digital and physical world. These examples involve adding subtle noise to original images or using manufactured objects to deceive the networks. In an attempt to evade person detectors, several methods based on patch attacks have been proposed, such as attaching a patch to a cardboard or printing adversarial patches on a T-shirt. However, these attacks are effective only when the adversarial patches directly face the camera. This limitation poses a challenge in attacking detectors from multiple viewing angles. To address this issue, this paper introduces the concept of the segment-missing problem, where a single adversarial patch on a piece of clothing may be only partially captured by the camera. Existing solutions, such as tiling multiple patches on the clothing, fail to fully mitigate this problem. Therefore, this paper aims to propose a novel approach to overcome the segment-missing problem and enhance the effectiveness of person detector attacks. By addressing this issue, it becomes imperative to reevaluate the safety and reliability of deep learning-based security systems in the face of such threats.