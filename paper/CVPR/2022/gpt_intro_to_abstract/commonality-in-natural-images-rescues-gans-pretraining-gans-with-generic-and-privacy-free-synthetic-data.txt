Generative adversarial networks (GANs) are widely used in computer vision tasks due to their impressive generation quality. However, GANs typically require a large amount of training data, which may not be feasible in practical applications with limited data availability. Transfer learning has been introduced to GAN training to improve generation quality using pretrained models. However, the effectiveness of transfer learning depends on the similarity between the source and target datasets. Existing source datasets may not fit real-world applications well and can also pose copyright and privacy issues. In this work, we address these critical issues by proposing a synthetic data generation strategy for acquiring pretrained GANs. Our synthetic dataset is free from copyright and privacy issues and ensures generalization to various datasets. We design our data synthesizer, Primitives-PS, based on the power spectrum distribution, structural property, and saliency of natural images. Pretraining GANs using our synthetic dataset and transferring the pretrained models to low-shot datasets show improved generation performance and convergence time compared to models pretrained with natural images. Our analysis of learned filters provides insights into transferability. The code is available for reproducibility.