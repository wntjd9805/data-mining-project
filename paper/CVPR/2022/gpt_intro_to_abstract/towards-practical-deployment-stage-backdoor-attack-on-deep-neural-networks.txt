The rapid advancement and deployment of deep learning models in various real-world applications has raised concerns about their vulnerability to attacks. The AI security community has focused on developing secure and reliable methods for producing and deploying deep learning models. While much attention has been given to backdoor attacks during the model production stage, the vulnerability of models during the deployment stage has not been adequately addressed.Current backdoor attack methods typically involve adversaries injecting poisoned samples during the training stage or providing pre-trained models with backdoors. However, the deployment stage, where models are frequently used on unprofessional user devices, is often overlooked. Existing deployment-stage attack algorithms heavily rely on gradient-based techniques and fail to consider real-world scenarios and physical triggers.To address these shortcomings, this paper proposes the Subnet Replacement Attack (SRA) framework for practical deployment-stage backdoor attacks. SRA does not require gradient information of victim models and instead replaces a narrow subnet of a benign model with a malicious backdoor subnet. The proposed framework is shown to be theoretically feasible and achieves high attack success rates while maintaining clean accuracy.Additionally, the paper demonstrates the application of SRA in realistic adversarial scenarios. From a system-level attack practitioner's perspective, the SRA framework is highly compatible with traditional system-level attack practices. This highlights the practical risk of a computer virus that can stealthily inject backdoors into DNN models in user devices.The main contributions of this work are three-fold: 1) identifying the lack of attention given to deployment-stage backdoor attacks, 2) proposing the SRA framework to improve the practicality of deployment-stage attacks, and 3) providing concrete real-world attack strategies and demonstrating the practical risk of a novel computer virus.