Deep neural networks (DNNs) have become a crucial tool in advancing artificial intelligence, but recent studies have shown that DNNs are vulnerable to adversarial examples. These examples are virtually imperceptible perturbations to an image that can cause a well-trained DNN to misclassify. As a result, there is growing interest in understanding the adversarial vulnerability and robustness of DNNs. Previous works have focused on white-box attacks, where full access to the model's parameters and architectures is available. However, this scenario is not practical in real-world systems due to privacy and security concerns. In a more practical black-box scenario, attackers can only query the target network and obtain its outputs. To generate adversarial examples, attackers train substitute models to imitate the target model based on the transferability of these examples. However, training substitute models requires access to target models' training data, which is often difficult to acquire due to privacy or transmission constraints. Recent research has proposed data-free black-box approaches using generative adversarial networks (GANs) to train substitute models. However, this training process is unstable and leads to model collapse, making it challenging to effectively learn a substitute model with a limited query budget. In this paper, we propose a powerful black-box attack framework that addresses the convergence and mode collapse problems in data-free attack methods. We reformulate the collaboration between the generator and substitute model, allowing them to have different objectives in a minimize-maximize game. We also introduce techniques to promote data diversity and balance data distribution to alleviate mode collapse. Additionally, we leverage synthetic data to improve the training efficiency of the substitute model and achieve higher attack success rates.Our empirical evaluations on six datasets demonstrate that our proposed method can efficiently imitate target models and successfully generate adversarial examples using the substitute model. For example, we achieve a 98.0% untargeted attack success rate on CIFAR10 with only 3.75% of the query budget used by the state-of-the-art DFME method. We also achieve a 100% attack success rate on the Microsoft Azure online model with only 0.46% of the query budget used by the previous SOTA method DaST. Our work provides insights into improving the security of DNNs in practical black-box scenarios with limited query budgets.