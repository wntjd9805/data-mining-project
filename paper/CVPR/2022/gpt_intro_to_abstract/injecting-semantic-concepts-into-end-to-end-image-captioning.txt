This paper introduces the task of image captioning, which involves generating descriptive text from an image. Recent developments in this field have focused on cross-modal fusion architectures, object-centric features and tags obtained from pre-trained object detection models, and general Vision and Language (VL) representations obtained from large image-text datasets. However, mainstream captioning models heavily rely on object detectors, which can lead to computational inefficiencies and limitations in training and application flexibility. To address these challenges, there is a growing trend to eliminate the detector and instead use a general visual encoder to produce grid features for cross-modal fusion. However, most existing works have primarily focused on the image understanding task, with few considering the generation task. This paper presents ViT-CAP, a detector-free image captioning model based on a vision transformer that leverages a lightweight Concept Token Network (CTN) to predict semantic concepts as tokens for captioning. The paper demonstrates that ViT-CAP outperforms existing detector-free models and approaches state-of-the-art detector-based models. The proposed model achieves high scores on multiple captioning datasets and offers advantages in terms of inference time. The contributions of this paper include the introduction of ViT-CAP as a detector-free captioning model, the injection of semantic concepts into end-to-end captioning through concept classification training, and extensive evaluations confirming the validity of the proposed method.