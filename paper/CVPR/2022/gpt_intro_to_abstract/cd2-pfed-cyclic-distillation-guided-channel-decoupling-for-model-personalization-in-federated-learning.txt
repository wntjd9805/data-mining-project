Deep learning techniques in computer vision tasks have gained popularity, but they require a large volume of training images. Collaboratively curating data from different sources is one solution, but centralized storage is costly and time-consuming. Additionally, privacy concerns and legal restrictions prevent the direct sharing of decentralized image data. To address these issues, distributed training paradigms like Federated Learning (FL) have emerged. FL allows a shared model to be globally trained while maintaining privacy by conducting local updates at each client. However, FL faces challenges due to data heterogeneity, which can lead to performance degradation. Personalized Federated Learning (PFL) aims to train models tailored to each client's data distribution without sharing data. Previous PFL approaches have focused on layer-wise personalization, but they require prior knowledge and struggle with generalization. This paper proposes CD2-pFed, a novel framework for model personalization in FL. CD2-pFed decouples model parameters at the channel dimension instead of using layer-wise personalization, offering a unified solution for addressing various forms of data heterogeneity. The framework combines a channel decoupling paradigm with a cyclic distillation scheme to enhance collaboration between private and shared weights. Experimental results on benchmark datasets demonstrate the superiority of CD2-pFed over previous PFL approaches.