Transformers, originally designed for sequence problems in natural language processing and speech recognition, have seen significant success in advancing state-of-the-art in vision tasks. These "vision transformers" are typically trained on large datasets using either supervised or self-supervised methods, and then fine-tuned on specific downstream tasks. However, fine-tuning the entire model can be problematic due to the large number of parameters, making it expensive to maintain and store. In this paper, we propose a method to augment pre-trained models with learnable tokens at each layer of the transformer block, acting as a permanent memory to improve predictions. Our approach achieves significant improvements in downstream task accuracy compared to head-only fine-tuning. Furthermore, our architecture design allows for the extension of trained models to perform new tasks while retaining the ability to perform previous tasks with incremental compute cost. We demonstrate the effectiveness of our approach through experiments and ablation studies. Overall, our method provides a novel way to extend and concatenate models for multi-task learning, with potential applications in lifelong and continual learning.