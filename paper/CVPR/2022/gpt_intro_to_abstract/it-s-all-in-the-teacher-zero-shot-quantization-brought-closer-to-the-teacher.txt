Deep neural network quantization is a powerful technique for improving the efficiency of deep neural networks by reducing latency and energy consumption. However, quantized models often suffer from a drop in accuracy due to quantization errors. One way to address this is through fine-tuning the model with training data, but this is not always feasible in practice. Zero-shot quantization, which only relies on the architecture and pre-trained weights, is therefore necessary. Current successful approaches use generative methods and knowledge distillation to achieve comparable performance to data-driven approaches. However, the loss function used in zero-shot quantization lacks detailed studies and may not be appropriate for the context. Additionally, the distribution of synthetic samples can differ from that of the original data, resulting in a generalization gap. In this paper, we perform in-depth analyses on the loss surface of zero-shot quantization and propose a method called AIT (All In the Teacher) to address its challenges. AIT eliminates the cross-entropy loss and applies gradient inundation with the KL divergence. Our method achieves state-of-the-art performance on various datasets and improves upon existing algorithms. Key contributions include analyzing the loss surface, identifying the trade-off between the two losses, and proposing a method to align the quantized model with the full-precision teacher model.