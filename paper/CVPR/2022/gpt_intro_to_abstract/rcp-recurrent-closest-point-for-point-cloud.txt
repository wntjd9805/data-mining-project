Motion estimation plays a crucial role in various applications, including robotics, augmented reality, and autonomous driving. By estimating 2D flow fields from optical flow estimation, we can obtain low-level motion cues that are valuable for higher-level tasks such as object detection and action recognition. While scene flow methods extend 2D optical flow to stereoscopic or RGB-D image sequences, recent attention has shifted towards direct 3D flow estimation on point clouds. This approach offers advantages such as low-latency estimation for autonomous driving and efficient computation distribution for augmented reality. Learning-based methods have been developed to customize scene flow estimation specifically for point clouds, utilizing recent advances in high-level tasks. However, defining a fixed search window for point clouds, as done in 2D optical flow, is challenging due to the irregular data structure. Previous approaches have employed complex layers to measure point-to-patch or patch-to-patch costs. In this paper, we propose a simple and effective method that decomposes the problem into two stages: point-wise optimization and global regularization using a recurrent network. By avoiding the discretization of the 6DOF solution space and measuring point-to-point costs, our method provides superior results in 3D scene flow estimation and point cloud registration, as demonstrated through experiments on benchmark datasets. The proposed zero-order method offers significant advantages for irregular point cloud data. This paper contributes to the field by introducing a novel approach to 3D flow estimation on point clouds and achieving state-of-the-art results in related tasks.