Deep neural networks (DNNs) have shown great success in various computer vision tasks, but most state-of-the-art methods require supervised training with a large amount of annotated data. However, collecting and annotating such data can be challenging and expensive. Many large-scale data collection techniques rely on web data that is automatically annotated but introduces label noise. Training with noisy labels is difficult because DNNs can memorize arbitrary noisy labels. This study focuses on combating label noise, which is a fundamental problem in deep learning. Previous approaches for training with noisy label data can be categorized into label correction and sample separation methods. Label correction requires estimating noise transition matrices, while sample separation filters out noisy samples from clean ones based on a small-loss criterion. However, the selection process in sample separation methods is biased towards easy classes and may result in class-disparity and poor precision. In this work, we propose a new uniform selection mechanism that enforces class-balancing and improves the quality of pseudo-labels. We also employ unsupervised contrastive learning to minimize the risk of label noise memorization. Our method, UNICON, achieves significant performance improvement over state-of-the-art methods, especially on datasets with severe label noise.