Before deploying a deep learning model, designers need to ensure that the model achieves a baseline performance threshold. Increasing the amount of training data can help improve model performance, but determining the optimal amount of data required is a challenge. Overestimating data requirements can lead to unnecessary costs and delays, while underestimating them can result in additional data collection and future costs. Previous research has proposed using power law functions to estimate data requirements, but our paper suggests using alternative regression functions that may be more accurate in practice. We demonstrate through simulations that incremental data collection over multiple rounds is essential to meet the target performance without exceeding it significantly. We also introduce a correction factor that can be learned from prior tasks to enhance the accuracy of data requirement estimates. Our empirical findings provide practical guidelines for data collection in real-world applications, recommending multiple rounds of data collection and the use of the correction factor alongside an optimistic regression function. By implementing our approach, significant cost savings can be achieved in the future.