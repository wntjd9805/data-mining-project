This paper proposes a novel approach for cross-domain continual learning with the capability of generalization to unseen domains. The authors introduce a setup consisting of a sequence of tasks with data originating from various domains. The proposed approach utilizes a discriminative domain alignment strategy called Mahalanobis Similarity Learning (MSL), which equips the classifier with class-specific Mahalanobis similarity metrics. This encourages the learning of semantically meaningful features across training domains. Additionally, the authors propose an exponential moving average framework for knowledge distillation to prevent excessive divergence from previously learned parameters. Extensive evaluations on four different datasets show that the proposed method consistently outperforms baselines on 10-task and 5-task protocols, achieving an improvement of up to 10%. The method also successfully mitigates catastrophic forgetting and achieves a low backward transfer rate. In summary, the contributions of this paper include providing a unified testbed for cross-domain continual learning, proposing a robust projection technique for domain generalization, and introducing a knowledge distillation framework to alleviate the impact of catastrophic forgetting and distributional shifts.