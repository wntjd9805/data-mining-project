This paper introduces a new approach to mapping in autonomous vehicles that focuses on learning a mapping from camera-view to a canonical map-view representation using a cross-view transformer architecture. Previous approaches have relied on explicit modeling of geometry and relationships between different views and maps, which can be challenging due to the error-prone nature of image-based depth estimates and the inflexibility of depth-based projections. In contrast, the proposed cross-view transformer architecture learns to map features from camera-view to map-view representation without explicit geometric reasoning. It uses a geometry-aware positional embedding and multi-head attention to perform the mapping. The model also learns an implicit estimate of depth through the camera-dependent map-view positional embedding. The simplicity of the model is highlighted, as it achieves state-of-the-art performance on vehicle and road segmentation tasks in the map-view and runs in real-time on a single GPU. The attention mechanism learns accurate correspondences between camera and map-views directly from data, making the model easy to implement and train.