Face recognition (FR) technology has been widely used in various applications such as access control and electronic payments. However, FR systems are vulnerable to presentation attacks (PAs) like print attacks and video replay. To address this issue, face anti-spoofing (FAS) methods have been developed. These methods range from hand-crafted descriptors to deep representation based approaches. While previous FAS methods have shown promising results in intra-domain scenarios, they often perform poorly in cross-domain settings due to dataset bias and limited generalization capability. Domain adaptation (DA) techniques have been used to reduce the discrepancy between source and target domains, but collecting sufficient unlabeled target data for training is inefficient. To overcome this limitation, domain generalization (DG) methods have been proposed. These methods focus on learning common feature spaces, disentangled representations, and learning to learn. However, they fail to fully utilize the subtle properties of global and local image statistics in FAS. In this regard, this paper introduces a new framework called shuffled style assembly network (SSAN) that utilizes global and local statistics for generalizable FAS. SSAN extracts content and style features using a two-stream structure and leverages adversarial learning for shared feature distributions and discriminative information. Contrastive learning is employed to enhance liveness-related style information and suppress domain-specific information. The proposed architecture and training approach are suitable for large-scale training. To evaluate the effectiveness of the proposed method, a large-scale benchmark is constructed by combining twelve public datasets. The performance is assessed using the True Positive Rate at False Positive Rate (TPR@FPR) metric. Experimental results demonstrate that the proposed SSAN achieves state-of-the-art performance on existing and proposed benchmarks.