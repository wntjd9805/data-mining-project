Attention-based deep neural networks have demonstrated impressive performance in various tasks, such as computer vision and natural language processing. The Transformer model, in particular, has shown promise in solving multimodal problems by utilizing self-attention mechanisms that allow for global information aggregation. While Transformers have been successful in NLP and computer vision tasks, the referring video object segmentation task (RVOS) presents unique challenges due to the dynamic nature of videos and the need to establish data associations across frames.Existing RVOS approaches typically rely on complex pipelines to handle these challenges. In this paper, we propose a simple, end-to-end Transformer-based approach to RVOS. By leveraging recent advancements in Transformers for textual and visual feature extraction, as well as object detection, our framework outperforms existing approaches. We employ a single multimodal Transformer and model RVOS as a sequence prediction problem. Our model generates prediction sequences for all objects in the video before determining the one referred to by the text query. We also introduce a temporal segment voting scheme to better align the video and text and focus on relevant parts of the video.Our contributions include the development of a Transformer-based RVOS framework called Multimodal Tracking Transformer (MTTR). Our framework simplifies the RVOS pipeline by being end-to-end trainable, free of text-related inductive bias modules, and requiring no additional mask refinement. We thoroughly evaluate our method on various datasets and demonstrate its superiority over existing methods. MTTR achieves significant improvements in performance across all metrics, including on challenging datasets that have not received much attention in the literature.Overall, our proposed approach offers a more efficient and effective solution for referring video object segmentation, leveraging the power of Transformers in handling multimodal tasks.