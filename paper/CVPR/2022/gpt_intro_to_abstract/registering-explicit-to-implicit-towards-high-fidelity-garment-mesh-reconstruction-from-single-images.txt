This paper introduces a novel geometry inference framework called ReEF, which aims to produce high-fidelity layered garments from single images. The generation of visually plausible 3D digital human assets has always been a laborious task, but recent advancements in single image body and clothed human reconstruction have made it possible to generate 3D human-related content efficiently. However, research on single image layered garment reconstruction is relatively sparse. The main challenges in garment reconstruction are generating garment styles and recovering surface details. Existing methods have limitations in generating novel garment styles and producing large-scale wrinkle deformations. Additionally, none of the existing methods can align garment styles and surface details with the appearance from the input image. To address these challenges, ReEF registers explicit garment template meshes to the full-body implicit fields predicted from single images. The paper proposes novel methods to generate boundary fields and semantic fields to align the explicit garment template with the implicit clothed body. Experiments demonstrate that ReEF can produce high-quality garment meshes with accurate styles and expressive surface details. The contributions of this work include the proposal of a novel geometry inference framework, a learning-based method for predicting implicit garment boundary fields, and experiments on synthetic and in-the-wild datasets that showcase the effectiveness of ReEF in generating high-quality layered garments.