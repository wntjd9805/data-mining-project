Scene text detection is a challenging task in computer vision, requiring the model to predict bounding boxes or polygons for text instances in an image. Deep learning-based methods have been widely used in this field due to their high research value and real-world applications. Despite recent advances in scene text detection, there are still remaining challenges. Various pre-training strategies have been proposed to enhance the generalization capability of models, including ImageNet pre-training and pre-training on synthetic text datasets. However, these methods often suffer from a domain gap between synthetic and real-world data. To address this issue, a new pre-training approach called STKM was proposed, which achieved promising results. However, STKM lacked the ability to effectively exploit contextual information and interactions between vision and text. To overcome these challenges, this paper introduces a novel Vision-Language Pre-Training framework for boosting Scene Text Detectors (VLPT-STD). This framework incorporates self-attention and cross-attention modules, along with three pre-training objectives, to align image and text representations and enhance the detection performance. The proposed approach requires only image-level text annotations, making it more cost-effective than traditional methods. Extensive experiments demonstrate the effectiveness of VLPT-STD, with consistent improvements over conventional and STKM pre-training techniques on various text detection datasets. Overall, this paper contributes a conceptually simple and flexible framework for pre-training scene text detectors, along with three novel pre-training objectives and demonstrated improvements in detection performance.