Multimodal Transformers have become the go-to choice for multimodal learning tasks, such as classification, segmentation, and cross-model retrieval. However, these models generally require complete data for each modality, which may not always be available due to privacy or security constraints. This paper aims to investigate the robustness of Transformer models against missing-modal data, a problem that has been seldom addressed in the literature. The authors empirically evaluate the performance of Transformer models on multiple datasets and find that the performance dramatically degrades when modalities are missing. Surprisingly, the multimodal performance is even worse than the unimodal performance when text data is severely missing. To address this issue, the authors propose a new method that optimizes Transformer models with both complete and incomplete data using multi-task optimization. They also develop a searching algorithm to determine the optimal fusion strategy for different datasets. Through extensive experiments and ablation study, the authors validate the effectiveness and robustness of their proposed method. This study is the first of its kind to investigate the Transformer robustness against missing-modal data and provides insights into the impact of fusion strategies on robustness.