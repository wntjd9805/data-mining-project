Recent advances in Generative Adversarial Networks (GANs) have led to significant progress in synthesizing photo-realistic images under various conditions, such as layout, text, and scene graph. In particular, the generation of images conditioned on text descriptions has gained attention from the computer vision and natural language processing communities due to its ability to bridge the gap between these two domains. However, text-to-image (T2I) synthesis remains a challenging task due to the cross-modal problem and the need to keep the generated image consistent with the given text.Existing T2I methods typically employ multi-stage refinement frameworks, where an initial image is generated from noise using sentence embedding and then refined with fine-grained word embedding in subsequent stages. While effective in synthesizing high-resolution images, these methods suffer from higher computation and unstable training processes due to the use of multiple generator-discriminator pairs. Additionally, the quality of the generated image is heavily influenced by the early stages, limiting the ability to improve image quality in later stages.To address these limitations, this paper proposes a one-stage T2I framework called Semantic-Spatial Aware Generative Adversarial Network (SSA-GAN). Unlike multi-stage frameworks, SSA-GAN utilizes a single generator-discriminator pair, resulting in improved efficiency and stability. Moreover, SSA-GAN only employs sentence embedding for image generation, reducing computation costs compared to methods that use word-level features.To ensure semantic and local consistency between the generated image and the text, SSA-GAN introduces a novel Semantic-Spatial Aware (SSA) block. This block learns semantic-aware channel-wise affine parameters and predicts a semantic mask to guide the image synthesis process at the pixel level. By complementing local details in pixel level rather than sub-region level, SSA-GAN achieves better semantic and local consistency with the text.Extensive experiments conducted on benchmark datasets validate the superior performance of SSA-GAN compared to previous methods. The proposed one-stage framework demonstrates lower computation requirements and improved training efficiency compared to multi-stage frameworks. The SSA block effectively fuses text and image features, and the weakly-supervised training of the semantic mask predictor eliminates the need for additional annotation.In summary, this paper presents a novel one-stage framework, SSA-GAN, for image synthesis from text. It utilizes sentence embedding and the SSA block to achieve improved efficiency, lower computation costs, and better semantic and local consistency. The proposed framework outperforms previous methods in both quantitative and qualitative evaluations.