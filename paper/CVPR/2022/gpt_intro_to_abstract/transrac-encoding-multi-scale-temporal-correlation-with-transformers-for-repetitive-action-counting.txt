Repetitive action counting in computer vision plays a crucial role in various applications, including fitness detection and planning. However, existing methods have limited exploration in this area, often focusing on counting actions in short videos lacking realistic scenarios. This limits their applicability to real-world situations where videos can be long and may contain anomalies such as interruptions or inconsistent action cycles. Inadequate annotations further hinder accurate performance evaluation and model interpretability. To address these challenges, we present a new dataset called RepCount, which includes a large number of videos with variations in length and real-world anomalies. The dataset also provides fine-grained annotations of action cycles for improved interpretability. Additionally, we propose a multi-scale temporal correlation encoding network with transformers that can handle high and low frequency actions as well as long and short videos. This approach allows the model to adaptively select the appropriate scale for computing correlation matrices and making count predictions. Moreover, we introduce a density map regression-based method for predicting action periods, which enhances both performance and interpretability. Our contributions include the RepCount dataset, the multi-scale temporal correlation encoding network, and the improved performance achieved on our dataset and other datasets. The proposed method outperforms state-of-the-art approaches, even on unseen datasets without fine-tuning.