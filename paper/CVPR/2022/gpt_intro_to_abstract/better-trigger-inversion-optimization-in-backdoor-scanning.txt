Backdoor attacks involve inducing misclassification of input samples by introducing a trigger pattern. These triggers can be injected through various methods, such as data poisoning or neuron hijacking, or they can naturally exist in normally trained models, known as natural backdoors. The presence of backdoors poses a significant threat, leading researchers to propose defense solutions, including backdoor scanning. Many existing scanning methods rely on trigger inversion, which aims to find a small input pattern that can flip clean samples to the target label. However, most trigger inversion methods, such as Neural Cleanse (NC), can fall into local optima and fail to find the optimal trigger. Triggers generated by NC are often not robust and become ineffective under transformations. This paper introduces a novel optimization method for trigger generation. Unlike existing methods, which optimize the product of the perturbation vector and the mask, our method focuses solely on optimizing the perturbation vector. We leverage the properties of the tanh function to represent the binary nature of perturbations, resulting in a smoother loss surface. Experimental results on the ImageNet dataset demonstrate that our generated triggers are significantly smaller and more robust compared to NC. Our method also achieves higher attack success rates and is faster in terms of computation time. We compare our method with other trigger generation methods, including UAP and CW, and show that ours is faster and more effective. The implementation of our method is publicly available.