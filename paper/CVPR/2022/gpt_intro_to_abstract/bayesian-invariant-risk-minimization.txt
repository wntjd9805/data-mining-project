Machine learning technology has achieved significant success in various domains such as computer vision and speech recognition. However, recent studies have revealed the limitations of these models due to the reliance on spurious features or shortcuts. For example, models may use non-invariant features like background to distinguish between animals. The assumption of independent and identically distributed (i.i.d) data, which is the common foundation of machine learning, does not always hold, especially when the testing distribution differs from the training distribution. This is known as the out-of-distribution (OOD) generalization problem. To address this, the invariance principle has been proposed to extract invariant features that remain stable even in the presence of distributional shifts. Invariant Risk Minimization (IRM) extends this principle to neural networks, regularizing them to focus on extracting invariant features and discarding spurious ones. However, recent empirical findings indicate that IRM methods are ineffective on deep models, mainly due to overfitting. Theoretical analysis confirms that IRM can degenerate to ERM (Empirical Risk Minimization) under overfitting. To overcome this limitation, we propose Bayesian Invariant Risk Minimization (BIRM) as a Bayesian treatment of IRM. BIRM estimates the posterior distribution of the classifier for each environment, helping to alleviate overfitting and make IRM practical for deep models. Our contributions include identifying overfitting as a key reason for the failure of IRM in large deep models, proposing a Bayesian formulation of IRM to mitigate overfitting, and demonstrating the effectiveness of BIRM through extensive experiments.