Automatic lip-reading (ALR) has garnered significant attention in the field of computer vision and pattern recognition due to its applications in biometric identification, improved hearing aids, and speech recognition in noisy environments. In this paper, we propose a novel approach to ALR using event cameras, which are biologically inspired optical sensors that capture per-pixel brightness changes asynchronously at a high temporal resolution. We introduce the Multi-grained Spatio-Temporal Features Perceived Network (MSTP), which converts event data into multi-grained event frames to recognize words. MSTP consists of two branches that take low-rate and high-rate event frames as input, allowing for the perception of both complete spatial structure and fine temporal features. We also devise a message flow module to merge the multi-grained spatio-temporal features learned by the different branches. To evaluate our approach, we collected the first event-based lip-reading dataset, called DVS-Lip, using the event camera DAVIS346. Experimental results demonstrate that MSTP outperforms existing event-based and video-based lip-reading models on the DVS-Lip dataset, showcasing the effectiveness of our proposed approach. The contributions of this work include the development of the first event-based automatic lip-reading framework, the design of a message flow module for feature fusion, the creation of the DVS-Lip dataset, and the demonstration of the superior performance of our method compared to existing approaches.