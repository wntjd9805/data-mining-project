This paper introduces a novel approach for region-based editing of real-world natural images using natural language text guidance. Existing techniques for text-driven image manipulation often rely on restricted domains and require image inversion into a generative adversarial network's latent space. In contrast, our method operates directly on real images and allows for specific region modification while preserving the rest of the image. We utilize pre-trained models, Denoising Diffusion Probabilistic Models (DDPM) and Contrastive Language-Image Pre-training (CLIP), to generate natural-looking results guided by user-provided text prompts. We propose a blending method that combines CLIP-guided diffusion latents with noised versions of the input image, ensuring coherence with the unaltered regions. Through extensive experiments, we demonstrate the effectiveness of our approach in achieving general-purpose region-based image editing while preserving background and mitigating adversarial results. Overall, our contributions include the introduction of a solution for diverse image editing guided by natural language, the preservation of unaltered regions, and the reduction of adversarial outcomes through augmentation techniques.