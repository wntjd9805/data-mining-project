Vision Transformers (ViTs) have achieved impressive results in various vision tasks, including image classification, object detection, and semantic segmentation. These models extract feature representations by splitting visual features into patches and projecting them into an embedding space. This is followed by transformer encoder layers, which consist of Multi-head Self-Attention (MSA) and Multi-Layer Perceptron (MLP) blocks. ViTs have been successful in learning global and local spatial dependencies. However, there is a lack of research on the vulnerability of sharing ViT's gradients in different settings.Previous studies have shown that convolutional neural networks (CNNs) are susceptible to gradient-based inversion attacks, where an attacker can intercept local model gradients and reconstruct private training data by matching compromised gradients. GradInversion has scaled gradient inversion to deep networks on large datasets but relies on batch normalization layers in models for better reconstruction. Since ViTs do not have batch normalization layers, existing inversion methods designed for CNNs do not work effectively when applied to ViTs, resulting in poor reconstruction quality.In this work, we aim to quantitatively and qualitatively demonstrate that ViT-based models are even more vulnerable to gradient-based inversion attacks than CNNs. We propose a novel method called GradViT that addresses the architectural differences between ViTs and CNNs by using an independently trained CNN to match feature distributions. We also introduce a patch prior loss to address the lack of inductive image bias and permutation invariance in ViTs.We evaluate the effectiveness of GradViT on various ViT-based models using the ImageNet1K and MS-Celeb-1M datasets. Our method achieves state-of-the-art benchmarks in terms of image reconstruction metrics and demonstrates the possibility of detailed recovery of facial images. We analyze the vulnerabilities of ViT components through layer-wise and component-wise analysis, showing that deeper layers' gradients are more informative and MSA gradients yield near-perfect input recovery.Overall, our contributions include the development of GradViT for ViT gradient inversion, an image prior based on contrastive loss-trained CNNs, a loss scheduling scheme, a patch prior loss function tailored to ViT inversion, and setting state-of-the-art benchmarks for ViT gradient inversion. Our findings provide insights for developing protection mechanisms against gradient inversion attacks and securing distributed training of ViTs in multi-node or federated learning scenarios.