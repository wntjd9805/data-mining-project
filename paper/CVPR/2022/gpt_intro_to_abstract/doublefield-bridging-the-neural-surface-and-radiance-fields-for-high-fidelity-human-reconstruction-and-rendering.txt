Surface fields and radiance fields have emerged as promising solutions for geometry modeling and texture rendering of 3D human models. However, these methods have limitations when it comes to simultaneous geometry and appearance reconstruction, particularly in sparse multi-view settings. Surface fields separate geometry learning from appearance learning, while radiance fields lack effective mutual constraints for consistent geometry reconstruction. Additionally, feature fusion strategies play a crucial role in reconstruction quality under multi-view setups. The limited representation power of features and calibration and geometry inference errors further deteriorate reconstruction performance. To address these limitations, we propose the DoubleField framework, which effectively bridges surface and radiance fields and enables shared learning space for both geometry and radiance reconstruction. This framework incorporates a feature embedding shared by the two fields and a surface-guided sampling strategy, allowing them to benefit from each other. Additionally, we introduce a view-to-view transformer that builds self attention between multi-view inputs and cross attention between input views and query viewpoints, improving feature fusion. This transformer is capable of utilizing high-resolution images and mitigating multi-view inconsistency issues. Our DoubleField framework improves reconstruction quality for both geometry and appearance and eliminates the need for pre-requisite SMPL fitting. It can handle loose clothing and make use of large-scale human scan datasets, enabling direct inference and fast finetuning for high-resolution free viewpoint rendering. Our proposed method achieves state-of-the-art performance in geometry reconstruction and texture rendering using sparse-view inputs.