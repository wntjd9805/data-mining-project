This paper introduces a new framework for Video Scene Graph Generation (VidSGG) that addresses the drawbacks of existing proposal-based methods. Scene graphs are graph-structured representations that capture visual relations between object instances in a video. The proposal-based methods used in VidSGG have limitations, such as partially correct ground-truth labels, breaking of high-order relations, and sensitivity to proposal generation rules. To overcome these limitations, the proposed classiﬁcation-then-grounding framework conducts predicate classiﬁcation based on the entire tracklets and then grounds the predicted predicate instances. This framework provides more accurate ground-truth predicate labels, preserves high-order relations, and avoids excessive proposals and heuristic rules. The authors also reformulate video scene graphs as temporal bipartite graphs, where entities and predicates are represented as nodes with time slots, and edges denote semantic roles. Entity nodes represent object tracklets, predicate nodes represent relation instances, and each entity node can be linked to multiple predicate nodes. A model called BIpartite Graph (BIG) is proposed, consisting of a classiﬁcation stage and a grounding stage. The classiﬁcation stage uses a Transformer-based model with role-aware cross-attention, while the grounding stage uses a multi-instance grounding head.Experimental evaluations on the VidVRD and VidOR benchmarks demonstrate the effectiveness of the proposed framework and BIG model, achieving state-of-the-art performance in VidSGG. In conclusion, this paper introduces a new framework and model that overcome the limitations of existing proposal-based methods, providing more accurate and context-aware results in Video Scene Graph Generation.