In recent years, deep learning techniques have enabled the widespread use of gaze estimation in various human-computer interaction systems. Appearance-based approaches have gained attention for their ability to estimate gaze direction from monocular images alone, eliminating the need for expensive eye model devices. However, the annotation requirements of these approaches limit their application in real-world scenarios. While large-scale gaze datasets and related estimation methods have been proposed to address this issue, they perform well within a single dataset but struggle when applied to different datasets due to domain differences. Collaborative model ensembles and additional annotations have been utilized to narrow the cross-dataset gap, but they add complexity to the learning pipeline. This paper proposes a self-supervised approach called Contrastive Regression Gaze Adaptation (CRGA) to address the cross-dataset gap without requiring additional labels or models. The authors derive a contrastive regression loss specifically for regression tasks and develop two modules, Contrastive Domain Generalization (CDG) and Contrastive Self-training Adaptation (CSA), to improve adaptation performance. Experimental results demonstrate that CRGA outperforms baseline models and state-of-the-art domain adaptation approaches, achieving remarkable performance improvements in gaze adaptation tasks. Specifically, CRGA shows improvements over the baseline ranging from 30.5% to 55.8% in various source domains when adapting to target domains. This paper introduces contrastive learning into regression tasks, significantly enhancing domain generalization and adaptation performance.