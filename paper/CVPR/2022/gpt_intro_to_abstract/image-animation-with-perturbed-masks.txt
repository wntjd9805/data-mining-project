The paper introduces the recent advancements in the field of reanimating still images based on driving videos. These methods aim to maintain source identity and replicate the motion pattern of the driver's frame with increased accuracy. The paper highlights that model-free methods have achieved impressive results by convincingly disentangling shape and identity from motion. However, there are areas where these methods still need improvement, such as noticeable artifacts, loss of source image identity, and mismatched motion in the generated video. To address these limitations, the paper proposes a new method that focuses on motion accuracy, identity and background preservation, and video quality. This method utilizes a mask-based representation of the driving pose and conditions on the source foreground mask. Both the source and driver masks are extracted using the same network, with an additional stage for manipulating the driver's mask to replace its identity information. The use of masks brings several advantages, such as eliminating identity cues from the driving video, explicitly modeling the region to be replaced in the source image, allowing training solely on source videos, and capturing detailed object pose and shape descriptions. Unlike previous methods that rely on GANs, this approach employs an encoder-decoder architecture to manipulate identity and direct the network's attention to specific parts of the input information.The contributions of this paper include an image animation method that generalizes to unseen identities, innovative use of perturbations on masks to interrupt and replace the driver's identity, and a comprehensive evaluation of various applications that demonstrates a significant improvement over the current state-of-the-art in image animation.