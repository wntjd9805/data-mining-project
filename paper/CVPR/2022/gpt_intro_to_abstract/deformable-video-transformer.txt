Transformers have gained popularity in computer vision with the introduction of the Vision Transformer (ViT), which offers the benefit of capturing longer-range dependencies compared to CNN architectures. However, the self-attention operation in ViTs causes high computational cost when executed globally over all pairs of image patches. This cost becomes even more significant in the case of videos, where the number of patches to consider scales linearly with the number of frames in the clip. Limiting the temporal attention to pairs of patches in different frames but at the same spatial location reduces the complexity but hampers temporal reasoning. Previous methods for reducing computational cost employ fixed, hand-designed attention strategies that neglect the dynamic nature of the video. In this work, we propose a novel space-time attention mechanism called deformable space-time attention, which leverages motion cues to dynamically determine which patches to compare. Motion cues are obtained from the compressed format of the video, resulting in a considerably lower computational cost compared to global attention. Our experiments show that deformable attention achieves higher accuracy than global attention and achieves state-of-the-art results on action classification benchmarks.