The task of instance segmentation involves locating objects in images and generating pixel-level binary masks for each object. Manual annotation of object masks for training is time-consuming, leading to the need for alternative forms of annotation. Weakly-supervised methods using easier-to-acquire annotation forms have made instance segmentation more accessible for new categories or scene types. Point clicks and squiggles have been used in interactive segmentation scenarios, and in this paper, we present a new instance segmentation annotation scheme that combines bounding boxes and point-based annotation. Unlike previous works, we randomly sample points inside an object bounding box and ask annotators to classify each point as object or background. We find that this point-based annotation scheme performs well on large-scale datasets and can be easily simulated with existing instance segmentation ground truth. We demonstrate the efficacy of our scheme on multiple datasets using Mask R-CNN, PointRend, and CondInst models, achieving 94%â€“98% of fully-supervised performance with only 10 annotated points per object. We also propose a point-based data augmentation strategy and explore self-training for further improvement. Additionally, we show that point-based pre-training matches mask-based methods in transfer learning. Our annotation scheme significantly reduces the annotation time compared to polygon-based mask annotation, making it a practical solution for training instance segmentation models. We introduce a new model, Implicit PointRend, which outperforms PointRend with point supervision by generating different parameters for a function that predicts the point-wise masks. Overall, our findings demonstrate the effectiveness of point-based annotation and its potential for improving instance segmentation models.