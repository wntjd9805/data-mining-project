The introduction of this computer science paper discusses the significant performance of AlexNet on the ImageNet image classification challenge, which sparked interest in convolutional neural networks (CNNs). This led to the development of powerful CNN backbones such as Swin-T based FCOS and Swin-T based Faster R-CNN. However, the performance of these detectors trained from scratch is not comparable to their ImageNet pre-trained counterparts. The paper explores the use of vision transformer-based detectors without pre-training and investigates the effectiveness of training from scratch. The limitations of using pre-trained networks in object detection are also discussed, including limited structure design space, learning bias, and domain mismatch. The paper highlights previous studies on training CNN-based detectors from scratch and raises two questions: whether these findings hold true for vision transformers and if it is possible to train vision transformer-based object detectors from scratch. The paper presents experimental results showing that naively applying the training methods from CNNs to vision transformers is not sufficient. Instead, architectural changes and more training epochs are found to be important. The paper proposes insights for training vision transformer-based detectors from scratch, including replacing RoIPooling with RoIAlign, introducing convolution blocks in the backbone model, gradient calibration, and more training epochs. The main findings of the paper include achieving consistency between proposal-based and proposal-free detectors, introducing the inductive prior of convolutions into the backbone model, gradient calibration, and the need for more training epochs. These insights are expected to be valuable for researchers and practitioners working in various fields of computer vision.