View synthesis of human activities has numerous applications in visual effects and telepresence, providing unique and immersive viewing experiences. However, current solutions for high-quality view synthesis still require complex capture setups or expensive per-scene training. This paper introduces HumanNeRF, a practical and high-quality neural free-view synthesis approach for general dynamic humans using only sparse RGB streams. The approach optimizes a more generalizable radiance field on-the-fly, enabling photo-realistic human rendering without the need for long-term per-scene training. The key idea is to combine the dynamic NeRF representation with neural image-based blending, using an implicit blending strategy to enhance texture results. The authors propose a two-stage framework that incorporates implicit feature aggregation and hybrid deformation to generalize the inference of motion and appearance in the dynamic NeRF framework. Additionally, they introduce a novel neural blending scheme that combines image-based rendering with NeRF-based volume rendering to accurately preserve texture detail from the input images. The paper presents significant contributions in terms of performance rendering, extending the generalizable NeRF framework to the dynamic and light-weight setting, and proposing a novel implicit blending scheme for realistic appearance rendering. Overall, HumanNeRF achieves superior performance compared to existing approaches in the field.