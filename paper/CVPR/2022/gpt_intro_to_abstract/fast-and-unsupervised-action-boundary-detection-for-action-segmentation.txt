Efficiently analyzing the contents of untrimmed videos is crucial for real-world deployment and has a wide range of applications. Supervised action segmentation has seen significant progress thanks to large-scale datasets and deep neural networks. However, the manual data labeling required for fully supervised learning models is slow, expensive, and error-prone, leading to the popularity of unsupervised action segmentation. In this paper, we propose an efficient and effective unsupervised action segmentation method called action boundary detection (ABD). Inspired by the temporal coherence of videos, we focus on detecting action boundaries as human attention would, using frame-wise similarities and change points. We introduce a bottom-up approach similar to the Canny detector in image processing and time series segmentation. By smoothing features and pinpointing temporal action boundary proposals based on frame similarity, we generate initial candidate action boundaries and refine them using a semantic segment-wise similarity method. Unlike previous attempts that rely on complicated neural networks or less-relevant pseudo labels, our method directly relies on frame similarity, offering reliability and interpretability. Our method has several advantages, including not requiring any training and achieving fast and state-of-the-art performance compared to other methods. It guarantees robustness and transferability, and can be extended to real-time action segmentation in untrimmed videos.