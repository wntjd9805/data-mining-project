The paper introduces the concept of gaze estimation, which involves determining the direction and point that a person is staring at. The authors highlight the potential information that can be inferred based on the object being stared at, such as urgency or purchase intention. They discuss the existing approach of employing separate backbones to process scene and head images, as well as recent advancements in improving gaze estimation performance. However, the authors note that existing models only predict the generalized gaze area, rather than precisely predicting the location of the object being stared at. To address this limitation, the authors propose a unified framework called GaTector, which combines gaze estimation, object detection, and gaze object detection. They introduce the specific-general-specific (SGS) mechanism to extract features from scene and head images using one backbone, reducing computational burdens. They also propose a Defocus layer to assist precise object detection and an energy aggregation loss to enhance gaze heatmap accuracy. The authors evaluate GaTector on a large-scale dataset and demonstrate improved performance in gaze estimation and object detection while reducing parameters and computational costs. They also provide a solid baseline for future research in gaze object detection.