The development of deep learning has led to advancements in video applications such as video recognition, retrieval, and segmentation. However, the manual annotation required for fully-supervised methods limits the utilization of uncurated videos on the internet. Therefore, there is a need for unsupervised video representation learning. Unsupervised learning in images has achieved competitive performance, especially with contrastive self-supervised learning formulations. Similarly, attempts have been made in self-supervised video representation learning, but vanilla contrastive learning suffers from background bias. In this paper, we propose a method called FAME that mitigates background bias and enhances contrastive learning. FAME extracts dynamic areas of videos and fuses them with random backgrounds from other videos to create new action samples. Our experimental results demonstrate that FAME improves contrastive video representation learning and achieves state-of-the-art performance on benchmark datasets. In summary, our contributions include demonstrating background bias, proposing FAME as an effective augmentation method, and achieving improved performance in unsupervised video representation learning.