Abstract: Object detection in 3D perception is a crucial component in various applications such as autonomous driving and robotic navigation. While significant progress has been made using lidar or stereo sensing solutions, their high cost and complex online calibration limit their mass applications. As a more affordable alternative, monocular-based sensing solutions have garnered attention. However, due to the ill-posed 2D-3D projection, the localization accuracy of monocular 3D object detection lags behind lidar and stereo-based approaches. In this paper, we propose a novel approach called Joint Semantic and Geometric Cost Volume (MonoJSG) that leverages pixel-level visual cues to refine bounding box proposals. We enhance the traditional 2D-3D constraint by estimating the location of each pixel in the normalized object coordinate and utilize this information to build pixel-level constraints for each bounding box. We enrich the constraints further with semantic error measurements to account for variant textureless and irregular regions. By constructing a 4D cost volume based on the joint geometric and semantic error, we refine object depth and achieve superior performance compared to state-of-the-art methods on the KITTI and Waymo datasets. Our contributions include the novel joint semantic and geometric error measuring approach, the adaptive 4D cost volume for depth refinement, and the demonstrated effectiveness of our approach with real-time performance.