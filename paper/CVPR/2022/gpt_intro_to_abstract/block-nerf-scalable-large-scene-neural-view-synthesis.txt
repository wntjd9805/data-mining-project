Recent advancements in neural rendering have allowed for photo-realistic reconstruction and novel view synthesis. However, these methods have been limited to small-scale and object-centric reconstruction, and do not easily scale up to city-scale environments. Reconstructing large-scale environments has important applications in domains such as autonomous driving and aerial surveying. To address the challenges of reconstructing large-scale environments, this paper proposes a modified model called Block-NeRF that extends Neural Radiance Fields (NeRF) with appearance embeddings and learned pose refinement. Additionally, exposure conditioning is added to provide the ability to modify exposure during inference. To overcome limitations and scalability issues, the paper suggests dividing large environments into individually trained Block-NeRFs, which are dynamically combined at inference time. This approach allows for maximum flexibility, scalability to arbitrarily large environments, and the ability to update or introduce new regions without retraining the entire network. To compute target views, a subset of the Block-NeRFs are rendered and composited based on their geographic location compared to the camera. An appearance matching technique is proposed to seamlessly composite different Block-NeRFs by optimizing their appearance embeddings. The proposed method opens up possibilities for high-fidelity scene reconstruction in large-scale environments for applications such as autonomous driving simulations.