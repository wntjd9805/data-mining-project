Automatic synthesis of realistic images from text descriptions is a significant goal in artificial intelligence. However, existing methods rely heavily on large datasets of image-text pairs, which require extensive human captioning and filtering. This presents challenges for custom domains where collecting such datasets is costly and impractical. To address this, recent approaches have explored zero-shot text-to-image generation by pre-training generative models on web-scale image-text pairs. However, these models require massive amounts of data, large model sizes, and extensive training, making them inaccessible to many researchers and environmentally unsustainable. To provide affordable solutions for building text-to-image generation models with limited image-text pair data, we propose LAFITE, a generative adversarial approach based on the pre-trained CLIP model. LAFITE leverages CLIP's image-text feature alignment properties to construct pseudo image-text feature pairs and utilizes a text-to-image GAN model for efficient learning. Our contributions include: (1) LAFITE works effectively in various text-to-image generation settings, including language-free, zero-shot, and fully-supervised learning; (2) LAFITE is the first work enabling language-free training for text-to-image generation, with two novel schemes for constructing pseudo image-text feature pairs; (3) In zero-shot settings, LAFITE outperforms prior art models with less than 1% of the trainable model parameter size; (4) In fully supervised settings, LAFITE surpasses state-of-the-art methods, even outperforming models trained with full image-text pairs. Overall, LAFITE provides an affordable and efficient solution for text-to-image generation, reducing the requirements on model size, data collections, and training, while achieving impressive performance in various settings.