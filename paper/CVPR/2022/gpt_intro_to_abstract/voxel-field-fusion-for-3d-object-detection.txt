In this paper, we propose a new cross-modality framework called voxel field fusion (VFF) for 3D object detection in real-world applications. Previous approaches have struggled with the sparse nature of LiDAR point clouds, resulting in frequent miss-detections. To address this issue, previous studies have introduced image features in the cross-modality fusion process. However, maintaining cross-modality consistency proves challenging due to context deficiency, density variance, and cross-modality misalignment caused by data augmentation.In our proposed VFF framework, we apply mixed augmentation for both modalities to preprocess the data. We project augmented image features onto the voxel grid and represent them in a point-to-ray manner in the voxel field. By doing so, we ensure that representations of both modalities are well-aligned and that surrounding spatial context is replenished in the voxel field. We utilize a learnable sampler and ray-wise fusion for efficient ray construction and cross-modality fusion, drawing inspiration from recent advances in neural rendering.Our VFF framework eliminates the modality gap and provides accurate 3D context, allowing for the detection of hard cases. It also efficiently samples high-responded features from augmented images, enabling the network to construct each ray on the fly. The framework can be easily instantiated with various voxel-based backbones for 3D object detection.We conducted extensive empirical studies to evaluate the effect of each component of our VFF framework. Experimental results on the KITTI and nuScenes datasets show consistent performance improvements over various benchmarks. Our VFF framework achieves a 2.2% AP gain over strong baselines on hard cases of the KITTI test set and surpasses previous fusion-based methods by a large margin, achieving leading performance on the nuScenes test set with 68.4% mAP and 72.4% NDS.Our work contributes to the field of image-based 3D detection and cross-modality fusion. Previous image-based methods have focused on monocular or multi-view images, while our VFF framework combines the strength of images and LiDAR data. We introduce a novel point-to-ray fusion approach in the voxel field that utilizes the strengths of both modalities and ensures sufficient context. Our proposed framework extends the current state-of-the-art in 3D object detection and offers promising results in real-world applications.