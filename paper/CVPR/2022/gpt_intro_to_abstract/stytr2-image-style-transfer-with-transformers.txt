Image style transfer is a popular research topic that involves rendering a content image with the style of another image. Traditional methods for style transfer, such as stroke appearance and painting process, are computationally complex. Therefore, researchers have turned to neural style transfer based on convolutional neural networks (CNNs) for more efficient and effective results. These optimization-based methods use learned style representation to iteratively render the input content image. However, they still struggle to capture the relationship between content and style in some cases. To address this limitation, recent methods have incorporated a self-attention mechanism for improved stylization results.CNNs have a limited receptive field, which means they struggle to capture long-range dependencies without a sufficient number of layers. Increasing the depth of the network can lead to the loss of fine details and feature resolution. This can negatively impact the preservation of content structure and style display. Recent studies have shown that typical CNN-based style transfer methods exhibit a bias towards content representation, leading to significant changes in the extracted structures of the input content during the stylization process.Inspired by the success of transformer architectures in natural language processing (NLP), this work proposes a novel image Style Transfer Transformer framework called StyTr2. Unlike the original transformer, StyTr2 incorporates two transformer-based encoders to capture domain-specific information. The transformer decoder is then used to generate the output sequences of image patches progressively. Additionally, a content-aware positional encoding scheme (CAPE) is proposed to address the challenges of positional encoding in vision tasks. CAPE dynamically expands the position to accommodate different image sizes while being conditioned on the input content and invariant to image scale transformation.The contributions of this work include the development of the StyTr2 framework, which generates stylization results with well-preserved structures and details of the input content image. The proposed content-aware positional encoding scheme provides a scale-invariant solution suitable for style transfer tasks. Comprehensive experiments demonstrate that StyTr2 outperforms baseline methods and achieves outstanding results with desirable content structures and style patterns.