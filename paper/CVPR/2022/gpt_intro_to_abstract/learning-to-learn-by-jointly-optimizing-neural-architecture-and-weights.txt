This paper addresses the problem of few-shot learning by using meta-learning, which allows deep learning models to fit unseen tasks using only a few training examples. Specifically, the paper focuses on the gradient-based meta-learning method called MAML, which aims to find initialization weights for the models (meta-weights) that can quickly adapt to new tasks with only a few gradient steps. To improve the architectures used in meta-learning, the paper proposes a Neural Architecture Search (NAS) approach.The goal of the paper is to find an optimal architecture and meta-weights for a meta-learner that can quickly adapt to new tasks with few training samples. The architectures are represented as connections, and each connection is weighted based on its importance. These weighted connections, called meta-connections, make up the adaptive architecture. The training process is a co-optimization problem of the connection parameters and network weights.Previous works have explored the impact of architecture in meta-learning but have either neglected the mutual impact between architecture and meta-weights or used biased updating rules. The paper introduces a progressive connection consolidation approach to preserve the mutual impact between architecture and meta-weights. During the searching phase, the supernet is pruned layer by layer, with the connections with the highest weights consolidated first. This allows for training the matched meta-weights on these consolidated connections and maintains the mutual impact between them. Additionally, the paper proposes Connection-Adaptive Meta-Learning (CAML) to update the meta-learner in an unbiased way, improving its generalization performance on all tasks.Experimental results demonstrate that the proposed method achieves state-of-the-art performance on FC100 and Mini-Imagenet datasets with significantly less computational cost. The effectiveness and efficiency of the method are highlighted, showcasing its potential for improving few-shot learning in various settings.