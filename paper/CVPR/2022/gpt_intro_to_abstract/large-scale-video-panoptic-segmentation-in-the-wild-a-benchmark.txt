Panoptic segmentation is an important task in computer vision that involves assigning semantic labels and instance IDs to every pixel in an image. While there have been many approaches proposed for image panoptic segmentation, video panoptic segmentation (VPS) remains a challenging problem. VPS requires not only unique and consistent semantic predictions within a video but also the association of instance IDs for the same object across frames. However, existing VPS datasets are limited in scale and diversity, hindering the development of effective VPS models.To address these limitations, we introduce a new dataset called VIPSeg, which aims to enable large-scale video panoptic segmentation in diverse real-world scenarios. VIPSeg includes 3,536 videos and 84,750 frames, with pixel-level annotations for both semantic categories and instance IDs. Notably, VIPSeg is the first dataset to consider diverse scenarios and contains a wide range of scenes and object categories. The dataset can be used not only for video panoptic segmentation but also for other video-related tasks such as video object segmentation and video semantic segmentation.However, annotating such a large-scale video panoptic segmentation dataset is challenging and expensive. To overcome this, we propose a Sparse-to-Dense Interactive Annotation strategy that combines human and computer collaboration. This strategy involves annotating instances at a sparse frame rate using a tracking model and manual correction, followed by refining the instance masks using a video object segmentation model.We evaluate existing video panoptic segmentation models on the VIPSeg dataset and find that most of these models rely on iterative inference, which can be inefficient for real-world applications with long videos. To address this, we propose a clip-based model that divides a video into non-overlapping clips and generates predictions for each clip individually. This clip-based approach enables parallel processing and improves efficiency in real applications. We employ the clip-based model to evaluate and analyze the VIPSeg dataset.Overall, the VIPSeg dataset and our proposed annotation strategy and clip-based model provide valuable resources for advancing research on video panoptic segmentation. These contributions address the limitations of existing VPS datasets and models, paving the way for more effective and efficient video understanding and analysis in various real-world applications.