Instance segmentation, a crucial task in computer vision, involves predicting pixels for individual objects in an image. While there have been significant improvements in instance segmentation performance through the use of pixel-level annotations, the process of obtaining these annotations is time-consuming and limits the development of instance segmentation for large numbers of novel categories. In contrast, box-level annotations are easier to obtain, leading to the emergence of partially-supervised instance segmentation methods that leverage both box-level and mask-level annotations. However, existing methods primarily focus on learning a class-agnostic mask segmentation model solely on base categories, neglecting the potential of training data from novel categories.To address this limitation, we propose ContrastMask, a novel partially-supervised instance segmentation method that learns a class-agnostic mask segmentation model on both base and novel categories within a unified pixel-level contrastive learning framework. Our approach fully exploits training data from all categories, allowing novel category data to contribute to the optimization of the segmentation model. We introduce a query-sharing pixel-level contrastive loss that utilizes region priors obtained from annotated masks of base categories or pseudo masks of novel categories. These region priors guide the separation of foreground and background and serve as shared queries, positive keys, and negative keys. Specifically, we establish a foreground query and a background query by averaging features within and outside the mask regions, respectively. Proper keys are selected using a special sampling strategy. The introduced loss pulls keys inside/outside the mask regions toward the foreground/background shared query and contrast them against keys outside/inside the mask regions. The learned features are then fused into a class-agnostic mask head for mask segmentation.ContrastMask offers several advantages over previous methods. Firstly, it fully exploits training data from all categories, enabling the optimization of the segmentation model using novel category data. Secondly, it effectively transfers the segmentation capability learned on base categories to novel categories through the unified pixel-level contrastive learning framework, thus enhancing feature discrimination between foreground and background for both base and novel categories. Experimental results on the COCO dataset demonstrate that ContrastMask outperforms previous state-of-the-art partially-supervised instance segmentation methods by large margins, achieving a 39.8 mAP for mask segmentation on novel categories.