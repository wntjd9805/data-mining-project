The extraction of 3D geometric information from images has long been a challenge in computer vision. Understanding 3D geometry is crucial for comprehending the physical and semantic structure of objects and scenes, yet achieving this understanding remains difficult. Previous work in this area has focused on deriving geometric understanding from multiple views or leveraging known geometry to supervise the learning of geometry from single views. In this paper, we propose a more ambitious approach: deriving 3D understanding in a generative model from single views of objects without explicit geometric information like depth or point clouds.One popular method, Neural Radiance Field (NeRF), has shown promise in geometry-based rendering but requires supervision from multiple viewpoints to avoid collapsing representations of scenes. However, acquiring multiple-view data is challenging. To address this, architectures combining NeRF and Generative Adversarial Networks (GANs) have been developed to enforce multi-view consistency without the need for multi-view training data.Surprisingly, our research demonstrates that training NeRF models without adversarial supervision is possible using only single views of objects, as long as a shared generative model is trained and approximate camera poses are provided. We align images in the dataset to a canonical pose using predicted 2D landmarks, which helps determine the appropriate view from which to render the radiance field. Our approach employs an auto-decoder framework to improve generalization and train separate models for foreground and background objects.Furthermore, we encourage our model to represent shapes as solid surfaces with clear outside-to-inside transitions. This improves the quality of predicted shapes. Notably, our method does not require rendering of entire images or patches during training. Instead, we train our models to reconstruct images from datasets while optimizing latent representations on a pixel-level basis. This allows our method to be trained with arbitrary image sizes without increasing memory requirements, unlike existing methods that utilize GANs.In summary, we propose a method for learning 3D reconstruction of object categories from single-view images that decouples training complexity from image resolution. We demonstrate that single views are sufficient for learning high-quality predictions of geometry without explicit geometric supervision. Additionally, our method surpasses adversarial methods in representing object appearance by reconstructing held-out images and novel views.