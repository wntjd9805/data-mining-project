The introduction of this computer science paper focuses on the importance of self-supervised learning in driving advancements in computer vision, natural language processing, and multi-modal representation learning. It highlights the limitations of current multi-modal datasets which lack diversity and adequately represent various modalities such as text, image, video, audio, and table data. The authors introduce the M5Product dataset, which is one of the largest and most diverse multi-modal product datasets to date, containing over 6 million samples from 6,232 categories. They propose a framework called SCALE (Self-harmonized ContrAstive LEarning) to address the limitations of existing methods in modality fusion and handling modality noise. The framework incorporates a self-harmonized strategy to adaptively integrate complementary modality information. The contributions of the paper include providing the M5Product dataset, demonstrating the effectiveness of the SCALE framework compared to baseline methods, and making interesting observations about the importance of dataset scale and diversity in multi-modal pre-training models for E-commerce.