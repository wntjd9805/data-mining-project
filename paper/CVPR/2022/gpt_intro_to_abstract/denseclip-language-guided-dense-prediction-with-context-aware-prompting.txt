The "pre-training + fine-tuning" paradigm has significantly advanced the field of computer vision, improving tasks such as image classification, object detection, semantic segmentation, and action recognition. However, dense prediction tasks require high annotation and computation costs, making pre-training even more crucial. This paper introduces DenseCLIP, a novel approach that leverages the knowledge learned from image-text contrastive learning to enhance dense prediction models. By converting the image-text matching problem in CLIP to a pixel-text matching problem and utilizing pixel-text score maps, DenseCLIP explicitly guides the learning of dense prediction models. Additionally, contextual information from images is employed to prompt the language model, optimizing the text embeddings and further improving performance. Experimental results demonstrate that DenseCLIP outperforms conventional ImageNet pre-trained models on tasks like semantic segmentation, object detection, and instance segmentation. The method can be easily integrated as a plug-and-play module into existing frameworks, providing substantial improvements in accuracy with minimal computational overhead. Moreover, DenseCLIP can be applied to various backbone models, making it a promising and generic approach for enhancing dense prediction models using pre-trained language models.