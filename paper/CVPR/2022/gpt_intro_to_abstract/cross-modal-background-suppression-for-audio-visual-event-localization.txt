Event location and action recognition in videos have become crucial for understanding and analyzing video content. While most methods rely on processing optical flow and RGB features, audio can also provide valuable clues. The combination of audio and visual modalities is essential for a comprehensive understanding of video content. The audio-visual event (AVE) localization task introduced by Tian et al. aims to determine the presence of an event and localize its boundary in the temporal dimension when it is both audible and visible. Challenges in the AVE task include merging complementary audio and visual features, dealing with sudden noise and complex backgrounds in videos, and handling unsynchronized audio and visual information. Early models attempted to solve these challenges by independently processing each modality or aligning and fusing audio and visual information. However, these methods still face errors in event category detection and inaccurate temporal event localization. In this paper, we propose a novel approach that addresses these challenges by considering cross-modal background suppression. This approach effectively mitigates the impact of sudden noise and unsynchronized audio and visual information, making it a promising direction for AVE localization.