The continuous learning of new tasks while maintaining knowledge of previously learned ones is crucial in real-world applications. Class-incremental learning (CIL), which aims to recognize new classes without forgetting old ones, has gained significant attention. However, methods that rely on joint re-training of old and new class samples in each phase are time-consuming and may not have access to all old class samples. Alternate approaches such as fine-tuning the network using only new class samples often result in catastrophic forgetting.To address this issue, recent CIL methods have introduced distillation losses and calibration techniques to maintain past knowledge and correct biases caused by class imbalances. However, most existing methods assume a certain number of exemplars can be stored in memory, which is not always practical.This paper focuses on non-exemplar class-incremental learning (NECIL), where old class samples cannot be preserved. We propose a self-sustaining representation expansion scheme that learns a structure-cyclic representation. This scheme includes a dynamic structure reorganization strategy to preserve old class space while accommodating new classes, and a main-branch distillation approach to maintain discrimination between old and new features. We also introduce a prototype selection mechanism to reduce confusion between similar classes.Experimental results on CIFAR-100, TinyImageNet, and ImageNet-Subset benchmarks demonstrate the effectiveness of our approach compared to state-of-the-art methods. The proposed NECIL scheme achieves a cyclically expanding optimization process and improves both forward transfer and feature discrimination.