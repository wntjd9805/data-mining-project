Explanations for deep neural networks (DNNs) remain a challenging problem in computer science. While DNNs are successful in many tasks, providing explanations that summarize the internal model computations in a human-interpretable manner is still an open research problem. Previous work focused on improving the visual quality of explanations but often sacrificed model-faithfulness. In this paper, we propose the B-cos transform as a replacement for linear transforms in DNNs to inherently provide high-quality explanations. The B-cos transform is designed as an input-dependent linear transform, and any sequence of these transforms induces a single linear transform that faithfully summarizes the entire sequence. By promoting weight-input alignment during optimization, the B-cos transform aligns model weights with task-relevant input patterns, resulting in clear interpretations of the induced linear transform. We make several contributions: (1) Introducing the B-cos transform, which highlights task-relevant patterns in the input for improved interpretability. (2) Designing the B-cos transform to allow explanations for not only output neurons but also neurons from intermediate layers. (3) Demonstrating that a plain B-cos convolutional neural network achieves competitive performance on CIFAR10 without additional non-linearities or regularization. The parameter B provides control over weight alignment and interpretability. (4) Showing that the B-cos transform can be integrated into commonly used DNNs while maintaining performance and outperforming other explanation methods in interpretability.