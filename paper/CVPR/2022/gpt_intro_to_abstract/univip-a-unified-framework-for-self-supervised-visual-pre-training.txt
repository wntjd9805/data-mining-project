This paper introduces a unified self-supervised pre-training framework called UniVIP for learning visual representations from both single-centric-object and non-iconic datasets. The paper addresses the challenge of semantic inconsistency in random views of non-iconic images by exploiting the co-occurrence relationship and semantic affinity between multiple instances in a single natural image. UniVIP generates candidate instances using the Selective Search unsupervised instance proposal method and creates two scene views with overlapping regions to ensure global similarity. The generated instances are grouped to approximate the semantics of the corresponding scene views, enabling the network to learn a variety of instances. The discrimination of different instances in the scene is formulated as an optimal matching problem using the optimal transport algorithm. Experimental results on single-centric-object and non-iconic datasets demonstrate that UniVIP outperforms previous state-of-the-art methods in various downstream tasks, including image classification, semi-supervised learning, object detection, and segmentation. UniVIP achieves impressive results, surpassing the supervised counterparts and even outperforming popular self-supervised object detection methods. The proposed framework effectively overcomes the semantic inconsistency of random views in non-iconic images, leveraging the similarity of scene-scene, the correlation of scene-instance, and the discrimination of instance-instance to improve model performance and generalization ability. Overall, the contributions of this paper include the development of a unified self-supervised representations learning framework, the simultaneous leveraging of various relationships in natural images, and extensive experimental validation of the effectiveness of UniVIP.