The recent Vision Transformer (ViT) models have achieved great success in computer vision tasks by partitioning images into patches and utilizing global self-attention for feature update. However, the quadratic memory cost of self-attention motivates the use of downsampling strategies in Transformer models, which may lead to information loss and difficulty in capturing small objects. Additionally, existing models lack the ability to capture features at different scales simultaneously, limiting their performance in complex scenes with objects of varying sizes. To address these issues, we propose a novel self-attention scheme called Shunted Self-Attention (SSA) that allows different attention heads within the same layer to focus on different granularity levels. This enables our model, called Shunted Transformer, to efficiently capture multi-scale objects while preserving fine-grained details. We demonstrate the effectiveness of our model in comparison to existing methods, achieving better performance on tasks such as image classification, object detection, and segmentation. Our Shunted Transformer outperforms state-of-the-art models while having a smaller model size. Overall, our work contributes SSA as a generic self-attention scheme and presents the Shunted Transformer as an efficient and effective model for multi-scale object recognition in computer vision.