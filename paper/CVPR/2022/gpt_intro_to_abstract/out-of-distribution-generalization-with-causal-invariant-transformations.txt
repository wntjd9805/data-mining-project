The introduction of this computer science paper discusses the problem of learning a model that can generalize well on unseen target distributions when data in different domains or environments are heterogeneous. The authors highlight the use of causality as a powerful tool to address this out-of-distribution (OOD) generalization problem. Existing works in this area have made assumptions about the causal mechanism or attempted to recover the causal feature from the data, but these approaches may be restrictive or fail to capture the invariance in practice. In this paper, the authors propose a new approach that does not rely on explicit recovery of the causal feature but instead learns a model that takes advantage of the invariant properties. They introduce the concept of causal invariant transformations (CITs), which are transformations that modify the input data but do not change their causal feature. The authors theoretically prove that if all the CITs are known, it is feasible to learn a model with OOD generalization capability using only single domain data. Additionally, they show that for OOD generalization, it suffices to know only an appropriate subset of CITs, referred to as the causal essential set. They propose to regularize training with the discrepancy between the model outputs of the original data and their transformed versions from the CITs in the causal essential set to enhance OOD generalization. The authors validate their findings through experiments on synthetic and real-world benchmark datasets, demonstrating the effectiveness of their proposed algorithm in terms of OOD performance.