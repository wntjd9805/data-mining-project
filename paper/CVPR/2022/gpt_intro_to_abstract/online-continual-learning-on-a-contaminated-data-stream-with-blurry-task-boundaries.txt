Continual learning (CL) is a practical scenario in computer science that involves learning from a continuous and online stream of annotated data. However, CL methods often face challenges such as catastrophic interference and resistance to learning new information due to continuously changing data distribution. Additionally, in real-world applications like e-commerce, the annotated labels are often unreliable due to less controlled data curation processes. Existing CL studies have largely assumed that training data has no annotation error, which limits the practical usability of CL methods in real-world applications.In recent work, a CL setup under a data stream with less reliable labels was proposed, but it assumed a disjoint class incremental scenario, which might not be practical for real-world applications. To address the challenges posed by continuously changing data distributions and falsely labeled data, this paper presents a novel CL task of online continual learning on a contaminated data stream with blurry task boundaries, which is a more realistic and practical scenario. Episodic memory-based CL methods rely on sample selection strategies, and diversity-aware sample selection policies have been shown to be effective in online blurry CL scenarios. However, on a contaminated data stream, diversity-based memory construction may lead to including examples with corrupted labels in the episodic memory, resulting in poor performance due to overfitting. To address this issue, this paper proposes a unified framework called PuriDivER (Purity and Diversity aware Episode Replay), which integrates memory sampling policy with robust learning using episodic memory and semi-supervised learning with unreliable data.The framework aims to construct a robust episodic memory that preserves a set of training examples that are both diverse and pure. The challenge lies in balancing purity and diversity, as clean examples generally have smaller losses than noise examples in deep neural networks. To address this trade-off, a score function is defined to promote label purity while also promoting diversity by optimizing the sample distribution to be similar to the one with noisy examples. Additionally, a robust learning scheme is incorporated to further promote purity and diversity when using samples from the episodic memory.Experimental evaluations on CIFAR-10/100, WebVision, and Food-101N datasets demonstrate that the proposed method outperforms combinations of prior arts in addressing the challenges posed by the combined scenarios. The contributions of this paper include proposing the first online blurry CL set-up with a contaminated data stream, establishing a baseline for this scenario, introducing a unified framework for considering diversity and purity in memory updates and usage, and proposing an adaptive balancing scheme for addressing various noise ratios in CL.