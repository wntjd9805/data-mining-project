Image animation, which transfers motion from a driving video to a static object in a source image, has various applications in video conferencing, movie effects, and entertainment videos. Previous research in motion transfer has relied on labeled data, making it limited to specific objects like faces and human bodies. Unsupervised methods have been proposed, but they still face challenges in dealing with large pose gaps between objects in the training process and inadequate inpainting capability. To address these issues, we introduce a new end-to-end unsupervised motion transfer framework that uses the thin-plate spline (TPS) transformation to approximate motion and combines it with affine background transformation. We also predict occlusion masks for feature maps to enable efficient feature fusion. Our proposed framework improves motion estimation accuracy and inpainting capability, outperforming previous unsupervised methods on various datasets.