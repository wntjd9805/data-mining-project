Graph matching (GM) is a significant research topic in the field of computer science, particularly in graph-based applications like vision and pattern recognition. The matching of visual graphs has been extensively studied, especially with the advancements in deep neural networks and GM techniques. However, deep neural networks are found to be vulnerable to small input perturbations that are imperceptible to humans. This vulnerability poses a challenge for GM tasks, as attackers can perturb graph structures and attributes to disrupt graph learning tasks such as node classification, community detection, and link prediction. Despite the growing interest in the vulnerability of deep GM for vision, there is limited research on the specific case of matching image keypoints. In this paper, we address this gap by proposing an effective adversarial attack on GM that simultaneously perturbs images and their hidden graphs. Our attack focuses on the perturbation of keypoint locality, as the location of keypoints plays a crucial role in the feature extraction process and the determination of graph structure in deep GM pipelines. We demonstrate the effectiveness of our attack through experimental evaluations on various deep GM baselines. To mitigate the impact of adversarial attacks, we propose a defense mechanism called Appearance and Structure Aware Robust Graph Matching (ASAR-GM). Our defense strategy leverages insights from the attack patterns and utilizes a regularization term called Appearance Aware Regularizer (AAR) to enhance the robustness of the model. The experimental results show that ASAR-GM achieves better clean accuracy and robustness compared to defense baselines. Additionally, our locality attack serves as a form of data augmentation, improving the generalization ability of deep GM models. Overall, our work contributes to the understanding of the vulnerability of deep GM for vision and provides effective attack and defense mechanisms for enhancing the robustness of GM models.