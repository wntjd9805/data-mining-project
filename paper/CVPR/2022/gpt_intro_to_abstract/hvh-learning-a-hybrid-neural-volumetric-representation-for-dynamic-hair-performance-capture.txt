This paper addresses the challenge of capturing and rendering realistic human hair in virtual avatars, which is notoriously difficult due to its complex physical interaction and interaction with light. While previous work has made progress in modeling facial features and skin, they struggle with representing hair geometry effectively. Recent volumetric representations have shown promise in 3D scene acquisition and rendering, but they suffer from high memory complexity, especially for high-resolution hair. This paper proposes a hybrid neural volumetric representation that focuses resolution on relevant regions of the 3D space, allowing for detailed modeling of hair appearance. To address the limitations of implicit volumetric representations, the authors introduce a hierarchical structure with coarse and fine-level radiance functions and utilize importance resampling to boost sample resolution. The paper also proposes a spatio-temporal modeling approach for dynamic upper head and hair, including hair strand tracking and the attachment of volumetric primitives. The model incorporates multi-view optical flow and enforces temporal consistency to capture hair dynamics accurately. The contributions of this work include the hybrid volumetric representation, a hair tracking algorithm, a volumetric ray marching algorithm, and a hair-specific volumetric decoder. Overall, this research aims to enhance the realism and believability of virtual avatars by improving the representation and rendering of human hair.