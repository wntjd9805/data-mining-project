Point clouds captured under different settings can exhibit prominent variations that cause performance drop when neural networks are tested on a domain that is different from the training ones. This can be troublesome if the network cannot be fine-tuned due to time constraints or limited computational budget, or if labels needed for fine-tuning on the test domain are simply unavailable due to high annotation cost. This situation is known as unsupervised domain adaptation (UDA) problem. Successful domain adaptation requires aligning the two domains statistically and semantically, so that the shared mapping to the output space can operate on the same ground. Existing UDA methods on point clouds mainly rely on adversarial training or self-supervised learning to align the domains, but both approaches have limitations. Adversarial training can result in negative adaptation gains and may distort geometric information, while self-supervised learning can have limited control over the invariants being learned. In this paper, we propose a method that leverages implicit function learning as a self-supervised task for unsupervised domain adaptation on point clouds. We introduce an adaptive unsigned distance field that enables training the implicits for arbitrary point clouds, especially for sparse and irregularly sampled ones. We also present effective training strategies to make our method robust to diverse artifacts exhibited in the point clouds. Our experiments on two major point cloud datasets, PointDA-10 and GraspNet, demonstrate the effectiveness of our method and show state-of-the-art performance.