Image editing plays a crucial role in various applications, including retouching, style transfer, language-guided editing, and colorization. However, current research focuses on these tasks independently and lacks a unified approach. Customized models for one specific task are difficult to extend to related tasks, and models trained on specific datasets struggle to generalize to out-of-domain samples. Pretrained architectures for vision and vision+language tasks have shown the potential for generalization and knowledge transfer. This paper aims to explore the possibility of a unified pretraining task or network architecture for image editing. Existing works like StyleGAN have successfully generated realistic images for closed-domain categories, but their generalizability to open-domain user photos remains unproven. The paper focuses on applying artistic styles to photos while preserving their original content, structure, and texture. To achieve this, the authors propose a pretraining task that transforms before-edited images into after-edited images with artistic styles controlled by random noise vectors. A large-scale dataset comprising before-and-after photos is collected, and a novel encoder-decoder network structure is proposed. The properties of the new latent space W are analyzed, and it is found that the W space emphasizes editing styles rather than image content. The inverted latent codes are useful for generation and recognition tasks. The pretrained model is tested on various open-domain image editing tasks, including language-guided image editing and free-form editing using CLIP. The contributions of the paper are: proposing a new pretraining task and network architecture, demonstrating the utility of the W space for editing styles, and showcasing the improved performance of the pretrained model on various downstream tasks.