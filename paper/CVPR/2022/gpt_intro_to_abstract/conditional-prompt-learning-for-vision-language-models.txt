Large-scale vision-language pre-training has shown remarkable performance in zero-shot image recognition, offering potential for learning open-world visual concepts. Unlike traditional supervised learning, which focuses on closed-set visual concepts, vision-language models utilize classification weights generated by a parameterized text encoder through prompting. This approach allows for open-set visual concepts and has proven effective in learning transferable representations. However, adapting these models to downstream datasets with fine-tuning is impractical due to their enormous size. Prompt engineering has been suggested as a solution, but it is time-consuming and inefficient. To automate prompt engineering, this paper proposes a novel concept called conditional prompt learning. The idea is to make a prompt conditioned on each input instance instead of being fixed once learned. The approach, called CoCoOp, achieves significant improvements over existing methods in terms of generalizability and performance on various visual recognition tasks. The experiments demonstrate the effectiveness of instance-conditional prompts in achieving better overall performance, reducing the gap between manual and learning-based prompts, and achieving stronger domain generalization. The research provides insights into prompt learning and offers potential for future research in generalizable and transferable prompt learning.