Few-Shot Learning (FSL) aims to enable machines to learn new concepts with limited experience. Typically, FSL assumes that the source and target data are in the same domain but belong to different classes. However, there is a need for FSL to generalize to different target domains, leading to the emergence of Cross-Domain Few-Shot Learning (CD-FSL). CD-FSL involves target data with different labels and domains than the source data. While many promising FSL methods have been developed, they often fail to perform well in CD-FSL due to the significant visual domain gap between the source and target data. Recent studies have attempted to improve model transferability by learning a generalizable feature extractor, but they do not adequately model the visual and label characteristics of the target domain. Other methods rely on data augmentation techniques to fine-tune the source domain feature representation but do not consider quantifying the cross-domain relevance of the pre-trained source domain representation. In this paper, we propose a new approach that treats CD-FSL as an image retrieval task and leverages the context of the target domain retrieval task to optimize model adaptation. Our method incorporates a k-reciprocal neighbor discovery and encoding process to calibrate the pairwise distances between unlabelled query images and their likely matches. It is orthogonal and complementary to existing generalizable model learning methods. We also propose a task-adaptive subspace mapping technique that minimizes transferring task-irrelevant representational information from the source domain, addressing the nonlinearity between the two domains. Through extensive experiments on eight target domains, we demonstrate that our proposed methods, Ranking Distance Calibration (RDC) and RDC with Fine-Tuning (RDC-FT), outperform state-of-the-art CD-FSL models in terms of classification performance.