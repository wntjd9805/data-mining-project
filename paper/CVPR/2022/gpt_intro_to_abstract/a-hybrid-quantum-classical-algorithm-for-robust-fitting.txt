Imperfections in computer vision sensing and processing often result in data containing outliers, necessitating the robustness of vision pipelines to mitigate their negative impact. In 3D vision, fitting a geometric model onto noisy and outlier-prone measurements is a fundamental task. The consensus maximisation framework is commonly employed for this purpose. However, existing algorithms, such as random sampling heuristics like RANSAC, cannot provide optimality guarantees or tight error bounds. Deterministic approximate algorithms have been proposed but lack error bounds, while learning-based methods do not offer optimality guarantees or generalization capabilities. In this paper, we propose a new approach that leverages quantum computing for consensus maximisation. Our core contribution is an algorithm that uses quantum annealing to solve a sequence of integer programs, providing either an optimal solution or a suboptimal solution with a known error bound. We present results using the D-Wave Advantage quantum computer, demonstrating the potential of quantum computing in computer vision applications. Although our technique does not currently outperform existing algorithms due to current quantum technology limitations, it paves the way for future efforts in this area.