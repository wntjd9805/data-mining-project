Video super-resolution (VSR) is a task that aims to generate high-resolution (HR) videos from low-resolution (LR) input videos, with the objective of recovering high frequency details in the frames. With the increasing demand for online video streaming services and in the movie industry, VSR has attracted significant attention. However, there are two major challenges in VSR.The first challenge lies in the dynamic nature of videos. To ensure temporal consistency and improve visual fidelity, previous methods have attempted to fuse information from multiple neighbor frames. Achieving alignment between neighbor frames before fusion is crucial but difficult, especially in videos with large motions. Existing VSR methods suffer from fusing misaligned frames, limiting their applicability in real-world videos such as sports videos and entertainment videos.The second challenge stems from the irreversible loss of high-frequency detail and the lack of useful information in low-resolution videos. While single image super-resolution (SISR) methods have explored visual reconstruction from LR images, applying these methods directly to each frame does not guarantee temporal consistency. Most VSR methods aim to fuse information from neighbor frames, resulting in superior results compared to SISR methods. However, the limitations of mining useful information from neighbor frames, particularly in videos with large motions, cause VSR to degrade to single image super-resolution.To address these challenges, we propose a Memory-Augmented Non-local Attention (MANA) framework for VSR. The MANA framework takes consecutive LR video frames as inputs and generates the HR version of the temporal center frame by incorporating information from its neighbor frames. Our framework consists of two novel modules designed to address the alignment and lack of information challenges in VSR.To tackle the frame-alignment challenge, we introduce the Cross-Frame Non-local Attention module, which fuses neighbor frames without explicit alignment. Unlike conventional non-local attention, this module employs a trainable Gaussian map to weight the correlations between pixels, aiding in balancing information sources and reducing mistaken correspondences. This allows our method to bypass the error-prone frame alignment process, leading to improved results in videos with large motions.To address the challenge of limited information from neighbor frames, we introduce a Memory-Augmented Attention module that leverages a 2D memory bank. This memory bank summarizes representative local details from the entire training set and serves as an external reference for super-resolving the current video frame. Our approach is the first to incorporate information beyond the current video through the memory bank mechanism, enabling the recovery of missing details in LR videos.