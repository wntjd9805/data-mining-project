This paper introduces a new method called Neural Point Light Fields for learning implicit volumetric scene representations. The method combines a coordinate-based neural network with a conventional volumetric rendering approach to predict density and radiance. Unlike traditional volumetric approaches, Neural Point Light Fields represent the volume as a learned function, which improves memory efficiency and differentiability. However, the method still requires volumetric sampling during training. To address this challenge, the paper proposes encoding a light field on a point cloud using lidar data as input. By formulating the light field based on the ray direction and a one-dimensional index, the method can evaluate a single radiance prediction per ray. The paper demonstrates the effectiveness of the proposed method on a large-scale automotive driving dataset and shows improved view synthesis quality. The contributions of this paper include introducing Neural Point Light Fields, lifting the restrictions of volumetric scene representations, and validating the method on novel video synthesis tasks. The code and trained models are available on the authors' website.