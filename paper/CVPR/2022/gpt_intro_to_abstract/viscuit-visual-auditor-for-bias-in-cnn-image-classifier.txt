Data classification algorithms are widely used in various practical applications, but they often suffer from biases and disparities in performance across different subgroups. This can lead to fairness violations and societal problems. Existing methods for mitigating biases in image classifiers based on convolutional neural networks (CNN) require manual specification of attributes, which can result in missing less sensitive attributes that can still contribute to biases. In this paper, we propose VISCUIT, an interactive visualization system that reveals and explains the biases in a CNN image classifier without pre-determining which attributes to inspect. VISCUIT provides visual summarization of underperforming subgroups and highlights the biases in the classifier. It also attributes the biases to the activation of specific neurons in the CNN and provides visual explanations of why the classifier underperforms on certain subgroups. VISCUIT is an open-source, web-based tool that can be easily accessed and extended to other model architectures and datasets.