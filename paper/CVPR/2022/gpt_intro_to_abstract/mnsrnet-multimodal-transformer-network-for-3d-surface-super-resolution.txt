This paper addresses the practical difficulty of acquiring high-quality 3D surface data for 3D vision-based applications. Most current 3D acquisition devices do not provide the desired quality, necessitating the development of low-cost computer vision methods to enhance the acquisition quality. The existing methods for improving the quality of 3D surface data can be classified into voxel-based, point cloud-based, and mesh-based methods. However, these methods have limitations in terms of equipment requirements, computation, and achieving dense and high-quality results. Additionally, existing methods that enhance the surface quality in 2D domain only explore a single modality, lacking the utilization of multimodal attributes of 3D objects. To address these challenges, the authors propose a multimodal transformer network for 3D surface super-resolution. The network considers the texture, depth, and normal modalities of the low-resolution 3D object surface. The texture and depth modalities are aligned with the normal modality using a transformer network, and the aligned features are fused into the main super-resolution backbone network. The enhanced normal map is used to reconstruct a fine-grained 3D object surface. The contributions of this work include the development of a multimodal-driven surface super-resolution network that fuses texture and depth modalities to enhance 3D object surfaces in 2D domain. The authors also propose a cross-modality transformer alignment module to align auxiliary modality information and a cross-modality affine fusion module for feature fusion. In addition, a new photometric stereo dataset is established to address the lack of multimodality training data. Experimental results demonstrate the superior performance of the proposed method compared to existing methods.