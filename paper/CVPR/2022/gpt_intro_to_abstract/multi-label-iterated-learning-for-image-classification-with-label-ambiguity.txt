Large-scale datasets with human-annotated labels have played a crucial role in the advancement of artificial perception systems based on neural networks. The improvement in performance on ImageNet has led to significant progress in various tasks and domains. However, these datasets and models often simplify the complex reality by projecting a single label onto each image, despite the presence of multiple labels. This simplification hinders model performance on real-world images that contain multiple objects.There is a growing recognition of the limitations of single-labeled datasets as a form of weak supervision, and researchers have been increasingly interested in exploring the boundaries of these benchmarks. Previous studies have highlighted the problem of label ambiguity in ImageNet and proposed multi-label evaluation sets to address the issue. They have identified softmax cross-entropy training as a major factor contributing to low multi-label performance and suggested alternative approaches, such as using sigmoid activations and treating the outputs as binary classifiers.To obtain a more comprehensive description of images from weakly-supervised or semi-supervised data, several methods leverage a noisy signal, such as pseudo-labels or textual descriptions from the web. In this work, we draw inspiration from the process of language emergence in cognitive science and propose a similar approach for building richer representations of images from weak or noisy supervisory signals. We introduce multi-label iterated learning (MILe) to learn to predict multi-label representations from single-labeled training data. This is achieved by replacing the softmax output with a hard multi-label binary prediction and transmitting these predictions through successive model generations with limited training iterations.Our experiments demonstrate that MILe addresses the label ambiguity problem and improves the F1 score of supervised and self-supervised models on the ImageNet ReaL multi-label validation set. We also show that iterated learning increases robustness to label noise and spurious correlations when evaluated on the WebVision dataset. Additionally, our approach proves effective in continual learning scenarios where newly introduced labels co-occur with known labels. Our contributions include the proposal of the MILe algorithm, showcasing its benefits in various setups, and providing insights on the predictions made by models trained with iterated learning.