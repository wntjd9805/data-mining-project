Image inpainting, or image completion, is the task of filling in missing regions within an image in a way that is visually coherent and semantically reasonable. The current State-of-the-Art approaches in inpainting rely on generative models such as GANs or autoregressive modeling. However, these methods often struggle to effectively handle various forms of masks, as they are typically trained on a specific mask distribution.In this paper, we propose an alternative approach for inpainting using Denoising Diffusion Probabilistic Models (DDPM). DDPM is a promising paradigm for generative modeling that has been shown to outperform GAN-based methods for image synthesis. Unlike existing inpainting methods that are trained with specific mask conditions, our approach leverages an unconditionally trained DDPM and conditions the generation process by sampling from the given pixels during the reverse diffusion iterations. By not specifically training for the inpainting task, our model exhibits two significant advantages. First, it allows for generalization to any mask during inference, enabling better adaptability to novel mask types. Second, it benefits from the powerful image synthesis capabilities of DDPM, resulting in more semantically meaningful inpainted regions. To further improve the semantic correctness of the inpainted images, we introduce an improved denoising strategy called RePaint. Instead of slowing down the diffusion process, our approach goes forward and backward in diffusion time, effectively harmonizing the generated image information and leading to more coherent and visually pleasing inpainted results.We evaluate our method on the CelebA-HQ and ImageNet datasets, comparing it with other State-of-the-Art inpainting approaches. Our experimental results demonstrate that our approach generalizes better and produces inpainted regions that are more semantically meaningful.