Weakly supervised semantic segmentation (WSSS) is a task that aims to generate accurate segmentation labels using weak supervision, instead of relying on pixel-level ground-truth annotations. Current WSSS methods typically use Class Activation Mapping (CAM) to extract object localization maps from Convolutional Neural Networks (CNNs). However, these methods still have limited performance in terms of completeness and accuracy. The Vision Transformer (ViT), a transformer model designed for computer vision tasks, has achieved significant breakthroughs in image recognition. ViT splits images into patches and transforms them into a sequence of vectors, using a class token to aggregate information from the patches. Although recent transformer-based approaches have removed the class token, this study highlights its importance in weakly supervised semantic segmentation.A recent work called DINO discovered that self-supervised ViT features contain explicit information about semantic segmentation, and attention maps of the class token can reveal the scene layout. However, it remains unclear how to associate a specific head in the transformer attention with a correct semantic class. Existing transformer-based methods suffer from using only one class token, making it challenging to accurately localize different objects in an image. This limitation arises from capturing context information from other object categories and the background, resulting in non-discriminative and noisy object localization.To address these challenges, this paper proposes a Multi-class Token Transformer (MCTformer) that employs multiple class-specific tokens to exploit class-specific transformer attention. A class-aware training strategy is employed to ensure that each class token learns discriminative representations for specific object classes. Class scores generated from the output class tokens are supervised by ground-truth class labels, establishing a strong connection between each class token and its corresponding class label. This enables the learned class-to-patch attention of different classes to serve as class-specific localization maps.Additionally, the proposed MCTformer leverages the patch-to-patch transformer attention as a patch-level pairwise affinity, which refines the class-specific transformer attentions and improves localization performance. The method also complements the CAM mechanism, resulting in high-quality object localization maps.Experimental results demonstrate that the proposed method achieves state-of-the-art performance on the PASCAL VOC and MS COCO datasets, with mean Intersection over Union (mIoU) scores of 71.6% and 42.0%, respectively. This approach represents a significant advancement in weakly supervised semantic segmentation by generating high-quality class-specific multi-label localization maps.