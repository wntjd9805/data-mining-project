Visual question answering (VQA) is the task of generating a natural language answer to a question about an image. While most VQA services focus solely on providing the answer, our work aims to include the region in the image that supports the answer, which we refer to as answer grounding. This addition of answer groundings has several potential applications. Firstly, it allows for the assessment of whether a VQA model is reasoning based on the correct visual evidence, which is valuable for explanation and model debugging. Additionally, answer groundings enable the segmentation of relevant content from the background, which can be useful for obfuscation to protect privacy. Furthermore, users with low vision can benefit from a service that magnifies the relevant visual evidence, aiding their ability to quickly find desired information. This is particularly valuable as VQA services can sometimes provide insufficient answers due to "reporting bias" where humans describe what they find interesting without truly understanding the user's intent. Existing datasets addressing answer grounding have been created using contrived scenarios, which may not accurately represent authentic VQA use cases and thus limit algorithm performance in real-world scenarios. To address this, we introduce the first answer grounding dataset derived from authentic use cases, specifically blind individuals who provided both image and question. We collected answer groundings for approximately 10,000 image-question pairs and analyzed their characteristics in comparison to existing datasets. We evaluated state-of-the-art VQA and answer grounding models on our dataset, highlighting the challenges posed, such as smaller answer groundings, higher quality images, and visual questions requiring text recognition skills. This work serves as a foundation for developing models that can handle a wider range of challenges in real-world VQA settings. Furthermore, we have organized a dataset challenge and provided a public evaluation server and leaderboard to encourage progress in addressing these challenges.