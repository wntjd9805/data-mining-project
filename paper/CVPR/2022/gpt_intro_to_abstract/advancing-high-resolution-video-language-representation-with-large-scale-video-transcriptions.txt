This paper introduces the concept of joint video and language (VL) pre-training to enhance video understanding in various tasks such as video search, recommendation, and editing. The existing video-language understanding models are limited in scale and scope of datasets. To address these limitations, the authors propose the HD-VILA-100M dataset, which is large, high-resolution, diverse, and balanced. They also propose a hybrid image sequence design for effective video representation and spatiotemporal feature learning. The learned features are combined with spatial features using a multimodal Transformer. Experimental results demonstrate the effectiveness of the learned cross-modality embedding in multiple video understanding and text-to-visual generation tasks. The dataset, model, and code are made available for further research.