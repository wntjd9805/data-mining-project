This paper introduces the task of video paragraph grounding (VPG), which aims to localize multiple events in an untrimmed video based on a paragraph describing those events. Existing VPG methods rely on temporal and contextual information from the paragraph, but fail to effectively exploit contextual cues and suffer from misalignment issues. To address these challenges, the authors propose a novel base model called Video-Paragraph TRansformer (VPTR), which incorporates contrastive learning and semi-supervised learning. They further extend VPTR to the Semi-supervised Video-Paragraph TRansformer (SVPTR) to reduce the reliance on temporal annotations. SVPTR utilizes hierarchical text features and a sentence-based query mechanism to extract contextual information from the paragraph. Contrastive learning is introduced to guide cross-modal fusion at the video-paragraph level, and a teacher-student framework is developed for semi-supervised learning. The proposed approach achieves promising results on three widely-used datasets, outperforming existing methods in both fully-supervised and semi-supervised settings. Overall, this paper contributes a novel approach for video paragraph grounding that effectively leverages contextual information and reduces the dependency on temporal annotations.