Automatic generation of personalized human avatars using a single RGB camera has numerous applications in virtual reality, augmented reality, entertainment, and gaming. While high-quality human models can be reconstructed using expensive 3D scanners or specialized cameras, these methods are costly and not widely accessible to consumers. Therefore, reconstructing avatars from a single RGB camera is a practical yet challenging approach. Several methods based on implicit representations have been proposed, but they often lack the ability to support animation and produce smooth reconstructions in unseen regions. To address these challenges, this paper proposes a coarse-to-fine framework that utilizes dynamic surface deformation and reference-based neural rendering to generate seamless and sharp texture maps for fully-textured avatars. The existing approach of using a fixed displacement for each vertex is unsuitable for capturing the changing pose and geometry of a person in motion. Furthermore, methods that directly average unwrapped texture maps result in coarse textures and artifacts. The proposed framework overcomes these issues by employing a dynamic surface network to recover pose-dependent surface deformations and decouple the shape and texture of the person. The optimization process incorporates a photometric constraint to guide the alignment of vertices and relieve geometry misalignment. Additionally, a reference-based neural rendering network is introduced to handle the complexity of textures and generate photo-realistic results. A bottom-up sharpening-guided fine-tuning strategy enhances the texture map, avoiding direct averaging and adding more texture details. Experimental results using public and collected datasets demonstrate that the proposed method outperforms existing techniques, both in terms of reconstructed avatars and novel view/pose synthesis. The key contributions of this paper include the introduction of a coarse-to-fine framework that combines neural texture with dynamic surface deformation, a dynamic surface network to model pose-dependent surface deformations, and a reference-based neural rendering network with a fine-tuning strategy. This framework enables the generation of high-fidelity personalized avatars and realistic texture maps while remaining compatible with traditional graphics pipelines.