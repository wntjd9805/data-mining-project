This paper focuses on the development of methods for automatically creating animatable personal avatars from monocular videos. These avatars are useful for applications in virtual reality (VR), augmented reality (AR), games, and telepresence. The goal is to create faithful representations of facial geometry, expressions, appearance, and overall head and hair structures. Previous methods based on morphable mesh models could fit 3D shape and texture parameters to images but faced limitations in resolution and were unable to handle topological changes caused by accessories like glasses or hair. More recent methods using neural radiance fields have shown promise in generating high-quality images but struggle with modeling deformations.In response to these challenges, the authors propose a novel approach called Implicit Morphable avatar (IMavatar) that combines the advantages of fine-grained expression control provided by 3DMMs with the high-fidelity geometry and texture details offered by resolution-independent implicit surfaces. IMavatars are represented using three continuous implicit fields that capture geometry, texture, and pose- and expression-related deformations. The authors draw inspiration from FLAME and use learned expression blendshapes, linear blend skinning weights, and pose correctives to represent deformation fields in a canonical space.To enable learning from monocular videos with dynamically deforming faces, the authors augment existing algorithms with a modified root-finding algorithm and a ray marcher to establish correspondence between pixels and 3D locations on the canonical surface. They utilize a canonical texture network for color prediction and employ a per-pixel image reconstruction objective for supervision. Gradients for the canonical points are obtained by leveraging implicit differentiation.The authors compare their method with various baselines and state-of-the-art approaches using image similarity and expression metrics. They also evaluate the generated geometry using a synthetic dataset that includes different poses and expressions. The results demonstrate that the proposed method produces more accurate geometry and generalizes better to unseen poses and expressions compared to existing techniques. When applied to real video sequences, IMavatar achieves more accurate reconstruction of target poses and expressions and exhibits better extrapolation ability.In conclusion, this paper contributes a 3D morphing-based implicit head avatar model that offers detailed geometry and appearance, a differentiable rendering approach for end-to-end learning from videos, and a synthetic video dataset for evaluation purposes.