Deep neural networks have achieved remarkable performance on visual recognition tasks due to the growing availability of data. However, the effectiveness of these models can be affected by the size and variability of the dataset. Models trained on specific distributions often fail to generalize to new domains, highlighting the need for large-scale datasets. This paper focuses on few-shot transfer learning and investigates the impact of replacing batch normalization (BN) layers with feature normalization (FN) layers in convolutional neural networks (CNNs). The authors hypothesize that the sparsifying effect of BN in conjunction with ReLU may contribute to the decrease in performance in few-shot transfer tasks. The experiments conducted on multiple few-shot transfer benchmarks confirm the superiority of FN over BN in similar settings. To address the challenge of generalizability and adaptation, the authors propose a novel methodology called "Fine-Affine" that combines FN during representation learning on the source domain and BN when adapting to the target domain. This approach achieves better results overall. The paper provides a comprehensive review of few-shot transfer and normalization-based approaches, presents formal definitions for FN and Fine-Affine, describes the experimental setups and evaluation results, and concludes with insights and future directions in few-shot transfer learning.