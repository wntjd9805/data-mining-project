Generative adversarial networks (GANs) have revolutionized deep learning by enabling the synthesis of realistic and high-resolution images with diverse styles and fewer artifacts. These GAN models also learn latent spaces that encode interpretable semantics, making them valuable for image manipulation. In order to apply these semantic directions to real-world images, a common practice is to first invert an input image to a latent code of the GAN model for accurate reconstruction, followed by manipulation of the latent code. This can be achieved through iterative optimization or inference with an encoder. However, there exists a trade-off between reconstruction fidelity and editability, depending on the embedding space used for inversion. Some methods sacrifice reconstruction quality for better editability, while others employ two-stage approaches with optimization or fine-tuning steps, resulting in expensive inference time. In this paper, we propose a novel pure encoder-based two-phase GAN inversion method for StyleGAN. Our approach utilizes a standard encoder to regress the image to the latent code in the W space, and then employs hypernetworks to predict residual weights that recover lost details and update the generator for final image synthesis. This approach not only achieves faithful reconstruction, but also preserves editability and significantly reduces processing time compared to existing methods. Extensive benchmarking demonstrates the superior performance of our method in GAN inversion. Overall, our contributions include a completely encoder-based two-phase approach for GAN inversion, a novel network architecture using hypernetworks for weight updating, and an extensive benchmark showcasing the effectiveness of our method.