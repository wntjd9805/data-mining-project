Multi-modal image fusion is an important technique for merging information from different imaging modalities to generate a single image with improved quality and comprehensive scene representation. However, existing fusion methods often fail to consider parallaxes caused by differences in device positions and angles, resulting in misalignments in the pre-registered images. This highlights the need for both multi-modal image registration and fusion in practical applications. Additionally, traditional approaches treat fusion as a downstream task of registration and do not provide feedback to improve registration accuracy. In this paper, we propose a mutually reinforcing framework called RFNet that combines image registration and fusion. Our framework utilizes the characteristics of fused images to improve registration accuracy, such as reducing modal diversity and encouraging gradient sparsity. We address the challenges of developing appropriate registration metrics for multi-modal data and ensuring practical constraints for network optimization. Furthermore, we introduce a novel unsupervised network architecture for coarse-to-fine registration and fusion, leveraging features from both stages to correct global parallaxes and local misalignments. Our contributions include exploiting image fusion to enhance registration accuracy, designing constraints for multi-modal registration performance, and integrating texture retention through a gradient channel attention mechanism and customized loss functions. Experimental results demonstrate the effectiveness of our approach in improving both registration quality and fusion results.