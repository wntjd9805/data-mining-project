This paper introduces NeuralHOFusion, a neural volumetric human-object capture and rendering system that utilizes light-weight consumer RGBD sensors. Existing solutions for reconstructing and rendering human activities under human-object interactions are either too complex and expensive or unsuitable for on-the-fly modeling. NeuralHOFusion addresses these challenges by combining traditional volumetric non-rigid fusion pipeline with neural implicit modeling and blending. It distinguishes between humans and objects using instance segmentation and reconstructs human and object geometry separately. The system also incorporates layer-wise neural blending for photo-realistic rendering of both humans and objects. The contributions of this work include the development of a novel neural volumetric capture and rendering system, a fusion-based neural implicit inference scheme for occlusion-aware reconstruction, and a layer-wise neural rendering scheme that combines volumetric and image-based rendering in both spatial and temporal domains.