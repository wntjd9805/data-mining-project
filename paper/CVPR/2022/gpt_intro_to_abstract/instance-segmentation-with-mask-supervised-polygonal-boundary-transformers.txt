Image segmentation and scene labeling are widely studied topics in computer vision, with applications in shape recognition, tracking, image understanding, medical imaging, and 3D reconstruction. The emergence of instance segmentation, which predicts the detailed extent of an object, has increased the practical significance of object segmentation. However, the representation and prediction of object segmentation pose challenges, leading to a preference for mask-centric models that rely on spatially dense functions to determine object boundaries. This paper introduces a model that predicts object boundaries in the form of polygons, treating instance segmentation as a regression problem with end-to-end differentiability. By representing boundaries as polygons, the model facilitates downstream tasks such as plane detection and provides a continuous output that can be differentiated in larger systems. Previous attempts to predict polygons within instance segmentation have not achieved parity with mask-based segmentation models. This paper aims to achieve parity in supervision, evaluation, and access by presenting a Transformer-based approach called BoundaryFormer. The model outperforms the baseline Mask R-CNN on the MS-COCO dataset and achieves competitive results on Cityscapes, all while being trained end-to-end without requiring additional supervision. The model uses pixel-wise masks as ground truth and can serve as a drop-in replacement for mask-based segmentation heads in various architectures. This paper contributes to the understanding of polygons' capabilities in segmentation pipelines and provides a performant solution accessible to different architectures.