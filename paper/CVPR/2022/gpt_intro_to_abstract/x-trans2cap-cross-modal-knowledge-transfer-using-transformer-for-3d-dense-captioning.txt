This paper introduces X-Trans2Cap, a cross-modal knowledge transfer framework for 3D dense captioning in computer vision. Dense captioning provides more detailed descriptions for each object in an image compared to traditional image captioning. Previous methods have used extra 2D input during both training and inference phases, but this is computationally intensive and not always available during inference. To overcome this, X-Trans2Cap utilizes a teacher-student framework where the teacher network takes multi-modal inputs and the student network only uses 3D inputs. A Transformer-based knowledge transfer framework is designed with flexible input control and better representation. A modified knowledge distillation operation with a cross-modal fusion module and cross-modal feature alignment objective is proposed for knowledge generalization. Experiments on the ScanRefer and Nr3D datasets show that X-Trans2Cap achieves better performance by leveraging 2D priors in the 3D object representation, outperforming previous state-of-the-art methods by significant margins. Overall, X-Trans2Cap provides an effective approach for 3D dense captioning by combining the strengths of both 2D and 3D modalities.