Powerful Generative Adversarial Networks (GANs) have been developed in recent years to generate high-fidelity and diverse images. However, these GANs typically require large datasets and computational resources to perform well. In this paper, we explore the possibility of generating diverse images with limited training data, specifically in the few-shot image generation task. This task is important in real-world applications with limited data, such as the artistic domain, and can also benefit downstream tasks like few-shot image classification. Previous approaches have focused on transfer learning from a GAN pretrained on a large-scale dataset, but these methods often struggle to achieve diversity in the generated images due to the limited number of target domain samples. To address this issue, recent methods have proposed diversity preserving techniques during generator adaptation. However, questions arise regarding the trade-off between quality and diversity in these methods, as well as the potential for further improvement. In this paper, we independently analyze the quality and diversity progression during adaptation and propose a novel regularization technique to slow down diversity degradation without compromising image quality. Our proposed method, called Dual Contrastive Learning (DCL), maximizes the mutual information between source and target image features at different semantic levels to preserve diversity. Experimental results demonstrate that DCL outperforms previous approaches and achieves state-of-the-art performance in few-shot image generation.