Recently, there has been growing interest in the field of vision and language understanding. Various tasks have been proposed, such as Moment Localization with Natural Language, Image Captioning, Visual Question Answering, and Visual Dialog. Visual Dialog aims to enable humans to interact with an unseen image through continuous communication. It involves two types of settings: a discriminative decoder that ranks answer candidates, and a generative decoder that synthesizes answers. Compared to visual question answering, visual dialog requires the agent to utilize previous dialog history and clues to produce a correct answer. Current visual dialog models focus on attention mechanisms in the discriminative setting or weakly capture the relation between generative and discriminative tasks. However, a unified model that can effectively train both tasks remains a challenge. One challenge is transferring semantic clues from the discriminative task to answer generation. Another challenge is aligning dialog context and answer. In this paper, we propose a unified model for Visual Dialog that handles interactions between entities in both discriminative and generative tasks. We introduce inter-task contrastive losses to facilitate bidirectional information flow and ease the training of both tasks. We conduct extensive experiments on VisDial benchmarks and demonstrate the improved performance of our model with inter-task contrastive learning.