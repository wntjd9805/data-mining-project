Establishing correspondences between images is a fundamental task in computer vision and has various applications such as 3D reconstruction and object recognition. Recent advancements in deep neural networks have led to improved performances in keypoint extraction and feature descriptors compared to traditional methods. Dense feature matching methods, which use all extracted features for matching, have also shown impressive performances. However, establishing reliable correspondences between images with intra-class variations remains a challenge. The idea of applying high-dimensional convolutional layers on the feature correlation map has been proposed and has shown promising results. Transformer networks, which have dynamic feature transformation and non-local interactions, have also shown success in computer vision. Inspired by the effectiveness of match-to-match consensus consideration and transformer networks, we propose a novel image matching pipeline called TransforMatcher. It leverages match-to-match attention, a self-attention mechanism, to capture global match-to-match interactions by utilizing 4D correlation maps. This allows for capturing long-range relevance and geometric consistency under challenging appearance variations. Our contributions include the introduction of TransforMatcher, the modeling of global interactions between dense correspondences using self-attention, leveraging multi-level correlation scores as features, and achieving state-of-the-art or on-par performances on benchmark datasets.