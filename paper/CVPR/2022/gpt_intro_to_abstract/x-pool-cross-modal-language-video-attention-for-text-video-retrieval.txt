This paper introduces the problem of text-video retrieval and proposes a solution to address the challenge of comparing text-based queries to video contents. Existing techniques often encode the entire video, leading to the inclusion of irrelevant information and potential performance reduction. To overcome this issue, the authors design a cross-modal attention model called X-Pool that allows for joint reasoning between text and video frames, enabling the model to focus on the most relevant video sub-regions. The proposed X-Pool model outperforms baseline methods and achieves state-of-the-art results on benchmark datasets. The paper also demonstrates the robustness of X-Pool to videos with diverse content, showcasing its superiority over text-agnostic pooling methods.