The field of continual learning (CL) focuses on training models incrementally to generalize across sequentially encountered scenarios or tasks, while avoiding training and maintenance costs. An important challenge in CL is catastrophic forgetting, where training on subsequent tasks can erase information pertaining to previous tasks. Various approaches have been proposed to address catastrophic forgetting, such as modifying the loss function, network architecture, and training procedure. Replay-based continual learning maintains a small data sketch from previous tasks to be included in the training mix. Previous methods for coreset selection in continual learning have focused on qualitative/diversity-based criteria or computationally intensive bi-level optimizations. In this paper, we introduce Gradient-based Coresets for Replay-based CL (GCR), a principled criterion for selecting and updating coresets. GCR selects a coreset that approximates the gradient of model parameters over the entire data seen so far. Our empirical evaluation shows that coresets selected using GCR mitigate catastrophic forgetting on par with or better than previous methods. We also explore the benefits of incorporating representation learning into the CL loss function. Extensive experimentation demonstrates the effectiveness of GCR in both offline and online/streaming settings, with improvements over the state-of-the-art. Furthermore, the benefits of GCR increase with the number of tasks, indicating its scalability. Finally, we compare GCR to other coreset methods for CL and show its superiority.