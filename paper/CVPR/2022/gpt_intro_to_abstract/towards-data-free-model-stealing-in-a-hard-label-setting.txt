Deep learning systems have made significant advancements in recent years, allowing them to be deployed in critical applications. One common practice is to provide pretrained machine learning models as a service (MLaaS), where users can query the model to access its predictions via APIs. However, this exposes the model to potential attacks, such as model stealing, where adversaries attempt to clone the model without accessing its gradients. Protecting the privacy of ML models is crucial, as organizations invest substantial resources in research and data collection. Recent works have shown that stolen models can be used to launch adversarial attacks or compromise user privacy through membership inference or model inversion attacks. This paper focuses on practical and challenging scenarios where only the top-1 prediction of the model is accessible, and presents a data-free model stealing (DFMS) attack. The proposed DFMS-HL attack leverages unrelated proxy data or manually crafted synthetic data to train a clone model, outperforming existing baseline methods and significantly reducing the number of queries to the victim model. The effectiveness and scalability of the approach are demonstrated on datasets such as CIFAR-10 and CIFAR-100. Comparisons with state-of-the-art model stealing attacks show comparable accuracy and a significant boost in performance with a soft-label variant of the proposed method (DFMS-SL). Overall, the paper provides a comprehensive approach to data-free model stealing with promising results.