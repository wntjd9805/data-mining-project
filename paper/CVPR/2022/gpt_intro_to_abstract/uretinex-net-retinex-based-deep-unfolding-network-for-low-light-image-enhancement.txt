Images taken in low-light conditions often suffer from low contrast and visibility, which present challenges for human visualization and high-level vision tasks. To address this, researchers have focused on contrast enhancement, texture recovery, and noise removal specifically for low-light image enhancement (LLIE). Retinex-based methods, which decompose an image into reflectance and illumination components, have gained attention for LLIE. However, existing methods often lead to unnatural and over-exposed results. Model-based approaches have been proposed to handle the decomposition problem, using hand-crafted priors as regularization terms. However, these methods lack adaptability and can be computationally expensive. As an alternative, deep learning-based methods have been used to restore low-light images. However, these methods often suffer from a lack of interpretability and flexibility. To overcome these limitations, we propose URetinex-Net, a Retinex-based deep unfolding network for LLIE in RGB color space. Our approach combines the strengths of model-based and learning-based methods by formulating the decomposition problem as an implicitly regularized model. We introduce deep networks to adaptively fit implicit priors and solve the optimization problem iteratively. Our approach avoids hand-crafted priors and incorporates an initialization module and an illumination adjustment module for flexibility. Experimental results demonstrate the effectiveness and superiority of URetinex-Net in noise suppression and details preservation.