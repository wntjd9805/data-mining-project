Deep neural networks (DNNs) have achieved remarkable results in computer vision applications, but their training requires vast amounts of data and computational resources. To address this, researchers have explored methods to construct smaller training sets, such as coreset selection and dataset condensation. While dataset condensation has shown promise, existing methods based on gradient matching have limitations in overlooking representative samples and lacking generalization. In this paper, we propose a new strategy called CAFE (Condensing dataset by Aligning FEatures) that incorporates distribution-level supervision to capture the entire dataset distribution. CAFE aligns layer-wise features between real and synthetic samples and introduces discriminative characteristics into the synthetic clusters. We also introduce a bi-level optimization scheme to dynamically adjust the number of SGD steps, mitigating under- and over-fitting issues. Experimental results on popular benchmarks demonstrate that CAFE outperforms existing methods, achieving superior performance, strong generalization, and robustness. CAFE provides an effective approach for dataset condensation, enhancing the efficiency and effectiveness of training deep neural networks.