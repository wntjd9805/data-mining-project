This paper introduces the concept of linking words to visual regions as a fundamental component of various applications in the field of computer science, such as human-robot interaction, visual question answering, and unsupervised neural machine translation. The training of visual grounding systems typically requires time-consuming and costly annotations, such as bounding boxes for groundable words. To address this challenge, the authors propose a framework that jointly models visual descriptions and word-to-region alignments, without the need for bounding box annotations. The framework can handle both weakly-supervised visual object grounding and grounded visual description tasks. Previous approaches have either focused on matching images to sentences or generating captions based on region proposals, limiting their ability to tackle both tasks simultaneously. The proposed framework overcomes the limitations of existing attention-based grounding approaches and leverages a conditional generative model to capture meaningful word-to-region alignments. This approach achieves significantly improved performance on challenging image and video datasets, demonstrating its effectiveness in visual grounding and captioning.