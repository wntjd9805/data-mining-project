Recently, there has been a growing demand for interactive photo editing tools on touch devices. Sketching is a popular method for expressing creative ideas and interacting with apps. Sketch-based image editing, which involves manipulating image structures based on user-drawn sketches, has seen significant progress with advancements in deep learning and generative models. However, there are still challenges in this area, such as identifying the precise modification region based on rough sketch inputs. In this paper, we propose a novel framework for sketch-based image manipulation that addresses these challenges. Our system enables both global and local image editing using only sketch inputs. For global editing, users can directly sketch on top of the original image, providing a more user-friendly interface. This approach also preserves the original image content in the modification region, resulting in a more consistent appearance. For local editing, we use a mask estimator to predict the modification region and a generator to synthesize new content within that region. The final manipulated image is created by blending the generator output with the original image using the predicted mask. To ensure the synthesized content follows the sketch structure while maintaining the style of the original content, we encode the modification region into a structure-agnostic style vector with a style encoder. We train our system in a self-supervised manner by reconstructing the target modification region based on the style vectors and sketches. We evaluate our method on multiple datasets, including CelebAHQ, Places2, Sketch-Face, and SketchImg, and demonstrate that it outperforms state-of-the-art approaches. Overall, our contributions include introducing a new paradigm of mask-free local image manipulation and developing a system that enables such manipulation with partial sketch inputs. We also propose a network architecture, data acquisition, and training strategy for this system. Extensive experiments validate the superiority of our approach compared to existing methods.