Convolutional neural networks (CNNs) with millions of parameters require high-performance devices for efficient utilization, even during the inference stage. To address this limitation and reduce latency and memory consumption, network compression techniques have been widely used in model deployment. These techniques include network pruning, quantization, and knowledge distillation methods, which have shown success in reducing computations and accelerating inference speed. However, these methods assume full access to the training data, which may not be feasible in non-academic scenarios due to data security concerns. In response, few-sample compression methods aim to compress models with limited samples, using non-sensitive data to protect data privacy. Recent approaches have adopted a layer-wise framework to obtain compact models, optimizing each layer individually. However, this approach can be cumbersome and lead to error accumulation. In this paper, we propose a new framework called Mimicking then Replacing (MiR), which advocates for holistic network pruning and optimization instead of layer-wise reconstruction. MiR first mimics the features outputted by a teacher model, and then replaces all other layers with a compact model, while keeping the classification or detection head intact. We show that mimicking features before the pooling layer can improve accuracy without additional computation. Our proposed MiR framework is simple to use, generalizable to different scenarios and architectures, unsupervised, and highly accurate, outperforming current state-of-the-art methods. We demonstrate the effectiveness of MiR in few-sample compression, achieving high accuracy within minutes and with few samples. In summary, our contributions include the proposal of the MiR framework, which outperforms existing methods in few-sample compression, its generalizability, and ability to avoid error accumulation, as well as the discovery of the benefits of mimicking features before the pooling layer.