Visual object tracking is an important task in computer vision with a wide range of applications. It can be categorized into two main paradigms: Single Object Tracking (SOT) and Multiple Object Tracking (MOT). Current methods in the tracking community treat these tasks separately by training models on individual datasets for either SOT or MOT. Siamese architecture is commonly used in SOT, while tracking by detection is popular in MOT. However, existing methods that combine these paradigms fail to address the challenges of both tasks effectively. In this paper, we propose a novel Uniﬁed Transformer Tracker (UTT) to solve both SOT and MOT concurrently. Our approach utilizes a shared backbone and multiple tracking heads to leverage the strengths of both paradigms. We introduce a small feature map proposal in the tracking frame based on the previous localization to update target representation and output target localization. This enables our UTT to track objects in both SOT and MOT with the same design. Our tracker utilizes a track transformer to encode target features and refine the localization of tracking targets iteratively. To train our UTT, we exploit training samples from both SOT and MOT tasks alternately. We evaluate the performance of our method on benchmark datasets for both SOT and MOT. Our proposed UTT achieves comparable performance to state-of-the-art algorithms on various benchmark datasets. Overall, our work presents a novel approach for uniﬁed object tracking, effectively addressing the challenges of both SOT and MOT tasks. Our UTT model shows promising results and opens up possibilities for real-world deployment in various fields.