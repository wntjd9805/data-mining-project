Vision Transformers (ViTs) have revolutionized computer vision and medical image analysis by demonstrating exceptional capabilities in learning pretext tasks, incorporating global and local information across layers, and providing scalability for large-scale training. Unlike convolutional neural networks (CNNs) with limited receptive fields, ViTs encode visual representations from a sequence of patches and leverage self-attention blocks for modeling long-range global information. Swin Transformers, a hierarchical ViT variant, allows for local computing of self-attention with non-overlapping windows, resulting in linear complexity and making it more efficient. Transformer-based models also outperform CNN-based counterparts in learning feature representations during pre-training and fine-tuning downstream tasks. Despite these advancements, medical image analysis has lagged due to the domain gap between natural images and medical imaging modalities, as well as the lack of cross-plane contextual information for volumetric (3D) images. To address these limitations, the authors propose a novel self-supervised learning framework for 3D medical image analysis. They introduce a new architecture called Swin UNEt Transformers (Swin UNETR) that directly utilizes 3D input patches and pre-trains the transformer encoder with tailored proxy tasks such as image inpainting, 3D rotation prediction, and contrastive learning. The framework leverages consistent contextual information in radiographic images to learn underlying anatomy patterns. The authors validate the effectiveness of the pre-training framework by fine-tuning Swin UNETR on two publicly available medical image segmentation benchmarks, where it achieves state-of-the-art performance. The contributions of this work include the introduction of a novel self-supervised learning framework with tailored proxy tasks, the proposal of the Swin UNETR architecture for pre-training, and the demonstration of improved performance in medical image analysis tasks.