Federated learning (FL) is a distributed machine learning paradigm that allows agents to collaboratively learn a centralized model without sharing their local data. While FL offers advantages in privacy compared to traditional centralized learning, it does not protect against inference attacks that can reveal sensitive information. To address this, differential privacy (DP) has been applied to FL, either at the instance level or the user level. Existing methods for user-level DP in FL often rely on the Gaussian mechanism, but this degrades the utility of the resulting models. In this paper, we propose two techniques to improve model utility with user-level DP guarantees in FL. The first technique, Bounded Local Update Regularization (BLUR), introduces a regularization term to the agent's local objective function to bound the l2 norm of local updates. This reduces the impact of the clipping operation during DP. The second technique, Local Update Sparsification (LUS), zeros out update values that have little effect on the local model's performance before clipping, further reducing the magnitude of local updates. We provide theoretical analysis on the convergence of our framework and rigorous privacy guarantees. Experimental results demonstrate the effectiveness and advantages of our proposed methods.