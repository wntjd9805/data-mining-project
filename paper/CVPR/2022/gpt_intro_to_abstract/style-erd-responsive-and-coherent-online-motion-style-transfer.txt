In this paper, we introduce the concept of Online Motion Style Transfer, which involves stylizing streaming input motion data for real-time applications. Current motion style transfer methods rely on offline processing and suffer from startup latency. We propose a novel framework called Style-ERD that addresses these issues by embedding knowledge of previous frames into the transfer module, allowing it to infer and track style and content even with only the current frame. We also introduce a new supervision module called FT-Att Discriminator to ensure temporal coherence in the post-transfer style. Our deep learning model demonstrates efficient and low-latency motion style transfer with high fidelity. The contributions of this work are the introduction of the online motion style transfer problem, the development of the Style-ERD framework, and the FT-Att Discriminator module, which provide significant improvements in compute time and style transfer quality compared to existing methods.