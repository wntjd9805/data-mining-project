Scene graph generation is a crucial task in visual understanding, aiming to detect instances and their relationships in images. By representing image contents in a graph structure, scene graph generation bridges the gap between visual scenes and human languages, benefiting various visual understanding tasks. Prior works have explored representation learning for scene graph generation, but the issue of biased prediction due to the long-tailed distribution of predicates in scene graph datasets remains challenging. Existing re-balancing strategies prefer predicates from tail categories and struggle with distinguishing hard-to-distinguish predicates. This is because differentiating these predicates requires exploring their correlations, which existing methods underestimate. To address this, we propose a Fine-Grained Predicates Learning (FGPL) framework that thoroughly exploits predicate correlations. Our framework includes a Predicate Lattice to capture complete predicate correlations and a Category Discriminating Loss (CDL) and an Entity Discriminating Loss (EDL) to discriminate hard-to-distinguish predicates. CDL focuses on differentiating these predicates, while EDL adapts the discriminating process based on entity predictions. Our FGPL framework significantly improves the performance of three benchmark models and outperforms existing methods on the Visual Genome dataset. Overall, our work contributes a novel approach to address biased prediction in scene graph generation and achieves superior performance in this task.