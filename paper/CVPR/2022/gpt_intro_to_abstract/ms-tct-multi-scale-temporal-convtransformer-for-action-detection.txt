Action detection in computer vision aims to identify the precise temporal boundaries of actions occurring in untrimmed videos, which is important for real-world applications. Public datasets with dense annotations have been developed to address this problem, but they present challenges such as concurrent actions with limited background information. Understanding both short-term and long-term temporal dependencies among actions is crucial for accurate predictions. While previous methods have utilized 1D temporal convolutions, they are limited in modeling long-range interactions between segments in a video. Recent approaches have employed multi-head self-attention (MHSA) inspired by transformers to capture long-term relations in videos. However, existing methods rely solely on input frames, which may not adequately capture complex temporal dependencies in action detection. To address these limitations, we propose the Multi-Scale Temporal ConvTransformer (MS-TCT) model that combines the strengths of convolutions and self-attention. MS-TCT utilizes convolutions in a token-based architecture to enable multiple temporal scales of tokens and enforce temporal consistency between neighboring tokens. The model is built on top of a 3D convolutional backbone that encodes temporal segments as single input tokens. Different stages of MS-TCT capture fine-grained relations between atomic actions and coarse relations between composite actions by incorporating temporal convolution layers, multi-head self-attention layers, and mixer modules. MS-TCT also introduces a heat-map branch to predict the relative temporal position of action instances, improving discriminative token classification in complex videos. In summary, the contributions of this work include the proposal of an effective and efficient ConvTransformer model for modeling complex temporal relations in untrimmed videos, the introduction of a new branch to learn the position relative to the instance center, and improvements on three challenging densely-labeled action datasets.