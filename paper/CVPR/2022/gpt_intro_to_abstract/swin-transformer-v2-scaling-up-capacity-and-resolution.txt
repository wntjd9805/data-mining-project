This paper explores the scaling up of vision models, which has lagged behind the scaling up of language models. While larger vision models generally perform better on vision tasks, the absolute size of vision models has recently reached only about 1-2 billion parameters. Additionally, existing large vision models are applied only to the image classification task. To address these issues, the authors propose two key techniques. Firstly, they introduce a new normalization configuration called res-post-norm, which produces milder activation values across network layers and improves stability and accuracy in training larger models. Secondly, they introduce a log-spaced continuous position bias (Log-CPB) to handle window size variations between low-resolution pre-training and high-resolution fine-tuning. This approach allows for better transfer across window sizes by sharing weights of a meta network. The authors also address the issue of high GPU memory consumption by incorporating techniques such as zero-optimizer, activation check pointing, and a novel implementation of sequential self-attention computation. With these techniques, they successfully train a 3 billion Swin Transformer model and achieve state-of-the-art accuracy on various vision benchmarks. The paper aims to stimulate more research in scaling up both the capacity and resolution of vision models, bridging the gap between vision and language models and facilitating joint modeling of the two domains.