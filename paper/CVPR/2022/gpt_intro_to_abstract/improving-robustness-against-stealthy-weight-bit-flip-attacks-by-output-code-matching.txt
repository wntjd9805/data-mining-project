Deep neural networks (DNNs) are widely used in artificial intelligence applications but are also susceptible to malicious attacks. Adversarial input attacks manipulate input perturbations to induce incorrect decisions in DNNs. Another category of attacks exploits the vulnerability of DNN parameter space by influencing the inference process through manipulating dynamic random-access memory (DRAM) chips. As weight parameters are often stored on DRAM, these hardware-induced attacks pose a threat to DNN predictions. Adversarial weight bit-flip attack algorithms have been developed to identify vulnerable quantized DNN bits. To make these attacks more efficient, algorithms aim to find the minimal number of bit flips that negatively impact targeted inputs while maintaining the expected behavior for other test samples. However, there is limited guidance on improving network robustness against such attacks. This study proposes a defense mechanism called output code matching that uses an alternative output coding scheme for multi-class classification with DNNs. The proposed approach predicts class-specific partially overlapping bit strings, which enhances resistance against stealthy weight bit-flip attacks. The contributions of this study include presenting a DNN defense mechanism that is compatible with any DNN backbone, outperforming state-of-the-art defenses, and demonstrating applicability to networks trained with existing defenses. Experimental results show that the proposed framework reduces the number of bits attacked in targeted stealthy attacks on large DNN architectures compared to vanilla networks and networks trained with the state-of-the-art defense.