Transfer learning has been a successful paradigm in computer vision, but zero-shot learning aims to develop models that can handle new tasks without task-specific data or adaptation protocols. Previous research has shown that web-sourced paired image-text data can be used to pre-train strong models for zero-shot transfer. In this paper, we propose a contrastive learning framework called contrastive-tuning, which trains an image model and a text model simultaneously to minimize a contrastive loss. We find that locking the image tower and using a pre-trained image model as the anchor works best. We call this specific instance of contrastive-tuning "Locked-image Tuning" (LiT). LiT achieves better results compared to from-scratch models and sets new state-of-the-art in zero-shot transfer accuracy on ImageNet and out-of-distribution ImageNet test variants. We believe LiT works well because it decouples data sources and techniques for learning image descriptors and vision-language alignment. We also propose a recipe for training high-performance zero-shot models using modest computational resources and public datasets. Our hope is that these contributions will facilitate wider research in zero-shot transfer.