Three-dimensional (3D) object detection plays a crucial role in various applications, including autonomous driving. Previous methods have achieved high performance by leveraging accurate depth information from sensors such as LiDAR signals or stereo matching. However, the cost of these sensors can be prohibitive. As a result, image-only monocular 3D object detection methods have been proposed but still struggle to achieve satisfactory performance without depth cues. To address this, some recent works have attempted to estimate depth from pre-trained models to aid in monocular 3D object detection. These approaches include pseudo-LiDAR-based methods that convert estimated depth maps into 3D point clouds and fusion-based methods that combine features from depth maps and images. However, these methods are limited by the accuracy of the depth maps and the additional computational cost of the depth estimator. In this paper, we propose MonoDTR, an end-to-end depth-aware transformer network for monocular 3D object detection. Our approach incorporates a depth-aware feature enhancement module that learns depth-aware features with auxiliary depth supervision, avoiding the need for inaccurate depth priors from pre-trained estimators. The module is lightweight yet effective, reducing computational time compared to previous methods. Additionally, we introduce a transformer-based fusion module that globally integrates image and depth information, leveraging the transformer's ability to capture long-range dependencies. We also utilize depth-aware features instead of object queries as input to the transformer decoder, providing more meaningful cues for 3D reasoning. We propose a novel depth positional encoding to inject depth positional hints into the transformer. Experimental results on the KITTI dataset demonstrate that our approach outperforms state-of-the-art monocular-based methods and achieves real-time detection. Furthermore, our depth-aware modules can be easily integrated into existing image-only frameworks to improve performance.