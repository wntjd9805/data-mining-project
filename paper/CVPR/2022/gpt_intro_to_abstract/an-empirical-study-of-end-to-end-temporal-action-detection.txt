With the rapid increase in the generation and access of videos, there is a growing need for automatic video understanding, particularly in the areas of human action recognition and temporal action detection (TAD). Action recognition involves predicting the action label of a short video, while TAD aims to determine the label and temporal interval of action instances in a long video. This paper focuses on the challenge of TAD and its practical applications in real-world actions.Deep learning methods, driven by the discriminative power of neural networks, have made significant advancements in the field of temporal action detection. However, most TAD methods follow a head-only learning paradigm, where the video encoder is pre-trained on a large action recognition dataset and then frozen for offline feature extraction. Only the detection head is trained for the TAD task on target datasets. This approach limits the performance by leaving the video features sub-optimal.While a few works have explored end-to-end learning for TAD, there is a lack of in-depth analysis and understanding of its benefits. Additionally, the effects of various factors in end-to-end TAD, such as the video encoder, detection head, image resolution, and temporal resolution, have not been systematically studied. This hinders the progress of end-to-end TAD research, and efficiency considerations are often disregarded.This paper aims to address these issues by conducting an empirical study of end-to-end temporal action detection. The study evaluates four video encoders and three detection heads with different designs on standard TAD datasets (THUMOS14 and ActivityNet). The benefits of end-to-end learning are uncovered, highlighting that medium-resolution end-to-end trained video encoders can perform as well as or better than pre-trained ones with standard image resolution. The study also examines the impact of various design choices on performance and efficiency, offering insights for achieving an efficiency-accuracy trade-off.Based on the findings, the paper presents a baseline detector that achieves state-of-the-art performance in end-to-end TAD while running more than four times faster. The detector can process a four-minute video in just 0.6 seconds. The goal of this work is to facilitate future research in temporal action detection and provide guidance for efficient and accurate TAD models.