Light field (LF) cameras capture 3D scenes in 4D LF images, providing rich spatial and angular information. Extracting depth information from LF images is crucial for various applications such as refocusing, view synthesis, 3D reconstruction, and virtual reality. Deep learning-based approaches have improved LF depth estimation performance by employing a four-step pipeline. However, the construction of matching costs, a key step in LF depth estimation, has received little attention and has limitations in terms of efficiency and handling spatially-varying occlusions. To address these challenges, we propose an occlusion-aware cost constructor (OACC) for LF depth estimation. Our OACC performs convolutions with designed dilation rates, eliminating the need for shifting operations and handling occlusions through dynamic modulation of pixels. We develop an OACC-Net based on the proposed OACC, achieving state-of-the-art accuracy with significant acceleration. Our contributions include the introduction of a cost constructor to replace the shift-and-concat approach, an occlusion-aware modulation of pixels, and the development of OACC-Net for LF depth estimation. Our method outperforms other state-of-the-art approaches on the 4D LF benchmark, providing high accuracy and accelerated performance.