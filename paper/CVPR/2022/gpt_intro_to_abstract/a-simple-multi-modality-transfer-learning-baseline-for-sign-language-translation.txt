Sign languages are crucial for communication among deaf and hard of hearing individuals, but automatic sign language translation (SLT) systems are still in early stages of development compared to machine translation for spoken languages. Existing SLT methods follow the neural machine translation (NMT) framework, but performance is limited due to the scarcity of training data. This paper proposes a multi-modal pretraining approach to address the data scarcity issue in SLT. The approach includes pretraining separate modules for visual action recognition and language translation tasks, followed by joint finetuning. By leveraging existing datasets and transfer learning, the need for large parallel data is reduced. The paper introduces a visual-language mapper to connect visual features with gloss embeddings, allowing for joint optimization and improved translation performance. Experimental results demonstrate the effectiveness of the proposed approach, surpassing existing methods on standard datasets.