Abstract:Image compression is a crucial technology in the digital information age, particularly with the increasing demand for high-resolution visual applications. This paper introduces a neural data-dependent transform for learned image compression, aiming to generate transform parameters dynamically based on the input sample. The model incorporates neural syntax to describe the image's rough contexts, resulting in a more compact distribution of encoded coefficients. The model also utilizes a continuous online mode decision mechanism to optimize coding efficiency. Experimental results demonstrate the superiority of the proposed method. This work contributes by introducing a neural data-dependent transform, proposing a joint optimization paradigm, and achieving online optimization of the encoded coefficients.