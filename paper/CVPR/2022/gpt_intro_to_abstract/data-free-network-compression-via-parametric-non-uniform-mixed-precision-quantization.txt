Deep neural networks (DNNs) have achieved remarkable success in various tasks, but their large size and complexity present challenges for deployment in resource-constrained devices. To address this issue, researchers have proposed several compression techniques, such as pruning, quantization, and knowledge distillation. However, many existing methods require expensive computations or large training datasets, limiting their practicality. This paper introduces a novel compression method called PNMQ, which achieves better results compared to existing data-free and training-free techniques. PNMQ utilizes a parametric family of non-uniform quantization grids, optimizing their parameters to improve quantization quality. Additionally, the paper presents a data-free algorithm for selecting optimal bitwidths for different layers of a model, enabling users to specify the desired compression ratio. PNMQ can be applied to any model without modifying its structure and is compatible with other weight compression techniques. Experimental results demonstrate the effectiveness and versatility of the proposed method.