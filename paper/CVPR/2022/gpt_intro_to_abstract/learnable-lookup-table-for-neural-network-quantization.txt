Deep neural networks have been successful in various fields, but their high computational cost and memory requirements limit their deployment on edge devices. To address this problem, several approaches such as efficient architecture designs, network pruning, weight decomposition, knowledge distillation, and network quantization have been proposed. Among these approaches, network quantization stands out as it significantly reduces computational cost and is widely used in real-world applications. Network quantization aims to obtain low-bit networks to reduce memory footprint and computational cost. However, reducing the bit-width introduces quantization errors that lead to accuracy loss. Network quantization can be achieved through uniform approaches or non-uniform approaches. This paper focuses on uniform quantization methods because they can be directly deployed on off-the-shelf hardware with integer arithmetic support.Early uniform quantization methods utilize fixed linear quantizers, but these quantizers do not fit the bell-shaped and long-tailed distributions of weights and activations well. The distributions in different layers can significantly vary, limiting the adaptability of fixed quantizers. To address this limitation, some works adopt trainable quantizers for joint optimization. However, these methods introduce considerable computational overhead as activation quantization needs to be conducted during inference.To overcome these challenges, this paper proposes the use of learnable lookup tables (LLTs) for network quantization. The quantization function is formulated as a lookup table, which is transformed into one-hot distributions for optimization. During training, one-hot distributions are softened into temperatured softmax distributions for the update of both the lookup tables and the network. Several training strategies, such as an exponential formulation of scale parameters and a gradient rescaling scheme, are introduced to enhance the convergence of the lookup tables. As the temperatured softmax distributions converge to one-hot distributions, the lookup tables learn an adaptive mapping from continuous float values to quantization levels. During inference, online activation quantization can be achieved through a simple lookup operation with minimal computational cost.Extensive experiments on image classification, image super-resolution (SR), and point cloud classification tasks demonstrate the state-of-the-art performance of the proposed method.