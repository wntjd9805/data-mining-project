Recent progress in vision-language models (VLMs) has opened up new opportunities for leveraging human language in downstream recognition tasks. VLMs learn aligned embeddings of image and text, encouraging similarity between image and language representations. Providing task-related content, such as category descriptions, can greatly improve recognition performance, even enabling zero-shot inference without training samples. However, determining the optimal text descriptions remains a challenging problem. Existing VLMs use hand-crafted prompt templates, but these may introduce artificial bias and be sub-optimal for specific tasks. Additionally, the diversity of visual content poses challenges, as a single prompt may not effectively capture the variance within a category. In this paper, we propose PROmpt Distribution leArning (ProDA), a data-driven approach for automatically learning diverse prompts from available data. ProDA learns soft prompts from a few downstream samples, resulting in less biased task-related content. Furthermore, instead of learning a single prompt, ProDA estimates a distribution over prompts to handle the variance of visual representations. We differentiate prompts on both construction and semantics to improve their diversity. To efficiently learn the prompt distribution, we focus on the output embeddings of the prompts rather than the input embeddings. We utilize a multivariate Gaussian distribution as a simple model to capture the high-level embeddings' distribution. We present a surrogate objective that upper bounds the original optimization objective, making the training process more tractable. Experimental results on 12 datasets demonstrate the effectiveness of ProDA, consistently outperforming existing baselines. Our method achieves a significant 9.1% average improvement compared to hand-crafted prompts with only 1 sample per category. Overall, ProDA provides an efficient and effective approach for leveraging VLMs in downstream recognition tasks.