This paper introduces L-Verse, a novel approach for learning a bidirectional cross-modal representation of image and text. The goal is to improve the performance and data efficiency of vision-language tasks, such as image-to-text and text-to-image generation. The approach combines a feature-augmented variational autoencoder (AugVAE) and a bidirectional auto-regressive transformer (BiART). AugVAE utilizes a visual codebook with diverse feature embeddings, resulting in improved image reconstruction performance. BiART employs segment embeddings to generate corresponding images for given texts or meaningful captions for given images. L-Verse outperforms existing image captioning models and does not require an object-detection framework. The approach also demonstrates promising results in text-to-image generation. The paper provides a detailed review of related work and presents quantitative and qualitative results. Future directions for research are discussed as well.