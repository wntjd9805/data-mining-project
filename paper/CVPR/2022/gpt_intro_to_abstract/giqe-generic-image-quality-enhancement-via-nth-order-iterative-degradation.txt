Image quality is crucial for the successful performance of computer vision algorithms that involve tasks such as object detection, semantic segmentation, and depth estimation. Environmental degradations, such as motion blur, illumination variations, rain, fog, snow, and water droplets, can significantly impact image quality and result in a decline in algorithm performance. These degradations can be modeled using a common mask-based approach since they affect the spatial properties of an image. However, the intensity and combinations of these degradations can be non-uniform, leading to varying levels of impact on different regions of an image. Current approaches to address these degradations have limitations. Constructing a labeled dataset that encompasses all variations for training perception algorithms is time-consuming and expensive. Image restoration algorithms are often degradation-specific and struggle to perform well outside the distribution of the training set. Convolutional neural networks (CNNs), which are widely used in image restoration algorithms, have limitations in capturing long-range dependencies and handling varying degradations due to their fixed convolutional filters. Swin transformers, which combine the advantages of CNNs and transformers, present an opportunity to develop a generic image restoration and reconstruction algorithm by utilizing their ability to handle large image resolutions and capture long-range dependencies. However, replacing convolutional blocks with transformer modules can lead to redundant computations. To address this, we propose a multiscale architecture that extracts features from different scales and uses transformer modules with various repetitions on each scale. We also introduce a feature selection module and a spatial compression mechanism to improve computational efficiency. Additionally, we highlight the importance of utilizing paired samples and designing an end-to-end architecture that predicts the spatial distortion mask, which represents the location of affected pixels. To evaluate the effectiveness of our approach, we utilize datasets with multiple co-occurring degradations and assess the impact of image restoration on downstream tasks such as semantic segmentation. Our contributions include a comprehensive image restoration and reconstruction architecture, an auxiliary degradation prediction branch, a Nth order degradation model, and techniques to reduce computational complexity. We also examine the effect of image restoration on extended training for achieving robust performance in downstream tasks.