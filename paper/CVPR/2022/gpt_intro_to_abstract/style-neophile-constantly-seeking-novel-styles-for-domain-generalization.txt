Convolutional neural networks (CNNs) have greatly advanced visual recognition, but their performance suffers when data distributions in training and testing differ. Domain adaptation is commonly used to address this issue by adapting models from source domains to a known target domain. However, domain adaptation models do not generalize well to unseen domains. Domain generalization (DG) aims to improve model generalization on arbitrary unseen domains. DG typically involves learning domain-invariant features, but the bias towards specific styles in images can hinder generalization. Style augmentation, which reduces bias towards specific styles, has been explored but has room for improvement in terms of style diversity. In this paper, we propose a novel framework that constantly generates diverse and plausible styles and augments training images with them. We utilize style queues and submodular optimization to efficiently generate and maintain styles. Our evaluation on multiple benchmarks demonstrates that our approach achieves state-of-the-art performance in domain generalization, particularly in domains with large discrepancies. Our contributions include a novel approach to domain generalization through constant synthesis of diverse and plausible styles, a framework based on style queues and submodular optimization for effective style generation, and outperforming existing DG techniques on multiple benchmarks.