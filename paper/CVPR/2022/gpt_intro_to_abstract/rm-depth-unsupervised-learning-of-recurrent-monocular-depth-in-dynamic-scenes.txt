Visual perception plays a crucial role in human understanding and perception of the world. Consequently, extensive research has been conducted on scene geometry, leading to the development of various applications such as autonomous vehicles, interactive robots, and virtual and augmented reality. Scene geometry involves estimating depth, camera motion, and optical flow from image sequences. These computer vision tasks are interconnected through geometric constraints.Unlike depth estimation through triangulation, single-image depth estimation is inherently ill-posed due to the existence of multiple possible 3D points along each light ray towards the camera center. Convolutional neural networks (CNNs) have shown great potential in leveraging the relationship between an image and its corresponding scene depth. Recently, unsupervised methods have exhibited better performance compared to early supervised approaches. These methods utilize structure from motion and typically require static scenes without moving objects.To address the limitations of previous approaches, this paper presents a novel unsupervised learning framework called RM-Depth for recurrent monocular depth estimation. The proposed framework jointly predicts depth, camera motion, and the motion field of moving objects without the need for static scenes. RM-Depth utilizes a recurrent modulation unit to enhance feature map fusion between the encoder and decoder, improving single-image depth inference. Additionally, multiple sets of filters are employed for residual upsampling, allowing for improved edge upsampling. Furthermore, a coarse-to-fine framework is proposed for estimating the 3D motion field of moving objects through a warping approach, eliminating the scene rigidity assumption and enabling unsupervised learning using general videos. An outlier-aware regularization loss is introduced to further enhance the unsupervised learning of motion fields.RM-Depth achieves state-of-the-art results on the KITTI and Cityscapes benchmarks while maintaining a small model size. The depth model requires only 2.97 million parameters, representing a significant reduction compared to Monodepth2 and PackNet. The code and trained models are publicly available for further exploration.In summary, this paper presents an innovative unsupervised learning framework for monocular depth estimation, offering advancements in feature map fusion, upsampling, and motion field estimation. The proposed approach achieves impressive accuracy while requiring fewer parameters and eliminates the need for static scenes.