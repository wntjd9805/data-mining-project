The introduction of deep convolutional neural networks (CNNs) and the Transformer model has revolutionized computer vision tasks and natural language processing (NLP) performance, respectively. While CNNs use convolution for local operation, transformers employ self-attention for modeling long-range dependencies. The Vision Transformer (ViT) introduced convolution-free architecture for image categorization, which has been adapted to video by extending the self-attention mechanism along the time axis. To achieve top performance, video transformers require large-scale pretraining on supervised image datasets. However, self-supervised learning has proven effective in eliminating the need for such pretraining in both NLP and image analysis. This work introduces Long-Short Temporal Contrastive Learning (LSTCL), a contrastive formulation that maximizes representation similarity between long and short video clips sampled from the same video. By training the short-clip representation to match the contextual information of the long clip, the model is forced to extrapolate and anticipate the future, improving video classification performance. The paper experiments with two different video transformer architectures, TimeS-former and Space-Time Swin transformer, demonstrating the effectiveness of LSTCL pretraining in outperforming supervised pretraining on large-scale datasets. Overall, the contributions of this paper include the introduction of LSTCL, the demonstration of its effectiveness in video transformers, and the proposal of the Space-Time Swin transformer for spatiotemporal feature learning.