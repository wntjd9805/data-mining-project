Interpretability in machine learning is crucial for understanding the reasoning behind a model's decisions. It increases trust in the models and improves machine learning systems, especially for high-stakes decisions. However, deep learning models such as CNNs have been criticized for their lack of interpretability due to their black-box nature. To address this, various methods have been explored, including model approximation using prototypes. This work specifically focuses on prototypical models and their interpretability. Two recent methods, ProtoPNet and ProtoTree, have been developed to add interpretability to prototypical models by learning class-specific prototypes and class-agnostic prototypes, respectively. However, there are still no efforts to transfer interpretability through prototypes to other models or to shallower networks. This transfer of interpretability could have applications in model compression, few-shot learning, and continual learning.Knowledge distillation is a well-known technique for transferring knowledge from a teacher model to a student network. Previous works have focused on improving the performance and accuracy of the student model without considering interpretability. This work aims to transfer interpretability from an implicitly interpretable teacher model to a student model using a novel distillation method.The proposed method, called Proto2Proto, transfers the interpretability of one prototypical part network to another. The teacher network's interpretability, represented by prototypes, is transferred to a shallower student network. Two novel losses, Global Explanation loss and Patch-Prototype Correspondence loss, are introduced to achieve this objective. The Global Explanation loss transfers global explanations or prototypes to the student, while the Patch-Prototype Correspondence loss ensures that the student's local representations align with the teacher's prototypes.To validate the effectiveness of Proto2Proto, three new metrics are introduced: Average number of Active Patches, Average Jaccard Similarity of Active Patches with Teacher, and Prototype Matching Score. These metrics evaluate the fidelity of the student model in terms of interpretability. The contributions of this work include being the first attempt to transfer interpretability from a prototypical teacher to a student model, proposing novel losses for knowledge transfer, introducing evaluation metrics for interpretability fidelity, and conducting extensive experiments on benchmark datasets to demonstrate the effectiveness of the proposed method.