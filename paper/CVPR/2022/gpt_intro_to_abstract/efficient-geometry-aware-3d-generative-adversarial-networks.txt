Generative adversarial networks (GANs) have made significant progress in generating high-quality images that are visually indistinguishable from real photographs. However, current state-of-the-art GANs operate in 2D and do not explicitly model the underlying 3D scenes. Recent work on 3D-aware GANs has begun to tackle the challenge of multi-view-consistent image synthesis and 3D shape extraction. However, existing 3D GANs still have limitations in terms of image quality, resolution, and 3D reconstruction. This is mainly due to the computational inefficiency of previous 3D generators and neural rendering architectures.In this paper, we propose a novel generator architecture for unsupervised 3D representation learning that addresses the computational efficiency and rendering quality limitations. Our approach combines a hybrid explicit-implicit 3D representation with a dual-discrimination strategy and pose-based conditioning. The hybrid representation improves computational efficiency while maintaining 3D-grounded neural rendering capabilities. We also introduce a training strategy that promotes multi-view consistency and faithfully models pose-correlated attribute distributions. Furthermore, our framework leverages state-of-the-art 2D CNN-based feature generators, such as StyleGAN2, to generalize over spaces of 3D scenes and benefit from 3D multi-view-consistent neural volume rendering.Our contributions include the introduction of a tri-plane-based 3D GAN framework that enables high-resolution geometry-aware image synthesis. We also develop a 3D GAN training strategy that promotes multi-view consistency and faithfully models pose-correlated attributes. We demonstrate state-of-the-art results in unconditional 3D-aware image synthesis and high-quality 3D geometry learning from 2D in-the-wild images.Overall, our approach improves the image quality, resolution, and 3D reconstruction capabilities of 3D GANs, making significant contributions to the field of 3D-aware image synthesis and representation learning.