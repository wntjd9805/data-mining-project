Knowledge distillation (KD) is a technique aimed at transferring the generalization ability of a powerful teacher model to a less-parameterized student model. This is typically achieved by aligning their logits or class predictions for the same inputs. KD has been successful in various applications including object detection, semantic segmentation, and transformer training. However, one limitation of traditional KD is the significant performance gap between the teacher and student models. To address this, several approaches have been proposed, most of which rely on additional supervision from the teacher model's intermediate layers. These approaches often involve the design of complex knowledge representations and require the optimization of hyperparameters. Additionally, the diversity of transferred knowledge makes it difficult to interpret the final improvement in student performance.In this paper, we present a simple knowledge distillation technique called "SimKD" that effectively bridges the performance gap between teacher and student models without the need for elaborate knowledge representations. Our technique is based on the observation that a teacher model's class prediction ability is not solely attributed to its expressive features, but also to its discriminative classifier. We propose training the student model through feature alignment in the preceding layer of the classifier and directly copying the teacher's classifier for student inference. By perfectly aligning the student features with those of the teacher model, the performance gap disappears, and the accuracy of student inference is solely determined by the feature alignment error. Hence, our knowledge transfer process becomes more comprehensible. We show through experiments that a simple â„“2 loss for feature alignment yields impressive results, eliminating the need for careful hyperparameter tuning.As the dimensions of features extracted from teacher and student models may differ, we introduce a projector after the student feature encoder to address this dimension mismatch. This projector incurs minimal cost to the pruning ratio in teacher-to-student compression and enables the application of our technique to various model architectures. In some cases, the pruning ratio can even be increased if the parameter number of the added projector and the reused teacher classifier is less than that of the original student classifier.We evaluate our SimKD technique on benchmark datasets and demonstrate its superiority over existing state-of-the-art approaches across different teacher-student architecture combinations. We also show its applicability in scenarios such as multi-teacher knowledge distillation and data-free knowledge distillation. Overall, our simple technique achieves significant improvements in student model performance while providing a more straightforward interpretation of the knowledge transfer process.