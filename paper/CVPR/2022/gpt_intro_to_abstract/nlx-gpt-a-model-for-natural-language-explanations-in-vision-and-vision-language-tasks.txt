In this paper, we address the need for explaining the decision-making process of deep learning models in the field of vision and vision-language tasks. These models have achieved impressive performance but are considered black box systems. To deploy these models in real-world applications, it is crucial to provide explanations for their decision-making processes to establish trust, ensure accountability, and understand model biases.While previous approaches focused on visual or textual explanations that highlight specific regions or tokens, we propose natural language explanation (NLE) models that provide detailed explanations through human-understandable natural language sentences. Specifically, we focus on explaining models designed for vision and vision-language tasks.Existing NLE models rely on a two-step process, utilizing a vision-language (VL) model to generate an answer and then feeding the answer to a language model to produce an explanation. However, this approach has drawbacks, including increased storage and memory requirements due to the VL-model and a disconnection between the explanation and the reasoning process for predicting the answer.To address these challenges, we propose NLX-GPT, a model that simultaneously predicts an answer and provides an explanation by formulating the answer prediction as a text generation task along with the explanation. This eliminates the need for a separate VL-model and associates the explanation with the reasoning process for predicting the answer.We evaluate our proposed model and compare it to previous works, demonstrating superior performance in terms of metrics. Additionally, our model is 15 times faster and requires fewer memory resources. We also conduct an ablation analysis to understand the contribution of different components of our model to its overall performance.Moreover, we introduce two new evaluation frameworks for NLE models that assess the correctness, reasoning, semantic meaning, and degree of biasness in the generated explanations.Overall, our contributions include the development of NLX-GPT, a model that predicts answers and provides explanations simultaneously, outperforming previous approaches. We also present evaluation frameworks that capture critical aspects of explanation quality.