Vehicle trajectory prediction is a critical component of self-driving cars, as it forecasts future outcomes based on the road structure and traffic participants. Existing models are often trained and evaluated on datasets from specific cities, limiting their generalization to other scenes. This paper presents a method for assessing the robustness of vehicle trajectory prediction models to different scenes through realistic adversarial scene generation. By modifying observed scenes, the authors aim to generate realistic examples where prediction models fail. The proposed approach involves using simple functions to transform scenes, generating multiple new scenes that challenge the models. The authors demonstrate that more than 60% of scenes can be modified to make state-of-the-art methods fail. Real-world locations resembling the generated scenes are found to validate their realism. The study also shows that these generated scenes can be used to improve the robustness of the models. The contributions of this work include highlighting the need for a more in-depth evaluation of the models, proposing an open-source evaluation framework, demonstrating the realism of the generated scenes, and leveraging these scenes to enhance model performance. This research complements existing studies on the robustness of self-driving systems by focusing on scene reasoning capability and providing an adversarial approach that allows extrapolation to new scenes.