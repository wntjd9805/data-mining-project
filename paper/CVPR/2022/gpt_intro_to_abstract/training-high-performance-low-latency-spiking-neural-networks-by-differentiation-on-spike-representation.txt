Spiking Neural Networks (SNNs) have gained attention due to their energy efficiency on neuromorphic hardware. However, training SNNs is challenging because they transmit information through non-differentiable spike trains. To address this issue, the surrogate gradient (SG) method and the ANN-to-SNN conversion method have been proposed but have limitations such as low performance and high latency. In this paper, we introduce the Differentiation on Spike Representation (DSR) method to train SNNs, which overcomes these issues. We treat the firing rate of spiking neurons as spike representation and derive a backpropagation algorithm based on this representation. By avoiding the calculation of gradients at each time step, our method effectively trains SNNs with low latency. We further study the representation error and propose techniques to reduce it, resulting in high-performance low-latency SNNs. Our experiments demonstrate the effectiveness of the DSR method on various datasets and network structures. This paper makes the following contributions: 1) We propose the DSR method, which uses spike representation to train SNNs without the non-differentiability problem; 2) We introduce techniques to reduce representation error; 3) Our model achieves competitive or state-of-the-art SNN performance with low latency.