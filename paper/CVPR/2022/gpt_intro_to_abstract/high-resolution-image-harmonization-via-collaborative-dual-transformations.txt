Image composition involves combining foreground and background elements from different images to create a composite image. However, inconsistencies in appearance between the foreground and background can degrade the quality of the composite image. Image harmonization techniques aim to adjust the foreground appearance to make it compatible with the background. While deep image harmonization methods have made significant progress, they typically only perform low-resolution harmonization, which can result in blurry outputs when upsampled. Training with high-resolution images to address this issue is computationally expensive. Additionally, traditional image harmonization methods rely on hand-crafted statistical features to determine color transformations, which can lead to local inharmony in the harmonization results.To overcome these challenges, we propose a high-resolution image harmonization network called CDTNet, which combines the advantages of pixel-to-pixel and RGB-to-RGB transformations. CDTNet consists of a low-resolution generator for pixel-to-pixel transformation, a color mapping module for RGB-to-RGB transformation, and a refinement module that combines the best aspects of both transformations. We utilize encoder features to learn RGB-to-RGB transformations instead of relying on hand-crafted methods. The CDTNet approach enables high-resolution harmonization without sacrificing computational efficiency or memory usage.In our experiments, we demonstrate that CDTNet achieves visually plausible and harmonious high-resolution harmonization results, outperforming existing methods while consuming fewer resources. Our contributions include being the first to focus on high-resolution image harmonization, achieving deep learning-based color-to-color transformation, and integrating pixel-to-pixel and color-to-color transformations in a coherent end-to-end network.