The development of conversational multimodal systems that can perceive the world and communicate with humans is a long-standing goal of the AI community. Multimodal video captioning is an emerging benchmark in this field, testing the ability to understand and generate natural language descriptions of 'multimodal' video content. However, a major challenge in vision and language learning is the lack of large-scale, manually annotated data for video captions. To overcome this limitation, recent works have pre-trained video-language models on instructional videos, leveraging their aligned speech and visual content. However, these models often lack a decoder for sentence generation, requiring the decoder to be learned from scratch. In this paper, we propose a multimodal video captioning framework that jointly optimizes the encoder and decoder, using a bidirectional generation objective that leverages both present and future utterances. Our framework achieves state-of-the-art results on four video captioning benchmarks and also performs well on other video understanding tasks. We make several contributions, including a novel pretraining objective, training the entire encoder-decoder model, training the encoder from raw pixels and words directly, and achieving strong multimodal video representations.