Guided depth completion is a task in computer vision that involves converting sparse depth observations to dense depth maps with corresponding color images. This task is crucial for various applications, such as 3D scene mapping and 3D object detection for robotic perception and autonomous driving. However, depth-sensing cameras face challenges due to factors like specular surfaces, quantization, occlusion, and noise. To address these challenges, researchers have proposed methods based on deep convolutional neural networks (CNNs). Early approaches focused on sparse depth input, while recent methods utilize multi-modal information, including color images and surface normals, achieving state-of-the-art performance.However, CNN-based methods have limitations due to their static convolutional layers that lack adaptability to diverse spatial contexts and struggle with modeling long-range dependencies. To overcome these limitations, recent attempts have been made to develop content-adaptive CNNs for depth completion. This paper introduces GuideFormer, a dual-branch architecture that leverages attention mechanisms for depth completion. Each branch consists of modified self-attention blocks that capture hierarchical and complementary tokens from sparse depth and color images, allowing the model to better capture adaptive intra-modal dependencies throughout the completion process.The paper also introduces the guided-attention module (GAM), an extension of the standard self-attention mechanism, to enable explicit information exchange between the two branches and capture inter-modal dependencies. Unlike existing methods that use CNNs as the backbone, GuideFormer is the first to propose a fully transformer-based architecture for depth completion. Experimental results on the KITTI benchmark demonstrate the effectiveness of the proposed method, surpassing state-of-the-art approaches.The main contributions of this paper are summarized as follows:1. The proposal of a dual-branch and fully transformer-based architecture for depth completion that effectively learns input-adaptive token representations from sparse depth and color guidance images.2. The introduction of the guided-attention module (GAM) to capture inter-modal dependencies and facilitate information flow between depth and color tokens. GAM proves to be a more powerful method for fusing multi-modal information compared to alternative approaches.3. The demonstration of superior performance compared to recent state-of-the-art methods on the KITTI benchmark, supported by extensive ablation studies and both quantitative and qualitative experimental analyses.