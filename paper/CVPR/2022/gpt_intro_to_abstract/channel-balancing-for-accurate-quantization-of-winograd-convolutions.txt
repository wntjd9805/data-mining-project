Convolutional Neural Networks (CNNs) have become essential for computer vision applications on mobile devices, and lightweight architectures and quantization techniques have enabled their deployment. The use of alternative convolution formulations, such as FFT or Winograd, has also contributed to faster processing. However, when using quantized Winograd convolutions, there is often a drop in quality due to numeric errors. Despite this, the significant speed and power consumption advantages of quantized Winograd convolutions have motivated further research in this direction. This paper focuses on the implementation and optimization of quantized Winograd convolutions with a 3x3 kernel and stride 1. The authors introduce a balanced quantized Winograd algorithm that incorporates a channel balancing operation to mitigate quality drops and quantify the complexity proportions of different stages of the algorithm. The paper demonstrates that the proposed method improves the quality of Winograd quantization and extends its application area. The technique is compatible with existing quantization techniques and can be used in both post-training quantization and quantization-aware training. Experimental results on super-resolution and image classification tasks show the effectiveness of the proposed balancing technique in improving the quality of Winograd quantization. Overall, this work contributes a lightweight, compatible, and universal method for balancing the channel ranges of inputs and filters in quantized Winograd convolutions, enhancing their quality and expanding their application possibilities.