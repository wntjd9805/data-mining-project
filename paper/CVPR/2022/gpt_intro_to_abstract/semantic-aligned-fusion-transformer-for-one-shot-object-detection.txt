Abstract:This paper addresses the problem of few-shot object detection (FSD), which involves learning a generalized model with limited labeled data. Existing approaches often rely on transfer learning or meta-learning, but they suffer from certain limitations and redundancies. In this work, we propose a Semantic-aligned Fusion Transformer (SaFT) that utilizes a novel semantic-aligned attention mechanism to facilitate feature fusion between query and support samples. The attention mechanism enables global feature interactions, improves alignment between query and support, and enhances object detection performance. Our SaFT model achieves promising results in both the PASCAL-VOC and MS-COCO datasets, outperforming state-of-the-art two-stage models. The contributions of this paper include the first proposal-free one-stage model for offline one-shot object detection, a unified attention mechanism to address spatial and scale misalignment, and the incorporation of cross-scale long-range relations for comprehensive meta-knowledge collection. Experimental results demonstrate the superiority of our semantic-aligned fusion approach compared to conventional methods.