This paper introduces a novel method called SE-mantic Energized Generation (SEEG) for generating semantic-aware co-speech gestures. The goal is to improve the realism and naturalness of digital humans by aligning their gestures with speech contents. The existing methods for co-speech gesture synthesis do not explicitly generate semantic gestures or model the relationship between speech and gestures. The challenges in generating semantic gestures arise from the difficulty in mining semantic cues and the lack of temporal alignment between gestures and their corresponding texts. SEEG addresses these challenges by utilizing a two-component framework: the DEcoupled Mining module (DEM) and the Semantic Energized Module (SEM). DEM decouples speech input cues into semantic-relevant cues and semantic-irrelevant cues, allowing for separate processing of semantic and beat gestures. SEM energizes semantic learning by constraining representational similarity and semantic similarity. It uses a semantic prompt gallery and a prompter network to guide the generation of gestures that align with the semantics conveyed by the speech.The main contributions of this work include the proposal of the SEEG framework, DEcoupled Mining, and Semantic Energized Module. The effectiveness of SEEG in generating semantic gestures is demonstrated through subjective metrics and objective human evaluations on different datasets. The results show that SEEG improves the expressiveness of gestures, with significant improvements in semantics. In conclusion, SEEG offers a promising approach for generating realistic and expressive co-speech gestures.