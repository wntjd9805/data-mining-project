Feature attribution is the problem of determining which features or patterns within a neural network's input are important for its output. There are several explanation methods in the literature, but these methods often point to different features as being important. This raises the question of which explanation is correct or if they are revealing the problem in different ways. To compare explanations, some approaches use ground truth annotations on the dataset, but there is no guarantee that what is important for a human is also important for the model. To address this issue, it is necessary to define what it means for a feature to be "important" for an output. One approach is to remove the feature and observe the output behavior. However, this can lead to ambiguities, especially when there are equivalent features where the removal of any of them does not affect the output value. To clarify the concept of importance, desirable properties for an importance assignment method can be formalized using axioms. The axiomatic view provides a framework for evaluating feature attribution solutions and allows for mathematical proofs of compliance with specific axioms. However, practical implementations may break these proofs, and experiments are required to test whether the solutions comply with the axioms. This work presents an experimental framework for evaluating attribution solutions axiomatically. The framework involves generating input features that impose a specific behavior on the network's input/output relationship and evaluating the solutions in terms of desirable properties such as the Null-player axiom, class-sensitivity, and feature-saturation. The experiments aim to reveal properties and drawbacks within existing explanations.