The large computational costs of deep learning have sparked interest in model compression, which aims to obtain smaller models that maintain the accuracy of larger models. Weight pruning is a popular compression method, where weights are set to zero to reduce model size without sacrificing accuracy. This study focuses on weight pruning and its impact on transfer learning, which involves leveraging information from a pretrained task to improve performance on a new task. The study considers two transfer learning variants: full finetuning and linear finetuning. The authors also explore the use of sparsity-aware inference engines and analyze the effect of different pruning methods and task characteristics on transfer performance. The study evaluates top-performing pruning methods and measures transfer accuracy on twelve transfer datasets. The main finding is that sparse models can achieve comparable accuracy to dense models in transfer tasks, but the effectiveness varies depending on the pruning method, transfer approach, model sparsity, and task type. The authors conclude that there is currently no single best pruning method for transfer, but existing methods can achieve significant compression without loss of accuracy, leading to potential speedups.