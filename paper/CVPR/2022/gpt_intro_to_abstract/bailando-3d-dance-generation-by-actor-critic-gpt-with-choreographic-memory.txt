Music-conditioned 3D dance generation is a challenging task with great potential for real-world applications, such as assisting choreographers and driving the performance of virtual characters. However, there are two main challenges: spatial constraint and temporal coherency with music. Existing methods have limitations in addressing these challenges effectively. In this paper, we propose a novel dance generation framework called Bailando. Bailando utilizes a choreographic memory, created using VQ-VAE, to encode and quantize 3D dancing poses. This enables the generation of visually expressive and emotionally infectious dance sequences. Additionally, we introduce a GPT-like network called motion GPT to ensure temporal coherency with music by translating music and pose codes to future pose codes. We enhance the motion GPT with a cross-conditional causal attention layer and use reinforcement learning to further improve temporal synchronization. Our experiments demonstrate that Bailando outperforms existing methods in terms of both automatic metrics and visualization judgements. Code and models will be made available upon acceptance.