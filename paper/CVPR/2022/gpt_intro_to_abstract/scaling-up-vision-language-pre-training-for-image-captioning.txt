Recent advances in image captioning have been attributed to vision-language pre-training (VLP), which is conducted on combined image-text datasets. However, the impact of the pre-training dataset on performance and its correlation with different model settings are unclear. Scale is believed to be important, and previous work has shown benefits of increasing model size. Contrastive image-text pre-training has also been scaled up, but its properties for image captioning are unknown. To study the scaling trend, a large-scale image-text dataset called ALT200M is constructed. Experiments are conducted to scale VLP for image captioning from both the data and model perspectives, resulting in the LEMON model. Results show that larger models benefit more when there is more than 10 million data for pre-training, but with only 3 million data, performance saturates early with increasing model size. The VLP scaling rule presented proves the effectiveness of learning from large-scale data and provides insights on improving performance by increasing both model and pre-training data sizes. The model achieves new state-of-the-art results on several benchmarks.