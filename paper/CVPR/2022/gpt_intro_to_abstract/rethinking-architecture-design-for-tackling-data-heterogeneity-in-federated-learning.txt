Federated Learning (FL) is a research paradigm that aims to train machine learning models on private data distributed over multiple devices while maintaining data privacy. FL allows collaborative machine learning across institutions without compromising privacy. This approach has found applications in domains such as healthcare, learning from mobile devices, smart cities, and communication networks. However, there are fundamental research challenges that need to be addressed for FL to be applicable to real-world data distributions. Existing methods for learning a single global model across non-IID devices face issues such as convergence problems and weight divergence for parallel FL methods, and catastrophic forgetting for serial FL methods. While most efforts focus on optimizing the FL process, this paper proposes a new perspective by considering the choice of architectures in federated models. The hypothesis is that Transformer architectures are well-suited for heterogeneous data distributions due to their robustness to distribution shifts. Transformers have been successful in self-supervised learning and multimodal learning tasks, where heterogeneity is present. To test this hypothesis, a large-scale empirical benchmarking is conducted, comparing different neural architectures across federated algorithms, real-world benchmarks, and heterogeneous data splits. The results indicate that Federated Learning with Vision Transformers (VIT-FL) performs exceptionally well in settings with high device heterogeneity compared to FL with ResNets. This improvement is attributed to the increased robustness of Transformer models to heterogeneous data, reducing catastrophic forgetting. VIT-FL shows immediate improvements without requiring additional training heuristics or hyperparameter tuning. Moreover, VIT-FL can be applied alongside existing optimization-based FL methods to further enhance performance. Therefore, Transformers should be considered a natural starting point for FL problems in future research.