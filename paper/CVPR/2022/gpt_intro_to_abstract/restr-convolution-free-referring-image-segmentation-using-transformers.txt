In recent years, there have been significant advancements in semantic segmentation, but its application to real-world tasks is limited. The task of semantic segmentation struggles with addressing undefined classes and specific entities of user interest. Referring image segmentation has been proposed as a solution to this limitation, as it allows for the segmentation of image regions corresponding to natural language queries. Referring image segmentation presents challenges in comprehending individual entities and their relations expressed in language expressions and leveraging structured and relational information in the segmentation process. Existing methods for referring image segmentation rely on convolutional and recurrent neural networks, but they have limitations in capturing long-range interactions and sophisticated interactions between modalities. To overcome these limitations, this paper proposes ReSTR, the first convolution-free model for referring image segmentation using transformers. ReSTR combines vision and language encoders to extract features and capture long-range interactions within each modality. A multimodal fusion encoder with self-attention layers is used to enable flexible interactions between modalities. ReSTR achieves state-of-the-art performance on four public benchmarks without additional complexities. The contribution of this work lies in the convolution-free architecture, the design of the multimodal fusion encoder, and the achieved performance on referring image segmentation benchmarks.