Pretrained vision and language transformers have achieved impressive performance on multimodal tasks but still lack a deep understanding. This paper focuses on the extent to which these models can conduct unimodal and multimodal compositional reasoning. While humans can easily distinguish visual differences between images with the same words in captions, it remains a challenge for machines. To address this, the paper introduces the Winoground task, where two images and two captions must be correctly matched. Models must effectively encode text and images while synthesizing information across modalities. Inspired by the Winograd Schema Challenge, this task aims to probe the capabilities of vision and language models. Evaluation of various state-of-the-art transformers and RNN-based models shows that their visio-linguistic compositional reasoning falls significantly short of expectations. The paper concludes with an analysis of model performance and the hope that insights from this work will improve future vision and language models.