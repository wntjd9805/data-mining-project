Automating the process of generating realistic 3D human motions from text is the focus of this paper. Existing efforts in this area have been sporadic and unsatisfactory, often limited to one short sentence and producing stationary and lifeless motions. Additionally, the motion lengths are typically fixed, and the available dataset is limited. Three challenges that need to be addressed include variable motion lengths, multiple possible behaviors for a given description, and a wide range of input description forms. To overcome these challenges, a two-stage pipeline is proposed: text2length sampling and text2motion generation. The former estimates the distribution function of visual motion length based on the input text, while the latter generates distinct 3D motions using a temporal variational autoencoder framework. A motion snippet code is introduced to characterize temporal motion semantics. A dedicated dataset, HumanML3D, is constructed, consisting of 44,970 textual descriptions for 14,616 3D human motions. Empirical evaluations on both HumanML3D and KIT-ML datasets demonstrate the superior performance of the proposed approach. The key contributions of this work are the stochastic generation of diverse and realistic 3D motions of variable lengths from text, the ability to handle input texts of varying complexity, and the construction of a large-scale human motion dataset.