This paper addresses the problem of catastrophic forgetting in deep neural networks (DNNs) when adapting to new tasks in open-world scenarios. The authors categorize existing continual learning methods into replay methods, regularization methods, and mask methods. However, most of these methods are tailored for convolutional neural networks (CNNs) and do not fully leverage the characteristics of vision transformers (ViTs), which have recently challenged the primacy of CNNs in computer vision. To address this gap, the authors propose a ViT-backed mask-based continual learning method called MEta-ATtention (MEAT). MEAT inherits the advantages of mask methods, such as dedicated parameters per task and insensitivity to task order, while also introducing innovations specific to ViTs. These innovations include leveraging the architectural characteristics of ViTs, adopting the Gumbel-softmax trick to resolve optimization difficulties, and introducing masks to only a portion of the parameters for efficiency.The proposed method is validated through extensive experiments on various image classification benchmarks using different ViT variants. The results demonstrate that MEAT outperforms state-of-the-art CNN counterparts, achieving significant accuracy improvements of 4.0% to 6.0%, while consuming lower storage costs for task-specific masks.The main contributions of this work are summarized as follows: (1) the proposal of MEAT as the first ViT-backed continual learning method, (2) the introduction of innovations to boost the performance of MEAT, and (3) extensive experimental validation showing the superiority of MEAT over CNN counterparts in terms of accuracy and storage costs.