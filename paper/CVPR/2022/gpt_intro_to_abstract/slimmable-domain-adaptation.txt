Deep neural networks are commonly trained on labeled source data and then deployed on resource-limited edge devices to test unlabeled target data. However, this paradigm leads to a performance degradation due to domain shift. Unsupervised domain adaptation (UDA) has been explored as a solution, aiming to align source and target data in a joint representation space. Unfortunately, existing UDA methods lack the ability to efficiently fit the requirements of various devices in real-world applications. In this paper, we introduce Slimmable Domain Adaptation (SlimDA), a framework that allows for flexible sampling of models with different capacities and architectures from a single trained model. To address the challenges of weight adaptation and architecture adaptation, we propose Stochastic EnsEmble Distillation (SEED) and Optimization-Separated Tri-Classifier (OSTC), respectively. Additionally, we propose an Unsupervised Performance Evaluation Metric to guide model selection under different computational budgets. Experimental results on popular UDA benchmarks demonstrate the effectiveness of our proposed framework, which achieves state-of-the-art performance and preserves performance improvement even with reduced computational complexity. Our contributions include the SlimDA framework, the SEED method, the OSTC for optimization modulation, and the Unsupervised Performance Evaluation Metric.