Synthesizing dynamic 3D (4D) facial expressions is an important task in computer graphics, with applications in 3D face modeling, augmented and virtual reality, animated films, and computer games. While generative neural networks have made significant progress in generating facial animations in 2D, there is limited research on generating facial animations in 3D.To achieve accurate and faithful 3D facial animation, several challenges need to be addressed. First, the identity of the subject should be maintained across time. Second, the applied deformation should correspond to the specified expression or motion and be applicable to any neutral 3D face. Lastly, modeling the temporal dynamics of the specified expression is crucial for realistic animations.Previous approaches have focused on either transferring facial expressions frame-by-frame or animating a 3D face mesh using input signals. However, these methods do not explicitly model the temporal evolution of the expressions.In this paper, we propose a novel approach to animating a face starting from a neutral face and an expression label. Our solution consists of two separate network architectures: Motion3DGAN and S2D-Dec. Motion3DGAN generates a temporally consistent motion of 3D landmarks based on the input label, while S2D-Dec generates a dense 3D face guided by the landmarks motion for each frame. By decoupling the temporal evolution and mesh deformation, we are able to effectively disentangle the identity and expression components.To maintain identity traits stable, the S2D-Dec expands the landmarks displacement to a per-vertex displacement, which deforms the neutral mesh. This approach ensures that structural face parts not influenced by facial expressions remain unchanged, allowing the network to focus on learning expressions at a fine-grained level of detail and generalize to unseen identities.Our main contributions include: (i) a method for generating dynamic sequences of 3D expressive scans from a neutral mesh and an expression label, with high generalization ability to unseen identities and expressions; (ii) the adaptation of a specific GAN architecture for dynamic 3D landmarks generation and the design of a decoder for expressive mesh reconstruction; (iii) a novel reconstruction loss that improves the accuracy of the decoder by weighting the contribution of each vertex based on its distance from the landmarks.Overall, our approach offers a novel solution to generating dynamic 3D facial expressions, with potential applications in various computer graphics domains.