When communicating, individuals often use non-verbal co-speech gestures to enhance the clarity and understanding of their message. Incorporating expressive body movements and gestures in AI agents, such as social robots and digital humans, is essential for effective human-machine interaction. The task of co-speech gesture synthesis involves generating human gestures based on speech audio and transcripts. Traditionally, this task is approached through creating one-to-one correspondences between speech and unit gesture pairs, which is time-consuming and not applicable to unseen speech scenarios. Recent studies have employed deep learning techniques to map audio and text representations to holistic human pose sequences. However, these methods fail to capture micro-scale motions and cross-modal information. To address this, we propose the Hierarchical Audio-to-Gesture (HA2G) pipeline that generates diverse co-speech gestures. Our approach leverages hierarchical cross-modal associations and generates gestures in a coarse-to-fine manner. This is achieved through the Hierarchical Audio Learner, which extracts hierarchical audio features and strengthens their representations through contrastive learning. The Hierarchical Pose Inferer associates multi-level audio features with the hierarchical structure of the human body, predicting the hierarchy of human upper limb movements in a cascaded manner. We also introduce bi-directional GRU generators and a physical regularization to enhance the realism of generated poses. Experimental results demonstrate that HA2G synthesizes realistic and smooth co-speech gestures. Our contributions include the proposed Hierarchical Audio Learner for extracting hierarchical audio features, the Hierarchical Pose Inferer for learning associations between multi-level features and human body parts, and extensive experiments that validate the effectiveness of HA2G compared to state-of-the-art methods.