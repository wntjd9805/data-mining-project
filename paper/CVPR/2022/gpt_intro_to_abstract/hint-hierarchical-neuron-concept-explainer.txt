Deep neural networks have made significant advancements in computer vision and machine learning tasks. However, interpreting the hidden neurons in a human-understandable way remains a challenge. This interpretation is crucial for understanding the reasoning process of deep networks and increasing trust in deep learning. Previous research has focused on explaining deep model predictions by analyzing input data, but the neurons themselves have remained unexplained. Recent efforts have attempted to associate hidden neurons with human-understandable concepts, but these methods rely heavily on human annotations and suffer from scalability issues. Additionally, these methods often overlook the connections among different concepts, missing the opportunity to discover neurons responsible for implicit higher-level concepts. To address these limitations, this paper proposes the HIerarchical Neuron concepT explainer (HINT), which explores hierarchical concepts harvested from WordNet. HINT aims to identify neurons for both low-level and high-level concepts by building a bidirectional association between neurons and concepts. The paper presents a saliency-guided approach to identify representations associated with hierarchical concepts, making HINT low-cost and scalable. Furthermore, classifiers are trained to separate responsible regions for different concepts, and a scoring method is used to evaluate neurons' contributions. HINT is the first attempt to associate neurons with hierarchical concepts systematically and quantitatively, enabling the study of how these concepts are embedded in deep network neurons. HINT identifies collaborative and multimodal neurons and discovers responsible neurons for both higher-level and lower-level concepts. The paper also verifies the neuron-concept associations with a Weakly Supervised Object Localization task and demonstrates the usefulness of HINT in various applications, such as saliency method evaluation, adversarial attack explanation, and COVID-19 classification model evaluation.