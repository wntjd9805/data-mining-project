Speech-driven 3D facial animation is a promising research area with applications in virtual reality, film production, games, and education. Most existing works focus on producing 2D videos of talking heads, but these are not directly applicable to 3D environments such as games and VR. Methods that rely on 2D monocular videos to obtain 3D facial parameters may result in unreliable results. Additionally, using short audio windows to formulate the input can lead to ambiguities in variations of facial expressions. In this paper, we propose a transformer-based autoregressive model for speech-driven 3D facial animation. Our model captures longer-term audio context, effectively utilizes self-supervised pre-trained speech representations, and considers the history of face motions for stability. We demonstrate the superiority of our approach through extensive experiments and a user study, showcasing realistic facial animation and lip sync. Our work addresses the limitations of existing methods and provides a novel solution for speech-driven 3D facial animation.