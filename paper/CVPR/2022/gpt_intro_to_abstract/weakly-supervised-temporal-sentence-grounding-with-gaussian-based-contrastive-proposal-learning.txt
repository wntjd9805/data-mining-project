Temporal sentence grounding, the localization of start and end times of moments described in natural language queries within videos, has valuable applications in video surveillance and summarization. Fully supervised approaches have seen success but require manual annotations, limiting scalability. Weakly supervised methods, which only use video and query during training, have lower annotation costs and increased efficiency. Existing weakly supervised methods use either multiple instance learning (MIL) or reconstruction paradigms but have limitations. They generate the same proposals for all samples, regardless of their contents and difficulty, and heavily rely on randomly selected negative samples, which are often easily distinguished. To address these limitations, we propose Contrastive Proposal Learning (CPL), which generates content-dependent proposals and mines negative samples from easy to hard within the same video. We use multiple learnable Gaussian functions to generate positive and negative proposals and introduce the entire video as a reference point. We also employ a controllable easy-to-hard negative proposal mining strategy. Our experiments on Charades-STA and ActivityNet Captions datasets show that CPL outperforms existing weakly supervised methods.