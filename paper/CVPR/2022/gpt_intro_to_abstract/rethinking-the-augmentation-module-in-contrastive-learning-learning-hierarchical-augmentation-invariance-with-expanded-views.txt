Contrastive learning has proven effective in learning meaningful visual representations without the need for human annotations. Previous methods have considered two views transformed from the same example as a positive pair and other examples as negative samples. However, recent works have shown that useful visual representations can be learned without the use of negative pairs or large batch sizes. The most essential component of contrastive learning is the data augmentation module, which aims to achieve representational invariance through a set of augmentations. Previous studies have identified the composition of multiple types of augmentations as crucial for contrastive learning. However, the pre-determined choices of augmentation types and strengths have limitations. This paper addresses these limitations by proposing a generic method that considers the "where" and "what" of contrast. First, the paper introduces hierarchical augmentation invariance to treat different augmentations differently and distribute fundamental invariances more widely while restricting insignificant invariances to deeper layers. Second, the paper expands the contrast content with augmentation embeddings, which augment the original labels with input transformations and help reduce unnecessary invariance and recover lost fine-grained information. The proposed method is applied to baseline contrastive learning architectures and evaluated on various benchmarks, showing consistent improvements in performance on downstream tasks.