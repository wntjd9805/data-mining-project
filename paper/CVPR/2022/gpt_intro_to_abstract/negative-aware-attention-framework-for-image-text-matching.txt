Image-text matching is a fundamental task in computer vision and natural language processing, aiming to bridge the semantic gap between images and text. Existing approaches either focus on global-level matching or local-level matching. Attention-based local-level matching has gained popularity, where word-region alignments are discovered to compute image-text similarity. However, these methods often underestimate the importance of mismatched textual fragments, leading to false-positive matching. In this paper, we propose a negative-aware attention framework (NAAF) that explicitly considers both matched and mismatched fragments for accurate image-text matching. NAAF consists of a two-branch matching module and an iterative optimization method. The module utilizes both negative and positive attention masks to measure the dissimilarity of mismatched fragments and the similarity of matched fragments. The optimization method mines the mismatched fragments by learning the optimal boundary between matched and mismatched distributions. Experimental results on Flickr30K and MS-COCO benchmarks demonstrate that NAAF outperforms compared methods in image-text matching.