Our study introduces a CLIP-Driven Referring Image Segmentation (CRIS) framework for the task of referring image segmentation, which aims to locate specific regions in an image based on textual input. We propose a vision-language decoder to propagate semantic information from text to pixel-level visual features, and employ contrastive learning to align text and pixel-wise features. Referring image segmentation is distinct from semantic and instance segmentation, as it focuses on finding a particular region rather than pre-determined categories. Previous approaches have concatenated text and visual features, but lacked effective interaction between the modalities. Our framework leverages the knowledge from the CLIP model, which has achieved state-of-the-art results in image-level tasks, to enhance cross-modal matching. We present experiments on three benchmarks, which demonstrate significant improvements in performance compared to previous methods. The contributions of our study include the CRIS framework, the vision-language decoder, and the text-to-pixel contrastive learning.