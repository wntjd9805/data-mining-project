The modeling and tracking of human hands have been extensively studied in computer vision due to the significant role hands play in human interaction with the physical world. Accurate and robust solutions to these problems have the potential to enable various applications such as human-robot interaction, prosthetic design, and virtual and augmented reality. Previous research efforts have heavily relied on the MANO hand model, which is controlled by shape and pose parameters but lacks resolution and texture coordinates for surface color representation. Similarly, the modeling and tracking of human bodies have utilized parametric meshes like the SMPL model, which also suffers from similar limitations. Recently, approaches based on implicit representations have shown promise in capturing both shape and appearance for modeling human bodies. However, the application of implicit representations to articulated objects like the human hand and their generalization to unseen poses is yet to be explored. In this paper, we propose LISA, the first neural model of human hands that can accurately capture hand shape and appearance, generalize to different hand subjects, provide dense surface correspondences, be reconstructed from images in real-world settings, and easily animated. We present a training method for LISA using a large dataset of multi-view RGB image sequences annotated with coarse 3D poses of the hand skeleton, minimizing shape and appearance losses. The disentangled representations in LISA allow fine control over specific aspects of the model. Experimental evaluation demonstrates the superiority of LISA over baselines in hand reconstruction from 3D point clouds and RGB images.