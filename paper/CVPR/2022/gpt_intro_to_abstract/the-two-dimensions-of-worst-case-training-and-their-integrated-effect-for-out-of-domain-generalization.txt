The introduction of this computer science paper discusses the challenges of out-of-distribution (OOD) generalization testing in deep learning models. Despite the success of deep learning on i.i.d data, OOD scenarios lead to underwhelming results. Previous works have explored strategies such as adding salient patterns, using adversarial attacks, or employing domain adaptation/generalization to improve OOD performance. However, these approaches have limitations and do not fully address the issue of models' incapability to learn what humans consider important in the data. To tackle this problem, the paper proposes a new training heuristic inspired by the idea of learning beyond one's comfort zone. The authors introduce the concept of "Worst-case along Two Dimensions" (W2D) as a simple and general method that can be applied to different deep learning architectures, optimizers, losses, or regularizations. By combining worst-case training at the feature dimension and worst-case training at the sample dimension, the W2D algorithm aims to improve the model's ability to learn hard-to-learn concepts.The paper is organized as follows: Section 2 provides background information on worst-case training along the two dimensions and its effect on OOD generalization. Section 3 presents the W2D algorithm, and Sections 4 and 5 demonstrate its empirical strength. Several related discussions are offered in Section 6, followed by the conclusion in Section 7.