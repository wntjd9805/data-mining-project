Surface reconstruction from 3D point clouds is a challenging task in computer vision, even with the aid of deep learning models. The standard approach involves training a neural network to learn a Signed Distance Function (SDF) from point clouds or ground truth signed distances, and then using the learned SDF to reconstruct surfaces. However, capturing local geometry details remains difficult when training the SDF to capture a global shape prior. To address this, recent methods have focused on learning local SDFs from local regions. These approaches split the global shape into overlapping or non-overlapping parts and learn a local region prior as a local SDF. While this improves generalization to unseen local reconstruction targets, the coverage of the learned local prior is limited, limiting its overall generalization ability. To overcome this limitation, we propose the use of Predictive Context Priors for highly accurate surface reconstruction from point clouds. Our method involves training a neural network to represent local SDFs of local regions in a large dataset of point clouds. During surface reconstruction at inference time, we specialize the pre-trained local context prior into a Predictive Context Prior. This allows us to predict query locations for the pre-trained local SDF, matching the learned local context prior to fit the given point cloud more accurately. This results in a global SDF for the specific point cloud. Notably, our method does not require ground truth signed distances or normals in the training process, nor does it require any additional post-processing steps. Our contributions include introducing a novel neural network architecture using Predictive Context Priors to learn SDFs for surface reconstruction from point clouds, demonstrating improved generalizability of a pre-trained local context prior through Predictive Queries, and achieving state-of-the-art results in surface reconstruction for both single shapes and complex scenes in popular benchmarks.