The Variational Autoencoder (VAE) is a successful generative model that incorporates a deep non-linear generative process. The VAE's deep inference network enables fast test time inference through a single feed forward pass, known as amortized inference. However, recent studies have shown that the accuracy of posterior approximation by the amortized inference network is often lower than that of other variational inference techniques. To address this, two common approaches are suggested: increasing the network capacity of the inference model or using a semi-amortized approach that combines the VAE's amortized inference with additional SVI steps at test time. These approaches have their own limitations, such as increased model complexity and computational overhead. In this paper, we propose a novel approach that retains the computational benefits of amortized inference but considers a random inference model. Specifically, we assume that the mean and variance functions of the variational posterior distribution are distributed as Gaussian processes (GPs). This idea is motivated by the suboptimality of the VAE's amortized inference network and the deviation of the VAE's variational posterior distributions from true posteriors. We model this deviation as uncertainty and treat it using a Bayesian neural network (GP), resulting in a faster and more accurate amortized model through principled uncertainty marginalization. Additionally, the Bayesian treatment allows us to quantify the discrepancy in approximation, serving as indicators for the quality of posterior approximations. Our inference approach is significantly faster than semi-amortized methods, requiring only a single feed forward pass through the GP posterior marginalized inference network. We demonstrate that our approach achieves higher test data likelihood scores compared to state-of-the-art semi-amortized approaches and high-capacity flow-based encoder models on various benchmark datasets.