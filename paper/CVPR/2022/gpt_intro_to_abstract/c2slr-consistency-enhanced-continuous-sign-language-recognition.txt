This paper introduces the field of video-based continuous sign language recognition (CSLR) and discusses the dominance of deep learning techniques in CSLR modeling. The paper explores the components of deep-learning-based CSLR models, including the visual module, sequential module, and alignment module. The main objective function used to train CSLR backbones is the connectionist temporal classification (CTC) loss. However, using only the CTC loss can lead to insufficient training and less accurate recognition results. To address this issue, the paper proposes two novel auxiliary constraints: spatial attention consistency (SAC) and sentence embedding consistency (SEC). The SAC constraint aims to enhance the visual module by leveraging pose keypoints heatmaps to guide a spatial attention module. This helps the visual module focus on informative regions, resulting in richer visual features. The SEC constraint aims to strengthen the cooperation between the visual and sequential modules by aligning their features at the sentence level. This enhances the representation power of both the visual and sequential features. The paper presents these two constraints as novel methods to improve CSLR backbones and achieve more accurate recognition results. Experimental results demonstrate that the proposed consistency-enhanced CSLR (C2SLR) model, which incorporates the SAC and SEC constraints, achieves state-of-the-art or competitive performance on three benchmarks. Importantly, the C2SLR model can be trained in an end-to-end manner, making it a practical and efficient solution for sign language recognition.