Collaborative training schemes, such as federated learning, have gained popularity for applications that prioritize data privacy. Split Federated Learning (SFL) is a recent scheme that combines the benefits of Split Learning (SL) and Federated Learning (FL), offering advantages in computation reduction and memory usage compared to FL. In SFL, a neural network model is split into a client-side model and a server-side model, allowing clients to operate on private inputs using their client-side model and pass intermediate activations to the server. However, SFL is reported to be vulnerable to Model Inversion (MI) attacks, where the server can reconstruct clients' raw data from intermediate activations. While prior works have focused on MI resistance during inference, achieving resistance during training is more challenging. To address this, we propose ResSFL, a two-step Split Learning framework that is resistant to MI attacks. The first step involves attacker-aware training to develop an MI-resistant feature extractor, which is then used to initialize client-side training in the second step. Our contributions include the novel two-step resistant SFL framework, the attacker-aware training method, and the use of resistance transfer through transfer learning to protect early-epoch vulnerability and reduce computation cost. We demonstrate that ResSFL successfully mitigates training-time MI attacks while achieving good accuracy in SFL.