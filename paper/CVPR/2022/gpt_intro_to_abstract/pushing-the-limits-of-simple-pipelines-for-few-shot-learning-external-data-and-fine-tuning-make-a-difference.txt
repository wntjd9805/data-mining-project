Mainstream supervised deep learning has achieved impressive results in applications with large annotated datasets. However, this is not always feasible in many applications due to limited data or the high cost of human annotation. As a result, few-shot learning (FSL) has gained significant attention as it aims to mimic the human ability to learn new concepts with minimal training examples. FSL research has explored various approaches such as metric learning, meta-learning, program induction, and neural optimization, among others.While most FSL research focuses on meta-learning, this paper suggests that transfer learning from pre-trained models can be more effective, especially when combined with stronger architectures like the vision transformer (ViT). The paper compares the performance of the ProtoNet + ViT backbone with contrastive language-image pretraining (CLIP) to ProtoNet with randomly initialized ViT. The results show the importance of pre-training and the potential of simpler baselines in achieving comparable performance to more sophisticated approaches.In this study, the authors extend previous research by investigating factors that influence the performance of simple few-shot pipelines. They specifically analyze the impact of pre-training data, neural architecture, and fine-tuning during meta-test. The influence of source data in FSL has been explored in other domains but not extensively in the computer vision FSL research community. Therefore, the authors examine the impact of exploiting unsupervised pre-training on external data, which significantly improves few-shot learning tasks. They also question whether the current problem definition of FSL is the most effective and propose alternative approaches.In addition to source data, neural architecture is another significant factor in FSL research. Common FSL benchmarks often use small networks that may not reflect state-of-the-art computer vision architectures. The authors investigate the benefits of using vision transformers and larger pre-training datasets to enhance few-shot performance.The study also explores the issue of fine-tuning during model deployment for individual tasks. Some studies advocate for fine-tuning, while others argue that a fixed feature representation should suffice. The authors suggest that fine-tuning is crucial when deploying foundation models to out-of-distribution tasks. They propose an algorithmic improvement to fine-tuning by automating learning rate selection through validation, resulting in a more performant pipeline for cross-domain few-shot learning.In conclusion, this paper makes advancements in few-shot learning by analyzing design choices in a simple pipeline rather than developing new algorithms. The authors investigate the impact of pre-training, the benefits of transformer architectures, and the necessity of fine-tuning.