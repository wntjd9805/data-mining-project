This paper introduces the concept of a Deformable Attention Transformer (DAT) as a powerful backbone network for various computer vision tasks. The authors address the limitations of excessive attention computation in visual recognition by proposing a data-dependent attention pattern that is more flexible and efficient. Existing approaches have used hand-crafted attention patterns to reduce computational complexity, but they may drop important keys/values while retaining less important ones. Inspired by the success of deformable receptive fields in convolutional neural networks, the authors explore the idea of a deformable attention pattern in Vision Transformers. However, a naive implementation would result in high memory and computation complexity. To overcome this, the authors present a simple and efficient deformable self-attention module that introduces a linear space complexity and enables the learning of shared groups of sampling offsets for all queries. This allows the candidate keys/values to be shifted towards important regions, enhancing the flexibility and efficiency of the original self-attention module. The proposed DAT model is evaluated on ImageNet, ADE20K, and COCO datasets, and demonstrates superior performance compared to competitive baselines, including Swin Transformer. The improvements are observed in image classification, semantic segmentation, and object detection tasks, with margins ranging from 0.7 to 2.1. Overall, this paper presents a novel approach to incorporating deformable attention patterns in Vision Transformers for improved performance in computer vision tasks.