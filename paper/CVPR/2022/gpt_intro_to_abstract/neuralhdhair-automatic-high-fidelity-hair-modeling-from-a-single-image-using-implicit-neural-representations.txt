The paper introduces a novel automatic monocular hair modeling framework called NeuralHDHair, which aims to reconstruct high-fidelity 3D hair models from a single image. The existing methods for hair modeling cannot meet the requirements of human digitalization in terms of flexibility, simplicity, and realism. This work addresses the limitations of data-driven methods and deep-learning based methods by utilizing a learning-based approach. The complex and intricate structure of hair makes it challenging to reconstruct a high-fidelity 3D hair model from a single view. Existing methods typically estimate a 3D orientation field based on a 2D orientation map extracted from the input image and then synthesize hair strands. However, this two-step mechanism leads to the loss of hair details and inefficient hair growth algorithms. The authors propose a fully automatic and efficient hair modeling method that can reconstruct a 3D hair model with fine-grained features. The authors leverage the use of implicit functions to represent and infer 3D shapes. They introduce IRHairNet, which uses a voxel-aligned implicit function and a high-resolution luminance map to extract global and local information for high-fidelity hair modeling. The hair growth process is handled by GrowingNet, a deep learning-based method that leverages a local implicit grid representation to generate strand models with arbitrary resolution.The proposed NeuralHDHair framework outperforms existing monocular hair reconstruction methods in terms of quality and efficiency. Extensive experiments and comparisons show the effectiveness of the proposed framework. The main contributions include the introduction of a novel monocular hair modeling framework, the development of IRHairNet and GrowingNet, and the achievement of high-quality hair modeling results.