Post-sync, or dubbing, is the process of re-recording dialogue in the film industry to improve audio quality. Automatic audio-visual dubbing involves generating synchronized video and speech by applying a text-to-speech (TTS) system and modifying frames to match the audio. This approach is challenging, particularly in generating photorealistic video. In contrast, this paper proposes a visually-driven TTS model called VDTTS that takes both text and facial video frames as inputs to generate speech that matches the facial movements of the video. The model retains the original prosody and synchronizes the audio with the video. The model consists of text and video encoders, a multi-source attention mechanism, and a vocoder. Experimental results on GRID and VoxCeleb2 datasets demonstrate the effectiveness of the proposed method, achieving state-of-the-art video-speech synchronization and recovering aspects of prosody in the generated speech. Demo videos are available for further exploration.