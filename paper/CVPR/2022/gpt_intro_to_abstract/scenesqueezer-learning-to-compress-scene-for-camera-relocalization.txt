Abstract:Visual localization is a key problem in 3D computer vision with applications in autonomous driving, augmented reality, and indoor navigation. Traditional methods for visual localization involve building 3D point cloud models of the scene, which requires significant memory usage for large-scale environments. To improve scalability while maintaining accuracy, previous methods have proposed compressing the 3D scene models. These methods can be categorized into three groups: descriptor compression, learning-based methods, and point selection. However, existing point selection criteria do not directly consider the impact on final pose estimation accuracy.Motivated by hierarchical localization methods, we propose a novel hierarchical strategy for 3D scene compression. Our approach compresses the 3D scene model in three levels. At the coarse level, we divide the scene into clusters to compress individually. In the middle level, we use a differentiable point selection method based on 2D observations and influence on pose estimation accuracy. This is done by designing a multi-view observation fuser that learns to extract and aggregate features for each 3D point. Finally, at the finest level, we employ differentiable quantization to compress feature descriptors further.Our experimental results demonstrate that our method outperforms existing compression-based localization methods. The contributions of this paper include the proposal of a hierarchical pipeline for scene compression inspired by hierarchical localization methods. We also present a novel differentiable point selection method that incorporates pose estimation accuracy in the selection process. Experimental evaluations on benchmark datasets show the superiority of our approach over previous methods.