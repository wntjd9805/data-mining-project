Deep neural networks (DNNs) are widely used in safety-critical applications but are vulnerable to adversarial attacks that add imperceptible perturbations to mislead the DNNs. This paper focuses on black-box attacks, which are more applicable to real-world scenarios where attackers have no access to model structures and parameters. Previous feature-level transfer-based attacks have limited transferability due to inappropriate neuron importance measures. To address this limitation, the paper proposes the Neuron Attribution-based Attack (NAA) method, which accurately measures neuron importance using the neuron attribution method. An approximation approach is devised to reduce computation costs, and each neuron is weighted based on its attribution results to minimize the weighted feature output. Experimental results demonstrate the effectiveness and efficiency of NAA in attacking both undefended and defended models.