Face recognition has become increasingly important in various applications, including face authentication on mobile devices. The advancements in face recognition have been facilitated by large-scale training datasets, improved network architectures, and the development of margin-based and mining-based loss functions. However, training on large-scale datasets poses challenges such as storage and computational limitations, resulting in increased training time or failure. To address this issue, this paper proposes a sparsely updated fully connected layer called Partial FC (PFC) for training large-scale face recognition. PFC reduces the updating frequency by sampling only parts of the negative class centers in each iteration, thus reducing computational requirements and the probability of inter-class conflict. Extensive experiments on different training datasets and backbones demonstrate the efficiency, robustness, and effectiveness of PFC under a range of sampling ratios. PFC achieves state-of-the-art performance on various benchmarks, demonstrating its accuracy in face recognition tasks. The proposed approach offers an efficient solution for training large-scale face recognition models, making it a promising technique for real-world applications.