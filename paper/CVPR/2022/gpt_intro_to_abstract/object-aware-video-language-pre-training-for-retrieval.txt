In this paper, we address the problem of learning scalable video-text representations for retrieval by leveraging both visual and textual clues, as well as the semantic alignment between these two modalities. While existing large-scale contrastive-based pre-training methods have achieved great performance advances, we identify the lack of regularization on fine-grained semantic associations as a hindrance to further improvements. Inspired by the progress in image-text pre-training, we propose a novel approach called Object-aware Transformer (OA-Trans) to enhance video-text pre-training. We introduce two key designs in our method. Firstly, we balance matching recall and efficiency by combining whole frames with a single anchor frame that encodes object information. Instead of replacing all video frames with extracted object regions, we only extract object regions in the anchor frame and mask out the non-object regions. Secondly, we introduce a novel 4-stream object-aware contrastive (OAC) loss, which contrasts the raw video stream with the object tags stream and the raw text stream with the anchor frame stream. Our proposed OA-Trans model achieves significant improvements in Recall@1 on four benchmarks with three downstream tasks. We are the first to successfully develop an object-aware dual encoder model for end-to-end video-language pre-training. Furthermore, our approach alleviates the computational cost of extracting object boxes by unifying whole frames with a masked anchor frame. We also design a unique object-aware contrastive loss based on our input streams to improve the correlation between visual and textual information.