This paper discusses the issue of unfairness in AI systems, where individuals are treated unequally based on protected attributes such as ethnicity, gender, and nationality. The existing approaches to mitigate these biases can be classified into pre-processing, in-processing, and post-processing methods. Pre-processing methods focus on mitigating biases in the training dataset, while in-processing methods introduce fairness-related penalties during the learning process. However, these approaches require retraining or fine-tuning the target models, which may not be feasible if the models are already deployed without access to their training set. Post-processing methods, such as the proposed boosting method, aim to produce a new classifier with equal accuracy in different groups but cannot ensure statistical and predictive parity.There is a need for a practical approach to mitigate unfairness in deployed models without changing their parameters and structures. This paper proposes a method called Fairness-aware Adversarial Perturbation (FAAP) to achieve this goal. FAAP leverages adversarial training to learn fairness-related attributes based on latent representations from deployed models. It also trains a generator to perturb input data to prevent the extraction of fairness-related features by the deployed models. This design effectively decorrelates fairness-related/protected attributes from predictions.Extensive experiments demonstrate the superior performance of FAAP in mitigating unfairness. Evaluation on real-world commercial APIs also indicates the transferability of FAAP, highlighting its potential in the black-box scenario. Overall, this paper presents a practical approach to improve fairness in deployed AI models without altering their parameters and structures.