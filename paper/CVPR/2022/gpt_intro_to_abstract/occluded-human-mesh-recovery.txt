Accurate estimation of 3D human meshes from single images is valuable for various applications in computer science, including modeling human-scene interactions, understanding human behavior, and AR/VR and robotics. While recent approaches have excelled in recovering human meshes from single-person images, the task becomes more challenging when dealing with complex real-world scenes containing multiple occluded individuals. The existing methods often make simplifying assumptions, such as expecting a single subject in the input image, which limits their robustness in crowded scenarios with severe person-person occlusion. This paper aims to address the limitations of the single-person assumption by proposing a method for human mesh recovery in multi-person scenarios. The current methods for human mesh recovery can be categorized as top-down and bottom-up approaches. Top-down methods simplify the problem by reducing it to the recovery of a single human mesh, relying on a person detector to detect individual bounding boxes for each person in the image. These methods are less sensitive to scale variations among subjects and can achieve accurate mesh alignment. On the other hand, bottom-up methods predict meshes for all subjects in the input image simultaneously but are constrained by a fixed input resolution. To overcome these limitations, this paper introduces a top-down approach called OCHMR, which predicts multiple meshes from the input bounding box. The method leverages image spatial-context in the form of body-center maps, which are helpful for reasoning about occlusions. The authors propose a novel Context Normalization (CoNorm) block to process the global and local center maps and inject contextual information at multiple depths in the network. Compared to early fusion methods, CoNorm effectively utilizes contextual information from the image. OCHMR is shown to outperform both top-down and bottom-up methods on various datasets, achieving state-of-the-art results on occluded scenarios. The method is also resilient to noisy body center estimates and demonstrates robust 3D reasoning using multi-person losses.