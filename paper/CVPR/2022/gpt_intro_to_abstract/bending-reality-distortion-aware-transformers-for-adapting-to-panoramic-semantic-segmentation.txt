Panoramic 360â—¦ cameras have gained significant attention in various fields, including automated vehicles and augmented/virtual reality displays. Unlike narrow-angle pinhole cameras, panoramic images offer omni-range perception, benefiting the detection of objects and elements in different scenes. However, performing semantic segmentation on panoramic images is challenging due to image distortions and the scarcity of labeled data. Previous convolution and attention-based models have attempted to mitigate these challenges but are suboptimal in handling pinhole-to-panoramic data deformations and establishing long-range contextual dependencies. To address these issues, we propose Trans4PASS, a Transformer architecture for Panoramic Semantic Segmentation. Our approach incorporates a Deformable Patch Embedding (DPE) and a Deformable MLP (DMLP) module to handle distortions and improve global context modeling. Additionally, we address the mismatch between pinhole and panoramic domains through unsupervised domain adaptation (UDA) using the Mutual Prototypical Adaptation (MPA) method. Our solution achieves state-of-the-art performance on benchmark datasets and demonstrates the capability for generalization to diverse scenarios. In summary, our contributions include distortion-aware transformers, dual-domain prototypical knowledge distillation, and significant improvements in performance on benchmark datasets.