Multimodal learning, which incorporates data from multiple sensors, has shown advantages in improving performance in various tasks such as action recognition, speech recognition, and visual question answering. However, recent studies have found that multimodal models may underperform compared to unimodal models in certain situations. This imbalance in optimization occurs when one modality dominates the learning process and hinders the optimization of the other modality. To address this issue, we propose the On-the-fly Gradient Modulation (OGM) strategy, which dynamically controls the optimization process of each modality to provide more focus on the under-optimized modality. Additionally, we introduce Generalization Enhancement (GE) through the application of Gaussian noise to improve the generalization ability of the model. Our proposed method, OGM-GE, consistently improves the performance of under-optimized unimodal representations in various multimodal tasks. We contribute by identifying the optimization imbalance phenomenon, proposing the OGM-GE method, and demonstrating its effectiveness in improving multimodal learning.