Abstract:In this paper, we introduce the concept of Visual Grounding (VG) in the context of 3D scenes. VG aims to localize target objects in a scene based on object-related linguistic descriptions. We highlight the increasing attention that the 3D VG task has received due to its wide applications in autonomous robots and human-machine interaction. However, localizing referred target objects in 3D scenes poses challenges due to the irregular and large-scale nature of point clouds. Existing 3D VG methods primarily follow a two-stage pipeline of detection and matching. However, the separation of these stages limits the accuracy and efficiency of the methods. We propose a new approach called 3D Single-Stage Referred Point Progressive Selection (3D-SPS) to address these limitations. Our method involves progressively selecting keypoints guided by the language description to localize the grounding target. We design a Description-aware Keypoint Sampling (DKS) module to focus on language-relevant keypoints and a Target-oriented Progressive Mining (TPM) module to fine tune the target selection. We utilize self/cross-attention mechanisms to model intra/inter-modal relationships and fuse the keypoint and point features of the scene for global localization perception. Our 3D-SPS method achieves state-of-the-art performance on benchmark datasets and establishes the first investigation of single-stage 3D VG. The code for our method is available on GitHub.