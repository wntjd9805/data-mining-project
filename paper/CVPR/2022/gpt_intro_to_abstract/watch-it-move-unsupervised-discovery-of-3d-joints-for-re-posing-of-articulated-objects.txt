Using images to infer both the appearance and functional structure of real-world objects is a fundamental goal in computer vision. This not only has practical applications in rendering and manipulating objects in the metaverse, but also pushes the boundaries of learning from data without direct supervision. Significant progress has been made in capturing the appearance and synthesizing novel views for static scenes, as well as capturing and reenacting motion in dynamic scenes. However, reposing an articulated object, which involves explicitly manipulating its pose, requires knowledge of joint locations and how different parts interact. Existing methods for predicting joint locations rely on expensive annotations or predefined models. In this paper, we propose an approach for reposing articulated objects using a single multi-view video and corresponding foreground masks, without any additional supervision or prior knowledge of the object's structure. Our approach treats the articulated object as a set of posed parts connected by joints, and uses indirect supervision for joint locations through image reconstruction loss. Inspired by neural implicit representations, our approach predicts the color and signed-distance function of any 3D point, allowing us to generate desired frames by volumetric rendering. We also explicitly learn certain properties of the object by modeling it as a set of ellipsoids, optimizing their geometric properties for each frame. Additionally, we estimate a residual with respect to this explicit part-based representation to improve accuracy. Our method is capable of reposing articulated objects from a single multi-view video sequence and corresponding foreground masks, without prior knowledge or supervision annotations. It can manipulate the pose of the object by applying appropriate roto-translations to the joints. Our results demonstrate that our method outperforms category-specific approaches that rely on prior knowledge. It is the first method to learn a re-poseable shape representation from multi-view videos and foreground masks without additional supervision or prior knowledge of the structure. Furthermore, it can discover the number and location of joints without annotations and can be applied to previously unseen articulated object categories.