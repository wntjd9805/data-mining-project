This paper introduces a method called BodyMap, which establishes accurate dense correspondences between a 2D image of a clothed human and the surface of a 3D human body template. The goal is to label every pixel covering the human body with semantic information, enabling various applications like video analysis, image editing, and style transfer. Previous methods have extracted sparse information such as 2D body keypoints or segmentation masks, but dense surface correspondence is necessary for pixel-level understanding. The existing method, DensePose, has limitations including visible seams and discontinuities between body parts, as well as inaccuracies in correspondence estimates. BodyMap addresses these limitations by using a novel transformer-based architecture and leveraging synthetic data to obtain ground-truth dense correspondence. Key contributions include achieving high-definition correspondences for fine details like hair and fingers, a transformer-based architecture that outperforms previous works, and state-of-the-art results on the DensePose COCO dataset. The method can be extended to learn layered representations and predict per-geometry surface correspondences.