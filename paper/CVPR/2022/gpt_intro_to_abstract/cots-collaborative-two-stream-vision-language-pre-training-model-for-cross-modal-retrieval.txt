The pretrain-then-Ô¨Ånetune paradigm has been successful in natural language processing (NLP) and vision-language (VL) tasks. In VL, vision-language pre-training (VLP) has shown significant improvements in various tasks by learning joint image-text representations. Existing VLP models can be divided into single-stream and two-stream models, each with limitations. Single-stream models are computationally expensive and rely on expensive object detectors, while two-stream models lack closer image-text interactions. To address these limitations, we propose a novel COllaborative Two-Stream (COTS) VLP model that retains real-time inference speed and enhances interactions between modalities. COTS introduces three levels of cross-modal interactions, including instance-level interaction, token-level interaction, and task-level interaction. Additionally, we propose an adaptive momentum filter (AMF) module to filter out noisy data in large-scale pre-training data. Experimental results show that COTS achieves the highest performance among two-stream models and is comparable to single-stream models in terms of inference speed. COTS also achieves state-of-the-art results in text-to-video retrieval tasks.