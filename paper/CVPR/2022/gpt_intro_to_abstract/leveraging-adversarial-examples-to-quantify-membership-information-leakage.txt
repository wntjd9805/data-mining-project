The introduction of this paper discusses the significant advancements in machine learning and its applications in various domains. However, it also emphasizes the importance of considering societal aspects such as fairness and safety while leveraging machine learning techniques. The issue of privacy in machine learning is addressed, highlighting the need to protect personal data while utilizing these models. The paper focuses on investigating membership inference attacks (MIAs), where an attacker tries to determine if a sample belongs to the training set of a target model. The authors propose an MIA strategy that achieves similar performance to state-of-the-art methods but without requiring access to training samples. They introduce the concept of adversarial distance and show that their approach is effective in determining membership without the need for additional training data. The contributions of this work include the proposal of a novel MIA strategy, a thorough evaluation of existing MIA methods, and empirical evidence demonstrating the performance of the proposed strategy compared to state-of-the-art methods.