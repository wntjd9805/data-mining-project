Point clouds have become widely used in various applications, such as augmented reality, automatic driving, and industrial robots. Point cloud quality assessment (PCQA) is essential for guaranteeing high-quality point clouds for tasks like virtual/augmented reality. However, PCQA is more challenging than image/video quality assessment due to the complex distortion present in point clouds. Several metrics have been proposed to address PCQA, but they all require reference samples. In many cases, reference samples are not available, making it necessary to study PCQA in a no-reference scenario. To overcome this challenge, we propose a no-reference PCQA method using unsupervised adversarial domain adaptation (UADA). We leverage the large-scale subjective ratings available in image quality assessment (IQA) databases as the source domain and infer point cloud quality using prior knowledge from the image field. We project point clouds into images and employ a hierarchical feature generator and a conditional-discriminative network to minimize domain discrepancy and produce robust features. We introduce a conditional cross entropy loss to ensure that the features are relevant for quality regression. Our proposed model, called image transferred point cloud quality (IT-PCQA), is the first deep-learning-based unsupervised no-reference PCQA metric via domain adaptation. The contributions of this paper include the proposal of IT-PCQA, the use of multi-perspective images for transfer learning, the introduction of a hierarchical feature encoder, and the application of a conditional-discriminative network with a novel loss function. We compare IT-PCQA with multiple full-reference metrics and two no-reference PCQA baselines, and our experimental results demonstrate its superior performance.