Shape representation is a fundamental element of geometry processing and learning algorithms, with different representations offering varying advantages based on the target application. Two primary classes of shape representations are lagrangian (explicit) and eulerian (implicit). In this paper, we present a novel approach that combines the benefits of both representations by incorporating the theory of currents from geometric measure theory. Our proposed representation enables the implicit representation of the surface interiors while maintaining an explicit representation of their boundaries. Lagrangian representations encode shapes using coordinate points or parameterized regions, offering high precision but limiting flexibility in representing shapes with varying topology. On the other hand, eulerian representations encode shapes via functions on a background domain, such as the level set of a scalar function sampled on a grid. However, traditional eulerian representations waste resolution on irrelevant regions away from the level set of interest. Recent advancements in neural implicit representations have addressed this limitation, leveraging the universal approximation and differentiability properties of neural networks to overcome the fixed discretization of background geometry. Nevertheless, most neural implicit representations lack the ability to encode surfaces with boundaries, which are important for manipulation and surface stitching purposes. To address this, we introduce a new neural implicit surface representation that incorporates explicit boundary curves, allowing for the encoding of complex hybrid surfaces. Our representation is based on the theory of currents, where k-dimensional submanifolds are defined by integrating differential k-forms, extending the concept of distributions defined by integration against smooth functions. The use of currents enables convenient optimization over surfaces and maintains linearity of the boundary operator. We utilize the mass norm, derived from Plateau's minimal surface problem, as the primary loss function to encourage the convergence of our neural currents to smooth surfaces. We demonstrate the effectiveness and flexibility of our representation through three applications: efficient computation of minimal surfaces using stochastic gradient descent, reconstruction of arbitrary surfaces from data using customized background metrics and additional loss terms, and encoding families of surfaces parameterized by their boundaries and a latent code. In summary, our contributions include the proposal of a new neural implicit surface representation with explicit boundary curves, a method for computing minimal surfaces using stochastic gradient descent, an approach for surface reconstruction from data using a custom background metric and additional loss terms, and a framework for learning families of surfaces with boundary control and a latent code.