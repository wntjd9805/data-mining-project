Video frame interpolation (VFI) is widely used in various applications to enhance the user experience by increasing the temporal resolution of videos. It synthesizes intermediate frames between consecutive original frames, reducing the need for high frame rate acquisition and improving slow-motion rendering, view synthesis, and rate-quality trade-offs in video coding. Deep learning has revolutionized VFI algorithms, with flow-based and kernel-based methods being the two main categories. However, these methods still face challenges in handling large motions, occlusions, and dynamic textures. Convolutional Neural Networks (CNNs) with limited receptive fields struggle to capture large pixel displacements, while occluded pixels are difficult to estimate accurately. Existing VFI methods also struggle with complex motion characteristics exhibited by dynamic textures. To address these issues, we propose the Spatio-Temporal Multi-Flow Network (ST-MFNet), a novel VFI model that decouples the handling of large and complex motions. ST-MFNet employs a two-stage architecture, with the Multi-InterFlow Network (MIFNet) predicting multi-interflows at multiple scales and the Bi-directional Linear Flow Network (BLFNet) approximating intermediate flows for large motions. In the second stage, the Texture Enhancement Network (TENet), a 3D CNN, performs spatial and temporal filtering to capture longer-range dynamics and predict textural residuals. The model is trained using the ST-GAN methodology to ensure spatial consistency and temporal coherence. Extensive experiments demonstrate that ST-MFNet outperforms current state-of-the-art VFI methods in capturing complex and large motions, as well as various texture types. The contributions of this work include the combination of multi-flow and single-flow based warping, a new CNN architecture for MIFNet, and the use of spatio-temporal CNN and ST-GAN for complex texture interpolation.