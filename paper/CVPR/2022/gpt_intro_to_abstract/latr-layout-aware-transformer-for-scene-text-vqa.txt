Scene-Text VQA (STVQA) is a task that aims to answer questions by utilizing the scene text in images. This requires reasoning over rich semantic information conveyed by vision, language, and scene text modalities. In this paper, we introduce Layout-Aware Transformer (LaTr), a multimodal encoder-decoder transformer-based model for STVQA. We explore the role of language and layout information in STVQA and propose a layout-aware pre-training and architecture. We demonstrate that most questions in current datasets fall into two categories: those that can be answered with just text tokens and those that require text and layout information. Through quantitative evaluation, we show that LaTr can already correctly answer over 50% of the questions with only text tokens. We also propose layout-aware pre-training, which addresses the domain gap and leverages the advantages of pre-training on documents. Our model utilizes a vision transformer for extracting visual features, eliminating the need for an external object detector. Additionally, we perform vocabulary-free decoding, allowing our model to handle out-of-vocabulary answers and overcome OCR errors. Experimental results show that LaTr outperforms state-of-the-art methods on multiple benchmark datasets. The key contributions of our work are: recognizing the importance of language and layout in STVQA, establishing a symbiosis between documents and STVQA through pre-training, addressing the challenge of out-of-vocabulary answers, and advancing the state-of-the-art performance on various datasets.