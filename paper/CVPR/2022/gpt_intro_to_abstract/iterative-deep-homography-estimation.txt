Homography estimation plays a crucial role in various computer vision tasks such as image/video stitching, video stabilization, augmented reality, GPS denied navigation, and multimodal image fusion. Existing approaches can be categorized into photometric-based and feature-based methods. Photometric-based approaches estimate homography from pixel intensities, while feature-based approaches involve feature extraction, feature matching, and homography estimation. Recently, deep homography methods have gained interest due to their superior performance. These methods use VGG-style networks to directly estimate homography, with cascading multiple networks to improve accuracy. However, the fixed number of cascades limits performance improvement. To address this limitation, some approaches combine the Lucas-Kanade algorithm with CNNs. However, these approaches have drawbacks, such as approximation errors and limited training. In this paper, we propose the iterative homography network (IHN) that is completely trainable. We present two versions of IHN: one for static scenes and another for dynamic scenes with moving objects. Both versions can be arranged in single- or multi-scale for efficiency or accuracy. Our experiments show that IHN outperforms existing methods in terms of average corner errors (ACEs) on different datasets. Additionally, IHN-mov improves accuracy by generating an inlier mask similar to RANSAC. Our results demonstrate the effectiveness of IHN and the benefits of the iterative framework. The contributions of this work include the proposal of the IHN network, the identification of the importance of the iterative framework, and the design of IHN-mov for moving-object scenes. Overall, our approach achieves state-of-the-art accuracy, stability, and parameter efficiency in homography estimation.