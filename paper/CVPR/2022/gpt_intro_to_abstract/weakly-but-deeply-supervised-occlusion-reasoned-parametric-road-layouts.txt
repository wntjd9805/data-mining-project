This paper introduces an end-to-end model for understanding road layout from images, which is crucial for applications such as autonomous driving and path planning. While non-parametric representations require labor-intensive supervision, parametric representations provide interpretability for higher-level reasoning. Previous methods estimating parametric layouts also rely on pixel-level supervision in perspective images or handle simple road layouts. In this paper, we propose a model that predicts parametric layouts in top-view using only parametric annotations and achieves state-of-the-art performance in complex road scenarios. We address the challenge of sparse parametric supervision by introducing two intermediate modules for perspective semantics and top-view semantics, which predict occlusion-reasoned layouts. We also develop a simple renderer to convert parametric annotations into occlusion-reasoned semantic annotations, enabling deep supervision. Our method achieves accurate parametric BEV layout and outperforms methods relying on dense perspective-view supervision. We validate our approach on KITTI and NuScenes datasets, achieving state-of-the-art accuracies. The key contributions of this paper include the end-to-end model, the intermediate module design with inductive biases, the deep supervision using cheap parametric annotations, and the state-of-the-art results.