Transformer has been a prevalent network model for various vision tasks, but its large model sizes and high training/inference costs limit its practical usage. Many studies have focused on compressing and searching for more efficient transformer architectures. However, commonly-used searching strategies such as reinforcement learning and evolutionary approaches are resource-consuming. In this work, we propose ViT-Slim, a joint sparse-mask based searching method that incorporates explicit soft masks to indicate the global importance of dimensions in a transformer. This allows for more efficient and accurate searching without the need for BN layers. Our method offers faster training and zero-cost subnet selection. We conduct comprehensive experiments on ImageNet, demonstrating that ViT-Slim can compress up to 40% of parameters and 40% FLOPs without compromising accuracy. We achieve state-of-the-art performance across a variety of ViT compression and search variants. Our contributions include introducing ViT-Slim, exploring structured slimming strategies, performing direct search over pre-trained transformers, and achieving state-of-the-art performance on ImageNet.