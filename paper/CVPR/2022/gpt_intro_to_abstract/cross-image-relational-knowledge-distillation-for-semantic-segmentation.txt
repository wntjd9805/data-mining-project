Semantic segmentation is an important task in computer vision, with applications in autonomous driving, virtual reality, and robotics. While state-of-the-art segmentation networks achieve high performance, they often have high computational costs, making them unsuitable for resource-limited mobile devices. To address this, lightweight segmentation networks have been proposed. This paper focuses on knowledge distillation (KD) to improve the performance of compact student networks for semantic segmentation. KD approaches have been studied extensively for image classification tasks but are less explored for segmentation tasks. Previous research has shown that directly applying classification-based KD methods to dense prediction tasks may not yield satisfactory results due to the structured context among pixels. Recent works have proposed specialized KD methods for semantic segmentation, leveraging spatial pixel correlations or dependencies. However, existing methods overlook cross-image semantic relations among pixels for knowledge transfer. To address this, the paper proposes Cross-Image Relational Knowledge Distillation (CIRKD), which aims to transfer global pixel relationships from a pre-trained teacher network to a student network. CIRKD introduces pixel-to-pixel and pixel-to-region distillation techniques, utilizing a memory bank to model long-range pixel relations. Experimental results on popular segmentation benchmark datasets demonstrate that CIRKD outperforms state-of-the-art distillation approaches, highlighting the value of transferring global pixel relationships in semantic segmentation. The contributions of this work include the proposal of cross-image relational KD, the introduction of pixel-to-pixel and pixel-to-region distillation with a memory bank mechanism, and achieving the best distillation performance among state-of-the-art methods on public segmentation datasets.