This paper introduces a novel approach called Object-Region Video Transformer (ORViT) to explicitly fuse object-centric representations into spatio-temporal representations in video understanding tasks. Unlike previous object-centric methods, ORViT integrates object information into the network starting from the early layers using self-attention. The ORViT block processes information from object bounding boxes and patch tokens, with separate streams for appearance and dynamics, and then re-integrates them into refined patch tokens. The effectiveness of ORViT is evaluated on various video understanding tasks, including action recognition and detection, and it is shown to outperform existing methods. The results validate that incorporating object representations throughout the network improves performance in video understanding tasks.