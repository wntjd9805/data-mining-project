This paper introduces a novel fusion module called Dual Cross-View Spatial Attention (VISTA) for improving the performance of 3D object detection in autonomous driving. The VISTA module utilizes an attention mechanism to capture global spatial context and model pairwise correlations between different views of LiDAR point clouds. Unlike previous fusion methods, VISTA considers local context in both views to achieve accurate multi-view feature fusion. The paper addresses the challenges of learning cross-view correlations and the trade-off between classification and regression tasks in 3D object detection. The proposed VISTA module replaces MLPs with convolutional operators to better handle local cues for attention modeling. The attention variance is constrained to guide the attention module to focus on meaningful regions in complex outdoor scenes. Experimental results on benchmark datasets demonstrate that VISTA-based multi-view fusion significantly improves the performance of 3D object detection algorithms, outperforming state-of-the-art methods in terms of overall performance and safety-critical object categories such as cyclists. The proposed VISTA module is a plug-and-play module that can be easily adopted in advanced target assign strategies.