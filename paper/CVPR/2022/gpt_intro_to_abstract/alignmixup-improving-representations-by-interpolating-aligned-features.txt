Data augmentation is a widely used method in computer vision that increases the quantity and diversity of data by applying transformations to images. However, traditional data augmentation techniques have limitations in exploring beyond the image manifold and combating memorization of training data. Mixup, a recently proposed method, interpolates between two or more examples in the input space or feature space, improving generalization performance and reducing sensitivity to adversarial examples. However, existing mixup methods have drawbacks such as unnatural image overlays and misalignment of semantic features.In this paper, we propose a novel mixup operation called AlignMixup, which focuses on interpolating local structure in the feature space. Instead of directly interpolating in the input space or decoding latent codes, we align the feature tensors of two images, resulting in soft correspondences between features. This allows for the retention of pose from one image and texture from the other, creating a continuous morphing effect. We demonstrate that this alignment-based mixup operation, combined with a vanilla autoencoder, can improve representation learning without the need for decoded images. Our experiments show that AlignMixup outperforms existing mixup methods in various tasks, including image classification, adversarial attack robustness, calibration, weakly-supervised localization, and out-of-distribution detection. We achieve state-of-the-art results on multiple networks and datasets. Our contributions include the introduction of AlignMixup, the advocacy for alignment in the feature space, the use of Sinkhorn distance for efficient alignment, and the demonstration of the benefits of autoencoder-assisted representation learning under mixup training.