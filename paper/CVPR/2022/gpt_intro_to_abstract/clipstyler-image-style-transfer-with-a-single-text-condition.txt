Style transfer is a popular technique in computer vision that aims to transform the content of an image by applying the visual style of another image. Previous approaches, such as neural style transfer, have achieved impressive results by calculating the style loss between the content and style features of the images. However, these methods require a reference style image, which may not always be available to users in practical applications.To address this limitation, researchers have explored using text conditions to convey desired styles. These methods leverage pre-trained text-image embedding models to extract semantic information from the text and transfer it to the visual domain. However, existing approaches often struggle to properly reflect the semantics of the text or are limited to specific content domains.In this paper, we propose a novel image style transfer method that leverages the recently proposed text-image embedding model CLIP. Instead of pixel optimization or manipulating layers, we train a lightweight CNN network to express texture information with respect to text conditions and produce realistic and colorful results. Our method allows for text-driven style transfer regardless of the content image and overcomes the limitations of previous approaches.We introduce several technical innovations in our implementation. First, we use a patch-wise CLIP loss to guide the network, enabling style transfer at the local level and generating more diverse and vivid patch styles. We also propose a threshold regularization to prevent over-stylization in certain patches that may negatively affect network training.Extensive experimental results demonstrate the effectiveness of our approach in transferring a wide range of styles based on text conditions. Our method expands the possibilities of style transfer beyond the reliance on reference style images, opening up new avenues for creative image manipulation.