State-of-the-art generative models in computer science operate in the image space using 2D CNNs. While these models, like Style-GAN, have achieved high photorealism, they lack direct control over the underlying 3D scene parameters such as camera and geometry. Some methods have added camera viewpoint control to image-based GAN models, but these are limited by the quality of 3D consistency. Recent approaches have explored learning GAN models directly in the 3D space, where the generator network synthesizes a 3D representation of the scene as output. However, controlling other scene properties like geometry and appearance independently remains a challenge. The proposed approach, inspired by non-rigid formulations for novel viewpoint synthesis, aims to address this limitation. The approach introduces D3D, a GAN with separate components for geometry and appearance and extends the non-rigid formulation to model multiple instances of a deformable object category. By disentangling geometric deformations from appearance variations, the proposed approach allows for greater control and accuracy. In addition, the formulation enables pose consistency, dense correspondences, and editing of input photographs using D3D. The paper highlights three key contributions: 1) A generative model that disentangles geometry, appearance, and camera pose; 2) A novel training framework for 3D GANs that ensures pose consistency and computes dense correspondences; 3) Editing of real images by mapping them to the GAN space, providing control over camera pose, appearance, and geometry.