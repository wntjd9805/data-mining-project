This paper addresses the challenge of enabling accurate long-term visual understanding in computer vision systems. While current systems can accurately parse visual content in short time periods, they struggle to do so over extended durations. To address this, we propose a memory-based approach called MeMViT (Memory-augmented Multiscale Vision Transformer), which efficiently models long videos.MeMViT achieves efficient long-term modeling by hierarchically attending the previously cached "memory" of the past. Rather than processing or training on the entire long video, we maintain a memory that the model can access for long-term context. This approach significantly improves efficiency compared to traditional methods that increase the temporal support of a video model by increasing the number of frames.To implement MeMViT, we utilize the "keys" and "values" of a transformer as memory. Each layer of the model attends further into the past, resulting in a longer receptive field. Additionally, we train a memory compression module to reduce the memory footprint by learning which cues are important for future recognition.We draw inspiration from how humans parse long-term visual signals, processing signals in an online fashion and associating them with past memory. Our experiments on various datasets demonstrate that augmenting video models with memory and enabling long-range attention significantly improves performance. MeMViT achieves state-of-the-art results in spatiotemporal action localization and action classification datasets.By enhancing our understanding of the long story told by our visual world, MeMViT takes us one step closer to comprehensive visual understanding.