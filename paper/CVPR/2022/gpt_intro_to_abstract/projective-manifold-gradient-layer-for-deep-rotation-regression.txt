Rotation estimation is a fundamental problem in computer vision with various applications. While deep neural networks have shown promise in accurately regressing rotations, the non-Euclidean nature of rotation space poses challenges. This paper addresses the gap between the Euclidean network output space and the non-Euclidean rotation manifold. Previous research has focused on designing rotation representations that are more amenable to learning, but the issue of optimization on non-Euclidean manifolds has been largely ignored. The authors propose leveraging Riemannian optimization and developing a manifold-aware gradient for the backward pass of rotation regression. This gradient incorporates a projection step that addresses the non-bijectivity of the mapping function and the ambiguity of multiple gradients resulting in the same update. The proposed method, termed Regularized Projective Manifold Gradient (RPMG), improves upon existing approaches and can be applied to various rotation representations. Experimental results demonstrate the effectiveness of RPMG across different tasks and rotation representations, including regression on other non-Euclidean manifolds. The contributions of this paper include the introduction of RPMG as a plug-in layer for rotation regression, extensive experiments showcasing its performance improvements, and the potential for application in regression tasks on other manifolds.