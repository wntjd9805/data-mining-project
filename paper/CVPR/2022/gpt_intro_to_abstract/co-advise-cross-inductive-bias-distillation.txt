Convolutional neural networks (CNNs) have revolutionized computer vision, but they have limitations. Recent research has focused on replacing convolution layers with self-attention-based architectures, such as ViT. However, transformers have fewer inductive biases than CNNs and struggle with limited training data. Knowledge distillation, as applied by DeiT, can help train vision transformers with the assistance of a powerful CNN teacher. However, DeiT has its own limitations, such as the over-influence of the teacher's bias and the requirement of a very large teacher CNN, resulting in heavy computational overhead. In this paper, we propose an approach that introduces inductive bias alignment to student models, improving their performance on ImageNet. We find that while introducing inductive bias directly improves performance, knowledge distillation helps the student model inherit more characteristics of the teacher. By leveraging the complementary inductive biases of convolution and involution, our method only requires two lightweight teachers (a CNN and an INN). The knowledge from these teachers in the distillation stage significantly improves the accuracy of the student transformer. Our findings show that the intrinsic inductive bias of the teacher model is crucial, and student models with fewer inductive biases can learn from multiple teachers with different biases. Additionally, knowledge distillation enables student transformers to perform similarly to diverse inductive bias teachers. Our cross inductive bias vision transformers (CiT) outperform previous vision transformers and only require lightweight teachers with a fraction of the parameters of the teacher models in DeiT.