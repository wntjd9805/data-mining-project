Image matting is a crucial task in computer vision for separating foreground objects from the background in order to facilitate image editing and compositing. This task becomes particularly challenging when dealing with complex objects such as human hair, animal fur, and transparent objects like glass or water. In this paper, we propose a transformer-based image matting model called MatteFormer. This model leverages global context information provided by a user-input trimap, which specifies the regions of the foreground, background, and unknown pixels in the image. We introduce prior-tokens, which represent the global context features of each trimap region, and incorporate them into the self-attention mechanism of each block in our network. The PAST (Prior-Attentive Swin Transformer) blocks in our encoder stage utilize the PA-WSA (Prior-Attentive Window Self-Attention) layer, which not only considers spatial-tokens but also involves prior-tokens for better self-attention computation. Additionally, we introduce a prior-memory mechanism that stores prior-tokens from previous blocks and incorporates them into subsequent blocks, enabling improved information exchange. Our proposed model achieves state-of-the-art performance on popular image matting datasets, namely Composition-1k and Distinctions-646. We also conduct extensive studies on the effectiveness of prior-tokens, visualization of self-attention maps, the usage of ASPP, and computational cost. In summary, our contributions include the introduction of MatteFormer, the first transformer-based architecture for image matting, the utilization of prior-tokens for global context information, the design of the PAST block with the PA-WSA layer and the prior-memory mechanism, and achieving superior performance on benchmark datasets.