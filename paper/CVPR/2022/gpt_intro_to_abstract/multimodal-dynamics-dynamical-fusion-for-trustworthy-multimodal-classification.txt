Multimodal learning has seen success in various applications, such as medical diagnosis, by leveraging information from different modalities. However, existing fusion strategies limit the reliability of traditional multimodal models, hindering their deployment in safety-critical applications. This motivates the need for more elegant and trustworthy multimodal fusion techniques. Traditional methods aim to obtain a common or joint representation by exploring the correlated and complementary information between modalities. However, these methods often lack the ability to dynamically perceive the informativeness of each feature and modality for different samples, which is crucial for enhancing the trustworthiness of multimodal classification. In this paper, we propose a novel algorithm called Multimodal Dynamics for trustworthy multimodal classification. Our approach models the informativeness of features and modalities to promote fusion stability and explainability. We introduce a sparse gating strategy to obtain informative features dynamically and evaluate the confidence of each modality based on sample-specific informativeness. We then introduce a unified multimodal fusion framework that dynamically fuses informative features and modalities while reducing the influence of noisy ones, resulting in a more robust and trustworthy model. Our contributions include proposing a dynamical multimodal fusion strategy, introducing mechanisms for estimating feature-level and modality-level dynamicities, and conducting experiments on multimodal medical classification datasets to demonstrate the effectiveness of our approach. The experimental results show significant improvements compared to state-of-the-art methods, and qualitative experiments validate the trustworthiness and interpretability of our approach in modeling multimodal dynamicity.