Deep networks have demonstrated remarkable performance in various computer vision and natural language processing tasks, thanks to large-scale datasets with high-quality annotated labels. However, acquiring such datasets is expensive, and alternative labels collected from web search and user tags are often noisy. Noisy labels negatively impact the generalization performance of deep networks by inducing inaccurate latent representations. Contrastive learning methods have been shown to improve representation learning, but existing approaches that address noisy labels do not fully leverage the pair-wise characteristic of supervised contrastive learning (Sup-CL). In this paper, we propose selective-supervised contrastive learning (Sel-CL) to address this issue. Sel-CL incorporates conﬁdent pairs, obtained from a reliable set of conﬁdent examples, to enhance representation learning with Sup-CL. By selectively choosing pairs with high representation similarity, Sel-CL effectively utilizes not only correct class labels but also pairs with misclassiﬁed examples that belong to the same class. Experimental results on synthetic and real-world noisy datasets demonstrate the superiority of Sel-CL over state-of-the-art methods. The proposed approach achieves robust pre-trained representations and improves generalization performance, offering a promising solution for learning with noisy labels.