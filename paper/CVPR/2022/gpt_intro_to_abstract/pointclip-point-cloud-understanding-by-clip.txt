Deep learning has been highly successful in computer vision tasks in the 2D and 3D domains. However, the transfer of methods from 2D to 3D is hindered by the sparsity and irregular distribution of 3D point clouds. Additionally, there is a growing need to recognize "unseen" objects in newly captured point clouds without the need for retraining models. Contrastive Vision-Language Pre-training (CLIP) has been successful in zero-shot recognition for 2D images by leveraging the correlation between vision and language. To address the challenge of zero-shot classification for "unseen" 3D objects, we propose PointCLIP, which transfers CLIP's 2D pre-trained knowledge to 3D point cloud understanding. We bridge the modal gap between unordered point clouds and grid-based images using online perspective projection. Our approach extracts multi-view features of the point cloud using CLIP's pre-trained visual encoder and obtains zero-shot predictions using a text-generated classifier. Weighted aggregation between views is used to obtain the final prediction for the point cloud. To improve performance, we introduce a learnable inter-view adapter that extracts features from multiple views in few-shot settings. PointCLIP achieves cross-modality zero-shot recognition without any 3D training, but its performance falls behind fully-trained point cloud networks. We leverage the inter-view adapter to improve the performance of classical fully-trained 3D networks. Experimental results demonstrate the effectiveness of PointCLIP in 3D understanding tasks. Our contributions include the introduction of PointCLIP for cross-modality zero-shot recognition, the use of an inter-view adapter for performance improvement, and the utilization of PointCLIP as a multi-knowledge ensemble module for enhancing the performance of existing 3D networks.