Data augmentation has been widely used to improve generalization in visual recognition and natural language understanding tasks. In the field of Visual Question Answering (VQA), previous studies have mainly focused on augmenting the question and answer pool by perturbing or masking image components. However, curating large-scale datasets and sourcing images come with practical challenges such as copyright and privacy. To address this, our work explores the use of synthetic data generated through modern 3D computer graphics. Specifically, we leverage the HyperSim and ThreeDWorld resources to generate realistic training samples for VQA. Although leveraging synthetic data has its challenges due to the domain gap between synthetic and real images, recent successes in tasks like eye gaze estimation, embodied agent navigation, and autonomous driving provide promising insights. While existing synthetic VQA datasets like CLEVR and VQA Abstract build closed worlds that do not generalize well to real-world images, our proposed Hypersim-VQA and ThreeDWorld-VQA datasets offer more realistic representations of real-world settings. Furthermore, we propose a feature swapping (F-SWAP) method to effectively augment existing VQA datasets with computer-generated examples. Unlike traditional domain adaptation methods that focus on aligning image distributions, F-SWAP swaps object-level feature representations, allowing for domain adaptation without requiring realistic style transfer. We compare our F-SWAP approach with other methods like adversarial domain adaptation and demonstrate superior results. In summary, our contributions include the generation of synthetic datasets, the introduction of feature swapping, and an empirical analysis of our proposed approach. Additional details on related work, dataset generation, feature swapping, experiments, and conclusions can be found in the paper. The synthetic datasets and code are available at the provided link.