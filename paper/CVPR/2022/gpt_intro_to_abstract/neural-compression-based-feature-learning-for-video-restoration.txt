Video restoration aims to recover high-quality videos from degraded inputs, such as noise, rain, and haze. While image restoration has been extensively studied, video restoration specifically focuses on utilizing temporal features for better quality. Recent methods have focused on network structure design for extracting these temporal features, but they often overlook the need for refinement. Noisy and irrelevant information in the temporal features can interfere with the restoration process. In this paper, we address this issue in the context of video denoising and propose a neural compression-based solution to refine the features and learn noise-robust representations. Inspired by neural codecs, which discard noisy and uncorrelated contents to save bitrate, we design a compression module to purify the temporal features and filter out noisy information, thereby improving video restoration. To ensure robustness to noise, we need to set the quantization step of the compression module appropriately. However, existing neural compression frameworks lack support for adaptive quantization, which harms the preservation of useful information. To address this, we propose an adaptive quantization mechanism at a spatial-channel-wise level, where the quantization step is learned by our prior model. During training, we employ cross-entropy loss to guide the learning process and preserve the most useful information. Experimental results demonstrate the effectiveness of our approach, as our framework significantly improves restoration quality compared to prior state-of-the-art methods. Our contributions include the proposal of a neural compression-based feature learning approach for video restoration, the design of a learnable spatial-channel-wise quantization mechanism, the introduction of an attention module and a motion vector refinement module for performance enhancement, and the development of a lightweight framework that achieves a superior quality-complexity trade-off in video denoising, deraining, and dehazing tasks.