Visual similarity is crucial in various computer vision applications such as image and video retrieval, face re-identification, and representation learning. Deep Metric Learning (DML) is commonly used in these fields to learn nonlinear distance metrics parametrized by deep networks. Proxy-based DML methods have shown high performance and fast convergence by employing generic class prototypes. However, these methods have limitations in capturing relations between samples within a class and optimizing for non-bijective similarity measures. The alignment of samples around a proxy can introduce local isotropy, hindering the learning of discriminative features and local structures. Incorporating multiple classes and proxies can partially address the isotropy issue but makes fine-grained resolution of local structures challenging. This work proposes Non-Isotropy Regularization (NIR) to explicitly learn unique sample-proxy relations and eliminate semantic ambiguity. NIR introduces a uniqueness constraint using a family of bijective translations, penalizing isotropy and ambiguity. Normalizing Flows and Invertible Networks are used to express the functional constraints of the translation models. Experimental results demonstrate that NIR enhances feature diversity, reduces overclustering, and learns more diverse class distributions compared to non-regularized approaches. NIR-equipped proxy DML achieves competitive or state-of-the-art performance on benchmark datasets while maintaining or improving convergence speeds.