Federated Learning (FL) has emerged as a machine learning paradigm that allows multiple clients to collaboratively train a global model while preserving data privacy. However, recent studies have shown that sensitive information can still be leaked through the shared gradients in FL. To address this issue, various defense strategies have been proposed to degrade the gradient information before sharing it. However, the effectiveness of these defenses in preventing the leakage of sensitive information is still unclear. In this paper, we model the gradient leakage process as an inverse problem and propose a new type of leakage called Generative Gradient Leakage (GGL). We leverage the manifold of a generative adversarial network (GAN) learned from a large public image dataset as prior information to reconstruct high-fidelity images from the shared gradients. We design an adaptive loss function and use gradient-free optimization methods to achieve better results compared to existing attacks. We demonstrate the feasibility of recovering high-resolution images from shared gradients with the proposed GGL, even under different defense settings. Our findings contribute to privacy auditing in FL and can assist in the future design of privacy mechanisms.