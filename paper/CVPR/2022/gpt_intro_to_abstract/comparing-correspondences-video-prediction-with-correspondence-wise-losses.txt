In recent years, there have been significant advancements in image prediction algorithms. However, these methods often struggle to effectively alter image structure, especially when it comes to modifying the positions or shapes of objects. This limitation poses challenges in tasks such as video prediction and video interpolation.One main issue is the uncertainty in determining the exact position of an object. When models attempt to predict these positions, they often produce blurry results. This undesired outcome is often encouraged by using simple loss functions, such as the L1 distance, where incorrectly positioned pixels are compared to pixels from different objects, resulting in a high penalty. As a result, models trained using these losses tend to average out all possible positions an object might occupy, leading to lower loss but poorer image quality.To address this problem, we draw inspiration from classic image matching methods like Hausdorff matching and deformable parts models. These methods allow input images to undergo small spatial deformations before comparison, which improves the accuracy of comparing object positions. We propose a similarity metric that incorporates optical flow to put predicted and ground truth images into correspondence, measuring the similarity between matching pixels rather than pixels in the same spatial positions.By using this correspondence-wise loss function, we achieve crisper and more perceptually accurate predictions. Blurry predictions are penalized as they cannot be easily matched with the ground truth through optical flow. Additionally, our approach encourages correct object placement, as positional errors lead to poor quality matches and penalties. Importantly, our method does not require modifying the network architecture itself, unlike flow-based video prediction architectures.We validate the effectiveness of our method through extensive experiments on various video prediction tasks. Our approach significantly improves the perceptual image quality when compared to traditional loss functions. We outperform state-of-the-art flow-based video prediction methods on perceptual quality metrics for driving datasets such as KITTI and Cityscapes using a simple network architecture. Moreover, we demonstrate the superiority of our loss in video interpolation tasks and its compatibility with stochastic variational autoencoder (VAE) video prediction architectures.