This paper introduces a cascaded object pose estimation framework called OVE6D, which aims to generalize to previously unseen objects without additional parameter optimization. The framework utilizes a depth-based object viewpoint encoder that captures the object viewpoint into a feature vector, which is invariant to in-plane rotations but sensitive to the camera viewpoint. The proposed approach involves three stages: encoding a 3D mesh model into a viewpoint codebook, inferring the camera viewpoint and estimating the remaining pose components conditioned on the obtained viewpoint. The framework requires no additional optimization for new object instances and achieves state-of-the-art results on the T-LESS dataset without using any images from the dataset for training. The contributions of this paper include the proposed cascaded framework, the robust viewpoint encoder, and the demonstrated state-of-the-art results.