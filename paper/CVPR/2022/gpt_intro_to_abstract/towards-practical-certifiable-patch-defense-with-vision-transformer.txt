Deep neural networks (DNNs) have demonstrated excellent performance in various computer vision tasks, but they are susceptible to adversarial examples, which are images with perturbations introduced by adversaries. One particularly threatening form of adversarial examples is patch attacks, where pixels in a continuous region are modified, enabling physical attacks on autonomous systems. While several practical defenses against patch attacks have been proposed, they only provide robustness against known attacks and may not be effective against more powerful attacks in the future. In this paper, we focus on certifiable defense against patch attacks, aiming to achieve guaranteed robustness against all possible attacks. Previous research has made progress in certifiable patch defense using methods like interval bound propagation (IBP) and small receptive fields. However, the accuracy gap between certifiable patch defense and normal models limits their practical application. To address this, we propose an efficient certifiable patch defense with the Vision Transformer (ViT) model. ViT has shown promising performance in computer vision tasks by capturing context and long-range dependencies using self-attention. We integrate ViT into a classic certifiable defense method called Derandomize Smoothing (DS) to improve both accuracy and inference efficiency. We introduce a progressive smoothed image modeling task to train ViT, which allows the model to capture local context while preserving global semantic information. We also renovate the global self-attention structure of ViT into isolated band-unit self-attention, enabling parallel calculation of multiple image bands. Our method achieves 78.58% clean accuracy on ImageNet and 41.70% certified accuracy under 2% area patch attacks, which is close to the accuracy of normal models like ResNet-101. Extensive experiments on CIFAR-10 and ImageNet demonstrate that our method outperforms existing approaches in terms of clean and certified accuracy while maintaining efficient inference. Our contributions include introducing ViT into certifiable patch defense, proposing a progressive smoothed image modeling task, and renovating the self-attention structure of ViT for improved efficiency.