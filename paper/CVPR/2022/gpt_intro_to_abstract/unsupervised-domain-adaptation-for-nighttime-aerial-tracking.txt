Object tracking is a fundamental task in computer vision, especially in the context of aerial robot applications. While there have been significant advancements in object tracking using large-scale datasets and deep learning techniques, these approaches primarily focus on favorable illumination conditions during the day. However, tracking objects at night poses unique challenges due to low contrast, brightness, and signal-to-noise ratio. Current state-of-the-art trackers struggle to generalize well to nighttime scenes, hindering the application of aerial robots in this domain. To address this domain gap, this paper proposes an unsupervised domain adaptive tracking framework called UDAT. Unlike previous approaches that require manually annotated labels for the target domain, UDAT leverages the unlabeled nighttime data to adapt SOTA tracking models trained on daytime data. The framework includes an object discovery strategy to identify potential objects in the unlabeled nighttime dataset and a bridging layer to align the feature distributions between different domains. Additionally, a day/night discriminator, based on Transformer structures, is used to distinguish between the feature domains during adversarial learning.To evaluate the proposed UDAT framework, the authors construct a benchmark called NAT2021, which consists of a test set with fully annotated video sequences and a train set with over 276k unannotated nighttime tracking frames. The benchmark serves as the first evaluation platform for unsupervised domain adaptive nighttime tracking. Extensive experiments conducted on NAT2021-test and the UAVDark70 benchmark demonstrate the effectiveness and domain adaptability of UDAT in nighttime aerial tracking.Overall, this work presents a novel approach to address the domain gap in object tracking, specifically focusing on nighttime aerial perspectives. The proposed UDAT framework, together with the NAT2021 benchmark, provides valuable insights and tools for improving tracking performance in challenging low-light conditions.