This paper presents a novel multimodal training framework for action recognition in videos. The framework leverages the power of NLP models, such as BERT, and pretrained audio classification models to automatically annotate the audio modality. By estimating the relevance between audio and visual modalities, the framework drops the irrelevant audio modality and fuses the relevant modalities based on their relevance level. An efficient two-stream video Transformer is designed for learning the visual modality with fewer parameters compared to traditional video Transformers. Additionally, an intra-class cross-modality augmentation method is proposed to generate more training samples by pairing audio and visual modalities from the same class. The proposed framework effectively trains audio-visual action recognition models on any vision-specific annotated dataset. The key contributions of this work include the multimodal framework, the learnable irrelevant modality dropout network, the efficient video Transformer, and the intra-class cross-modality augmentation method.