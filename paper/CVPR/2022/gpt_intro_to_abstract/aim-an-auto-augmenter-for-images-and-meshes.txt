Deep neural networks have become prevalent in computer vision tasks, particularly in the analysis of digital images and 3D graphics. While these networks aim to mimic human cognition, they still fall short of the robustness of human vision. In order to improve learning methods for vision-based tasks, it is common to preprocess input data to neural networks using augmentation approaches.Certain augmentations, such as affine transformations, random horizontal flipping, and random cropping, are commonly used for image processing to increase the network's tolerance to geometric changes in data. Similarly, for mesh analysis, jittering of mesh elements and affine transformations are performed. These augmentation techniques, along with more advanced strategies and frameworks, are task-agnostic and non-learnable.In contrast, task-aware augmentation approaches optimize jointly with neural networks, learning which transformations are suitable for a specific task, where to apply them, and to what extent. However, existing learnable augmentation methods are often limited to specific dimensions of data representation, making them unsuitable for 3D data. This paper addresses these limitations and proposes an Auto-Augmenter for Images and Meshes (AIM).AIM draws inspiration from human eye movements, categorizing eye movements into fixation, saccades, stabilization, and smooth pursuit. By fixating on specific locations, the human visual system enhances resolution to process fine spatial details. AIM mimics this process by inferring critical regions in images and meshes that contain essential information for the task. It then increases spatial resolution in these regions while reducing spatial coverage of non-crucial areas.The main contributions of this paper include the development of AIM's novel and differentiable spatial warper, an attention module for graph data, and a novel directional consistency loss to constrain deformations produced by AIM. The proposed method is evaluated through experiments on multiple datasets for image classification, mesh classification, and mesh segmentation.