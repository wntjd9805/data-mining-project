In recent years, Vision-and-Language Navigation (VLN) has gained significant attention in the fields of computer vision, natural language processing, and robotics. VLN involves guiding an agent to navigate to a target location in a 3D simulated environment based on natural language instructions. Various VLN tasks have been proposed, including navigation with low-level instructions, communicative and cooperative instructions, and high-level instructions for remote object grounding. These tasks primarily focus on the sequential text-to-image grounding problem, where the agent selects the next node based on the maximum correspondence between the instruction and the image representation. Inspired by the success of Vision-Language BERT (VL-BERT) pre-training on visual-textual matching tasks, several pre-training methods have been developed for VLN. However, these methods have limitations and do not explicitly consider the historical context or temporal order in pre-training. To address these issues, we propose a novel history-and-order aware pre-training paradigm for VLN. Our approach includes the Action Prediction with History (APH) task, which incorporates history visual observations to improve action prediction accuracy. We also introduce two order-aware proxy tasks: Trajectory Order Modeling (TOM) and Group Order Modeling (GOM), which enhance the model's understanding of temporal order within instructions. Experimental results on four downstream VLN tasks demonstrate the effectiveness of our proposed pre-training methods, with superior performance on both in-domain and out-of-domain tasks. Our approach achieves favorable results on tasks such as R2R, RxR, NDH, and REVERIE, indicating the significant potential of our pre-training paradigm for improving VLN performance.