Talking face generation is a popular research area in neural video generation, where the goal is to synthesize a face video that synchronizes with the input speech. This technology, when combined with a text-to-speech (TTS) system, allows users to create talking videos with just a text input. Such systems have potential applications in news broadcasting, virtual lectures, and digital concierge services. Expanding the capabilities of talking face generation to support multiple languages would greatly increase its usefulness and reach a wider global audience. However, recent studies claim to support input speeches in any language, but fail to generalize to certain languages like Korean. This raises the question of whether the robustness of these models depends on the degree of similarity between the training speech language and the input speech language. In this paper, we aim to validate the generalization capabilities of multilingual face generation models using speeches from different language families. We also address the challenge of preserving the vocal identity of the speaker in multilingual talking face generation, as obtaining multilingual speech data for desired speakers is often unavailable. We propose a multilingual talking face generation system that utilizes a multilingual adaptation of VITS for cross-lingual speech synthesis while preserving the vocal identity of the speaker. Additionally, we introduce a talking face generation model capable of generating face videos from synthesized speeches, regardless of the language. Our contributions include a system that can synthesize talking face videos in four languages (Korean, English, Japanese, and Chinese) for a monolingual speaker, a robust talking face generation model for different input speech languages, and a demonstration that generates high-quality facial image sequences at a speed of 25 fps.