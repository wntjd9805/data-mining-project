In the field of unpaired image-to-image translation (I2I), the goal is to translate images from a source domain X to a target domain Y. However, this is an ill-posed problem due to the infinite number of potential translators that can map the source distribution PX to the target distribution PY. To address this, various constraints on the translation function G have been proposed, such as cycle consistency, mutual information maximization, and geometric consistency. However, these existing models either have limited applicability or overlook spatial variations in image translation.To overcome these limitations, we propose a novel regularization technique called the maximum spatial perturbation consistency (MSPC). MSPC introduces a spatial perturbation function T that adaptively transforms each image with an image-dependent spatial perturbation. The insight behind MSPC is that consistency on hard spatial perturbations enhances the robustness of the translator G. To achieve this, we use a differentiable spatial transformer T to compete with G in a mini-max game, generating the hardest spatial transformation for each image.To align the spatial distribution of the content, T and G cooperate with a discriminator Dpert in another mini-max game. T helps align the distribution between the translated images and the target images by adjusting object size, removing background noise, and reducing undesired distortions in G.We evaluate our proposed MSPC on several benchmark datasets and also introduce a Front Face â†’ Profile dataset to highlight challenges in real-world applications. The experimental results demonstrate that MSPC outperforms existing models on various I2I tasks and exhibits stability across different tasks. Our approach effectively addresses the ill-posedness of the unpaired I2I problem and considers spatial variations in image translation.