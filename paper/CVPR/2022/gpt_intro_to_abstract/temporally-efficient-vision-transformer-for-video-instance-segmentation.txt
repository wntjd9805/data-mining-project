Video Instance Segmentation (VIS) is a challenging task in video understanding that involves detecting, segmenting, and tracking video instances across frames. Temporal context information is crucial for achieving high-performance VIS systems. Vision transformer (ViT) has shown strong long-range context modeling ability and success in various instance-level video recognition tasks. However, designing ViTs for VIS, especially considering temporal context modeling, is an emerging problem. Existing approaches have focused on 2D contextual information modeling and have limitations in efficiently capturing temporal information. To address these challenges, this paper presents Temporally Efficient ViT (TeViT) for efficient and effective VIS. TeViT incorporates a transformer backbone and query-based VIS heads. The backbone stage uses messenger tokens and a messenger shift mechanism for intra-frame information extraction and early temporal feature fusion. In the head stages, the QueryInst instance segmentation head is converted into a VIS head, utilizing multi-head self-attention for instance-level temporal information interaction. Experimental results demonstrate that TeViT achieves state-of-the-art performance on large-scale VIS datasets. Contributions include being the first video instance segmentation transformer that efficiently captures temporal contextual information, the proposed temporal modeling modules that are compatible with image-level pre-trained models, and the nearly convolution-free framework that achieves superior VIS results. TeViT also sheds light on effective video transformers for instance-level recognition tasks by introducing the concepts of "early temporal feature fusion" and "a video instance as a query".