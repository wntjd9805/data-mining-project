Effective video representation is critical for various video understanding tasks such as action recognition, temporal localization, and video retrieval. Pre-training models on large-scale video recognition datasets has been a common practice but labeling such datasets is costly and time-consuming. Self-supervised learning (SSL) has gained attention as a way to learn video representation without human annotation. SSL involves pretext tasks, with most falling into two categories: recognizing transformation types and instance discrimination. While instance discrimination has become dominant, there are questions about the differences between representations learned via these tasks, the cause of RecogTrans's inferior performance, and the potential of RecogTrans-based approaches. In this paper, a comprehensive study is conducted comparing representative methods of RecogTrans and InstDisc. It is found that representations learned via RecogTrans have unique properties, with poor performance in semantic-related tasks but a strong capability in temporal modeling. The TransRank framework is proposed to address the limitations of RecogTrans by considering transformations in a ranking formulation. TransRank outperforms previous state-of-the-art work on recognition tasks. The paper concludes that RecogTrans is still worth exploring and introduces several good practices to improve RecogTrans-based SSL.