In this paper, we address the problem of restoring all-in-focus images from defocused versions, which is important for various artificial intelligence applications. The depth of field (DOF) is decreased when using a large camera aperture, resulting in defocus blur. Restoring all-in-focus images from defocused versions is challenging due to the spatially varying nature of defocus blur. Previous methods involve estimating blur kernels and applying non-blind deconvolution, but these approaches have limitations. Recently, deep neural networks have been used to directly restore sharp images from defocus blur, showing improved performance and efficiency. However, existing datasets used for training these networks lack accurate correspondence between defocused and all-in-focus image pairs. To address this, we analyze two datasets, Dual-Pixel Defocus Deblurring (DPDD) and LFDOF, and propose a novel training strategy that leverages the advantages of both datasets. We also propose an end-to-end network architecture with a dynamic residual block. Experimental results demonstrate the effectiveness of our approach, achieving state-of-the-art performance on multiple test sets.