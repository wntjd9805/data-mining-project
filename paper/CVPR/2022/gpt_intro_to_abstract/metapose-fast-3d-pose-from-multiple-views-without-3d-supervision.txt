This paper addresses the problem of estimating 3D coordinates of human joints from RGB images captured using synchronized cameras with unknown positions, orientations, and intrinsic parameters. The authors propose a neural network approach that predicts both the 3D human pose and relative camera poses from multiple views. The network leverages human body joints as a source of information for camera calibration and takes into account joint occlusions and prediction uncertainties. The proposed approach requires only 2D joint annotations for training and adds minimal latency to the pipeline. Experimental results show that the method achieves state-of-the-art performance on benchmark datasets, while also providing faster inference compared to classical optimization-based methods. Overall, the proposed approach offers an efficient and accurate solution to the problem of estimating 3D human pose from synchronized camera views.