This paper introduces Point-BERT, a scheme for learning point cloud Transformers with minimal inductive bias. While Transformers have been successfully used in natural language processing and image processing, their application to 3D point clouds is relatively unexplored. Existing point cloud models based on Transformers introduce inductive biases from local feature aggregation and neighbor embedding, deviating from the standard Transformers. The adoption of Transformers on point clouds directly does not yield satisfactory performance due to limited annotated 3D data. To address this, the authors propose self-supervised pre-training techniques to leverage the scalability and generalization of Transformers for 3D point cloud representation learning. Inspired by BERT, they introduce BERT-style pre-training for 3D point cloud understanding. However, since there is no pre-defined vocabulary for point clouds, the authors devise a point cloud Tokenizer using a dVAE-based point cloud reconstruction to convert point clouds into discrete point tokens. They also propose a 'masked point modeling' task to pre-train Transformers by reconstructing missing point tokens. The experiments show that Point-BERT accurately predicts masked tokens and infers diverse, holistic reconstructions. Furthermore, Point-BERT achieves high accuracy on ModelNet40 and ScanObjectNN datasets, surpassing other point cloud models with fewer human priors. The representations learned by Point-BERT transfer well to new tasks and domains, advancing the state-of-the-art in few-shot point cloud classification. The authors hope that a unified transformer architecture across images and point clouds can facilitate joint modeling of 2D and 3D visual signals.