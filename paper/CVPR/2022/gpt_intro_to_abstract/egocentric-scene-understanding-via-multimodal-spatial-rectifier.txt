We will be discussing the problem of egocentric 3D scene understanding in this paper. While humans have a natural ability to perceive and understand the geometry of the 3D scenes around them, existing computer vision systems struggle with this task. These systems are trained on static and well-organized scenes captured by controlled cameras, making them fragile when faced with egocentric images from everyday activities. To address this, additional sensors such as IMU and depth sensors are necessary in augmented/mixed reality devices. The specific problem we focus on is predicting depths and surface normals from a single view egocentric image. However, egocentric scene understanding poses additional challenges compared to classic scene understanding problems. Firstly, egocentric images are often depicted in a tilted manner due to head movements, making them different from existing data distributions. Secondly, these images include not only background objects but also dynamic foreground objects such as humans and hands, which are key in understanding evolving activities.We propose a solution that incorporates the concept of equivariance, called a spatial rectifier. This is an image warping technique that transforms a titled image to a canonical orientation, allowing a prediction model to learn from upright images. However, the current spatial rectifier struggles with predicting 3D geometry in egocentric images that involve significant head movements, as it leads to excessive perspective warping. To overcome these challenges, we introduce a multi-modal spatial rectifier that learns multiple reference directions based on the orientations of the egocentric images. This helps minimize the impact of excessive perspective warping and enables more accurate predictions. We also present a new dataset called EDINA (Egocentric Depth on everyday INdoor Activities), which includes synchronized RGB, depth, surface normal, and 3D gravity direction data. We use this dataset to train our multimodal spatial rectifier and geometry prediction models, and demonstrate their improved performance not only on EDINA but also on other datasets.Our contributions in this paper include the development of a multimodal spatial rectifier, the creation of the EDINA dataset, and a comprehensive evaluation of our methods on depth and surface normal prediction for egocentric scenes.