Compositional reasoning is a critical aspect of human visual event representation, as well as the language used to communicate about visual events. The computer vision community has developed video benchmarks to measure compositional reasoning using question answering. However, existing benchmarks do not explain why models struggle with compositional reasoning. Standard evaluation schemes and model behavior dissection approaches are unable to provide a complete understanding of the challenges faced by these models. In this paper, we introduce a question decomposition engine that breaks down compositional questions into a directed acyclic graph (DAG) of sub-questions. Each sub-question represents a subset of the reasoning steps required to answer the original question, allowing us to analyze model performance on intermediate reasoning steps. We create the AGQA-Decomp dataset using our engine, which includes 4.55 million question-answer pairs associated with 9.6K videos. To evaluate compositional reasoning, we design handcrafted programs and templates for each sub-question and define composition rules to combine sub-questions. We also develop a suite of metrics, including internal consistency, to evaluate model performance. We assess three state-of-the-art video question answering models using our DAGs and metrics: HCRN, HME, and PSAC. Our analysis reveals that these models struggle with many compositional reasoning steps, either failing to complete the step or relying on faulty reasoning mechanisms. They often exhibit self-contradiction and achieve high accuracies even when failing at intermediate reasoning steps. Additionally, the models struggle with tasks such as choosing and comparing between options.We find that there is no correlation between internal consistency and accuracy for HCRN and PSAC, suggesting a lack of self-consistency in the models. However, for HME, there is a weak negative correlation, indicating frequent inaccuracy that propagates due to internal consistency.In conclusion, our question decomposition engine and DAGs provide insights into the challenges faced by video question answering models in compositional reasoning. These insights can facilitate further research in transparency, consistency, and interactive model analysis tools.