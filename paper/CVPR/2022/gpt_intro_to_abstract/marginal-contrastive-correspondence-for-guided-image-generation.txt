Image-to-image translation using Generative Adversarial Networks (GANs) has shown great promise in generating high-quality images conditioned on specific inputs. However, achieving faithful control of the generation style remains a challenge. Variational autoencoders (VAEs) have been used to regulate image style, but often suffer from compromised generation quality. Another approach involves using styles extracted from exemplars, but they struggle to capture detailed structures. Dense correspondence, achieved through dense semantic correspondence or feature alignment, has emerged as a promising approach for faithful style control. However, existing methods do not explicitly model domain-invariant features for correspondence building. In this paper, we propose Marginal Contrastive Learning Network (MCL-Net) for exemplar-based image translation. MCL-Net introduces contrastive learning to extract domain-invariant features for building cross-domain correspondence. Contrastive learning maximizes the mutual information between condition features and image features, yielding domain-invariant features. We also propose a marginal contrastive loss to enhance feature discriminability and curb over-smoothed or inaccurate correspondence. Furthermore, we introduce a self-correlation map mechanism to explicitly represent scene structures and preserve texture patterns during correspondence building. The contributions of this work are threefold. First, we introduce contrastive learning for explicit domain-invariant feature learning. Second, we propose a novel marginal contrastive loss to enhance feature discriminability. Third, we design a self-correlation map to facilitate the preservation of scene structures and texture patterns during correspondence building.