Detecting 3D objects from monocular images is a crucial task for machine perception in various applications such as virtual reality, robotics, and autonomous driving. However, accurately estimating 3D information from a single image is challenging. Despite the limitations, the potential benefits of using a cheap and easy-to-deploy monocular 3D detection solution have captured the attention of many researchers. Pseudo-LiDAR detectors that utilize a pre-trained depth estimation network to generate Pseudo-LiDAR representations have shown significant progress in improving monocular 3D detection performance. These representations, such as pseudo point clouds and pseudo voxels, are then fed to LiDAR-based 3D detectors. Although enhancing depth perception capabilities has shown promise, there is still a significant performance gap between Pseudo-LiDAR and LiDAR-based detectors due to errors in image-to-LiDAR generation. In addition to LiDAR-based detectors, stereo 3D detectors have also proven effective in accurately localizing 3D objects. Furthermore, the gap in image-to-image generation for stereo views is smaller compared to image-to-LiDAR generation. Taking these factors into account, we propose a novel Pseudo-Stereo 3D detection framework for monocular 3D detection. Our framework generates a virtual view from a single input image to create Pseudo-Stereo views, which are then used by stereo 3D detectors for detecting 3D objects. We employ LIGA-Stereo, one of the most advanced stereo 3D detectors, as the base detection architecture. The key component of our framework is the virtual view generation process.We demonstrate the generation of a virtual view using the KITTI-3D dataset as an example. Notably, our virtual view generation does not require ground-truth actual views during the training phase. We present two virtual view generation methods: image-level and feature-level. In the image-level method, we convert the estimated depth map from the input left image into disparities and use them to forward warp the input left image into a virtual right image, creating the Pseudo-Stereo views. In the feature-level method, we propose a disparity-wise dynamic convolution that uses dynamic kernels sampled from the disparity feature map to adaptively filter the left features, resulting in the generation of virtual right features. This approach mitigates the impact of depth estimation errors on feature degradation. We summarize our contributions as follows:1.