This paper introduces a method for synthesizing dynamic dressed humans with physically plausible secondary motion. Existing approaches focus on static poses and fail to generate realistic secondary motion. The authors propose a representation for dynamics that can be learned from limited observations by enforcing equivariance. They rearrange 3D features in a canonical coordinate system, the UV map, which captures semantic meaning and reduces geometric ambiguity. The authors also address factors that impact appearance, such as garment type and local geometry, by using a compositional decoder. Experimental results demonstrate the effectiveness of the proposed method, showing superior generalization ability and better handling of complex motion sequences. The intermediate representations predicted by the method enable applications such as free-viewpoint rendering and relighting.