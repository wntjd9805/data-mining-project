Recent studies have shown that deep neural networks (DNNs) used in computer vision tasks are vulnerable to adversarial examples, even with small perturbations. This vulnerability is of great concern in safety-sensitive scenarios such as autonomous driving and medical diagnosis, as it can lead to erroneous decisions and potentially dangerous situations. While there are numerous attack strategies investigated in the literature, the focus has shifted towards physical-world attacks, which are more practical and challenging. One popular approach is the "sticker-pasting" strategy, where adversarial perturbations are printed as stickers and attached to target objects. However, this approach has limitations, such as the inability to access the target object and imperfect printing of perturbations. To address these limitations, recent research has explored the use of light-based attacks, leveraging natural phenomena like shadows. This paper introduces a new type of optical adversarial example that uses shadows to achieve naturalistic and stealthy physical-world attacks. The effectiveness of this attack is validated in the context of traffic sign recognition. This work is the first to demonstrate that shadows can be a threat to machine learning-based vision systems, providing a meaningful reminder to prevent such attacks in practical systems. The contributions of this paper include proposing a new light-based physical-world adversarial attack using shadows, offering optimization strategies to generate digitally and physically realizable adversarial examples, conducting thorough evaluations in simulated and real-world environments, and discussing the limitations and defense mechanisms of this attack.