The Visual Relationship Recognition (VRR) task involves understanding relationships between objects in a visual scene. This comprehensive understanding benefits various vision tasks such as image captioning, VQA, image generation, and 3D scene synthesis. However, current models in VRR are dominated by frequent relationships, lacking generalization on low-shot relationships. Previous approaches rely on graph structures to model relationships but are limited by their focus on nearby neighbors. We propose an approach called RelTransformer that uses self-attention mechanisms to tackle the VRR challenges. This allows the relation to have a larger attention scope and learn richer contextualized representations, benefiting long-tail visual relationship understanding. Our approach reconstructs the scene graph into relation triplets and global scene context, and uses self-attention to pass messages among them. We also introduce a memory attention module to augment relation representation. We evaluate our model on VG200, GQA-LT, and VG8K-LT datasets and achieve state-of-the-art results, demonstrating the effectiveness of our approach.