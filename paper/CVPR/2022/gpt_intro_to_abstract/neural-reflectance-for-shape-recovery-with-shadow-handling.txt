Recovering the 3D shape of non-Lambertian objects from multiple photometric images remains challenging due to the presence of specularities and shadows. Previous methods have relied on restrictive assumptions or supervised deep learning approaches, which do not fully exploit the underlying physics principles of image formation. Additionally, the lack of interpretability in deep learning methods hampers the understanding of object appearance and surface normals. Processing real-world images with both specularities and shadows is an open challenge.In this paper, we propose an unsupervised neural network method to address these issues. Our framework takes image coordinates as input and directly outputs surface normal, reflectance parameters (diffuse albedo and specular parameters), and depth for each surface point. We introduce neural specular basis functions to account for different types of specularities, providing an accurate surface normal estimation. Moreover, our framework explicitly parameterizes shadowed regions and excludes them from computation to avoid rendering artifacts.Following the inverse graphics rendering idea, we use the estimated surface normal and neural reflectance to re-render pixel intensities under different light directions. Our framework is optimized by minimizing the difference between reconstructed and observed images, without requiring ground truth data or pre-training. Experimental results on the DiLiGenT dataset demonstrate that our method outperforms both supervised and self-supervised state-of-the-art methods, while being ten times faster than other self-supervised deep methods.