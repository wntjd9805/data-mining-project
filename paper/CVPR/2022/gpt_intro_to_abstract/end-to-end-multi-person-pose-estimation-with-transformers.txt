Multi-person pose estimation is a fundamental computer vision task with various applications such as action recognition, human-computer interaction, pedestrian tracking, and re-identification. Existing methods for multi-person pose estimation rely on two-stage frameworks, either top-down or bottom-up approaches. However, top-down methods suffer from accuracy limitations and high computational cost, while bottom-up methods involve heuristic and hand-crafted grouping processes. In recent years, there has been interest in single-stage approaches that directly estimate multi-person poses. Although these methods have achieved good accuracy and efficiency, they still rely on hand-crafted post-processing steps. In this paper, we propose a fully end-to-end multi-person pose estimation framework called PETR, which utilizes transformers to unify person instance and fine-grained body joint localization. PETR hierarchically attends to relevant features, addressing the feature misalignment issue and achieving improved performance. We demonstrate the superiority of PETR compared to existing methods on the COCO and Crowd-Pose datasets, establishing a new state of the art in crowded scenes. Our contributions include the first fully end-to-end learning framework for multi-person pose estimation, hierarchical decoders to handle feature misalignment, and state-of-the-art performance on benchmark datasets.