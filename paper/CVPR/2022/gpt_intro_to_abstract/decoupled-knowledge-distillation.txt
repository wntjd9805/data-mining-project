In recent years, deep neural networks (DNN) have greatly impacted the field of computer vision, achieving impressive results on tasks such as image classification, object detection, and semantic segmentation. However, these powerful networks often require large model capacities, which can lead to high computational and storage costs. To address this issue, knowledge distillation (KD) has emerged as a potential solution.KD focuses on transferring knowledge from a heavier, more complex model (teacher) to a lighter, more lightweight model (student), improving the performance of the student model without introducing extra costs. The concept of KD was initially proposed as a way to minimize the KL-Divergence between the prediction logits of teachers and students. While most research in KD has concentrated on distilling knowledge from deep features of intermediate layers, little attention has been given to logit-based methods.In this paper, we explore the mechanism of KD and propose a novel logit distillation method called Decoupled Knowledge Distillation (DKD). We divide the classical KD loss into two parts: Target Class Classification Knowledge Distillation (TCKD) and Non-target Class Classification Knowledge Distillation (NCKD). By separate analysis of these two parts, we uncover limitations in the classical KD loss, including the suppression of effective knowledge transfer due to the coupling of NCKD with the teacher's confidence on the target class, and the limited flexibility in balancing the contributions of TCKD and NCKD.To address these limitations, DKD decouples the NCKD loss from the teacher's confidence, improving the distillation effectiveness of well-predicted samples. Furthermore, DKD allows for separate consideration of the importance of TCKD and NCKD by adjusting the weight of each part. Experimental results show that DKD achieves state-of-the-art performance on various tasks and exhibits higher training efficiency and better feature transferability compared to feature-based distillation methods.Overall, this work provides insights into logit distillation by dissecting the classical KD loss and analyzing the effects of its components. We identify limitations in the classical KD formulation and propose DKD as an effective approach to overcome these limitations.