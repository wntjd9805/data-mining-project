In recent years, significant progress has been made in vision-and-language tasks and datasets, particularly in understanding textual expressions grounded in 2D images. However, developing models that understand the spatial information of 3D scenes poses several challenges. Existing models based on 2D images lack accurate perception of relative directions, occlusions, and object localization. While some 3D spatial-understanding models exist, question answering datasets for 3D environmental annotations are limited in terms of size and question variety.To address this gap, this paper proposes a 3D question answering (3D-QA) task that utilizes 3D spatial information to comprehend real-world information. The proposed 3D-QA task involves answering questions about a 3D scene along with object localization. A novel ScanQA dataset based on RGB-D scans and annotations from the ScanNet dataset is introduced. The dataset includes auto-generated questions from object captions, which were filtered and refined. Humans provided free-form answers and object annotations using an interactive 3D scene viewer.A 3D-QA model with textual and 3D scene encoding, as well as baseline models, including 2D image models, a combination of 3D object localization models, and a question answering model, were developed. The ScanQA model outperformed the baseline models in various evaluations, such as exact matching and image captioning metrics.This research contributes to the development of models that comprehend 3D scenes and can ask and answer questions about the 3D environment, similar to human capability. The proposed task and dataset are applicable to real-world services that utilize 3D scene information, such as virtual room-viewing services or searching in indoor scenes.