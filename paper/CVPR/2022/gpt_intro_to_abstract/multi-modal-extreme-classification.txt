Extreme Classification (XC) is a challenging task in computer science that aims to annotate datapoints with the most relevant subset of labels from a large set of labels. This paper focuses on multi-modal XC, where datapoints and labels are characterized by both visual and textual descriptors. Existing methods for multi-modal XC are often limited to using embeddings obtained from neural architectures, neglecting the potential benefits of training classifiers alongside these embeddings. Additionally, most XC research focuses on text-based categorization, leaving a gap in the field for multi-modal XC architectures. To address these challenges, the MUFIN method is proposed. MUFIN combines a novel embedding architecture with a novel classifier architecture. The embedding architecture employs multi-modal attention, while the classifier architecture utilizes datapoint-label cross attention and high-capacity one-vs-all classifiers. The training process of MUFIN scales to tasks with millions of labels through pre-training and hard-positive and hard-negative mining. Remarkably, MUFIN achieves predictions within 3-4 milliseconds per test point, even on tasks with millions of labels. In addition to the proposed method, this paper introduces the MM-AmazonTitles-300K product-to-product recommendation dataset, which consists of over 300K products with titles and multiple images. Comparative evaluations demonstrate that MUFIN outperforms leading text-only, image-only, and multi-modal methods by at least 3% in terms of accuracy. This superiority extends to zero-shot tasks, further validating the effectiveness of MUFIN's classifiers and embedding model. The results suggest that MUFIN is a promising approach for multi-modal XC tasks with millions of labels.