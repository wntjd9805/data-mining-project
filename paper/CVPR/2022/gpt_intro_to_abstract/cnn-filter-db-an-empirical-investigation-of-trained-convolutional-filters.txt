The practical deployment of convolutional neural networks (CNNs) still faces challenges, such as the reliance on large amounts of annotated training data and the issues of robustness and generalization. These problems are interrelated, as fine-tuning pre-trained models by small datasets from the target domain is a common solution to the lack of annotated data. However, finding suitable pre-trained models that match the target data distribution remains a challenge. In this paper, we propose investigating distribution shifts in the 2D filter-kernel distributions of CNNs themselves, as they reflect the sub-distributions of the input image data. We introduce a dataset of over 1.4 billion filters with meta data from trained CNNs, enabling systematic investigations of learned filters. Through our analysis, we show that many publicly provided models suffer from degeneration, with overparameterization leading to sparse and non-diverse filters. We also demonstrate that learned filters do not significantly differ across models trained for various tasks, indicating that pre-training can be performed independent of the target data. Furthermore, we find that the most variance in learned filters occurs in the beginning and end of classification models, while object/face detection models exhibit significant variance in early layers. Our paper contributes a diverse filter database, a data-agnostic method to identify degenerated convolution layers, insights into distribution shifts in filters, and a deeper understanding of CNN model structure.