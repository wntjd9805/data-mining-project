Human-centric computer vision tasks such as face alignment, human pose estimation, and 3D human mesh reconstruction have gained significant research attention due to their widespread applications. Vision transformers, inspired by the success of transformers in natural language processing, have emerged as a promising approach to solving these tasks. Transformers offer advantages such as long-range attention between image patches, which is crucial for capturing relationships between different body parts. However, most existing vision transformers use a fixed grid-based token generation approach, which is suboptimal for human-centric visual analysis. In this paper, we propose a novel vision transformer called Token Clustering Transformer (TCFormer) that generates tokens dynamically by progressive token clustering. Unlike the grid-based tokens, our approach allows tokens to have varying locations, sizes, and shapes, enabling a focus on important regions such as the human body. We introduce a Clustering Token Merge (CTM) block that groups tokens with similar semantic meanings and then merges them to create new tokens. We also propose a Multi-stage Token Aggregation (MTA) head to efficiently aggregate token features in multiple stages while preserving image details. Experimental results demonstrate that TCFormer outperforms existing methods on various human-centric tasks and datasets, including whole-body pose estimation and 3D human mesh reconstruction.