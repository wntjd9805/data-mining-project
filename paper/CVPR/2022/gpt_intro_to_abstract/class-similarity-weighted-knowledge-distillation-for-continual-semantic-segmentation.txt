Semantic segmentation is a crucial task in computer vision, with applications in areas such as self-driving cars and medical image diagnostics. However, traditional segmentation models face the challenge of catastrophic forgetting, where the performance on old tasks significantly degrades when new data is continuously learned. Recently, research on continual learning for semantic segmentation (CSS) has focused on addressing this problem in the medical imaging and general scene understanding domains. In CSS, there are two main problems causing catastrophic forgetting: a strong bias towards new classes and the tendency to forget visually similar old classes. Existing CSS methods use knowledge distillation to transfer the knowledge of previous models, but they do not prioritize revising the affected old knowledge and put less emphasis on visually similar old classes. To overcome this limitation, this paper proposes a novel class similarity weighted knowledge distillation (CSW-KD) method. CSW-KD computes the similarity between new and old classes and reweighs the predictions of the previous model based on this similarity. The proposed approach has three benefits: increased resilience to forgetting visually similar old classes, improved learning of new tasks by capturing class similarity, and the ability to learn an underlying class hierarchy. The paper presents the REMINDER CSS framework, which consists of CSW-KD for knowledge distillation and a feature knowledge distillation module to encourage feature reuse. Extensive experiments show that REMINDER outperforms state-of-the-art methods on datasets such as Pascal-VOC 2012 and ADE20k, demonstrating a better trade-off between rigidity and plasticity. The contributions of this paper include the use of semantic similarity for continual learning, the novel CSW-KD method, and the REMINDER CSS framework.