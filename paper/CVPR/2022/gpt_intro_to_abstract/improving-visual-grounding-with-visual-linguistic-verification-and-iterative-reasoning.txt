Visual grounding is a task that aims to localize a referred object or region in an image based on natural language expressions. This task has gained significant attention due to its potential in bridging the gap between linguistic expressions and visual perception. In this paper, we propose a transformer-based visual grounding framework that retrieves the feature representation of the target object directly for accurate localization. Our framework consists of a visual-linguistic verification module and a language-guided context encoder to establish discriminative features for object identification. Additionally, we introduce a multi-stage cross-modal decoder that iteratively compares and integrates visual and linguistic information to reduce ambiguity during inference. We evaluate our approach on multiple benchmark datasets and demonstrate significant performance improvements over previous state-of-the-art methods. Our proposed components are validated through extensive experiments and ablation studies. The code for our framework is publicly available.