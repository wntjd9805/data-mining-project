Deep learning has achieved remarkable success in various domains but has also exposed the vulnerability of deep neural networks to distribution shifts. Adversarial attacks, which involve finding minimal perturbations to corrupt network outputs, showcase this vulnerability by moving samples out of the training distribution. While methods have been proposed to enhance robustness, they only alleviate the problem without solving it. While white-box adversarial attacks are primarily academically relevant, physical adversarial attacks have serious consequences for safe deployment as they involve placing confounding patterns in the real world to deceive machine learning systems.Previous work focused on adversarial attacks in recognition problems, and it was believed that correspondence problems, such as optical flow estimation, were not a suitable target. However, recent studies demonstrated successful physical adversarial patch attacks on optical flow networks. These attacks involved optimizing adversarial local patches that could be inserted into images, causing large errors in estimated optical flow fields. Different network architectures showed varying degrees of vulnerability to these attacks, while conventional optical flow methods remained unaffected. The vulnerability was hypothesized to be related to the common encoder-decoder architecture of FlowNet and its derivatives, but a thorough analysis was lacking.In this paper, we aim to delve deeper into the vulnerability of optical flow networks to understand the true cause of adversarial patch attacks. We investigate whether the patch-based attack can be constructed without optimization for a specific network architecture, and explore if specific design choices or the avoidance of design mistakes can mitigate the severe vulnerability. Our analysis leads us to discover the underlying reason behind the vulnerability and propose architectural changes to improve robustness, as illustrated in Figure 1.Having answered these questions, we then explore global adversarial perturbation attacks, which modify the entire image instead of using local patches. We demonstrate the generation of target optical flow fields using this attack strategy (Figure 11). However, we also show that universal attacks, which are input-agnostic, do not apply to optical flow networks and instead, global attacks must leverage the structure of input images. This is different from unprotected recognition networks, which are vulnerable to imperceptible universal attacks.Overall, this paper contributes to a deeper understanding of the vulnerability of optical flow networks to adversarial attacks and proposes methods for improving robustness against both patch-based and global attacks.