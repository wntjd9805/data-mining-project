Instance segmentation, the task of grouping pixels into object instances, poses challenges in both closed-world and open-world settings. While closed-world setups involve segmenting objects from a predefined taxonomy, open-world scenarios require segmenting objects of arbitrary categories, including unseen ones. Current computer vision systems struggle to accurately segment objects they cannot recognize, resulting in a performance gap between seen and unseen domains. In contrast, humans can readily segment objects they cannot categorize. Previous models, pre-deep learning era, were category-agnostic, achieving moderate recall across the board. However, they lacked precision and relied on hand-crafted features. To bridge the gap between closed-world recognition-based models and category-agnostic models, a novel approach called Generic Grouping Networks (GGNs) is proposed. GGNs use learned pairwise affinity predictions and a simplified version of the MCG algorithm to extract and rank segments without annotations. These segments serve as pseudo ground truth and are combined with curated annotations to train a Mask R-CNN instance segmentation module. This approach yields impressive performance gains in open-world instance segmentation across multiple benchmarks, surpassing state-of-the-art results. The contributions of this paper include the introduction of GGNs, comprehensive ablation experiments, and the achievement of state-of-the-art performance in open-world instance segmentation.