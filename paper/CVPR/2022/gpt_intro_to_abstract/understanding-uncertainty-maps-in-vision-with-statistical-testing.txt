With the increasing use of deep neural network models in production systems for vision tasks, there is a growing need to understand the uncertainties associated with these models. Uncertainty estimation is crucial for decision making in various domains, including autonomous driving, medical imaging, and general decision support systems. This paper focuses on the design of mechanisms for model calibration and uncertainty estimation from deep neural network models used in vision tasks. The uncertainties can be categorized into aleatoric (statistical) and epistemic (systematic). Aleatoric uncertainty captures inherent and irreducible data noise, while epistemic uncertainty accounts for uncertainty in model parameters. Various strategies have been proposed, including heteroscedastic models, variational autoencoders (VAE), Bayesian neural networks (BNN), and hybrid approaches combining heteroscedastic neural networks (NN) and BNN. However, the practical applications of uncertainty estimates require statistically sound schemes to generate "significant" uncertain regions and enable actionable information. This paper addresses the limitations of existing frameworks and proposes a probabilistic framework based on Neural ODE and Wasserstein distance to perform hypothesis tests on uncertainty maps generated by different probabilistic DNN models. The proposed method, called Warping Neural ODE, allows for mapping results back to the domain of uncertainty maps and provides actionable information for decision making. The contributions of this work include demonstrating how existing DNN tools, when combined with Random Field Theory, can perform hypothesis tests on uncertainty maps and develop a framework for uncertainty estimation in deep neural network models used in vision tasks.