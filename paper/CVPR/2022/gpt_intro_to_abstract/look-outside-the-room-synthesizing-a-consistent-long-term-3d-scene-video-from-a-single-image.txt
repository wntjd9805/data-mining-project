Single-image view synthesis has gained significant attention in computer vision and computer graphics. It involves extrapolating pixels beyond the input image to generate new pixels that adhere to the scene's geometric structure while maintaining semantic coherence. Existing view synthesis methods that utilize 3D geometric representations have shown promising results in generating high-quality novel views. However, these approaches are limited in their ability to generate views for large camera motions.In this paper, we propose an extended view synthesis problem that aims to generate a consistent video given a single image of a 3D scene and a long-term camera trajectory. Our goal is to ensure that the generated images exhibit consistency as the camera moves through the scene. This task has practical applications in content generation, editing, and the development of differentiable simulators for model-based planning and control in robotics.To address this problem, we leverage autoregressive models that have demonstrated success in extrapolating image content beyond the input. We build upon the work of Rombach et al., who proposed using an autoregressive Transformer for large geometric transformations in view synthesis. However, their approach led to inconsistent and diverse outputs due to the probabilistic sampling.To synthesize consistent long-term videos, we propose leveraging the autoregressive Transformer for sequential modeling with locality constraints. Instead of learning the model between only two views, we perform sequential modeling with multiple video frames. By conditioning on a sequence of input images and camera trajectories, our proposed probabilistic framework predicts future frames by sampling from the conditional distribution.To ensure consistency between generated and historical views, we incorporate a locality constraint based on the relative cameras. We compute a bias using a multi-layer perceptron (MLP), known as the Camera-Aware Bias, which guides the self-attention operation in the Transformer model. This bias strengthens the dependencies between relevant patches connected by the camera, facilitating optimization and enforcing consistency during frame generation.Experimental evaluations conducted on RealEstate10K and Matterport3D datasets, which focus on 3D indoor scenes, validate the effectiveness of our approach. Our model successfully synthesizes new views with large camera motion and generates long-term videos from a single input image. Our method outperforms state-of-the-art techniques in terms of both standard view synthesis metrics and long-range future frames.To summarize, our contributions include a novel Transformer model for synthesizing consistent long-term videos, a locality constraint using the Camera-Aware Bias to enforce consistency, and state-of-the-art performance in view synthesis.