3D hand mesh estimation from a single RGB image is a challenging task, especially in the presence of occlusion. While spatial attention mechanisms have been successful in occlusion-robust 2D human body pose estimation, their applicability to 3D hand mesh estimation remains unexplored. This paper introduces HandOccNet, a novel framework that leverages a feature injection mechanism to improve the robustness of 3D hand mesh estimation under occlusion. Unlike existing methods, HandOccNet utilizes both primary and secondary features, representing features of hand regions and occluded regions, respectively. By injecting the information of primary features into the locations of secondary features, HandOccNet provides a richer representation for occluded regions. To achieve this, the paper proposes two Transformer-based modules: the feature injecting transformer (FIT) and the self-enhancing transformer (SET). The FIT computes a correlation map between primary and secondary features, while the SET refines the output feature map. Experimental results on hand-object interaction datasets demonstrate that HandOccNet outperforms state-of-the-art methods in 3D hand mesh estimation accuracy under severe occlusion. Overall, this paper presents a novel framework for occlusion-robust 3D hand mesh estimation and contributes to the existing literature by introducing a new approach based on spatial attention and feature injection.