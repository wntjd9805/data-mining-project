3D human pose estimation from monocular observations is a crucial task in computer vision that reconstructs the 3D joint locations of the human body from 2D input images or video. This task has various applications, including action recognition, virtual human creation, and human-robot interaction. Most existing methods follow a 2D-to-3D lifting pipeline, where 2D keypoints are detected and lifted to 3D. However, due to the depth ambiguity of monocular data, multiple potential 3D poses can be mapped from the same 2D pose, making it challenging to recover accurate 3D poses from a single frame. Recent approaches have exploited temporal information in videos to address this issue but often overlook the differences in motion between body joints, leading to insufficient learning of spatio-temporal correlation. Additionally, existing methods either only estimate the pose of the central frame or regress the 3D pose sequence from the input 2D keypoints, resulting in limited sequence coherence and low efficiency. In this paper, we propose a novel approach called MixSTE, which leverages a transformer-based seq2seq model to learn the separate temporal motion of each body joint and promote sequential coherent human pose estimation. Our method achieves state-of-the-art performance on benchmark datasets and demonstrates outstanding generalization.