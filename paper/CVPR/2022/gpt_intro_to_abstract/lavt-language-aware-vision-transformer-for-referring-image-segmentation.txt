Referring image segmentation is a valuable task in computer vision that involves predicting pixel-wise masks for object delineation based on image and text descriptions. This task presents unique challenges due to the diversity and complexity of natural language expressions. Previous methods have used cross-modal fusion techniques to incorporate both visual and linguistic features, but they fail to fully leverage the potential of Transformer-based encoders. To address this limitation, we propose a Language-Aware Vision Transformer (LAVT) network that integrates linguistic features into visual features during the encoding process. This hierarchical language-aware visual encoding scheme allows for the extraction of helpful multi-modal context, eliminating the need for a complicated cross-modal decoder. Experimental results on popular referring image segmentation datasets demonstrate that LAVT outperforms existing methods, achieving new state-of-the-art results. The proposed framework not only improves the accuracy of referring image segmentation but also demonstrates its effectiveness and generality. The source code for LAVT is available for further exploration.