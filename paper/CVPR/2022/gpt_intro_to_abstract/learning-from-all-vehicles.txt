Abstract:Autonomous driving is a highly anticipated technology, but its widespread deployment has been hindered by the challenge of generalization. Hand-designed classical planning approaches fail to handle diverse driving scenarios, while learning-based methods struggle with a long tail of uninteresting behaviors. In this paper, we propose an orthogonal approach that leverages the observation of accidents and safety-critical scenarios experienced by other vehicles. We present a mapless, learning-based end-to-end driving system called LAV that learns from multi-modal sensor readings and predicts future trajectories for all detected vehicles. We address the challenge of partial observability by using a privileged distillation approach, where LAV learns a perception model for viewpoint-invariant representation and a privileged motion planner for predicting motion based on future waypoints. The two models are combined using privileged distillation, allowing the motion prediction model to drive from raw sensor inputs alone. Our method achieves state-of-the-art results on the CARLA driving simulator, with a high driving score and route completion rate, showcasing its effectiveness in handling diverse driving scenarios. Our method has also won the 2021 CARLA Autonomous Driving challenge. The code is available at https://github.com/dotchen/LAV.