In this computer science paper, the authors address the problem of inferring shape programs from visual data. They define shape program inference as obtaining a latent program that generates a given observed shape. To tackle this challenging problem, the authors propose modeling the posterior distribution of the program given the shape using deep neural networks. However, since paired (shape, program) data is typically unavailable, the authors explore different fine-tuning strategies to train the models.The authors discuss the limitations of current approaches that rely on synthetic programs and shapes, which often have a distributional mismatch with real shapes. To overcome this, they propose a collection of methods that create (shape, program) data pairs for training using maximum likelihood estimation (MLE) updates and treating the program executor as a black box. These methods include wake-sleep and self-training, which make compromises in the formulation of paired data. In self-training, the authors take advantage of the known distribution of shape given the program and use a model to infer latent programs for unlabeled shapes.The authors categorize all these fine-tuning regimes under the conceptual framework of PLAD (Pseudo-Labels and Approximate Distributions). They evaluate the PLAD methods experimentally in various shape domains, such as 2D and 3D constructive solid geometry and assembly-based modeling. The results show that PLAD training regimes outperform policy gradient reinforcement learning in terms of shape reconstruction performance while requiring less computation time. Additionally, the authors find that combining the training updates from a mixture of PLAD methods leads to even better performance.In summary, the contributions of this paper are: 1) introducing the PLAD framework to group a family of self-supervised learning techniques for shape program inference, 2) proposing latent execution self-training as a PLAD method to leverage the unique properties of shape program inference, and 3) conducting experiments in different shape domains to demonstrate the superiority of PLAD methods over policy gradient reinforcement learning and the benefits of combining multiple PLAD techniques. The code for the proposed method and experiments is made available on GitHub.