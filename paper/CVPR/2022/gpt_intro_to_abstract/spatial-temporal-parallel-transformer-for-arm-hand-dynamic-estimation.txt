Human arm-hand dynamics play a crucial role in full human motion capture and human-machine interface control. Existing methods for capturing full human motion and hand gestures often treat these tasks separately, resulting in inaccurate predictions in challenging scenarios. Although some approaches have incorporated body-hand correlation, they have primarily focused on predicting 2D hand keypoints and 3D body keypoints separately. In this paper, we propose a spatial-temporal Parallel Arm-Hand Motion Transformer (PAHMT) to capture accurate and plausible arm-hand dynamics from monocular video. Our model leverages arm-hand correlations and inter-frame information, utilizing a spatial transformer for capturing global and local correlations and a temporal transformer for exploiting inter-frame dynamics. We also introduce two losses to encourage smooth and accurate predictions. To train our model, we collected a dataset of 200K frames of human motion, including hand gestures. Our proposed model demonstrates its efficacy in estimating arm twists and hand gestures in challenging scenarios, such as occlusion and motion blur. Our contributions include capturing arm and hand dynamics simultaneously using arm-hand correlations, designing a spatial-temporal parallel transformer model that utilizes inter-frame information, and demonstrating superior performance compared to existing approaches in various challenging scenarios.