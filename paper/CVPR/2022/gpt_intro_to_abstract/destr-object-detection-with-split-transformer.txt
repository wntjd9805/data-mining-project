This paper introduces a new approach to object detection in images using the Detection Split Transformer (DESTR). The authors address three limitations of existing Transformer-based detectors, including the cross-attention between object queries and the entire image context. DESTR splits the estimation of cross-attention into two independent branches for classification and regression, allowing each branch to focus on different optimal features. Additionally, DESTR incorporates a mini-detector after the encoder to initialize the object queries, and includes pair self-attention to better constrain the hypothesis space. Experimental results show that DESTR outperforms other Transformer-based detectors and is competitive with CNN-based detectors. The paper concludes with a review of related work, a description of DESTR, experimental evaluation, and final remarks.