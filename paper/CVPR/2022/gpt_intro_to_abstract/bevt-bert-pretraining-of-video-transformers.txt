Transformers have emerged as dominant network structures in natural language processing (NLP) and have shown significant success in various NLP tasks. The ViT model introduced the use of transformers for image recognition by tokenizing images into patch-based tokens. This approach has further demonstrated the effectiveness of transformers as generic vision backbones for various vision tasks. While transformers have shown promise in image understanding, there have been limited studies exploring their potential for video understanding. This paper explores the application of BERT pretraining, a successful pretraining task in NLP, for video transformers. Unlike static images, videos present additional challenges due to their dynamic nature. Learning representations from scratch on videos is computationally expensive and requires large-scale datasets. Instead, self-supervised models pretrained on image datasets have been shown to benefit video recognition. However, the spatial context relationships learned from image pretraining may be modified during video feature learning.To address this issue, this paper introduces BEVT (BERT pretraining of video transformers), a model that decouples video representation learning into spatial representation learning and temporal dynamics learning. BEVT builds upon the Video Swin Transformer and is trained with a BERT-style objective. The model includes an image stream for spatial modeling and a video stream for temporal modeling, which interact with each other for video modeling. The image stream learns spatial priors through predicting masked image patches, while the video stream learns temporal dynamics through predicting masked 3D tubes. Both streams are jointly trained on video data.Extensive experiments were conducted on three challenging video datasets, and BEVT achieved impressive results, outperforming state-of-the-art methods. The experiments also showed that different video samples have different preferences towards spatial and temporal clues. The contributions of this paper include exploring BERT-style training for video transformers, introducing a novel two-stream network, identifying the different preferences of video samples, and achieving comparable or better results than existing methods on challenging video benchmarks.