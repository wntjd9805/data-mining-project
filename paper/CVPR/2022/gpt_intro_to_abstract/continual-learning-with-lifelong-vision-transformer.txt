Humans have the ability to continuously learn new concepts and accumulate visual knowledge throughout their lives. However, artificial neural networks often suffer from catastrophic forgetting, where they forget previously learned information when learning new tasks. This can result in a drop in performance on the previous tasks. Continual learning, also known as lifelong or incremental learning, aims to address this challenge by studying how to learn from a non-stationary stream of data while maintaining and extending acquired knowledge over time.Previous works in continual learning have focused on task-incremental learning (task-IL) and class-incremental learning (class-IL). Task-IL methods rely on oracle knowledge of the task identity for selecting the corresponding classifier, while class-IL methods evaluate the network on all classes observed during training without requiring task identity. Rehearsal-based methods that store a small portion of observed data in memory for replaying and distillation-based methods that use knowledge distillation to maintain representation have shown promising results in class-IL.However, existing methods are primarily designed for convolutional neural networks (CNNs) and have not fully utilized the potential of vision transformers, which have recently shown superiority in certain computer vision tasks. Vision transformers, based on the self-attention mechanism, bring a new perspective to the development of continual learning. This work proposes a novel framework called Lifelong Vision Transformer (LVT) that leverages the attention mechanism in continual learning to achieve a better stability-plasticity trade-off.Unlike vanilla self-attention in vision transformers, LVT introduces an inter-task attention mechanism that computes attention maps based on affinities between self-queries and a learnable external key with an attention bias. This mechanism implicitly absorbs previous task information and saves parameters compared to self-attention. Additionally, LVT utilizes two classifiers: an injection classifier to inject new task representation and mitigate interference with previous tasks, and an accumulation classifier to integrate previous and new knowledge in a balanced manner.The paper also proposes a confidence-based memory update strategy to store impressive exemplars in limited memory. These exemplars have distinctive characteristics of their classes and recalling them helps consolidate previous knowledge and reduce forgetting. Experimental results demonstrate that LVT outperforms state-of-the-art methods in accuracy and forgetting, even with fewer parameters. Ablation experiments validate the effectiveness of the proposed components. Overall, this paper presents a novel framework that combines the strengths of vision transformers and attention mechanisms for continual learning, achieving state-of-the-art performance in various benchmarks.