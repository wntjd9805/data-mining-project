State-of-the-art 3D object detectors commonly use LiDAR-produced point clouds as the main input due to their precise depth information and robustness. However, the sparsity of point clouds in small and distant objects makes it difficult to predict their boundaries and semantic classes. On the other hand, camera-produced images offer clear appearance and texture, allowing image-based detectors to recognize object boundaries and classify objects effectively. However, image-based detectors lack depth information and are less accurate and robust than LiDAR-based detectors. Some recent works have explored fusion of 3D point clouds and 2D RGB images to improve feature quality, but this comes at the cost of inference efficiency and sensor calibration. In this paper, we propose a novel approach to train a single-modality network to produce simulated multi-modality features solely from LiDAR input. By learning from a multi-modality LiDAR-image detector during training, our approach achieves high efficiency, precision, and robustness in 3D object detection with only LiDAR input. We address technical challenges related to foreground knowledge transfer, computational feasibility, and knowledge transfer for objects of various sizes and shapes. Our proposed Simulated Single-to-Multi-Modality Single-Stage 3D object Detector (S2M2-SSD) framework effectively trains the single-modality network and outperforms state-of-the-art single-modality detectors in the evaluation on the nuScenes test set. This work fills a significant gap between single- and multi-modality detectors and demonstrates the effectiveness of our approach in autonomous driving applications.