Deep image synthesis has made significant progress in recent years, with Generative Adversarial Networks (GANs) leading the way in synthesizing high-fidelity images at fast speeds. However, GANs still face challenges such as training instability and mode collapse, resulting in a lack of sample diversity. Inspired by the success of Transformer and GPT in NLP, generative transformer models have gained interest in image synthesis. These models treat images as sequences and use autoregressive models to generate image tokens sequentially. Unlike GANs, these models are learned through maximum likelihood estimation and offer stable training and improved distribution coverage. Current research in generative transformers mainly focuses on the quantization of images and borrowing the second stage from NLP. However, treating images as sequential sequences may not be optimal or efficient. Images are not inherently sequential like text and encoding them as a flat sequence can lead to excessively long sequences and decoding intractability. To address this issue, this paper introduces a bidirectional transformer model called Masked Generative Image Transformer (MaskGIT). MaskGIT is trained on a proxy task similar to mask prediction in BERT and adopts a non-autoregressive decoding method during inference. This method allows MaskGIT to generate an image in a fixed number of steps by predicting all tokens simultaneously in parallel and refining the predictions iteratively. The bidirectional self-attention of MaskGIT enables generating new tokens in all directions and improves generation quality. The paper empirically demonstrates that MaskGIT is significantly faster and capable of generating higher quality samples than autoregressive transformers and GAN models on the ImageNet benchmark. It also showcases the multidirectional nature of MaskGIT, which allows it to handle image manipulation tasks that are challenging for autoregressive models. Overall, MaskGIT offers a new approach to efficient and high-quality image synthesis and manipulation.