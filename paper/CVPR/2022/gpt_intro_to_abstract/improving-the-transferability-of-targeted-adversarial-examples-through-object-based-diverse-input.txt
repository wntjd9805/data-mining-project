Deep learning models have shown impressive performance in various domains, but they are susceptible to adversarial examples, which are small perturbations crafted to deceive the models. Interestingly, these adversarial examples designed to attack one network can also disrupt other networks, demonstrating transferability. This property allows adversaries to target black-box models without knowledge of their internal mechanisms. Targeted attacks are particularly challenging as they aim to deceive models into predicting a specific harmful target class. Research on transfer-based targeted attacks is crucial for service providers to protect their models against potential threats and assess their robustness. The success rates of these attacks vary depending on the dissimilarity between the source and target models. Approach and techniques have been proposed to enhance transferability, such as introducing momentum and different loss functions, data augmentation, and using an ensemble of source models. However, existing methods that focus on input transformation have limitations in terms of input diversity. To address this limitation, we draw inspiration from humans' ability to perceive images printed on 3D objects. We propose the object-based diverse input (ODI) method, where we project adversarial examples onto the surfaces of 3D objects and vary the rendering conditions to improve the transferability of the attack. Our contributions include the introduction of the ODI method, which utilizes 3D objects as canvases for adversarial examples and demonstrates that the choice of object affects the attack success rate. Experimental results on the ImageNet dataset show that combining the ODI method with state-of-the-art techniques significantly boosts the targeted attack success rate. We also demonstrate the effectiveness of the ODI method in the face verification task.