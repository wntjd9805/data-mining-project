Present day Deep Neural Networks (DNNs) perform well when training and testing data belong to the same distribution. However, when there is a domain shift and the testing data comes from a different domain, neural networks struggle to generalize. Even a slight distribution shift can significantly degrade the performance of neural networks. One approach to overcome this drop in performance during domain shifts is to obtain labeled data from the shifted domain and retrain the network. However, manual labeling of large amounts of data is costly and time-consuming. Unsupervised Domain Adaptation (UDA) approaches aim to modify network parameters in such a way that they can adapt to out-of-distribution testing data without the need for labeled training data. However, in many practical scenarios, access to both labeled training and large amounts of unlabeled testing data is not feasible. This work highlights the statistical difference between train and test data as a hindrance in domain generalization. The authors propose adapting the running mean and variance of batch normalization layers during inference in an online manner using a small fraction of test data. They also present an adaptive update schema for stable adaptation and fast convergence. The authors show that this online adaptation of batch normalization parameters provides a strong performance gain, even with less than 1% of unlabeled test data. The proposed method, called Dynamic Unsupervised Adaptation (DUA), is simple, unsupervised, dynamic, and does not require back propagation. The computational overhead is negligible, making it suitable for real-time applications. DUA is evaluated on various domain shift benchmarks and achieves state-of-the-art results on most benchmarks while remaining competitive on the rest. The authors also demonstrate the effectiveness of dynamic adaptation for object detection.