Navigating through dynamic environments is crucial for autonomous vehicles, requiring an understanding of the surroundings and the ability to predict future motions of nearby traffic agents. However, accurately predicting the behavior of multiple agents in complex traffic scenarios is challenging. Recent progress in learning-based methods for motion prediction has shown promise, with some approaches rasterizing scenes into bird's eye view images and applying CNNs for predictions. However, these methods are computationally expensive and have limited receptive fields. Other vectorized approaches have been proposed, which extract vectors or points from trajectories and map elements and process them using graph neural networks or Transformers. However, existing vectorized approaches struggle with real-time predictions in rapidly changing traffic conditions, and modeling all-to-all relationships becomes computationally expensive with increasing entities. To address these challenges, we propose a new framework that leverages symmetries and hierarchical structures in multi-agent motion prediction. Our approach involves dividing the scene into local regions centered around each agent, extracting context features locally, and performing global message passing between agent-centric regions using the Transformer encoder. By employing rotation-invariant spatial learning modules and a translation-invariant scene representation, our approach can learn representations robust to translation and rotation of the inputs. This enables faster and more accurate predictions with fewer parameters compared to existing approaches. We validate the advantages of our proposed approach through extensive experiments on large-scale driving data.