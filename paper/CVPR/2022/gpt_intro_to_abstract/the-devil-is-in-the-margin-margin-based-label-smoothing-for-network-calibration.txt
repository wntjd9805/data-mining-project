This paper focuses on the problem of calibrating deep neural networks (DNNs) to improve their predictive uncertainty. High-capacity DNNs often produce over-confident predictions, overestimating the likelihood of correctness. Various methods have been proposed to better calibrate network outputs, such as post-processing steps and explicit maximization of Shannon entropy during training. Recent studies have shown that losses that modify hard-label assignments, such as label smoothing and focal loss, implicitly integrate entropy maximization and improve model calibration. However, these methods have limitations, such as dependence on the dataset and network. This paper introduces a unified constrained-optimization perspective of current calibration losses, viewing them as approximations of a linear penalty imposing equality constraints on logit distances. It also proposes a flexible generalization based on inequality constraints, which adds a controllable margin to logit distances. Comprehensive experiments and ablation studies are conducted on standard image classification benchmarks, fine-grained image classification datasets, semantic segmentation datasets, and an NLP dataset. The experimental results demonstrate the superiority of the proposed method compared to state-of-the-art calibration losses, especially for complex datasets like fine-grained image classification, where the margin-based method significantly improves calibration.