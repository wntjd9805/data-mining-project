Scene parsing is an important task in computer vision that involves assigning class labels to each pixel in an image. Due to variations in texture, lighting, and position, achieving accurate segmentation is challenging. In recent years, convolutional neural networks (CNNs) have been used to address this issue, employing strategies such as pyramid pooling, dilated convolution, and self-attention.Attention-based models have shown considerable promise in scene parsing tasks. These models typically involve two steps: calculating pixel relations and augmenting features based on these relations. Current approaches either use pairwise affinity or coarse segmentation to calculate pixel relations. However, these methods have limitations in terms of computational intensity and the ability to ensure global consistency within the same category.To address these issues, this paper focuses on two main questions: whether there is a different approach to modeling pixel relations, and how to improve global consistency while considering local specificity. The authors propose a new method called Partial Class Activation Attention (PCAA) which combines local and global representations. PCAA utilizes Partial Class Activation Maps (partial CAMs) to capture spatial information within non-overlapping patches of the input image. These partial CAMs are then used to compute pixel-to-class similarity maps and aggregate local representations into global class centers for feature augmentation. This approach effectively handles local variations while ensuring global consistency.The authors validate the effectiveness of PCAA through extensive experiments on challenging public benchmarks, including Cityscapes, Pascal Context, and ADE20K. Their proposed method achieves state-of-the-art results on these datasets. The contributions of this paper include the introduction of Partial Class Activation Maps as a new strategy for representing pixel relations and the design of Partial Class Activation Attention for enhanced feature representation. This research provides a novel perspective on the attention mechanism for semantic segmentation in scene parsing.