Recent unsupervised learning methods have shown better transfer learning performance on visual tasks compared to supervised learning with the cross-entropy loss. This raises the question of why unsupervised pretraining surpasses supervised pretraining despite the latter using annotations with rich semantic information. Some works have suggested that unsupervised pretraining is more effective because it learns without semantic information in annotations, which reduces overfitting to semantic labels, and because of the special design of the contrastive loss, which incorporates low/mid-level information. However, the effectiveness of the multilayer perception (MLP) projector in unsupervised pretraining on transfer tasks has not been thoroughly explored. In this paper, we analyze the transferability of different models using the concept generalization task and identify the MLP projector as a core factor for transferability through empirical and theoretical analysis. We introduce SL-MLP, a method that adds an MLP projector to supervised pretraining with the cross-entropy loss, and show that it can significantly improve the transferability of the model on various downstream tasks. Experimental results demonstrate the benefits of the added MLP in preserving intra-class variation, reducing feature distribution distance, and decreasing feature redundancy in the pretraining dataset. Theoretical analysis further supports the improvement in performance on the target dataset. Our contributions include revealing the significance of the MLP projector in the transferability gap between unsupervised and supervised learning methods, empirically demonstrating the effectiveness of adding an MLP projector to supervised pretraining methods, and theoretically proving the impact of the MLP projector on improving transferability.