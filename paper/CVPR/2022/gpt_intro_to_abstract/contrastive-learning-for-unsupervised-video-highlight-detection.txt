Video highlight detection is a crucial task in computer vision for automatically identifying interesting moments within videos. With the increasing popularity of video content, this task has gained significant importance in various applications such as video retrieval, recommendation, browsing, and editing. Existing methods predominantly rely on manually annotated highlight moments in training videos, making them expensive to obtain. In recent years, weakly supervised methods have been proposed, utilizing video-level labels as a weak supervision signal. However, these methods often rely on large-scale external data. This paper presents a novel unsupervised framework for video highlight detection based on contrastive learning. The proposed framework breaks down each video into fixed-length clips and uses a pre-trained feature extractor to obtain vector representations for each clip. By applying dropout as a random transformation within the network, the framework learns to map two embeddings of the same video with different series of dropouts close together, while mapping different videos farther apart. The authors motivate their approach by highlighting that highlight clips contain more information about the video content, leading to better clustering under random dropout perturbations. Experimental evaluation on three widely-used highlight detection benchmarks demonstrates the superior performance of the proposed framework compared to state-of-the-art methods that rely on large amounts of external data. The framework offers a promising approach for video highlight detection without the need for manually annotated training data or video-level labels.