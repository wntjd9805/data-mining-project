The deployment of neural networks has been challenging and laborious, despite advancements in network designs. In this paper, we propose a solution to bridge the gap between deep learning research and actual deployment. We introduce a set of optimized Convolution Neural Networks (CNNs) tailored for different GPUs, prioritizing inference latency. We leverage Neural Architecture Search (NAS) to automate the design process and measure post-processed TensorRT engine latency for model optimization. Our approach is designed for NVIDIA enterprise GPUs, aiming for widespread adoption. We built a distributed NAS system with a comprehensive search space that includes various factors affecting latency and accuracy. Our system achieves superior performance in terms of latency and accuracy compared to existing models. We validate our approach on COCO detection tasks, where our optimized CNNs consistently outperform other models. We have also developed a model hub with tiered latency levels, which will be released along with this paper.