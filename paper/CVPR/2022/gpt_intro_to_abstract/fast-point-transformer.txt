3D scene understanding is a crucial task in various fields, such as robotics, intelligent agents, and AR/VR. The use of deep learning frameworks for this task has become prevalent, but processing large-scale 3D scenes as a whole remains challenging due to computational and memory constraints. Existing methods often employ cropping or approximation techniques, which lead to increased inference time or degraded output quality. This paper introduces the Fast Point Transformer, a novel local self-attention-based network that efficiently encodes continuous positional information of large-scale point clouds. By leveraging voxel hashing architecture and centroid-aware voxelization and devoxelization techniques, the proposed approach reduces quantization artifacts and ensures dense predictions regardless of rigid transformations. A reformulation of the standard local self-attention equation further reduces space complexity. The Fast Point Transformer exhibits fast inference time and collects rich geometric representations, proving its effectiveness and efficiency in large-scale scene understanding tasks. Experimental results on the S3DIS and ScanNet datasets demonstrate competitive accuracy in semantic segmentation and improved 3D object detection performance compared to other baselines. Additionally, a novel consistency score metric, CScore, is introduced to evaluate the coherence of predictions under rigid transformations. The contributions of this work include the introduction of Fast Point Transformer, a lightweight local self-attention module, the demonstration of more coherent predictions compared to voxel-based approaches, and the achievement of significantly faster inference speed compared to Point Transformer while maintaining reasonable accuracy trade-offs in 3D semantic segmentation tasks.