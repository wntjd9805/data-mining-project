Self-supervised learning (SSL) has gained significant attention in recent research. It has been found that SSL can extract powerful visual representations that compete with supervised learning and achieve superior performance on various visual tasks. Different types of SSL methods have emerged, including contrastive learning, asymmetric network, and feature decorrelation methods. These methods employ siamese networks and aim to reduce the distance between positive samples while pushing apart negative samples. Asymmetric network methods suggest that using positive samples alone is sufficient, while feature decorrelation methods focus on reducing redundancy between different feature dimensions. Despite their differences, these methods share a similar working mechanism and can be unified under a gradient analysis framework. To compare the performance of these methods, fair and detailed experiments are conducted, revealing that different gradient formulas result in similar performance, with momentum encoder being a key factor. From this analysis, a concise and effective gradient formula named UniGrad is proposed, which maximizes the similarity between positive samples and expects zero similarity between negative samples. UniGrad does not require a memory bank or asymmetric network, and can easily incorporate popular augmentation strategies. Extensive experiments demonstrate the competitive performance of UniGrad in various tasks, including linear evaluation, semi-supervised learning, and downstream vision tasks. This work contributes by providing a unified framework for SSL methods, comparing them under controlled conditions, and proposing the effective UniGrad gradient formula.