The development of high quality 6 DoF pose estimation methods is rapidly progressing in the field of computer science. Deep learning methods dominate the field, as indicated by the BOP challenge, which evaluates and compares pose estimation datasets. However, the performance of these methods is limited by the availability of labeled training data, which is a complex and time-consuming process. To simplify this process, many methods have started training on synthetically rendered images. However, this still requires repetitive rendering and model training for each new object of interest. Existing approaches for one-shot object detection, which detect novel objects not seen during training, have shown promising results but are limited in their ability to generalize to new objects. Most related works focus on objects from the same category or with similar geometry, rely on partial training, or only perform viewpoint estimation. In this paper, we propose a one-shot object detection method that extends to full 6 DoF pose estimation. Our method is trained once and automatically generalizes to new objects without the need for synthetic or real data preparation and training. We present a 4-stage pipeline that includes one-shot object localization, initial viewpoint estimation, dense 2D-2D matching, and 6 DoF pose estimation. The evaluation of our approach on multiple datasets demonstrates its ability to generalize to new objects and scenes not seen during training. Our contributions include a truly scalable RGB-based one-shot pose estimation pipeline, a novel architecture and attention mechanism for one-shot semantic segmentation, and an architecture for dense 2D-2D matching that enables correspondence transfer from a template with known pose to a target image with unknown pose.