Machine learning algorithms are frequently trained on sensitive data, posing privacy risks. Even with secure storage and processing, access to these trained models can expose private information. One particular privacy attack is model inversion, where the attacker aims to recreate training data or sensitive attributes using the trained model. Model inversion attacks can reveal fine-grained information and have been demonstrated in various scenarios, such as personalized medicine prediction and image reconstruction. Existing model inversion attacks assume either complete knowledge or the ability to query the model's output, referred to as whitebox and blackbox threat models, respectively. However, in practice, machine learning models often function as blackboxes, only providing hard-label outputs. This label-only threat model poses a more realistic challenge for model inversion attacks due to limited attacker information. In this paper, we propose BREP-MI, a general algorithm for label-only model inversion attacks. BREP-MI utilizes a technique where the synthesized image iteratively moves away from decision boundaries, allowing successful attacks within a given query budget. We provide theoretical justification and empirical evidence demonstrating the efficacy of BREP-MI against various model architectures. Our attack outperforms confidence-based blackbox attacks and achieves comparable performance to state-of-the-art whitebox attacks. We also contribute by releasing data, code, and models to support future research in this area.