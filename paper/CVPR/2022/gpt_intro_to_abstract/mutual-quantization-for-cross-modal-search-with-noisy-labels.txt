Cross-modal hashing efficiently transforms high-dimensional data from multiple modalities into compact binary hash codes, enabling large-scale multi-modal data storage and search. Supervised deep learning-based methods have shown promising results but are often trained on noisy labels, which can lead to overfitting and degrade model generalization. Existing methods for handling noisy labels are limited to unimodal tasks and continuous representations, making them unsuitable for robust binary cross-modal search. In this paper, we perform an empirical study on the impact of noisy labels on deep cross-modal hashing models. We propose a robust framework called Cross-Modal Mutual Quantization (CMMQ) that addresses the challenges of noisy labels and the heterogeneous gap simultaneously. The framework utilizes a proxy-based contrastive loss to narrow the heterogeneous gap, exploits the memorization effect of deep models to select examples with small losses, and incorporates mutual quantization to improve model agreement. Experimental results on benchmark datasets demonstrate the effectiveness of our method compared to state-of-the-art approaches. The contributions of this paper include the proposal of a proxy-based contrastive loss, leveraging the memorization effect of deep models to combat the impact of noisy labels, and the introduction of a mutual quantization loss to improve code quality.