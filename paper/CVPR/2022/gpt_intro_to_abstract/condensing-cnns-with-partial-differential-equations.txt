This paper introduces a novel approach to reducing the computational and storage costs of deep convolutional neural networks (CNNs) by embedding a new layer called the Global feature layer. This layer approximates the solution to a partial differential equation (PDE) constraint that couples the input and output feature maps. By replacing multiple convolutional blocks with a single global feature layer, the network becomes much shallower without significant performance loss. The authors demonstrate the effectiveness of this approach by applying it to the Resnet32 architecture and achieving computational savings without sacrificing accuracy. They also propose an efficient PDE solver and provide pseudo-code for implementing the Global layer in popular deep learning libraries. The experimental results show that the proposed approach leads to shallower networks with fewer parameters and lower computational costs.