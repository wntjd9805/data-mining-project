Supervised end-to-end learning has been successful in various computer vision, speech, and machine translation tasks. However, limited data poses a challenge in training models for inference, especially when annotation is scarce or requires expert knowledge. In contrast, humans can learn new objects from just a few examples, leading to the development of few-shot learning (FSL). FSL algorithms can be categorized into metric learning, meta-learning, and transfer learning. Transductive few-shot methods have shown better performance by jointly inferring class labels for all unlabeled query samples. However, these methods ignore the structure among data points in the support set and the query set. In this paper, we propose a new approach based on the assumption that features in the inference step can be drawn from multiple subspaces, resulting in a block-diagonal prior for the similarity matrix. We introduce the EASE model, which learns a discriminant subspace by maximizing inter-class distance and minimizing intra-class distance. Additionally, we propose the SIAMESE clustering method to estimate class centers and query predictions. Experimental results demonstrate that our model outperforms state-of-the-art methods in various settings, datasets, and training models. Moreover, our transductive inference is fast and applicable to large-scale tasks. Our contributions include the proposed assumption, the EASE model, and the SIAMESE clustering method.