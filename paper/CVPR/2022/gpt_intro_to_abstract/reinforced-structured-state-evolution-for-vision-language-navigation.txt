Embodied-AI (E-AI) has gained significant attention in recent years, particularly in the computer vision and natural language processing communities. This growing interest has led to the construction of various datasets that simulate realistic environments for embodied tasks, such as navigation, interactive learning, and multi-agent cooperation. One attractive application of E-AI is Vision-and-Language Navigation (VLN), where an embodied agent in a 3D environment must navigate to a specific location based on natural language instructions. Previous methods for the VLN task have typically used sequence models to make sequential decisions based on a vector-based navigation state. However, these methods discard important structured layout information from the environment during the navigation process. This lack of structured layout memory can cause confusion for the agent when it encounters landmarks that are not currently observable. To address this issue, we propose a Structured state-Evolution (SEvol) model that maintains a structured navigation state capable of preserving layout memory. The SEvol model introduces several innovations: (1) it adopts a graph-based feature as the navigation state, enabling structured layout memory; (2) it designs a Reinforced Layout clues Miner (RLM) to mine crucial layout information according to the instruction; and (3) it devises a Structured Evolving Module (SEM) to store and update the structured navigation state during the navigation process. Our experiments on Room-to-Room (R2R) and Room-for-Room (R4R) datasets demonstrate that the proposed SEvol model significantly improves VLN performance compared to existing models. Overall, this work contributes to the VLN community by providing insights into the importance of structured navigation states and their impact on task performance. Furthermore, the RLM and SEM modules offer effective strategies for extracting critical layout information and maintaining long short-term layout memory, respectively.