Neural networks have shown remarkable performance in computer vision tasks, but they heavily rely on annotated data for effective training. Semantic segmentation, in particular, requires labeling every pixel, leading to expensive annotation costs. Training with synthetic data has been proposed as a solution, but conventional CNNs struggle with domain shift from synthetic to real data. Unsupervised domain adaptation (UDA) aims to adapt a network trained on synthetic data to real data without target labels. However, previous UDA methods have mostly used outdated network architectures, limiting their overall performance and potentially misguiding UDA benchmarks. This work investigates the influence of network architecture on UDA and introduces DAFormer, a tailored architecture based on Transformers, which have shown robustness compared to CNNs. The proposed architecture incorporates context-aware multi-level feature fusion, enhancing UDA performance and revealing the significant potential of Transformers for UDA semantic segmentation. To address adaptation instability and overfitting, three training strategies are introduced: Rare Class Sampling (RCS) to handle the long-tail distribution of source domain, Thing-Class ImageNet Feature Distance (FD) to regularize source training and leverage expressive features, and learning rate warmup to stabilize the learning process and improve feature transfer. DAFormer outperforms previous methods significantly, demonstrating the importance of network architecture and appropriate training strategies in UDA. The framework achieves substantial improvements on GTA→Cityscapes and Synthia→Cityscapes datasets, even for difficult classes. Furthermore, it simplifies training compared to previous methods, making it more accessible for practical usage.