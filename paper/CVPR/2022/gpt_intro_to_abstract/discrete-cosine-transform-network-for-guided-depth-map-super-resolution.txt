Consumer-oriented depth estimation sensors, such as Time-of-Flight (ToF) and Kinect cameras, have gained popularity and have contributed to advancements in various fields including autonomous driving, pose estimation, virtual reality, and scene understanding. However, these depth maps often suffer from low resolution and noise. In contrast, high-resolution RGB images are readily available in the same scene. Therefore, guided depth map super-resolution (GDSR) using RGB images has become an important topic in multi-modal image processing and super-resolution. This paper proposes a Discrete Cosine Transform Network (DCTNet) for GDSR, inspired by coupled dictionary learning and physics-based modeling. The DCTNet consists of four components: semi-coupled feature extraction, guided edge spatial attention, discrete cosine transform module, and depth reconstruction module. The proposed network leverages the correlation between intensity edge in RGB images and depth discontinuities, while preserving unique properties in both modalities. It also introduces a novel DCT module to improve the explainability of the DL architectures used in GDSR. The DCT module solves an optimization model for GDSR and extracts HR depth map features guided by RGB features. Additionally, an enhanced spatial attention block is employed to address the issue of texture over-transfer from RGB images. Experimental evaluations on four popular RGBD datasets demonstrate that the DCTNet achieves state-of-the-art performance in GDSR with a relatively small number of parameters.