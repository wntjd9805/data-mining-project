In recent years, there has been a significant development in autonomous driving, with scene understanding being a crucial task in its key technologies. One recent task in this field is LiDAR panoptic segmentation, which aims to unify semantic and instance segmentation in a single framework. While most existing methods either rely on object detection or heuristic clustering algorithms, they have limitations such as dependency on object detection performance or high computational requirements. In this paper, we propose Panoptic-PHNet, a novel approach that directly generates a clustering pseudo heatmap from shifted thing points without the need for separate learning tasks. This pseudo heatmap allows for efficient and accurate clustering of instances. We also introduce a knn-transformer module to model the interaction among thing points for improved offset regression. Additionally, our backbone network leverages both fine-grained voxel features and 2D bird's eye view (BEV) features at different scales, enhancing the accuracy of the segmentation results. Experimental results on benchmark datasets show that our approach achieves state-of-the-art performance in real-time scenarios.