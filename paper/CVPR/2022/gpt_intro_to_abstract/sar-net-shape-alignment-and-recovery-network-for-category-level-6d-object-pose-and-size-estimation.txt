Accurate 6D pose estimation of objects is essential for augmented reality, scene understanding, and robotic manipulation applications. However, existing methods often rely on exact CAD object models, limiting their practical applicability. In this paper, we propose a Category-level 6D Object Pose and Size Estimation (COPSE) task that reduces reliance on instance-level CAD models. The main challenge in COPSE lies in the color and shape variations within the same category. Previous works have addressed intra-class variations using RGB(-D) features and synthetic training data. However, the domain gap between synthetic and real images affects the performance of COPSE models. To tackle this, we propose encoding shape information predominantly from the depth channel using shape alignment and symmetric correspondence. Shape alignment is achieved by recovering object rotation, while symmetric correspondence exploits the symmetric structure and geometrical cues of objects. We introduce a novel Shape Alignment and Recovery Network (SAR-Net) that incorporates these ideas, using RGB-D scene images as input. SAR-Net takes advantage of Mask-RCNN for object segmentation and 3D-GCN for 3D point cloud generation. The network predicts the implicit rotation representation and infers symmetric point clouds, enabling obtention of a coarse object shape for accurate object center and size estimation. Extensive experiments conducted on the NOCS dataset demonstrate that our synthetic-only SAR-Net outperforms state-of-the-art methods. Our contributions include the efficient encoding of shape information, the lightweight SAR-Net model, and the successful generalization of our synthetic-only approach to real-world scenarios.