The success of modern object detectors and instance segmentors has been demonstrated on balanced datasets, but their performance declines significantly when applied to datasets with a long-tailed and large-vocabulary category distribution. This decline is due to classification prediction bias caused by imbalanced training sample volumes among foreground classes. Previous works have used class-wise sample frequencies in the training set as an indication for debiasing, but this approach does not consider factors like optimization hardness and inter-class similarities. To address this issue, we propose using a categorical confusion matrix, which captures the misclassification probabilities between pairs of classes, as an indicator for model bias. We conduct a proof study on the LVIS v0.5 dataset to verify the effectiveness of confusion matrix calibration in improving performance, particularly for rare classes. However, using the confusion matrix from the training set for calibration is not practical as it does not match the patterns of the test set. To overcome this, we develop an online pairwise bias-driven calibration method called Pairwise Class Balance (PCB), which maintains a confusion matrix during training and adjusts the pairwise bias for each proposal learning. We also introduce a prediction-dependent iterative paradigm to enhance the features recurrently and achieve progressive pairwise class balancing. Our experiments on the LVIS v0.5 and LVIS v1.0 datasets demonstrate the effectiveness of our method for long-tailed instance segmentation. Overall, our contributions include exploring the use of the confusion matrix for indicating model bias, proposing the PCB method, and conducting extensive experiments to validate the effectiveness of our approach.