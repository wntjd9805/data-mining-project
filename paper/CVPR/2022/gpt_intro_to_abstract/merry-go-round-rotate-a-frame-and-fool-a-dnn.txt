Deep Neural Networks (DNNs) have achieved impressive performance in computer vision tasks. However, they are vulnerable to imperceptible adversarial perturbations. Adversarial attacks aim to create perturbations that, when added to clean images, generate adversarial samples that deceive DNN models into making incorrect predictions. These perturbations should be imperceptible to humans. This paper proposes a novel method for mounting black box adversarial attacks on video analysis (VA) systems. Previous techniques for AA on VA systems use frame-wise attacks with intensity-based noise, which fail to coordinate the perturbations between consecutive frames. This paper introduces a parameterized perturbation approach that allows coordinated changes in intensity levels across frames. Geometric transformations are used to implement the perturbations, which preserve the semantic integrity of the frames while making them imperceptible to humans. The paper also proposes a new DNN architecture for predicting intensity and geometric perturbations, which successfully fools VA systems in fewer queries compared to state-of-the-art techniques. Experimental results on benchmark datasets for egocentric and third-person videos demonstrate the effectiveness of the proposed architecture.