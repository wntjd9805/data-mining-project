3D face reconstruction has gained increasing attention in recent years due to applications in digital human, games, and mobile photography. The pioneering method, 3DMM, provides reliable facial priors, allowing reconstruction through optimization and fitting. With the advancement of deep learning, recent methods have emerged that learn to regress 3DMM parameters from input images. Additional efforts have been made to improve non-linear modeling and multi-view consistency. In addition to 3DMM-based approaches, recent work attempts to model 3D faces without shape assumptions, which have the potential to overcome the limitations of 3DMM. However, existing learning-based methods require differentiable renderers, which often suffer from approximation or ill-posed decomposition. This introduction proposes a novel Physically-guided Disentangled Implicit Rendering (PhyDIR) framework for 3D face reconstruction. By disentangling 3D physical pipelines from neural reasoning, PhyDIR achieves robust and photo-realistic 3D modeling/editing from input facial photos. The framework combines a texture modeling network and a 2D-aware neural appearance renderer, with explicit 3D pipelines acting as guidance. PhyDIR leverages a multi-image shading module to compensate for monocular ambiguity, allowing for unsupervised learning of lighting variations. The neural appearance renderer uses a projected 2D texture for image formation, constrained by 3D consistency losses. PhyDIR ensures explainable 3D controls and photo-realistic image formation without hand-crafted rules. The contributions of this work include the proposal of the PhyDIR framework, which integrates the advantages of graphics/neural renderers and overcomes the limitations of existing methods. Additionally, novel modules and consistency losses are introduced to guarantee fine-grained 3D controls and robust rendering under 3D operations.