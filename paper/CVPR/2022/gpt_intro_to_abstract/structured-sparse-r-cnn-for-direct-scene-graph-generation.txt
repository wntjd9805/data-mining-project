Scene graph generation (SGG) is an important task in computer vision, aiming to detect objects and their pairwise relations in images. This structured representation has various applications in high-level visual understanding tasks such as image captioning and visual question answering. Most existing methods for SGG follow a multi-stage pipeline, which involves object detection, fully-connected relation graph construction, and relation classification. However, these methods rely heavily on object detection performance and often involve redundant computation.In addition to structure information, the sparsity of relation detection in natural images is another important property. Previous dense or dense-to-sparse detection methods for SGG have failed to effectively capture the sparse and semantic priors. Inspired by recent advances in sparse object detectors, this paper presents a new perspective on SGG by treating it as a direct sparse set prediction problem. The proposed framework, called Structured Sparse R-CNN, eliminates the need for explicit object detection and relation graph construction. Instead, it uses learnable triplet queries and a structured triplet detector to capture the general sparse detection prior and encode spatial and appearance information.Training Structured Sparse R-CNN directly from scratch is challenging due to the sparse annotations of relations in current datasets. To address this, a Siamese Sparse R-CNN is introduced to guide the training process using knowledge distillation. Pseudo-labels generated by the Siamese Sparse R-CNN are used for training, and a relaxed matching criteria is designed for the set prediction loss to enhance stability. Additionally, an adaptive focusing parameter and post-hoc logit adjustment are proposed to handle the imbalance distribution of object and relation categories.Extensive experiments on Visual Genome and Open Images datasets demonstrate that the proposed framework achieves state-of-the-art performance in scene graph generation. Ablation studies provide insights into the effectiveness of the structure modeling in the proposed design. In summary, this paper presents a novel sparse and unified framework for direct scene graph generation, overcoming the limitations of existing methods and achieving superior performance.