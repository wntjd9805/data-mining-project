Deep convolutional neural networks (CNNs) have shown impressive performance in various intelligent applications, but their high demands on storage and computational resources make them unsuitable for deployment on mobile and embedded devices. To address this issue, extensive research has been conducted on model compression and acceleration methods, including pruning, tensor decomposition, knowledge distillation, and quantization. Quantization, in particular, has been effective in reducing network complexity by representing weights and activations with low-bit integer values. However, current quantization methods often overlook the diversity of each instance in the dataset. In this paper, we propose a novel network quantization scheme called Dynamic Quantized Network (DQNet) that dynamically allocates bit-widths for quantized neural networks based on each input sample. This instance-aware approach allows for better trade-offs between performance and computational complexity. DQNet generates hidden sub-networks with various bit-width configurations from the given network architecture, and uses a lightweight bit-controller to predict the optimal bit-width sequence for each layer. The quantized neural network and the bit-controller are trained together in an end-to-end manner. Experimental results on CIFAR-10 and ImageNet datasets demonstrate that DQNet achieves comparable or even better classification accuracy than static quantization methods, while significantly reducing computational costs. The dynamic computation resource allocation of DQNet offers improved trade-offs between accuracy and computational cost compared to conventional static quantization methods.