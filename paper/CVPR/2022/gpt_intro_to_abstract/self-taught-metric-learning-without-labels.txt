Understanding the similarities between data is crucial in various machine learning tasks such as data retrieval, face verification, person re-identification, few-shot learning, and representation learning. Metric learning plays a vital role in capturing semantic similarity by learning an embedding space where the distance between data pairs represents their inverse semantic similarity. However, recent advances in metric learning heavily rely on supervised learning with large-scale datasets, which can be costly and limit the generalization capability of models. To address this, unsupervised metric learning has been explored, but existing methods often fail to capture intra-class variation or impose computational burdens. In this paper, we propose a novel end-to-end self-taught metric learning (STML) approach that overcomes the limitations of previous works and achieves state-of-the-art results. Our framework leverages self-exploration within each mini-batch to predict class-equivalence relations between data and uses these predictions as synthetic supervision for metric learning. The pipeline of our STML framework consists of two embedding networks, a teacher and a student, where the teacher provides estimated semantic similarities as soft pseudo labels for the student model. To enhance the quality of predicted semantic similarities, we compute contextualized semantic similarities considering the overlap of contexts and pairwise distance. This approach provides rich information and requires no external modules or memory banks, making STML efficient and concise. The asymmetric design of the teacher and student networks allows the teacher to provide robust supervision by learning a higher dimensional embedding space, while the student model remains compact. STML is the only unsupervised metric learning method that considers semantic relations between data in end-to-end training without requiring off-the-shelf techniques. Compared to instance-level surrogate classes and previous pseudo labeling approaches, STML generates and exploits pseudo labels that are more suitable for capturing semantic relations and allows for efficient and less hyper-parameter sensitive training. We evaluate STML on standard benchmarks for metric learning and demonstrate its superiority over existing unsupervised learning methods. Remarkably, STML even outperforms some supervised learning models. Additionally, STML demonstrates its efficacy on benchmarks for semi-supervised metric learning, surpassing previous works. The proposed STML framework provides a promising approach for unsupervised metric learning and can also be applied to semi-supervised scenarios.