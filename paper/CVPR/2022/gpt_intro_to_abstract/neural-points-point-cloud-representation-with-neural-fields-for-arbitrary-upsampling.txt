This paper introduces Neural Points, a novel representation for point clouds in 3D scenes. Traditional point cloud representations are limited by their resolution, but Neural Points overcomes this limitation by encoding each point as a local surface patch represented via neural fields. These neural fields allow for a continuous representation of shape information, providing several advantages over traditional discrete point cloud representations. The trained model of neural fields is shared for all local patches, resulting in minimal storage overhead. The paper also presents an integration strategy to combine the local neural fields and generate a global shape. The proposed Neural Points representation proves to be effective in capturing geometric details and outperforms existing methods in point cloud upsampling tasks. Overall, this work contributes a novel point cloud representation with improved representation ability and low storage overhead, demonstrating robustness and generalization capabilities.