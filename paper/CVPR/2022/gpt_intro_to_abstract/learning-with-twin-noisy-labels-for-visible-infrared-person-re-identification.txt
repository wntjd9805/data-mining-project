This paper addresses the problem of person re-identification (ReID) in the context of visible-infrared person re-identification (VI-ReID). While existing ReID methods focus on RGB images captured by visible cameras, they often fail to achieve satisfactory results in poor illumination environments, such as at night. To overcome this limitation, some VI-ReID methods have been proposed, which aim to find corresponding identities across visible and infrared modalities. However, the success of VI-ReID heavily relies on high-quality annotated data, which is challenging and even impossible to obtain due to the poor recognizability in the infrared modality.This paper introduces a new problem for VI-ReID, termed Twin Noisy Labels (TNL), which considers both noisy annotations (NA) in the category and noisy correspondences (NC) between cross-modal pairs. The authors argue that existing methods for rectifying noisy annotations are not applicable to VI-ReID due to the large number of categories and the difficulty of avoiding noisy correspondences. To solve the TNL problem, the authors propose a novel method called DuAlly Robust Training (DART), which consists of co-modeling and pair-division modules with a new objective function. The co-modeling module computes clean confidence for each sample using the memorization effect of deep neural networks, while the pair-division module rectifies noisy correspondences and divides noisy pairs into subsets. The proposed method employs a soft identification loss and adaptive quadruplet loss to achieve robust VI-ReID representation and alleviate the modality discrepancy.The contributions of this work include the identification and investigation of the TNL problem in VI-ReID, the proposal of a novel method (DART) for learning with TNL, and the evaluation of the method on SYSU-MM01 and RegDB datasets, demonstrating its effectiveness compared to five state-of-the-art methods. This paper presents a promising solution to the challenging TNL problem in VI-ReID, which has not been addressed before.