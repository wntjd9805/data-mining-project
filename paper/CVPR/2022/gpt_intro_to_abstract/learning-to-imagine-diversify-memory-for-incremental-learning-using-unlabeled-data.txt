Recent years have seen the significant progress of deep neural networks (DNNs) in various tasks. However, when a pretrained deep model learns a new task, it often forgets the knowledge acquired from previous tasks in the absence of the corresponding training data. This phenomenon, known as catastrophic forgetting, limits the practical application of deep models since it is impractical to maintain the training data of each task due to privacy concerns and other factors. To overcome this problem, incremental learning methods have been developed, which typically employ the rehearsal strategy of storing a limited number of exemplars from the original training dataset and reusing them to mitigate forgetting when learning new tasks. However, due to privacy concerns, only a few exemplars with limited variations can be stored, hindering the effectiveness of these methods. In this paper, we propose a learnable feature generator that leverages unlabeled data to adaptively diversify the exemplars. By exploiting semantic-irrelevant information from the unlabeled data, the feature generator can generate various counterparts for the exemplars, thereby increasing their diversity and addressing the forgetting problem. Our method follows a two-stage training schedule, where first, the feature generator is trained to generate diverse counterparts of the exemplars based on the exemplars and massive unlabeled data. Semantic contrastive learning and semantic-decoupling contrastive learning are employed to ensure that the generated samples are diverse and semantically consistent with the exemplars. In the second stage, when a new task starts, the frozen feature generator is used to generate diverse samples to prevent forgetting. Our method does not introduce extra inference costs and is insensitive to unlabeled data. Experimental results demonstrate the effectiveness of our approach, outperforming existing methods.