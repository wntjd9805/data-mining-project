Cross-modal retrieval, specifically text-to-image retrieval, is an important task with valuable applications. Recent advancements in transformer-based architectures have significantly improved the performance of cross-modal retrieval. However, these deep interaction approaches are computationally expensive for large-scale retrieval tasks. To address this issue, CLIP, ALIGN, and WenLan have employed cross-modal contrastive pre-training with separate encoders for images and text. While these methods have improved retrieval performance, they struggle to learn specific fine-grained visual concepts, particularly scene text semantics. A new cross-modal retrieval task that incorporates scene text features has been proposed, but existing models designed for this task face challenges in generating reliable similarities for images without scene text instances. To address these challenges, we propose a Vision and Scene Text Aggregation (ViSTA) framework that utilizes a full transformer architecture to encode image patches and fuse scene text embeddings. We introduce a token-based aggregation approach to share necessary scene text information via a fusion token and employ dual contrastive supervisions to strengthen visual modality. ViSTA improves retrieval performance for both scene text aware and scene text-free scenarios, outperforming existing fusion methods. We make three key contributions: 1) an effective transformer architecture for aggregating vision and scene text, applicable in both scenarios; 2) a fusion token-based aggregation design and dual contrastive losses to enhance visual features; and 3) a cross-modal retrieval framework that achieves state-of-the-art performance for both scene text aware and scene text-free retrieval tasks. This paper presents the first transformer-based solution for these tasks and demonstrates the effectiveness of the ViSTA framework.