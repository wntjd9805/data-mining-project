In this paper, we address the challenge of training deep neural networks for visual classification while ensuring privacy for individual training samples. We introduce the concept of Mixed Differential Privacy (MixDP) and propose a method called AdaMix that utilizes public data in addition to private data to achieve a balance between privacy and accuracy. We show that using public data for pre-training and few-shot learning can lead to significant performance gains compared to using only private or only public data. We also analyze the impact of long-tailed data on privacy and demonstrate the effectiveness of our approach in achieving high classification performance. Furthermore, we present a convergence proof for our algorithm and provide new theorems and empirical analysis to evaluate its utility and privacy guarantees. Overall, our contributions include the introduction of AdaMix, improved performance on MixDP vision tasks, the integration of text information for private models, and theoretical and empirical analysis of our method.