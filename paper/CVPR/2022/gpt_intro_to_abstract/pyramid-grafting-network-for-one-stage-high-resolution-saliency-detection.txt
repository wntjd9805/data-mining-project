Salient object detection (SOD) is an important task in computer vision that aims to identify and segment the most attractive objects in a given scene. While deep neural network-based SOD methods have achieved significant progress, most existing methods are designed to perform well within a specific input low-resolution range. With the increasing availability of high-resolution images, these methods struggle to adapt directly to such inputs, resulting in incomplete segmentation and loss of detail. This limitation arises from the fact that most low-resolution SOD networks are designed in an Encoder-Decoder style, which fails to capture global semantics necessary for SOD when the input resolution increases.To address these challenges, previous high-resolution SOD methods have adopted multi-stage architectures, dividing the SOD task into semantic and detailed phases. However, this approach leads to inconsistent transfer of contextual semantics between stages and increased computational complexity. In this paper, we propose a novel one-stage deep neural network called Pyramid Grafting Network (PGNet) for high-resolution saliency detection. PGNet employs both Resnet and Transformer as encoders, extracting features with different spatial sizes in parallel. The Transformer branch decodes the features in FPN style and passes the global semantic information to the Resnet branch through feature grafting, which completes the decoding process. To facilitate the feature grafting between different models, we introduce the Cross-Model Grafting Module (CMGM) based on attention mechanisms, along with the Attention Guided Loss to guide the grafting process.To promote future research in high-resolution salient object detection, we provide a new challenging Ultra High-Resolution Saliency Detection dataset (UHRSD) containing over 4K resolution images and corresponding pixel-wise salient annotations. Experimental results on existing datasets as well as UHRSD demonstrate that our PGNet outperforms state-of-the-art methods in terms of accuracy and speed. Our contributions include the introduction of a one-stage framework for high-resolution SOD, the development of the Cross-Model Grafting Module for information transfer, the design of the Attention Guided Loss for feature grafting, and the creation of the UHRSD dataset.