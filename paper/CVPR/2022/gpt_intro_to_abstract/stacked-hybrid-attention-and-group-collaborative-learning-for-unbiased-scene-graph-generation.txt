Scene Graph Generation (SGG) is a task in computer vision that aims to organize objects and their pairwise relationships into a concise summary graph. SGG has applications in various vision-and-language tasks such as cross-modal retrieval, image captioning, and visual question answering. However, current SGG approaches still face challenges related to modality fusion and biased relationship predictions. To improve modality fusion, previous methods have simply fused visual and semantic features without taking into account their interaction information. In this paper, we propose a Stacked Hybrid-Attention (SHA) network that incorporates Self-Attention (SA) and Cross-Attention (CA) units to capture intra-modal and inter-modal information. These units are organized into a Hybrid-Attention (HA) layer, and multiple HA layers are stacked to build the encoder. This enhances the model's multi-modal interaction capabilities and improves relationship prediction performance.Another issue in SGG is biased relationship predictions caused by imbalanced data distribution. Some predicates dominate the training procedure, resulting in scene graphs with limited information about tail predicates. Existing debiasing approaches are vulnerable to overfitting on tail classes or sacrificing head classes. To address this issue, we propose the Group Collaborative Learning (GCL) strategy. GCL divides biased predicate classes into balanced subsets and introduces additional classifiers for each subset. We use the class-incremental learning approach to continuously extend the classification space, and the Median Re-Sampling strategy to provide balanced training sets for each classifier. These nested classifiers work together to mitigate biases and improve relationship predictions.Our contributions are threefold. First, we present a Stacked Hybrid Attention network that enhances modality fusion in SGG. Second, we propose the Group Collaborative Learning strategy that optimizes the decoder in SGG by introducing multiple classifiers and cooperatively optimizing them. Finally, experimental results on VG and GQA datasets demonstrate that our approach achieves state-of-the-art unbiased metric performance and significantly outperforms baseline methods when employing the model-agnostic GCL strategy.