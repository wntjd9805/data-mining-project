This paper introduces a novel method for discovering generic event boundaries in videos. The prevailing convention of dividing videos into short non-overlapping snippets neglects the semantic continuity of the video. In contrast, human perception views videos as a set of events, suggesting the need for a video parsing method that preserves semantic validity and interpretability of video snippets. The Generic Event Boundary Detection (GEBD) approach aims to identify content change moments that humans perceive as event boundaries. This paper presents the Kinetics-GEBD dataset, which includes event boundary labels annotated by multiple annotators to capture the subjectivity of human perception. Several baseline methods for GEBD are also included in the dataset. However, existing methods that use pre-trained features have limitations, as they focus on class-specific or object-centric information and may not effectively capture event boundaries not entailing scene change. To address this, the paper introduces a novel method, Recursive TSM Parsing (RTP), which utilizes the Temporal Self-similarity Matrix (TSM) to detect event boundaries. The paper also proposes a supervised approach, Supervised Boundary Contrastive (SBoCo) learning, that utilizes TSM as an interpretable intermediate representation. The results demonstrate that the proposed methods outperform previous unsupervised and supervised approaches, achieving state-of-the-art performance. Overall, this paper makes contributions in leveraging TSM for GEBD, introducing RTP and UBoCo as unsupervised frameworks, and proposing SBoCo for supervised event boundary detection.