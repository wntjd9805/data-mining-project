This paper introduces a new generative model called VolumeGAN, which aims to achieve 3D-aware image synthesis through explicit learning of a structural and textural representation. The existing approach of directly employing Neural Radiance Field (NeRF) in the generator poses challenges related to representing global structure and computational cost for high-resolution image generation. The proposed VolumeGAN addresses these challenges by using a 3D convolutional network to generate a feature volume that encodes the relationship between spatial regions. This feature volume compensates for the limited receptive field of NeRF's Multi-Layer Perceptron (MLP). The structural information for each 3D point is represented using a coordinate descriptor obtained from the feature volume, and a NeRF-like model is employed to create a feature field. This feature field is then accumulated into a 2D feature map to represent texture, and a CNN with a 1x1 kernel size is used to render the output image. The use of separate representations for structure and texture enables the disentangled control of shape and appearance. The proposed VolumeGAN achieves superior performance in terms of image quality, with significantly better Fr√©chet Inception Distance (FID) scores compared to existing alternatives. Evaluation on various datasets showcases the model's ability to synthesize 3D-aware images and demonstrate stable control of object pose and consistency across different viewpoints. Additionally, an empirical study analyzes the trade-off between image quality and the 3D property of learned representations.