Embodied visual navigation, which involves using visual sensing to navigate the world, has seen significant advancements in recent years. One popular form of embodied visual navigation is ObjectNav, where an agent must navigate to a specific object in an unmapped 3D scene. This task requires semantic reasoning about the environment and serves as a precursor to more complex object manipulation tasks. While reinforcement learning (RL) approaches have made progress in solving ObjectNav, they suffer from high computational costs, poor sample efficiency, and limited generalization to new scenes. Modular navigation methods have emerged as promising alternatives to RL, but they still require extensive computational resources for training. To address these challenges, this paper introduces a new method called Potential functions for ObjectGoal Navigation with Interaction-free learning (PONI). The authors propose that determining "where to look" for an unseen object is fundamentally a perception problem and can be learned without interactions. They introduce a potential function, which represents the value of visiting each location in the environment to find the object, and use a potential function network to estimate this function from a partially filled semantic map. The network leverages geometric and semantic cues to predict two complementary potential functions: an area potential for efficient exploration and an object potential to determine how to reach the object. Experiments conducted on Gibson and Matterport3D environments demonstrate that PONI outperforms state-of-the-art modular RL and end-to-end RL methods in terms of training cost and performance. PONI achieves the highest scores on the Habitat ObjectNav challenge leaderboard compared to previously published methods. The proposed method offers a promising approach to addressing the challenges of ObjectNav by leveraging perception-based learning and modular navigation techniques.