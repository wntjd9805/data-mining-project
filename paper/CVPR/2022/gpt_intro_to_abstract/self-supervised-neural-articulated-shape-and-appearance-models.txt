Reconstructing articulated 3D objects from 2D image observations is a challenging problem in computer vision with various applications in robotics and augmented/virtual reality. Most existing approaches rely on object-specific priors learned from large datasets with 3D ground truth, which can be costly and difficult to obtain. These approaches often encode shape, appearance, and motion using a low-dimensional latent space to constrain the reconstruction problem. However, most techniques focus on reconstructing static objects, and dynamic objects with controllable articulation are less explored. Previous methods also require dense 3D ground truth for training and do not model object appearance.In this paper, we propose a novel approach to jointly learn the geometry, kinematics, and appearance manifold of a class of articulated objects using only color images as input. Our approach is self-supervised, meaning it does not require explicit geometry supervision. It enables independent control of learned semantic dimensions and handles a variety of joint types. We outperform state-of-the-art methods that rely on ground truth geometry and our representation allows for various applications, such as few-shot reconstruction and novel view synthesis. Our model, trained on synthetic data, can also be fine-tuned for real-world articulated objects.