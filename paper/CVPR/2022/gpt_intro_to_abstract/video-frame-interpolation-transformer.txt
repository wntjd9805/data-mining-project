Video frame interpolation is a crucial problem in computer vision, aiming to generate intermediate frames between existing frames in a video. This task has various applications, including image restoration, virtual reality, and medical imaging. Most state-of-the-art methods for video frame interpolation rely on deep convolutional neural networks (CNNs). However, these CNN-based approaches have two major limitations. Firstly, the content-agnostic nature of CNNs restricts their ability to model complex and content-dependent motion-compensation processes involved in video interpolation. Secondly, CNNs usually employ small convolution kernels, which are inefficient in capturing long-range dependencies necessary for handling large motion fields in video interpolation.To overcome these limitations, we propose the Video Frame Interpolation Transformer (VFIT). Transformers, initially designed for natural language processing, have been successfully adapted to computer vision tasks and offer potential benefits for video interpolation. In our approach, we make several key design choices to generate visually realistic and temporally coherent frames. Firstly, we introduce a local attention mechanism, inspired by the Swin Transformer, to address the computational complexity of the global self-attention used in traditional Transformers. This local attention mechanism facilitates modeling long-range dependencies while significantly reducing memory and computational costs. Secondly, we extend the local attention mechanism to the spatial-temporal domain to handle video inputs effectively. To enhance memory efficiency, we propose a space-time separable version of spatial-temporal Swin attention, called Sep-STS. This version not only reduces memory usage but also improves video interpolation performance. Finally, we propose a multi-scale kernel-prediction framework that can handle multi-scale motion and structures in videos and generate high-quality video interpolation results in a coarse-to-fine manner.Our experiments show that VFIT achieves superior performance compared to state-of-the-art methods with a more compact model size. Even the small model variant of VFIT outperforms the FLAVR method with only a fraction of its parameters. Our proposed VFIT offers a concise, flexible, and memory-efficient solution for video frame interpolation, demonstrating its effectiveness in generating high-quality intermediate frames in videos.