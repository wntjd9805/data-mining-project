Egocentric vision presents unique challenges in computer vision, including human-object interaction, action anticipation, action recognition, and video summarization. With the availability of large-scale datasets, new tasks such as wearer's pose estimation and egocentric videos anonymization are being explored. However, RGB-based deep models suffer from environmental bias, where their performance decreases when the training and test data have different distributions. This bias is caused by appearance-based networks relying on background cues and object texture, which can vary in different environments. As a result, motion-based modalities have become more favorable in egocentric vision systems. However, computing optical flow from RGB frames introduces significant computational costs. Event-based cameras, on the other hand, offer reduced motion blur, low latency, and low power consumption, making them suitable for egocentric scenarios. Yet, these sensors remain unused in this context. In this paper, we introduce N-EPIC-Kitchens, the first dataset that enables the use of event data in egocentric vision. This dataset allows us to analyze environmental bias and compare event data to other modalities. We propose two approaches, E2(GO) and E2(GO)MO, to exploit the motion characteristics of event data for egocentric action recognition. We release N-EPIC-Kitchens, benchmark it on popular architectures, and demonstrate the robustness of event data to environment changes. Our results show that event data can outperform RGB in unseen environments and are competitive in known environments, suggesting the viability of using event data in egocentric vision.