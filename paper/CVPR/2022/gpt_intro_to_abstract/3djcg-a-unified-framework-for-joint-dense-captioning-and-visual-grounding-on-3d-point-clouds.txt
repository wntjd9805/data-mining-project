This paper introduces a unified framework for jointly solving the tasks of 3D dense captioning and 3D visual grounding in the intersection field of 3D visual understanding and natural language processing. The framework leverages the shared and complementary information between the two tasks to enhance their performance. It consists of three main modules: a 3D object detector, an attribute and relation-aware feature enhancement module, and a task-specific grounding or captioning head. The framework treats each task as a proxy task of the other, allowing for the integration of object attribute information and complex object relations. Experimental results on benchmark datasets demonstrate that the proposed framework achieves state-of-the-art results for both 3D dense captioning and 3D visual grounding tasks.