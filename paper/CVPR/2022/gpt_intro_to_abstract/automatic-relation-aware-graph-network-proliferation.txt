Graph neural networks (GNNs) have gained traction in handling graph-structured data and have shown impressive performance in various relation reasoning tasks. However, the manual design of GNN architectures requires extensive expert knowledge and trial and error. To address this, researchers have turned to automated neural architecture search (NAS) to discover advanced GNN architectures with reduced human intervention. This paper focuses on graph neural architecture search, which consists of two critical components: the graph search space and the search strategy. While current graph search space mainly focuses on node-learning operations, the latent hierarchical relational information associated with edges is often neglected. This paper explores the design of the micro graph search space from the perspective of learning both hierarchical relations and node features. The search strategy plays a crucial role in determining the efficiency and effectiveness of the search process. Previous works have applied reinforcement learning-based strategies, which are time-consuming, while one-shot differentiable strategies have shown promise due to their high computational efficiency. However, these strategies suffer from limitations such as subnet interference, high space-time complexity, and a shrink in the search space. To address these limitations, the paper proposes the Automatic Relation-aware Graph Network Proliferation (ARGNP), which efficiently searches for optimal GNN architectures with relation-guided message passing mechanisms. The paper introduces a dual relation-aware graph search space and a novel search paradigm called network proliferation. Network proliferation progressively explores the graph search space by dividing and differentiating network structures. The proposed method outperforms human-crafted and other search-based GNNs in classical graph learning tasks across multiple datasets.