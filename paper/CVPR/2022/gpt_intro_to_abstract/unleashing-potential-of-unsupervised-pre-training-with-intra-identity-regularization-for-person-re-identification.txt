Person re-identification (ReID) is a fine-grained classification problem that aims to identify individuals across non-overlapping camera views. While there have been successful ReID methods in both supervised and unsupervised domains, these approaches often rely on pre-training weights from ImageNet, which may not be optimal for ReID tasks due to the coarse-grained classification nature of ImageNet and the large domain gap between ImageNet and ReID datasets. To address these challenges, this paper proposes a ReID-specific pre-training framework called UP-ReID. UP-ReID introduces an intra-identity (I2-) regularization that consists of a global consistency constraint and an intrinsic contrastive constraint. The global consistency constraint ensures that the pre-training model is invariant to augmentations by narrowing the similarity distance between augmented and original person images. The intrinsic contrastive constraint explores the fine-grained information of person images by partitioning augmented images into multiple patches and computing an intrinsic contrastive loss among these patches. To improve the stability of training, a hard mining strategy based on the horizontal symmetry of the human body is established for the contrastive loss calculation. The main contributions of this paper include: 1. UP-ReID is the first attempt towards a ReID-specific pre-training framework that explicitly addresses the differences between general pre-training and ReID pre-training. 2. UP-ReID introduces an intra-identity regularization, incorporating global consistency and intrinsic contrastive constraints, to exploit both global and local discriminative clues in person images. Experimental results on several widely-used ReID benchmarks demonstrate the effectiveness of UP-ReID, outperforming state-of-the-art pre-training methods and showcasing its potential for improving downstream ReID-related tasks.