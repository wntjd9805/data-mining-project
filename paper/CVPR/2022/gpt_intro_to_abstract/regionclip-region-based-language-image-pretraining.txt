The recent progress in vision-language representation learning has resulted in impressive models such as CLIP, ALIGN, and Florence. These models have been trained on large-scale image-text pairs and have shown remarkable performance in recognizing concepts without manual labels. However, their applicability to reasoning about image regions, particularly for tasks like object detection, remains an open question.To address this question, we propose a simple object detection framework inspired by R-CNN, where we utilize a pretrained CLIP model to detect objects by matching visual features of candidate regions to text embeddings of object categories. Our experiments on the LVIS dataset reveal that the performance of the pretrained CLIP model drops significantly in object detection tasks, indicating the need for improvement.We hypothesize that the limitation lies in the training of vision-language models, such as CLIP, which are primarily trained to match images with their image-level text descriptions. These models lack precise alignment between local image regions and textual tokens, resulting in an inability to ground textual concepts to specific image regions. Moreover, the cropping and matching approach used in existing models disregards the critical visual context required for object recognition and incurs high computational costs.To address these challenges, we propose a novel approach to learning region representations for object detection through vision-language pretraining. Our approach involves explicitly aligning image regions and text tokens during pretraining, despite the lack of fine-grained alignment information in image-text pairs. We bootstrap from a pretrained vision-language model to align image regions and text tokens and fill in missing region descriptions using pre-defined templates.We pretrain our RegionCLIP model on image captioning datasets and evaluate its performance on open-vocabulary object detection benchmarks such as COCO and LVIS. Results show that our pretraining approach achieves state-of-the-art performance on these benchmarks, outperforming previous methods by a significant margin. Furthermore, our model supports zero-shot inference and demonstrates promising capabilities in object detection. Our contributions include a novel method for aligning image regions and textual descriptions, a scalable approach using text prompts for alignment without manual annotations, and a pretrained model that outperforms existing methods in open-vocabulary object detection tasks.