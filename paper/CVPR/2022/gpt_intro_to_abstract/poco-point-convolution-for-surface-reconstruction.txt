Constructing surface or volume representations from 3D point data has various applications in fields such as digital twins, augmented reality, and virtual reality. With advancements in cheaper sensors and multi-view stereo techniques, the opportunities for 3D reconstruction have expanded. Traditional approaches involve expressing the target surface as a solution to an optimization problem, but they have limitations in completing less densely sampled or missing parts of the scene. Hand-crafted priors and data-driven methods have been used to address this issue but have limited applicability.To overcome these limitations, learning-based methods with explicit shape representations have been proposed. Voxel-based approaches suffer from resolution limitations, while generating a mesh with a neural network is challenging. Implicit representations have been used for point cloud generation but provide weaker geometrical and topological information. More success has been achieved with explicitly-designed implicit representations, which encode a function expressing volume occupancy or distance to the surface. However, most existing approaches operate on a single object and cannot be applied to arbitrary scenes.Recent methods have achieved translational equivariance via Convolutional Neural Networks (CNNs) but operate on voxelized discretization, losing the direct connection with the surface points. Our approach, based on point convolution, overcomes these issues. It attaches features representing the implicit function to input points, preserving their positions and concentrating information close to the surface. Features are computed using point convolution, ensuring natural coverage and scalability to scenes of arbitrary size. Interpolation is used for query-relative features, leading to better results. An efficient test-time augmentation is proposed to handle inputs of high density or large size.Our approach outperforms other methods on object and scene datasets, demonstrating robustness to domain shift and faster inference compared to methods that overfit to a scene or infer from scratch for each query. Overall, our approach offers improved reconstruction with finer details.