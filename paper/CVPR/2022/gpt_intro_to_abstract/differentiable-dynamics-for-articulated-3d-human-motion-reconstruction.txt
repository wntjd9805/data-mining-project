This paper aims to contribute to the development of physics-based methodology for accurate and robust 3D visual human sensing systems. By incorporating the laws of physics into the visual reasoning process, the plausibility of estimated motion can be improved, leading to more efficient use of training examples. The focus is on articulated human motion, which is a well-studied and challenging real-world prediction task. While existing state-of-the-art approaches demonstrate high accuracy in joint position estimation, they often produce physically implausible predictions.Physics-based reasoning introduces substantial modeling and inference complexity. Many methods rely on rigid body dynamics (RBD), which introduces auxiliary variables corresponding to forces acting at the body joints. In the presence of physical contact, non-smooth effects and fluctuations in model parameters can result in substantially different motions. This makes inferring physics variables from monocular video under contact discontinuities difficult algorithmically and computationally. Despite these challenges, some recent methods successfully apply physics-based constraints for articulated human motion estimation.In this paper, the authors propose a methodology, called DiffPhy, that leverages recent progress in differentiable simulation to incorporate physics-based constraints into articulated 3D human motion reconstruction. DiffPhy utilizes gradient-based optimization, connects end-to-end with images, and avoids simplifying assumptions on contacts or the introduction of external non-physical residual forces. The approach involves reconstructing motion in physical simulation by minimizing a differentiable loss. DiffPhy optimizes the control trajectory containing joint angle targets to PD controllers, which in turn compute torque vectors to actuate motors in the simulated body. A differentiable simulator, TDS, that supports complex contacts is integrated into DiffPhy. Each subject is represented by a personalized physical model, and the initial state is also optimized to enhance robustness to low-quality initial estimates. The output of DiffPhy is 3D pose estimates that align with visual evidence and respect physical constraints.