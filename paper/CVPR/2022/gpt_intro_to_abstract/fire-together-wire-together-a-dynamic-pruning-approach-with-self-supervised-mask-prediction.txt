Convolutional Neural Networks (CNNs) have achieved remarkable success in numerous fields over the past decade. However, their high computational and memory requirements hinder their deployment on edge and embedded platforms. To address this issue, there have been significant advancements in model compression research, including lightweight design, low-bit precision, architecture search, and model pruning. While most compression techniques optimize for static efficient models, recent efforts in pruning literature propose dynamic pruning, where different routes are activated based on the input, offering more flexibility. Current dynamic pruning approaches often introduce a regularization term or adopt policy gradient methods, but these methods require careful tuning and suffer from issues related to training stability and biased handling. Additionally, the selection of sparsity hyperparameters lacks transparency and makes it challenging to achieve a target reduction in Floating Point Operations (FLOPs). In this paper, the authors propose a novel approach to dynamic pruning by formulating the problem as a self-supervised binary classification task inspired by Hebbian theory. They generate binary masks for each layer based on the activation of the previous layer and use binary cross-entropy loss for channel gating. By generating ground truth masks based on the heatmap mass per sample, they achieve several advantages. Firstly, the proposed loss formulation is SGD-friendly and does not require gradient weighting tricks. Additionally, the channel gating loss adapts to the backbone's status, leading to improved training stability. Moreover, the reduction in FLOPs can be estimated before training, simplifying the hyperparameter selection process. The main contributions of this work include a novel loss formulation with self-supervised ground truth mask generation, a dynamic signature based on heatmap mass without predefined pruning ratios, and a simple hyperparameter selection method that enables FLOPs reduction estimation before training. These contributions provide a promising approach for efficient model pruning in CNNs, enhancing their applicability on resource-constrained platforms.