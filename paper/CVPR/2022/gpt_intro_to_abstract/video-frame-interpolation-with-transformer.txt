Video frame interpolation (VFI) is a popular video processing technique used to synthesize intermediate frames between consecutive frames in order to improve the frame rate and alleviate motion blur and judder. This task is crucial for various applications such as novel view synthesis, video compression, video restoration, and slow motion generation. Existing methods, which often rely on convolutional neural networks, face limitations in handling large motion due to the inherent locality of convolution operations.Inspired by the success of Transformers in natural language processing and computer vision tasks, we propose the application of Transformers in the context of video frame interpolation. We introduce a novel network called VFIformer, which uses the attention mechanism as its core operation to model pixel correspondence between frames and capture long-range dependencies. Our method outperforms leading VFI methods in terms of visual quality.To address the high memory and computational requirements of vanilla Transformers, we design VFIformer in a UNet architecture, processing features at different scales to reduce complexity and increase the receptive field. Furthermore, we propose a cross-scale window-based attention mechanism to enable information interaction between different windows and enhance the receptive field. This mechanism leverages the fact that features at coarser scales contain smaller displacement and serve as informative motion priors for the original scale.Our contributions include the introduction of a novel framework integrating the Transformer for the VFI task, the introduction of a cross-scale window-based attention mechanism to handle large motion, and the achievement of state-of-the-art performance on multiple benchmarks.