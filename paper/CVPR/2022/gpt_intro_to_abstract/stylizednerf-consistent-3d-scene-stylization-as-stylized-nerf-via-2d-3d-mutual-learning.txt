Controlling the appearance of complex 3D real scenes has become a significant area of research in recent years. Existing works have focused on tasks such as texture synthesis and semantic view synthesis. In this paper, we address the problem of stylizing complex 3D scenes, which is useful for applications in virtual reality and augmented reality. We utilize advanced 3D representation methods, such as neural radiance fields (NeRF), to represent complex scenes as point clouds or implicit fields. We aim to stylize a 3D scene using a given set of style examples, allowing for the generation of stylized images from novel views while maintaining consistency across different views. However, there are challenges in leveraging NeRF for stylization, including memory limitations and the lack of 3D information in existing image stylization methods. To overcome these challenges, we propose a mutual learning framework that combines NeRF and a 2D image stylization method. Our approach involves training a NeRF network to model the opacity field of the scene and distilling the 3D coordinates of rendered pixels to the 2D stylization method. We replace the color prediction module of NeRF with a style module and co-train the stylized NeRF network with the 2D stylization network. We introduce a mimic loss to align the outputs of the stylized NeRF and the 2D method, aiming to share the stylization knowledge and the inherent geometric consistency of NeRF. Additionally, we propose a conditional probability modeling for learnable latent codes to handle inconsistencies in the 2D stylized results and enable conditional stylization. Our contributions include a novel stylized NeRF approach, a mutual learning strategy, and a conditional probability modeling for handling ambiguities in 2D stylized results. Overall, our method produces superior stylization results in terms of visual quality and 3D consistency.