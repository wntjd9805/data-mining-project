Video generation has seen significant advancements in recent years, particularly in the area of controllable video generation. This type of video generation allows users to express their intentions regarding the appearance and motion of the generated videos. Controllable video generation has various applications, including artistic creation and data augmentation for machine learning. Existing controllable video generation tasks fall into three categories: Image-to-Video generation (I2V), Video-to-Video generation (V2V), and Text-to-Video generation (T2V). Each of these tasks provides different levels of control over the appearance and motion information. However, there is a need for a more challenging and versatile video generation task. In this paper, we propose a novel task called Text-Image-to-Video generation (TI2V). TI2V involves using a single static image to set the scene and a natural text description to provide motion guidance. This task requires aligning visual objects with corresponding text descriptions and transforming the implied object motion into an explicit video. We aim to achieve controllability and diversity in the generated videos. To address the TI2V task, we introduce an auto-regressive framework called MAGE, which incorporates a spatially aligned Motion Anchor (MA) to integrate appearance and motion information. The MA stores all necessary motion information for video generation in a cross-attention mechanism. We also introduce explicit conditions and implicit randomness to improve controllability and diversity in the generated videos. We utilize an axial transformer to inject and fuse the MA into visual tokens and generate videos in an auto-regressive manner. To evaluate the TI2V task and our generation model, we propose two datasets with synthetic videos and fine-grained text descriptions. These datasets allow us to evaluate the performance of both deterministic and diverse video generation. Our contributions include the introduction of the TI2V task, the development of the MAGE framework for controllable and diverse video generation, and the creation of two video-text paired datasets for evaluation purposes. Experimental results on these datasets demonstrate the effectiveness of our approach.