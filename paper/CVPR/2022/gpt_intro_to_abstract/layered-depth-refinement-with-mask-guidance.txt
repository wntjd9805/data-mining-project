Recent advancements in deep learning have made it possible to predict depth maps from single RGB images with reasonable accuracy. However, existing models for single image depth estimation still have limitations, including blurry and inaccurate depth boundaries, missing thin structures, and imprecise depth values in certain background regions. In this paper, we propose a novel approach to address these issues by incorporating an additional cue of a high-quality mask to refine depth maps generated by existing single image depth estimation models. This mask can be obtained through readily available auto-masking tools or off-the-shelf segmentation models. We introduce a layered refinement strategy that processes the mask and its inverse separately to generate two layers of depth maps, which are then combined using the mask to preserve boundary details and accurately fill in the depth values in isolated background regions. We also address the challenge of hole filling in the depth maps and propose a degradation-aware layered depth completion and refinement framework that learns to identify and correct inaccurate regions based on the context of the mask and the image. To train our model, we devise a self-supervised learning scheme that utilizes RGB-D training data without paired mask annotations. Experimental results on synthetic datasets and real images demonstrate the effectiveness and robustness of our proposed method. Our contributions include the introduction of a novel mask-guided depth refinement framework, a layered refinement approach for challenging areas, and a self-supervised learning scheme for training without paired mask annotations.