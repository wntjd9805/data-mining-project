Data-driven deep neural networks have achieved significant progress in semantic segmentation. However, these models are limited to a fixed number of classes and cannot be dynamically extended to identify new classes. Rebuilding the training set and retraining the model with all available data, known as Joint Training, is a straightforward solution, but it is costly and raises privacy concerns. In this paper, we propose a method for continual learning in semantic segmentation, which allows a model to recognize new categories without forgetting the old ones. We introduce a representation compensation module that remembers the old knowledge while accommodating new knowledge. This module has no additional parameters or computation cost during inference. We also incorporate a knowledge distillation mechanism, Pooled Cube Distillation, to alleviate catastrophic forgetting. Experimental results on continual class segmentation and continual domain segmentation demonstrate the superiority of our method compared to state-of-the-art approaches on three different datasets.