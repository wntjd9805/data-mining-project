Deep networks have achieved impressive results in learning visual tasks within the same data distribution. However, their ability to generalize to unseen data is hindered by domain shift. Domain adaptation aims to address this issue by transferring knowledge from a labeled source domain to a related target domain. This work focuses on the problem of test-time or source-free domain adaptation, where the source data is no longer available during adaptation to unlabeled test data. Test-time adaptation is appealing in real-world applications due to its ability to work with limited access to source data. Previous approaches have explored various methods such as generative models, entropy minimization, pseudo labeling, and self-supervised auxiliary task training. However, these methods have limitations in terms of computational requirements, model calibration, and leveraging the strength of self-supervised representation learning during the adaptation stage. In response, this paper introduces AdaContrast, a novel test-time adaptation strategy that combines self-supervised contrastive learning with pseudo labeling. AdaContrast optimizes the target representation in a joint manner, allowing the model to quickly adapt by reusing source knowledge while benefiting from pseudo labeling. The paper also introduces a new online pseudo label refinement scheme that enhances the accuracy of pseudo labels by using soft k-nearest neighbors voting. AdaContrast achieves state-of-the-art performance on major domain adaptation benchmarks, demonstrating its effectiveness in test-time adaptation.