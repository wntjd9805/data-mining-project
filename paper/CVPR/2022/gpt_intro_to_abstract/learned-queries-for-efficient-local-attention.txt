This paper introduces a new aggregation layer called Query and Attend (QnA) to revisit the design of local attention mechanisms in image processing. The goal is to leverage the locality and shift-invariance of convolutions while incorporating the expressive power of attention mechanisms. The QnA layer uses learned queries to compute aggregation weights, allowing for linear memory complexity regardless of the window size. Additionally, combining different queries enables capturing richer feature subspaces with minimal computational overhead. Through rigorous experiments, it is shown that the QnA layer imposes locality while maintaining accuracy, making it an effective and flexible general-purpose layer for up- and down-sampling operations. Furthermore, replacing self-attention layers with QnA layers in an attention-based object-detection framework leads to improved precision, particularly for small-scale objects. Comparative analysis demonstrates that QnA layers integrated with vanilla transformer blocks result in hierarchical Vision Transformers (ViTs) that achieve comparable or better accuracy compared to state-of-the-art models, with increased throughput and reduced parameters and floating-point operations. Overall, QnA offers an efficient aggregation mechanism that incorporates locality into existing transformer-based frameworks.