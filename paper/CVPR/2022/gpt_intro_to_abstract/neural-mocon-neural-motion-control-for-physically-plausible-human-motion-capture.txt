Recent years have seen significant advancements in marker-less motion capture, enabling a wide range of applications in character animation, human-computer interaction, personal well-being, and human behavior understanding. While existing techniques can accurately capture human pose from monocular videos and images, they often suffer from artifacts that do not adhere to biomechanical and physical plausibility, such as jitter and floor penetration. To address these issues, some works have incorporated physics-based constraints in motion capture, either as soft constraints in optimization frameworks or using non-differentiable physics simulators with deep reinforcement learning. However, these methods have limitations in accurately estimating human pose in scenarios with scene interactions and subject variations. In this paper, we propose a physics-based motion capture framework called Neural Motion Control (Neural MoCon), which combines sampling-based motion control with physical supervisions. Our approach overcomes challenges associated with noisy and physically implausible motion estimation by training a motion distribution prior with physical supervisions. We introduce a human-scene interaction constraint that adjusts the distance between meshes via Signed Distance Fields to enforce appropriate contacts. Additionally, we propose a two-branch decoder to address stochastic errors and train the distribution prior using a non-differentiable physics simulator. Our framework can capture physically plausible motion in complex terrains, diverse body shapes, and various behaviors. The main contributions of this work include the explicit physics-based motion capture framework, the novel two-branch decoder, and the interaction constraint based on Signed Distance Fields for accurate human-scene contacts.