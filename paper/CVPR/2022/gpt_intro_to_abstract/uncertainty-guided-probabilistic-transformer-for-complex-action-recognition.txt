This paper focuses on the challenging task of complex action recognition, which involves recognizing high-level activities composed of a sequence of atomic actions. The existing approaches often overlook the underlying dynamics and interactions among atomic actions, making it difficult to capture long-range dependencies and intra-class variations. Additionally, traditional sequence modeling architectures struggle with the long temporal durations of complex actions. To address these issues, we propose the use of a Transformer model, originally designed for language translation, as the backbone of our model. The Transformer's self-attention mechanism enables it to capture complex and long-range dynamics, making it suitable for modeling complex actions. However, the conventional Transformer lacks the ability to quantify prediction uncertainty, which is important for handling noisy and imbalanced data. To overcome this limitation, we introduce a probabilistic Transformer that treats attention scores as random variables, allowing us to capture stochastic dependencies and uncertainty in the inputs. By training a multilayer perceptron to produce the distribution parameters for attention scores, we can accurately quantify the model's prediction uncertainty. Based on this uncertainty, we propose a novel training and inference strategy using two models, the majority model and the minority model. These models focus on low-uncertainty and high-uncertainty samples, respectively, and their combination during inference is dynamically determined based on input uncertainty for improved predictions. Experimental results demonstrate that our probabilistic Transformer achieves state-of-the-art performance, even under noisy and insufficient data. The key contributions of this paper include the proposal of using the self-attention mechanism of a Transformer for modeling complex actions, the introduction of a probabilistic Transformer to quantify prediction uncertainty, and the novel strategy for training and inference with the majority and minority models. Our method achieves state-of-the-art performance on benchmark datasets, showcasing its effectiveness and robustness under various training data conditions.