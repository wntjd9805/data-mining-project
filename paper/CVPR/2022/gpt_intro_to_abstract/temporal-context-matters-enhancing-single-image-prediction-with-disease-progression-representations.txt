The goal of predicting clinical outcomes from medical images has long been a focus in the medical vision community. In recent years, deep neural networks (DNNs) have been utilized to improve diagnostic and prognostic performance. While previous approaches trained DNNs from scratch, more recent frameworks have employed knowledge distillation and self-supervision techniques to pre-train models, leading to improved performance. However, most medical imaging datasets only contain single-timepoint images, limiting their ability to capture the temporal evolution and prognosis of a pathology. This paper proposes leveraging deep learning approaches to make accurate predictions about disease trajectories even when temporal data is limited. Traditional temporal models tend to overfit on small datasets, resulting in poor generalizability. To address this, the authors propose using a Temporal ConvNet with a hierarchical attention mechanism to obtain the optimal representation of a temporal image sequence. They also introduce a feature matching technique called maximum mean discrepancy (MMD) loss to align the snapshot feature space with the optimal temporal representations. The contributions of this work include being the first to learn representations from limited temporal medical images and utilizing them to improve clinical prediction tasks from single-timepoint datasets. The proposed framework combines a Temporal ConvNet with attention mechanisms and a vision transformer pretrained in a self-supervised fashion. The results demonstrate the effectiveness of the approach in improving predictive performance using limited temporal data.