This paper focuses on the task of image captioning, which involves generating a natural language description based on a given image. Existing approaches typically use an encoder-decoder framework, where visual features are encoded and then used by a decoder to predict the next word in the caption. However, these approaches often suffer from a lack of reliable visual information, leading to generated descriptions that are irrelevant to the actual visual content. To address this issue, the proposed Dual Information Flow Network (DIFNet) introduces the visual representation of dual information flow. This network takes both grid features and segmentation features as input, allowing for a more comprehensive understanding of the image. To effectively integrate these features, a feature fusion module called Iterative Independent Layer Normalization (IILN) is introduced. By leveraging the segmentation map as spatial semantic guidance, DIFNet enhances the contribution of visual information for more accurate predictions.The proposed method is evaluated on the MSCOCO benchmark for image captioning and achieves state-of-the-art performance, with a CIDEr score of 136.2 on the COCO Karpathy test split. Additionally, Layerwise Relevance Propagation (LRP) is used to analyze the contributions of visual information and partially generated captions to the prediction process, further demonstrating the effectiveness of the proposed approach.In summary, the contributions of this paper include the introduction of DIFNet, which enhances the contribution of visual content in image captioning, the development of the IILN fusion module, and achieving significant improvements over existing methods on the MSCOCO benchmark.