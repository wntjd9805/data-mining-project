Multi-view Stereo (MVS) is a vital task in computer vision, aiming to reconstruct 3D scenes or estimate dense depth maps using multiple views. With the increasing availability of high-quality cameras, there is growing interest in developing reliable and efficient stereo algorithms for various applications. Recent research has shown that deep neural networks, particularly convolutional neural networks (CNNs), outperform traditional solutions in terms of accuracy and robustness. State-of-the-art CNN-based MVS approaches can be categorized into three main categories: variants of a standard 2D UNet architecture with feature correlation, differential 3D cost volume construction, and global scene representation with ray-casting features. However, these approaches have limitations such as difficulty in extending to multi-view scenarios, heavy computational burden, and the presence of salient artifacts in the predicted depth maps.In addition to multi-view depth estimation, there has been significant progress in single-view depth prediction networks. Combining the strength of single-view and multi-view depth estimations is a natural question. This paper introduces MVS2D, which integrates single-view and multi-view depth cues using an attention mechanism. The attention mechanism aggregates features along epipolar lines of each query pixel on the reference images, capturing rich signals from the reference images. Importantly, the attention mechanism can be easily integrated into standard CNN architectures, introducing low computational cost. MVS2D demonstrates superior efficiency compared to state-of-the-art algorithms while achieving state-of-the-art accuracy. Experimental evaluations on challenging benchmarks confirm the effectiveness of MVS2D in terms of performance metrics and the production of high-quality 3D reconstruction outputs.