This paper introduces the crucial aspect of accurately discerning the performance of generative modeling techniques, specifically focusing on visual data. Metrics such as Inception Score (IS), Kernel Inception Distance (KID), and Fr√©chet Inception Distance (FID) have become standard practice in the field for evaluating models. However, seemingly innocuous operations in low-level image processing, such as resizing and compression, can introduce significant discrepancies in high-level statistics. The authors demonstrate the impact of different image processing libraries and JPEG compression on model evaluation and propose a standardized protocol to avoid inconsistencies. The findings highlight the importance of considering these factors when training and evaluating generative models.