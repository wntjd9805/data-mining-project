Video virtual try-on has gained much attention in recent years due to its potential applications in e-commerce and the short video industry. Previous methods focused on image-based operations, achieving success in different aspects. However, image-based methods applied to videos often result in inconsistent frames and lack immersion. Several attempts have been made to design video virtual try-on methods that address spatio-temporal consistency, but these methods still produce flickering frames and lack smoothness. Moreover, they struggle to handle occlusions and complex backgrounds. To address these challenges, we propose a novel framework called ClothFormer. This framework introduces a clothing-agnostic person representation that preserves background and occlusion information. We employ a frame-level TPS-based warp method to predict and mask the occlusion region of the target clothes. We also utilize appearance-flow-based methods to obtain an accurate and anti-occlusion dense flow mapping between body and clothing regions. Spatio-temporal consistency is enforced in both the warp module and try-on module. The warp module utilizes ridge regression and optical flow correction to produce temporally smooth warped clothes. The try-on module incorporates a Multi-scale Patch-based Dual-stream Transformer (MPDT) generator to synthesize realistic videos. The MPDT generator extracts clothing color and texture features from the warped clothing sequence, and preserves person features and environmental information from the clothing-agnostic sequence. The integration of clothing features and background content features in the MPDT block generates more harmonious results. We conduct experiments on a wild virtual try-on dataset with occlusion and complicated backgrounds to validate the performance of our framework. The results demonstrate that ClothFormer outperforms existing methods in generating videos, both quantitatively and qualitatively. Our contributions include the design of a novel warp module that combines TPS-based methods and appearance-flow-based methods, a tracking module based on ridge regression and optical flow correction, and the application of the MPDT generator in the try-on module, which is the first time transformer has been used in video virtual try-on.