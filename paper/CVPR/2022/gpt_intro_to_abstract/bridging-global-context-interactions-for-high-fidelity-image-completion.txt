Image completion is the task of filling missing regions in an image with photorealistic content. Previous methods have relied on propagating information from neighboring visible regions or using deep neural networks to generate content. However, a major challenge is bridging and exploiting visible information globally, especially when the visible information is degraded by arbitrary masks. This challenge becomes even more difficult when dealing with irregular masks that cover partial information. Two-stage approaches have been proposed to address this challenge, but they have limitations in terms of information flow and long-distance message delivery. In this paper, we propose a new perspective on image completion by treating it as a directionless sequence-to-sequence prediction task. We introduce a content inference model called TFill, which utilizes a Transformer-based architecture to fill missing regions. The Transformer model allows for equal attention to be given to all visible pixels, reducing the dominance of nearby pixels in the completion process. However, applying Transformer models to visual tasks is a challenge, particularly in terms of token representation. To address this, we embed the masked image into an intermediate latent space and utilize a restrictive CNN for token representation, ensuring independent representation of visible information and explicit perception of long-range context relationships between tokens. Additionally, we propose a fully convolutional network for refining the visual appearance. Our approach outperforms existing state-of-the-art models for image completion. The contributions of this paper include the introduction of a restrictive CNN head for token representation, the explicit modeling of long-range interactions through a transformer-based architecture, the introduction of a novel attention-aware layer for adaptive attention balancing, and the demonstration of superior performance through extensive experiments.