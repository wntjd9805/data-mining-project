Visual Place Recognition (VPR) is a challenging problem in autonomous driving and robot localization systems. It involves determining whether a query image has been seen before and identifying corresponding images from a database. Two types of image representations are commonly used in VPR tasks: global image features and patch-level descriptors. Global features abstract the entire image into a compact feature vector, while patch-level descriptors describe specific patches or keypoints within an image. A common strategy is to first retrieve candidates using global features and then re-rank them using patch-level descriptor matching. However, existing methods often suffer from the inability to identify task-relevant regions in an image, leading to reduced robustness. To address this, we propose TransVPR, a Transformer-based model that adaptively extracts robust image representations from distinctive regions. Inspired by previous studies on CNNs, we fuse multi-level attentions that focus on different semantically meaningful regions to generate global image representations. The model's components are tightly coupled, allowing for end-to-end optimization with only image-level supervision. Experimental results demonstrate that TransVPR achieves superior performance on VPR benchmark datasets, outperforming state-of-the-art approaches by significant margins. The proposed model offers improved robustness, low computational time, and minimal memory requirements.