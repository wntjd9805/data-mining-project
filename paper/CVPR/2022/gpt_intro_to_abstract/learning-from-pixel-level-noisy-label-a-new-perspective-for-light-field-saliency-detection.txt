Saliency detection is a key problem in computer vision that aims to identify visually distinctive regions in an image or video. It has various applications, including image segmentation, visual tracking, and robot navigation. Existing saliency detection methods can be categorized based on the type of input images they utilize, such as 2D (RGB), 3D (RGB-D), and 4D (light field) images. Light field data, in particular, provides a rich source of information with its multi-view images and depth information. However, most existing light field saliency detection methods require pixel-level annotations for training, which is a time-consuming and expensive process. In this paper, we propose a new approach to learning light field saliency prediction using single per-pixel noisy labels. These noisy labels are produced by unsupervised saliency detection methods and may have biases or inaccuracies. Our method incorporates intra-light field features fusion and across-scenes correlation to effectively utilize the noisy labels and identify clean labels in a unified framework. We introduce a pixel forgetting guided fusion module to explore the interactions among the all-focus central view and focal slices of a light field image. This module helps identify noisy pixels and refine the saliency predictions. Additionally, we propose a cross-scene noise penalty loss to capture the global correlation of the noise space and enable more robust saliency predictions. Our experimental evaluations show that our proposed model achieves comparable performance with state-of-the-art fully supervised light field saliency prediction methods. This work is the first to consider light field saliency detection as learning from pixel-level noisy labels, providing a new direction for this research area. Our contributions include the formulation of saliency prediction as a joint optimization problem, the introduction of a pixel forgetting guided fusion module, and the proposal of a cross-scene noise penalty loss.