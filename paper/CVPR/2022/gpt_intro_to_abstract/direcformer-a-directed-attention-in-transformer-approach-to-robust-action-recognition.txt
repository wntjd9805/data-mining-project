Abstract:Video understanding has gained significant attention in the computer vision community, with an emphasis on action recognition and temporal modeling. Current methods rely on 3D convolution or two-stream structures, but these approaches suffer from intensive computation or the need for optical flow computation. Moreover, these methods have limitations in terms of robustness and generalization. In this paper, we propose a new end-to-end Transformer-based approach called DirecFormer, which focuses on ordered temporal learning in action recognition. We introduce a Directed Attention mechanism to provide human action attentions in the correct order and incorporate conditional dependencies in action sequence modeling. Our approach achieves State-of-the-Art results on three standard action recognition benchmarks.