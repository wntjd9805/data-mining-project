The introduction of this computer science paper focuses on the application of Transformer-based models, specifically the Vision Transformer (ViT), in vision tasks. While ViT has shown success in image classification, there are challenges in high-resolution tasks and capturing local relations. One proposed solution is the use of local-window self-attention, which performs self-attention within non-overlapping windows but poses limitations in receptive field and modeling capability.To address these challenges, the authors propose a Mixing Block that combines local-window self-attention with depth-wise convolution in a parallel design. This parallel design enhances the modeling of intra-window and cross-window relations simultaneously. Additionally, bi-directional interactions are introduced across branches to improve modeling ability in the channel and spatial dimensions.The proposed Mixing Block aims to achieve complementary feature mixing across windows and dimensions to improve representation learning. The authors present MixFormer, a series of models with different computational complexities, to validate the efficiency and effectiveness of the Mixing Block. MixFormer achieves competitive results in various vision tasks, such as image classification, object detection, instance segmentation, and semantic segmentation.On ImageNet-1K, MixFormer outperforms EfficientNet, RegNet, and Swin Transformer by a large margin. In dense prediction tasks on MS COCO, MixFormer-B4 shows significant improvements in box mAP and mask mAP compared to Swin-T with lower computational cost. MixFormer also achieves gains in mIoU for ADE20k and performs well in other tasks like keypoint detection and long-tail instance segmentation.In summary, the proposed MixFormer with the Mixing Block achieves state-of-the-art performance as an efficient general-purpose vision transformer in multiple vision tasks.