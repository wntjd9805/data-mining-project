Deep Convolutional Neural Networks (CNNs) have shown impressive performance in various visual tasks. However, their success heavily relies on the assumption that training and testing data distributions are identical. This limitation makes it challenging for CNNs to generalize well to different real-world scenarios, resulting in performance drops when applied to target domains. Additionally, the difficulty of collecting labeled target domain training data hinders CNNs from directly learning from it. To address these issues, domain adaptation (DA) algorithms aim to transfer knowledge from a fully annotated source domain to a target domain with unavailable annotations. Traditional DA methods require both source and target domain data simultaneously, making them infeasible when source data is inaccessible due to privacy concerns. Recent works have explored source-free domain adaptation (SFDA) approaches, which only require unlabeled target domain data and a model pretrained on the source domain for knowledge transfer. However, existing SFDA methods do not explicitly align the source and target domain distributions for adaptation. In this paper, we propose SFDA-DE, a method for image classification under SFDA setting. We estimate the source distribution without accessing the source data by utilizing the domain information captured by the pretrained model. We use the source classifier weights as class anchors and perform spherical k-means clustering on target features to generate robust pseudo-labels. Moreover, we dynamically estimate the feature distributions of source domain class-wise using semantic statistics from target data and their corresponding anchors. We simulate unknown source features by sampling surrogate features from these distributions and align them with target features using a contrastive adaptation loss function. Experimental results on three public DA benchmarks demonstrate that SFDA-DE achieves state-of-the-art performance and outperforms traditional DA methods that require source domain data access.