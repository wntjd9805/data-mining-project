Abstract:Locating the active object in human-object interactions is a critical task, particularly in egocentric videos where only the hands are visible. This task is crucial for various downstream applications, including pose estimation, reconstruction, activity recognition, and imitation learning. However, accurately localizing the active object is challenging due to occlusions caused by the hands during interactions. Therefore, we propose a pixel-wise voting function to improve the active object bounding box estimate while considering occlusions. This function takes an initial bounding box estimate of the active object and predicts an improved bounding box by allowing each pixel inside the input box to regress a new bounding box. The collection of predicted boxes is referred to as the Relational Box Field (RBF), which represents a field of bounding boxes related to the pixels in the input bounding box. To overcome inconsistencies across the RBF, we use a majority voting scheme inspired by the Hough transform, which minimizes the influence of outliers. We show that our voting scheme is more robust compared to standard regression methods. Additionally, we employ reinforcement learning (RL) to learn the proposed voting function iteratively. By using RL, we mitigate the issue of covariate shift caused by repeated application of a one-step predictor. Through experiments on two large-scale hand-object interaction datasets, we demonstrate that our approach achieves state-of-the-art performance in both hand-object detection and active object detection tasks. We also validate the generalization ability of our model and conduct a comprehensive ablation study to analyze design choices.