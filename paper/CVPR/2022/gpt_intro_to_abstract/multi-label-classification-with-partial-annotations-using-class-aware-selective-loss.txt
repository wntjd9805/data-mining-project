Recent advancements in multi-label classification have shown significant progress in various approaches, including dedicated loss functions and transformers. However, in large-scale multi-label classification tasks, fully annotating each image becomes impractical due to the increasing amount of samples and labels. This leads to partially labeled data, where only a subset of positive and negative labels are annotated and the rest are considered unknown. The treatment of these un-annotated labels in the learning process has a considerable impact. The traditional approach is to ignore their contribution, but this may result in a limited decision boundary. Treating them as negative labels can improve discriminative power but introduces label noise and imbalance in the training. Additionally, certain frequent classes that are not sufficiently annotated may be trained with incorrect negative samples. In this paper, we propose a selective approach that addresses these limitations by adjusting the training mode for each label individually based on label likelihood and label prior probabilities. We also propose a method for estimating the class distribution from the data. Our approach outperforms previous methods in various partially labeled datasets and achieves a state-of-the-art result in the OpenImages dataset. The contributions of this paper include introducing a novel selective scheme, addressing the challenge of estimating class distribution accurately, and proposing a partial asymmetric loss to control the impact of annotated and un-annotated negative samples.