Video segmentation tasks involve partitioning video frames into tubes with different predicted labels, such as semantic categories or both semantic categories and instance identities. However, the underlying similarity of these tasks has been overlooked, leading to divergent models. In this paper, we propose TubeFormer-DeepLab, a single model based on mask transformers that directly predicts class-labeled tubes for various video segmentation tasks. TubeFormer-DeepLab extends the mask transformer to generate pairs of class predictions and tube embedding vectors, which are multiplied by video pixel embedding features to yield tube predictions. Our model tackles multiple video segmentation tasks without task-specific designs. We introduce a latent dual-path transformer block to handle multi-frame features and a global dual-path transformer block for attention learning. Additionally, we split the global memory into thing-specific and stuff-specific memories to capture different characteristics. During inference, we stitch video clips and enforce consistency using a Temporal Consistency loss. We also propose a data augmentation policy called clip-paste, which randomly pastes thing or stuff regions from one video clip to another. Experimental results on multiple datasets demonstrate the effectiveness of TubeFormer-DeepLab, surpassing state-of-the-art performance on various benchmarks. Our approach simplifies video segmentation systems and achieves high performance across different tasks.