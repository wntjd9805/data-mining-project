This paper introduces a unified architecture called Uni-Perceiver for processing multiple modalities and tasks using a single siamese model with shared parameters. The authors argue that current methods still require task-specific design and training, hindering the development of generic perceptual models. To address this limitation, the authors propose encoding task inputs and targets from different modalities into a unified representation space and modeling the joint probability through the similarity of their representations. The proposed Uni-Perceiver is pre-trained on various uni-modal and multi-modal tasks and evaluated on downstream tasks, including novel tasks that were not part of the pre-training stage. Results show that the pre-trained model achieves reasonable performance even on novel tasks and can be further boosted with prompt tuning or full-model fine-tuning. When fine-tuned with 100% of the target data, the model achieves comparable or superior results to state-of-the-art methods on most tasks, demonstrating its strong representation ability.