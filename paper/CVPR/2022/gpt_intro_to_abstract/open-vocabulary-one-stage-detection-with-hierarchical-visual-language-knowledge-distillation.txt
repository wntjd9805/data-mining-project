In this paper, we address the challenge of recognizing objects from unseen categories (novel categories) in the context of object detection. Traditional detectors typically rely on training sets with annotations from seen categories, and to detect more object categories, the usual solution is to label more categories in the training sets. However, this approach can be costly and also exacerbate the long-tail distribution of object categories.To overcome these limitations, zero-shot and open-vocabulary object detection tasks have been proposed. These tasks aim to recognize objects from unseen categories while the detector is only trained with annotations from seen categories. The open-vocabulary detector, in particular, has seen more rapid development recently and outperforms the zero-shot detector.There have been previous works attempting to redesign traditional detectors for these detection tasks. These works can be categorized as two-stage methods and one-stage methods, similar to traditional detection. Although one-stage detectors have comparable performance and a more concise pipeline, the current best performing open-vocabulary detector is a two-stage method, which significantly surpasses the similar one-stage method.In this paper, we analyze the reason behind this performance gap and propose ways to narrow it by constructing a high-performance open-vocabulary one-stage detector. We introduce a weakly supervised global-level language-to-visual knowledge distillation method (GKD) to learn novel category knowledge beyond training labels for one-stage detection. GKD leverages visual captions that potentially contain semantic knowledge of novel categories and performs language-to-visual knowledge distillation.We combine GKD with the commonly used instance-level visual-to-visual knowledge distillation (IKD) to create an end-to-end hierarchical knowledge distillation mechanism-based detector, called HierKD. Our proposed HierKD significantly outperforms previous best open-vocabulary one-stage detectors, with 11.9% and 6.7% AP50 gains under the zero-shot detection and generalized zero-shot detection settings, respectively, on the MS-COCO dataset.Overall, our contributions include exploring a weakly supervised global-level language-to-visual knowledge distillation method, proposing an end-to-end hierarchical visual-language knowledge distillation mechanism, and achieving state-of-the-art performance on open-vocabulary one-stage detection tasks.