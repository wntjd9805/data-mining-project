Clothed body reconstruction from monocular video is a challenging research topic in computer science. While high-fidelity human reconstruction can be achieved in the film and gaming industry through pre-captured templates and complex setups, these requirements are impractical for general customers who need personalized avatars for applications like telepresence, AR/VR, anthropometry, and virtual try-on. This paper proposes a novel approach called SelfRecon that combines explicit and implicit representations to reconstruct high-fidelity digital avatars from monocular video. Existing marker-less monocular human performance capture methods rely on explicit mesh representation and actor-specific templates, which limits their applicability to unseen human sequences. In contrast, recent neural implicit representation-based approaches can handle various topologies but require high-quality 3D data and lack space-time coherence. To address these limitations, SelfRecon utilizes a learnable signed distance field (SDF) as the representation for the canonical shape and employs forward deformation to map canonical points to the current frame space. By periodically extracting explicit canonical meshes and warping them with deformation fields, the overall shape is recovered using mask loss and smooth constraints. The implicit part is refined using a differential formulation to intersect the deformed surface and a neural rendering technique. Evaluation of SelfRecon on self-rotating human videos shows superior performance compared to existing methods. This work opens up possibilities for combining implicit and explicit representations in 3D reconstruction, particularly for articulated objects. SelfRecon contributes to the field by reducing the dependence on actor-specific templates and generating a space-time coherent mesh sequence from monocular video.