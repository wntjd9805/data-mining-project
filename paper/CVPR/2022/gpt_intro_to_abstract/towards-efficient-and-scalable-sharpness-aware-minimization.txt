In this paper, the authors address the issue of sharp local minima leading to decreased generalization performance in deep networks. They focus on the Sharpness Aware Minimization (SAM) algorithm, which penalizes sharp minima and guides convergence to a flat region. While SAM has shown promising results, it involves two sequential gradient computations at each step, resulting in increased training time. To improve the efficiency of SAM and make it applicable to large-scale training problems, the authors propose a novel algorithm called LookSAM. LookSAM decomposes SAM's update direction into two components: one parallel to the original SGD direction and another orthogonal component capturing the differences between SAM and SGD updates. The authors demonstrate that this second direction remains similar across nearby iterations, leading to the development of LookSAM, which reuses this direction periodically, significantly reducing computational complexity while maintaining similar generalization performance.Additionally, the authors address the non-uniform instability problem introduced by large-batch training by incorporating a layer-wise scaling rule for weight perturbation. They introduce Look-LayerSAM, which successfully trains Vision Transformer models (ViTs) with a batch size of 64k within a comparatively short time.The contributions of this paper can be summarized as follows: 1. The development of LookSAM, an algorithm that speeds up SAM training by approximating the original SAM's direction while reducing computational complexity.2. The introduction of Look-LayerSAM, which scales up the batch size of LookSAM and achieves a new record of 64k batch size for ViT training.3. The demonstration of speed improvements over previous training settings, with the ViT-B-16 training completed in only 0.7 hours, setting a new speed record for ViT training.