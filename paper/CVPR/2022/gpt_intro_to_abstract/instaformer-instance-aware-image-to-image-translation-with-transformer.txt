Image-to-image translation (I2I) has gained significant attention in computer science over the past decade. This research focuses on translating an image from one domain to another, with applications in style transfer, super-resolution, inpainting, and colorization. Recent works have aimed to improve the disentangled representation of multi-modal translations using unpaired training data. However, most existing methods treat images as a whole and do not consider the presence of multiple object instances, leading to limited performance in content-rich scene translation. To address this limitation, some methods explicitly consider object instances within deep convolutional neural networks. These methods improved instance-awareness to some extent but still inherit limitations related to local receptive fields and limited encoding of pixel or patch relationships. In this paper, we propose InstaFormer, a novel approach that utilizes Transformer architecture to effectively integrate global and instance-level information in image translation. Our framework extracts content and style vectors using disentangled representation approaches and incorporates them into Transformer-based aggregators to consider both global context and instance-level features. We also employ adaptive instance normalization and convolutional patch embedding to reduce computation requirements. Additionally, we introduce an instance-level content contrastive loss to improve the quality of translated images at object regions. Experimental results on various benchmarks demonstrate the effectiveness of our proposed model for instance-aware I2I.