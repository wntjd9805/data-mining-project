This paper focuses on the cross-modal retrieval of e-commerce products, exploring the challenges faced in aligning visual and textual data semantically. The authors highlight the unique characteristics of e-commerce images and language, including the need for fine-grained features in fashion domains and the shortcomings of current methods in detecting relevant regions in images. They propose a model that improves cross-modality product retrieval by addressing two motivations related to the language aspect of e-commerce: the biased pretrained language models and the uneven contribution of metadata to retrieval. The authors conduct empirical analyses to support their arguments and introduce an Entity-aware Intervention-based contrastive learning framework (EI-CLIP) with two specific module designs to address these challenges. The paper presents its contributions as the first work to tackle the challenges introduced by e-commerce special entities in language modality, the formulation of the entity-aware retrieval task in a causal view, the introduction of the EI-CLIP framework, and the achieved competitive performance on the Fashion-Gen benchmark dataset.