Abstract:Objects are fundamental components of perception and understanding the world. Object-centric representation plays a crucial role in higher-level cognitive abilities and is essential for generalization and adaptation. While progress has been made in object recognition in computer vision, existing approaches heavily rely on manual labels and a fixed vocabulary of object categories. Discovering objects and their extent in data that generalizes across domains remains a challenging problem. The ambiguity and context-dependency of objects further complicate the task, leading to ineffective separation from the background. This is particularly evident in realistic scenes where classical graph-based methods tend to over- or under-segment objects. Learning-based approaches have shown promise but are limited to toy images and fail in realistic scenarios. In this paper, we propose leveraging motion segmentation as a means to group objects, even in static scenes. We extend an existing unsupervised object discovery framework by introducing a spatio-temporal memory module and simplifying the grouping mechanism to handle realistic scenes. Our approach demonstrates the importance of incorporating independent object motion as an inductive bias for discovering objects. We show that even sparse and noisy motion segmentation can guide the attention operation to identify both moving and static instances of objects. Our method only requires videos for training and can segment objects in static images during inference. We evaluate our approach on a new synthetic dataset as well as real videos from the KITTI dataset, comparing it with existing approaches. The results demonstrate the effectiveness and generalization capabilities of our method. The code, models, and synthetic data are publicly available.