Anomaly detection (AD) is an important task in various domains, such as medical image analysis, industrial inspection, video surveillance, and autonomous driving. Most existing AD methods are unsupervised, relying on normal training samples due to the difficulty of collecting large-scale anomaly data. However, in many real-world applications, a small number of labeled anomaly examples are available, providing valuable knowledge about application-specific abnormality. Unsupervised detectors are unable to utilize this information effectively, resulting in less discriminative features. Recently, supervised detection paradigms have been explored to exploit these anomaly examples, achieving improved performance in detecting similar anomalies. However, these models often fail to generalize to unseen anomaly classes, becoming less effective than unsupervised detectors. To address this issue, this paper proposes an open-set supervised anomaly detection approach called DRA. DRA learns disentangled representations of abnormalities, including anomalies similar to seen anomalies, anomalies similar to pseudo anomalies, and unseen anomalies. A multi-head neural network model is designed to learn each type of abnormality, allowing for diversified abnormality representations. Additionally, a latent residual-based abnormality learning module is introduced to detect hard anomalies that cannot be detected in the original feature space. Experimental results on various real-world datasets demonstrate that DRA outperforms five state-of-the-art models in different settings. This paper presents new baselines for future research in this emerging field of open-set supervised anomaly detection.