The visual question answering (VQA) task involves providing a natural language answer to a question based on an image. While cross-modal methods have achieved impressive results in VQA, the knowledge-based visual question answering (KB-VQA) task requires incorporating external general knowledge to answer questions. Most KB-VQA datasets come with pre-defined knowledge bases, and each question is annotated with supporting knowledge facts. Additionally, the recently proposed outside-knowledge visual question answering (OK-VQA) task allows for any external knowledge to be used in answering questions.This paper addresses unique challenges in the OK-VQA task. Firstly, it explores a paradigm shift by transforming everything into the language space before leveraging large textual knowledge for question answering. This approach outperforms previous methods that only use captions or object labels. Secondly, it tackles the challenge of effectively retrieving relevant knowledge passages from massive knowledge bases. Instead of relying on weak supervision signals, it utilizes a state-of-the-art dense passage retrieval model. Lastly, it consolidates all multi-source inputs, including the question, visual context, and retrieved knowledge passages, to predict answers. By formulating the problem as a multi-passage question answering task and using a generative question answering model, the paper avoids the limitations of extractive methods.The proposed Transform-Retrieve-Generate (TRiG) framework aligns all information in the language space to take advantage of the rich semantics of textual knowledge. It starts with image-to-text transformations, followed by dense passage retrieval and answer generation. The TRiG framework achieves state-of-the-art performance on the OK-VQA dataset, outperforming other supervised methods by 11.1%. Additionally, the framework provides transparent and interpretable results by utilizing cross-attention scores to rank and highlight the top supporting knowledge passages. This helps diagnose errors and understand the reasoning behind the model's predictions.In summary, this paper presents a new paradigm and a framework for the OK-VQA task, transforming images into plain text and performing knowledge retrieval and question answering all in the language space. The proposed TRiG framework achieves superior performance and provides interpretable results.