Monocular depth estimation using convolutional neural networks (CNNs) has made significant advancements in recent years. These networks have shown great potential in learning complex geometric relationships from data, similar to how humans perceive depth using visual cues. While monocular depth estimates have already been applied successfully in various areas such as 3D photography and novel-view synthesis, most existing approaches are limited to low-resolution perspective images with a narrow field-of-view.However, with the rising popularity of 360° cameras, there is a growing need for monocular depth estimation in omnidirectional images. The wide field-of-view captured by these cameras makes them attractive for tasks like robust SLAM, scene understanding, and VR applications. Though some state-of-the-art methods exist for monocular depth estimation in 360° images, they are currently restricted to resolutions of around 0.5 megapixels, which is insufficient for high-quality VR applications that require at least 2 megapixels.To address this limitation, this work proposes a general and flexible framework for monocular depth estimation in high-resolution 360° images. It draws inspiration from tangent images and uses projection techniques to convert the input 360° image into a collection of perspective tangent images. Each tangent image is then processed by state-of-the-art perspective monocular depth estimators to obtain detailed depth maps. These depth maps are then aligned and merged to create a seamless high-resolution 360° depth map using gradient-based blending.The technical contributions of this work include a practical framework for high-quality, multi-megapixel 360° monocular depth estimation, support for increased resolutions using tangent images, and forward compatibility with future depth estimation approaches. Additionally, the authors provide ground-truth depth maps for Matterport3D's stitched skyboxes to facilitate the development and advancement of future high-resolution depth estimation methods. This work aims to bridge the gap in monocular depth estimation for 360° images and enable more immersive VR experiences.