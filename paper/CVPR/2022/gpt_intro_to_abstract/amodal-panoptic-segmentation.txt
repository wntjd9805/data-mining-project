Humans have the ability to perceive complete physical structures of objects even when they are only partially visible, known as amodal perception. In contrast, robots are limited to modal perception, which restricts their ability to emulate the visual experience of humans. To bridge this gap, this paper proposes the amodal panoptic segmentation task. This task aims to concurrently predict pixel-wise semantic segmentation labels of visible regions of "stuff" classes (e.g., sky, road) and instance segmentation labels of both visible and occluded regions of "thing" classes (e.g., cars, pedestrians). Amodal panoptic segmentation is challenging as it requires complex occlusion reasoning, especially for non-rigid classes like pedestrians. The paper introduces two benchmark datasets and proposes evaluation metrics for this task. Additionally, the paper presents the APSNet architecture, which includes shared backbone and task-specific heads for semantic and amodal instance segmentation. The proposed approach incorporates occluder and occlusion features to identify occluded regions and refine visible masks. The paper also discusses the use of dilated convolutions for context aggregation and provides publicly available code and models for further research.