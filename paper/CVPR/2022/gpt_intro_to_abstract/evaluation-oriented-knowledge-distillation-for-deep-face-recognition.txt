With the increasing demand for compact yet discriminative models in recognition systems deployed on mobile and edge devices, the performance gap between resource-intensive networks and compact networks needs to be addressed. Knowledge Distillation (KD) is a widely-used technique that utilizes the knowledge of a large network to improve the performance of compact models. Previous KD methods have focused on reducing the Kullback-Leibler divergence between the probabilities at the output layers of the teacher and student networks, but these instance-based methods have limited improvement on student model performance. Relation-based KD methods have been proposed to utilize the correlations between instances as knowledge, but they still have too strict constraints on knowledge transfer. In this paper, we propose a novel Evaluation-oriented Knowledge Distillation (EKD) method for deep face recognition, which aims to reduce the performance gap between teacher and student models. We adopt the False Positive Rate (FPR) and True Positive Rate (TPR) evaluation metrics as the performance indicator and use a rank-based loss function to constrain the critical pair relations in the student model. By directly optimizing the critical relations that cause the TPR and FPR difference, the EKD method achieves better performance compared to state-of-the-art competitors on popular facial benchmarks. The contributions of this paper include the introduction of the EKD method, the novel rank-based loss function, and extensive experimental results that demonstrate the superiority of EKD.