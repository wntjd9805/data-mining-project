Virtual human characters can now possess realistic facial animation thanks to advancements in facial performance capture and speech-driven animation. However, achieving realistic representation and motion of the inner mouth, specifically the tongue, remains a challenge. Traditional animation approaches fail to accurately animate the tongue due to the partial observability of inner mouth articulators and the complexity involved in manual animation. As a result, rule-based or procedural animation techniques are commonly employed, which often result in poorly lit inner mouth regions and approximate tongue articulation. To address this issue, this paper proposes an automatic speech-driven tongue and jaw animation solution using a data-driven sequence to sequence approach. The input to the model is streaming speech audio waveform, and the output is the corresponding 3D landmark locations of motion-captured speech articulators. The paper also introduces a new dataset consisting of labeled speech recordings, which is publicly available for further research. The study leverages deep learning-based speech audio feature representations and compares them with traditional features, highlighting the superior generalization and resilience to noise offered by deep learning approaches. The predicted landmark locations can be used to animate any facial animation rig, and the paper demonstrates the rig solving process using a professional facial rig. The results can be easily integrated into game engines or digital content creation software. Overall, the contributions of this work include the introduction of a framework for speech-driven tongue animation, the analysis and comparison of diverse audio representations, the presentation of a pipeline approach for driving a parametric 3D face model, and the release of code and a large-scale speech-to-tongue mocap dataset for training animation models.