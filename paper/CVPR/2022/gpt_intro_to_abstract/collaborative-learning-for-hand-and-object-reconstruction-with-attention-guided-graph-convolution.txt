Understanding human hand and object interaction is crucial for interpreting human actions and behavior. While there have been advancements in pose estimation of isolated hands using deep learning and RGB-D sensors, joint reconstruction of hand and object has received less attention, despite its real-world applications in augmented and virtual reality. In this paper, we address the problem of reconstructing hand and object meshes from a single RGB image. Joint pose estimation is challenging due to occlusion from both the hand and object, and erratic camera motion in first-person-view scenarios. Previous works have tackled some challenges but often lead to erroneous pose estimation and mesh reconstructions. To better understand hand-object interactions, recovering 3D information is essential. There have been improvements in hand mesh estimation from RGB images, but existing approaches are limited to scenarios where the hand and object are already in contact. We propose a collaborative learning framework that allows the hand and object branches to iteratively share mesh information, boosting each other's performance and addressing occlusion challenges. We introduce a novel unsupervised associative loss for information transfer between branches and an attention-guided graph convolution for unsupervised training to handle occlusions. Our contributions include an end-to-end trainable strategy, attention-guided graph convolution, unsupervised training strategy, and achieving highly physically plausible results without contact terms. We evaluate our method on multiple datasets and demonstrate its superiority over state-of-the-art approaches.