Time of Flight (ToF) cameras are devices used to capture depth information by measuring the time it takes for emitted light to travel back after intersecting with an object. However, consumer-level ToF cameras perform indirect measurements due to the need for precise and costly hardware. The most common type of ToF camera is the Amplitude-Modulated Continuous-Wave (AMCW) camera, used in devices like the Kinect. AMCW cameras emit a periodically modulated light signal and measure the phase shift of the received signal to determine the object's distance from the camera. However, continuous illumination of the scene leads to Multi-Path Interference (MPI), affecting depth estimation. Additionally, AMCW cameras suffer from low Signal to Noise Ratios (SNR) on dark surfaces and the mixed pixel effect along sharp object edges. Previous works have explored the use of 2D neural networks to correct ToF depth images, but they do not consider the explicit 3D information in their computations. In this paper, we propose a new neural network architecture that projects the problem into the 3D domain and utilizes point convolutional neural networks to analyze and adjust the erroneous reconstruction. We also introduce a novel fine-tuning procedure for Unsupervised Domain Adaptation (U-DA) based on self-training methods to transfer knowledge from synthetic to real-world ToF data. Our approach outperforms existing methods and is evaluated on both synthetic and real datasets. Additionally, we provide a large-scale high-resolution synthetic ToF dataset that includes challenging scenes with high MPI levels, low SNR captures, and high-frequency details. The code, trained networks, and synthetic dataset are available at our GitHub repository.