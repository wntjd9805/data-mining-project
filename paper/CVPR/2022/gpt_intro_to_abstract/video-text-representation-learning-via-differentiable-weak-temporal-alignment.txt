Learning video-text representations is a key challenge in computer vision, driven by the abundance of video data and wide-ranging applications. Previous works have achieved promising results but often require labor-intensive manual annotations. To address this scalability issue, self-supervised learning approaches with large-scale unlabeled data have emerged as a promising alternative. In this paper, we focus on the HowTo100M dataset, which contains 100 million video clips and captions. However, this dataset poses challenges due to its uncurated nature and weakly correlated video-text pairs. To handle this, we propose a new weak temporal alignment algorithm based on Dynamic Time Warping (DTW), which allows for flexibility and globally optimal alignment. Our algorithm is differentiable, enabling its integration into representation learning. We introduce a novel multi-modal self-supervised learning framework named VT-TWINS, which leverages our alignment algorithm to handle the correspondence between noisy and weakly correlated captions and video clips. Extensive experiments on benchmark datasets demonstrate the effectiveness of our method in various downstream tasks. Our contributions include the novel self-supervised learning framework, an analysis of local neighborhood smoothing in our alignment algorithm, and improved joint video-text representations.