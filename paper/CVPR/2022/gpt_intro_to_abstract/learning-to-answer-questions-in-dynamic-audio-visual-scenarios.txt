This paper introduces the topic of integrating audio and visual modalities for scene perception and understanding. The authors highlight the limited ability of existing methods in cross-modal reasoning and propose a computational model for audio-visual question answering (AVQA). They present a large-scale MUSIC-AVQA dataset of musical performance videos and develop a spatio-temporal grounding model to understand and reason over audio and visual modalities. The proposed model outperforms recent AVQA approaches and demonstrates the benefits of multisensory perception in AVQA. The contributions of the paper include the dataset, the spatio-temporal grounding model, and experimental results showing the superiority of the proposed approach.