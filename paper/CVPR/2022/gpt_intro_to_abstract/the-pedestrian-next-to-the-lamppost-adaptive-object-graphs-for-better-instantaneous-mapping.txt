The problem of generating top-down birds-eye-view (BEV) maps from images is essential in the field of autonomous driving. These maps provide a compact representation of the spatial configuration and other agents in a scene, making them ideal for navigation and planning purposes. Recently, there has been increasing attention towards inferring semantic BEV maps from images, which involves mapping "things" like traffic cones and pedestrians, as well as "stuff" like road lanes and pavements.Existing BEV estimation techniques have achieved impressive accuracy in generating semantic maps for both "things" and "stuff" using texture-based models. However, these models are more effective for large textured classes dominating the scene, such as road and pavement. They tend to struggle with smaller and potentially dynamic objects at greater distances, resulting in low recall and large localization errors.In contrast, the field of monocular 3D detection excels in object localization accuracy by adopting an object-based approach. To address the limitations of texture-based models, one possible solution is to utilize off-the-shelf monocular 3D detectors to generate BEV object bounding boxes. Surprisingly, this approach improves object intersection-over-union (IoU) accuracy in BEV estimation by approximately 20%. This raises the question of why not combine both methods to reason about objects in object space and enhance background "stuff" estimates.To tackle this challenge, we propose a novel BEV estimation method that leverages object graphs to reason about the scene layout. These graphs provide additional context and improve object localization by propagating information between objects. Our model predicts BEV objects by spatially reasoning about an object, considering the long-range context provided by other objects in the scene.The key contributions of our work are as follows: 1. We introduce a novel application of graph convolution networks for spatial reasoning to localize BEV objects from a monocular image.2. We demonstrate the importance of learning both node and edge embeddings, as well as their mutual enhancement and edge supervision, for accurate object localization.3. We incorporate positional-equivariance into our graph propagation method, leading to state-of-the-art results across three large-scale datasets. Notably, there is a 50% relative improvement in BEV estimation of "things" or objects.Overall, our proposed method effectively combines texture-based models and monocular 3D detection approaches, utilizing object graphs to improve BEV estimation accuracy and provide valuable context for scene understanding in the context of autonomous driving.