Adversarial attack poses a significant security threat to Deep Learning (DL) applications. Traditionally, perturbations applied to the input to create adversarial examples are kept imperceptible by having bounded perturbations in the pixel space. However, recent research has shown that adversarial examples with large pixel distances can be generated, beyond the bounds of existing defense techniques. These techniques mutate meta-features of original inputs, such as colors and styles, but offer limited protection against imperceptible perturbations on individual content-features/neurons. In this paper, we propose a new attack vector in the feature space that performs imperceptible content feature perturbation by bounding individual neuron perturbations. We achieve a uniform perceptual bound using a distribution quantile bound. Our results show that our technique can mutate content features in a less human perceptible way, achieving a high attack success rate while being difficult for humans to distinguish from benign examples. Our attack generates perturbations that piggy-back on existing semantically meaningful features, making them difficult to detect. Furthermore, our attack has better imperceptibility and persistence compared to existing attacks when evaluated against detection techniques.