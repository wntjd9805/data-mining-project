Deep neural networks (DNNs) have achieved significant successes in various areas, but their robustness is challenged by noises, particularly in real-world scenarios. Adversarial noise, designed to mislead DNN decisions, is becoming a significant threat. Additionally, DNNs show weak robustness against common corruptions in daily environments. To address these issues, researchers have proposed defenses from both the model-end and data-end. However, existing data-end defenses have limitations in terms of generalization to diverse noises and transferability across multiple models. This paper introduces a data-end defensive patch generation framework that can effectively defend against diverse noises and work across different models. The authors improve the defense ability of the patches by better exploiting local and global features. To enhance generalization against diverse noises, the authors optimize patch priors to include more class-specific identifiable patterns. This allows the defensive patches to better preserve recognizable features and resist the influence of different noises. To improve transferability across multiple models, the authors guide the defensive patches to capture more class-wise global feature correlations. Experimental results show that the proposed defensive patch outperforms other methods by a significant margin in terms of adversarial and corruption robustness. The contributions of this work include generating data-end defensive patches to improve application robustness against diverse noises, enhancing robustness by injecting local identifiable patterns and enhancing global perceptual correlations, and achieving higher accuracy in both digital and physical world scenarios.