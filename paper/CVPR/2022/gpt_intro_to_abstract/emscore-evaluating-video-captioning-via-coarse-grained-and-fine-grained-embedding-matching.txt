Video captioning has made significant progress in recent years, driven by the neural encoder-decoder paradigm. However, accurately evaluating generated captions is essential for further advancements in this field. While human evaluation is the ideal metric, it is time-consuming and labor-intensive. As a result, automatic metrics are commonly used for video caption evaluation. However, most of these metrics come from other tasks like machine translation and image captioning, which may not fully capture the specific characteristics of video captioning. Additionally, these metrics require human-labeled references, which has inherent drawbacks. To address these limitations, we propose a reference-free metric called EMScore (Embedding Matching-based score) for evaluating video captions. EMScore utilizes a pre-trained large-scale vision-language model to extract visual and linguistic embeddings, mimicking the human evaluation process. It considers both coarse-grained (video and caption) and fine-grained (frames and words) levels of matching to provide a comprehensive comparison between the video and caption. Furthermore, we extend EMScore to the case where human-labeled references are available, naming it EMScore ref. To facilitate the development of video captioning evaluation metrics, we introduce the VATEX-EVAL dataset, which contains 54,000 human ratings for video-caption pairs. Experimental results on VATEX-EVAL demonstrate that EMScore has higher human correlation and reduced reference dependency compared to popular automatic metrics like BLEU and ROUGE. We also collect the ActivityNet-FOIL dataset to test EMScore's effectiveness in identifying "hallucinating" captions. Our overall contributions include the proposal of EMScore as a reference-free video captioning metric, the development of the VATEX-EVAL dataset for evaluation, and thorough experimental results validating the efficacy of EMScore.