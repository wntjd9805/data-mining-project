3D hand pose estimation plays a crucial role in enhancing the intuitiveness of virtual reality, augmented reality, and human-computer interaction. While significant progress has been made in single-hand pose estimation using depth maps and RGB images, the challenge of estimating poses for two interacting hands has received less attention. Identifying and localizing joints accurately becomes extremely challenging due to the similarities in appearance between the joints of the two hands and occlusions caused by the close interaction. Existing approaches that detect and predict the poses of each hand independently or rely on heatmaps struggle to achieve accurate results in close interaction scenarios. In this paper, we propose a novel approach called the "Keypoint Transformer" that addresses these challenges. Our approach estimates the 3D poses of the hands in three stages: (1) detecting keypoints, which are potential joint locations, (2) associating keypoints with the corresponding joints or background using a self-attention mechanism, and (3) predicting the 3D hand poses using a cross-attention module. Our architecture explicitly disambiguates keypoints and outperforms previous methods by a large margin with a smaller model size. We evaluate our approach on existing hand interaction datasets and introduce a new challenging dataset called H2O-3D. The results demonstrate that our method achieves state-of-the-art performance with fewer model parameters. Ablation studies and comparisons with strong baselines further confirm the effectiveness of our approach.