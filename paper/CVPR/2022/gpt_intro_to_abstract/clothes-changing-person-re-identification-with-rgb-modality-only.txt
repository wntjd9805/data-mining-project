Person re-identification (re-id) aims to search for specific individuals across different surveillance videos. Existing methods assume that pedestrians do not change their clothes frequently, but the clothes-changing problem becomes significant when attempting to re-identify individuals over a long period of time. This problem is also relevant in real-world scenarios, such as criminal suspects changing their clothes to avoid identification and tracking. Clothes-changing person re-id has attracted attention due to its importance in intelligent surveillance systems. Humans can recognize acquaintances despite their changing clothes by utilizing clothes-irrelevant features such as face, hairstyle, body shape, and gait. Prior methods address this issue by modeling body shape and gait or using multi-modality inputs, but they require additional models or equipment. This paper proposes the Clothes-based Adversarial Loss (CAL) to better leverage clothes-irrelevant information in RGB images. CAL is a multi-positive-class classification loss that penalizes the predictive power of the re-id model regarding different clothes of the same identity. By minimizing CAL during training, the learned feature map highlights more clothes-irrelevant features compared to using only identification loss. Extensive experiments demonstrate that CAL, using RGB images alone, outperforms state-of-the-art methods on clothes-changing person re-id benchmarks. Additionally, this paper addresses the need for video-based settings in real-world scenarios and introduces the Clothes-Changing Video person re-ID (CCVID) dataset, which includes fine-grained clothes labels and provides richer appearance and temporal information. Evaluations on CCVID show that utilizing this information improves the performance of clothes-changing person re-id.