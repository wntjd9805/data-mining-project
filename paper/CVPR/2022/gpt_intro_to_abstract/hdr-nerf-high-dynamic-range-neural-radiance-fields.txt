Novel view synthesis is a highly pursued topic in computer graphics and computer vision. However, traditional methods are limited by the low dynamic range of rendered novel views compared to the human visual system. To address this, recent works have focused on recovering the radiance field using deep neural networks, known as neural radiance fields (NeRF), to render high-quality novel views. But the dynamic range of NeRF is still limited compared to the broader dynamic range of the physical world scene. On the other hand, High Dynamic Range (HDR) imaging techniques can recover HDR images from multiple Low Dynamic Range (LDR) images with different exposures. However, conventional HDR synthesis methods may introduce artifacts, and they are unable to render novel views. Thus, this paper proposes an end-to-end method called HDR-NeRF that recovers the high dynamic range neural radiance field from a set of LDR images with different exposures. The proposed method combines NeRF and HDR imaging techniques, featuring a differentiable tone mapper to model the process of radiance becoming pixel values in the image. The HDR-NeRF pipeline enables joint learning of the radiance field and tone mapping, crucial for recovering the HDR radiance field. The method is evaluated on a new HDR dataset containing synthetic and real-world scenes, showcasing its ability to render LDR novel views with arbitrary exposures and impressive HDR views. The main contributions of this paper are: (1) proposing an end-to-end HDR-NeRF method for recovering the high dynamic range neural radiance field, (2) modeling the camera response function to render both HDR and LDR views with varying exposures, and (3) presenting a new HDR dataset and achieving the best performance compared to state-of-the-art methods. The dataset and code will be made available for further research in the community.