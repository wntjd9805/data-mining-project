Face reenactment is the process of transforming the action of a reference face to a source face, involving pose and facial expression. This topic has gained significant attention in the field of computer vision and has numerous applications in virtual reality, animation, and entertainment. Several approaches, such as Landmark-Assisted Generation (LAG), have been proposed to address the challenges of similarity between actions and preservation of source identity. However, existing LAG approaches have limitations in handling pose transformation and accuracy of facial landmarks. Additionally, methods without landmarks, like the MGOS, still face challenges in achieving ideal performance metrics. To tackle these issues, we propose a Dual-Generator (DG) network that consists of an ID-preserving Shape Generator (IDSG) and a Reenacted Face Generator (RFG). The IDSG transfers pose and expression from a reference face to a source face, generating target landmark estimates. The RFG then takes these estimates and the source face as inputs to generate reenacted faces with the same action as the reference face but the identity of the source face. Our approach incorporates a 3D-landmark detector and an objective function to handle large-pose variations and capture the pose-dependent local shape variation. We train the DG network on a dataset with full pose variation to learn landmark motion and identity preservation across large poses. We summarize our contributions as the effectiveness of IDSG and RFG in generating identity-preserving facial shape and target faces, the use of 3D landmarks to handle large-pose face reenactment, and better performance compared to state-of-the-art approaches. Detailed evaluations on benchmarks and additional resources are available in our GitHub repository.