This paper introduces the research topics of video moment retrieval and video highlight detection, which address the challenges of finding relevant moments in videos and quickly scanning large amounts of video content. The existing model for joint solving both tasks has limitations in terms of assuming the presence of a text query and only considering the visual modality. To overcome these limitations, this paper proposes a unified multi-modal framework called Unified Multi-modal Transformers (UMT) that incorporates text, video, and audio inputs. UMT effectively handles different modality reliability situations and combinations, such as when the text input is unavailable or noisy. The effectiveness and superiority of the proposed framework are demonstrated through experiments on datasets for joint video moment retrieval and highlight detection as well as popular public datasets for moment retrieval and highlight detection. Comparative analyses and ablation studies are conducted to evaluate the essential components of the proposed scheme.