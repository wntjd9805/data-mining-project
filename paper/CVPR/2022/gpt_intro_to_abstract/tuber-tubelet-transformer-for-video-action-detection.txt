This paper addresses the problem of spatio-temporal human action detection in videos, which has significant applications in advanced video search engines, robotics, and self-driving cars. The paper explores two prevalent approaches for spatio-temporal action detection: frame-level detection and tubelet-level detection. Frame-level detection involves detecting and classifying the action independently on each frame and then linking these detections into coherent action tubes. Tubelet-level detection, on the other hand, directly generates spatio-temporal volumes to capture the coherence and dynamic nature of actions. However, existing tubelet-level methods are limited by their inability to track persons over longer periods of time. To overcome this limitation, the authors propose a tubelet-level detection approach called TubeR, which utilizes a tubelet-transformer framework. TubeR learns a set of tubelet queries to pull action-specific tubelet-level features from a spatio-temporal video representation. It includes a specialized spatial and temporal tubelet attention to allow tubelets to be flexible in their spatial location and scale over time. TubeR also incorporates a context aware classification head that considers both short-term and long-term contextual information for accurate action classification. Experimental results on popular action detection datasets demonstrate that TubeR outperforms existing state-of-the-art methods. The contributions of this paper include the proposal of TubeR, the formulation of tubelet query and attention, the introduction of a context aware classification head, and the achievement of state-of-the-art results in action detection.