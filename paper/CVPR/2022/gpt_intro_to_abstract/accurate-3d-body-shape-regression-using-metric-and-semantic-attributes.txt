The field of 3D human pose and shape (HPS) estimation has made significant progress in recent years, with methods now able to accurately regress 3D pose from a single image. However, less attention has been paid to the estimation of body shape, leading to the production of body shapes that do not represent the person in the image accurately. This is due to several reasons, including the focus on pose rather than shape in evaluation datasets, the lack of training datasets with 3D ground-truth shape, the challenge of estimating shape from clothing-obscured images, and the scale ambiguity in 2D images. However, realistic body shape estimation is critical for various applications such as AR/VR, apparel design, virtual try-on, and fitness. In order to address this gap and democratize avatars, the authors propose SHAPY, a new deep neural network that accurately regresses 3D body shape and pose from a single RGB image. To train SHAPY, the authors introduce two novel datasets and corresponding training methods. The first dataset consists of images of people with diverse body shapes for which anthropometric measurements such as height and circumference are available. The second dataset involves linguistic shape attributes provided by modeling agencies, which rate images according to shape attributes. Using these datasets, SHAPY is trained with three novel losses: a loss that penalizes mesh measurements that differ from ground-truth measurements, a function that maps 3D bodies to linguistic attribute scores, and a function that maps attributes to shape. Evaluation is performed on a new dataset called "Human Bodies in the Wild," which contains ground-truth 3D body scans and in-the-wild photos of 35 subjects. The results show that SHAPY estimates much more accurate 3D shape. The models, data, and code for SHAPY are made available for further research.