Real-world data often exhibits imbalanced distributions over classes, with a small number of classes containing a large number of instances (head classes) and most classes containing only a few instances (tail classes). This is particularly relevant for critical applications like medical diagnosis, autonomous driving, and fairness, where the minority classes play a significant role. Imbalanced and long-tailed datasets pose challenges for classification tasks, leading to a drop in performance. Techniques such as data resampling and loss re-weighting have been explored to address this issue, but they often harm the performance of head classes. Supervised contrastive learning has emerged as a promising approach for long-tailed recognition, offering performance gains. However, applying contrastive loss to imbalanced data can result in poor uniformity, which hinders performance. Uniformity, the property of classes being uniformly distributed in the feature space, is desirable as it maximizes the margin and improves generalizability. Imbalanced data naturally puts more weight on the loss of majority classes, leading to ununiform distributions in the feature space. In this paper, we propose targeted supervised contrastive learning (TSC) for long-tailed recognition. TSC generates optimal class centers in advance to avoid bias from head classes and utilizes an online matching-training scheme to perform contrastive training while adaptively matching samples to class targets. TSC achieves a class-balanced feature space regardless of the training set's imbalance ratio. We evaluate TSC on benchmark datasets and demonstrate its state-of-the-art performance in long-tailed recognition. This paper introduces TSC as a novel framework, highlights the poor uniformity issue in supervised contrastive learning, and showcases TSC's effectiveness in achieving superior long-tailed recognition performance.