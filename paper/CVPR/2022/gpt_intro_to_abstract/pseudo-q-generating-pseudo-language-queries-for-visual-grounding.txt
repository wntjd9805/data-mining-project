Visual grounding (VG) task has made significant progress in recent years by leveraging advancements in computer vision and natural language processing. VG aims to localize objects referred to in natural language queries, which is crucial for various vision-language tasks. Existing VG methods can be categorized as fully-supervised or weakly-supervised, both of which rely heavily on manually annotated datasets. However, manually annotating large quantities of data, especially natural language queries, is expensive and time-consuming. To address this challenge, we propose a pseudo language query-based approach (Pseudo-Q) for visual grounding. Inspired by previous works in image captioning, we leverage an unlabelled image set, a sentence corpus, and an off-the-shelf object detector to generate pseudo region-query pairs. We also introduce a query prompt module and a multi-level cross-modality attention mechanism to tailor and fuse visual and language features. Experimental results show that our approach reduces human annotation costs while achieving comparable performance with state-of-the-art weakly-supervised visual grounding methods. Our contributions include introducing a pseudo-query based visual grounding method, proposing a query prompt module and a visual-language model with multi-level cross-modality attention, and demonstrating the effectiveness of our approach through extensive experiments.