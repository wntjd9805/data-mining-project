Deep Neural Networks (DNNs) have made significant advancements in recent years, revolutionizing various real-life applications. Neural networks heavily rely on activation functions, which are crucial in determining the effectiveness and training dynamics of deep neural networks. While hand-designed activation functions are commonly used in neural network models, the widely used Rectified Linear Unit (ReLU) suffers from the "dying ReLU" problem, where a large percentage of neurons become inactive during training. Researchers have proposed several alternatives to address the limitations of ReLU, including Leaky ReLU, Parametric ReLU, ELU, Softplus, and Randomized Leaky ReLU. Despite these advancements, there is still room for improvement in performance. Swish and GELU are two activation functions that have shown promise as smooth approximations of ReLU. Additionally, a few non-linear activations have been introduced that further enhance the performance of ReLU, Swish, or GELU. These functions are either hand-designed or smooth approximations of Leaky ReLU, such as Mish, ErfAct, Padé activation unit, and Orthogonal Padé activation unit. The development and exploration of these activation functions aim to overcome the limitations of ReLU and enhance the efficiency and effectiveness of deep neural networks.