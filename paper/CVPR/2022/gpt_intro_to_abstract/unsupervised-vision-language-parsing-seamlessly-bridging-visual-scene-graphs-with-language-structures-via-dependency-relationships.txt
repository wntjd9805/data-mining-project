Visual scene understanding has been a primary objective in computer vision, with efforts focused on higher-order visual understanding beyond individual object detection. Scene graphs have emerged as popular visual structures for representing objects and their relationships in scene images. However, existing scene graph generation models face limitations in dataset coverage, labeling efficiency, and inducing semantically-consistent graphical structures. In this paper, we propose a new task, unsupervised vision-language parsing (VLParse), to bridge visual scene graphs with linguistic dependency trees. We leverage the structured information in natural language and introduce a novel contrastive learning-based architecture, Vision-Language Graph Autoencoder (VLGAE), to construct a multimodal structure and align visual-language information. Our framework consists of feature extraction, structure construction, and cross-modality matching modules. We evaluate our approach on a new dataset and demonstrate significant improvements in structure induction and cross-modality alignment. Our contributions include the design of a joint VL structure, the introduction of the VLParse task, the creation of a dataset using a two-step paradigm, the benchmarking of our dataset with VLGAE, and empirical evidence of improved performance.