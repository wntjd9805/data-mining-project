In recent years, deep convolutional neural networks have made significant advancements in medical image segmentation. However, the distribution shift between training and test data often leads to a degradation in performance when deploying segmentation models. Unsupervised domain adaptation (UDA) based segmentation has been extensively studied to address domain shift, but it requires access to the target domain, which is not always feasible in real applications. To overcome this limitation, we propose a more practical yet challenging approach called domain generalization (DG) for generalizable medical image segmentation. Existing DG models mainly focus on cross-central settings with minor domain variations, while larger domain shifts such as cross-modality have been less explored. We introduce two types of generalizable segmentation scenarios, namely cross-center and cross-modality, and demonstrate the limitations of existing DG models on the cross-modality task. We then present a novel cross-modality medical image segmentation method that is trained on a single source domain and directly applied to an unseen target domain without retraining. Our method utilizes BÃ©zier Curves to augment the source domain into source-similar and source-dissimilar images, addressing the gray-scale distribution discrepancy in medical images. We develop a dual-normalization network to preserve the style information of these two domains, and a style-based path selection scheme for optimal segmentation in the target domain. Extensive experiments on different datasets demonstrate the effectiveness of our method, outperforming state-of-the-art DG methods and achieving competitive results in challenging cross-modality scenarios. Our proposed method has the potential to address domain shift in medical image segmentation tasks without requiring access to the target domain or retraining the model.