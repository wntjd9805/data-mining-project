This paper introduces a few-shot incremental learning method for label-to-image translation (FILIT). The task is to generate images from semantic label maps, which depict the layouts and semantic classes of images. Existing approaches in this area have made progress in areas such as spatial alignment, diversity, fine details, and style controls. However, these approaches still suffer from two main issues: the requirement of a large amount of labeled data for training and the inability to learn new semantic classes without retraining the model. Manual labeling of data is costly and complicated, making it expensive to acquire semantically fine-annotated data. Additionally, existing label-to-image translation models require that all training samples of all classes are prepared beforehand and learned at once. In practice, a trained translation model is often expected to perform new image generation tasks by learning novel semantic classes.To address these issues, the paper proposes FILIT, a method that enables a pre-trained translation model to learn novel semantic classes incrementally. Inspired by the human learning process, incremental learning is used, which continuously updates a trained model with samples from new tasks. To prevent catastrophic forgetting, regularization, rehearsal, expansion, and few-shot incremental learning methods are used. FILIT adopts semantically-adaptive normalization and convolution filters in the generator, allowing customization of filters and normalization for each pixel based on its semantic class. During new task learning, only a few modulation parameters for base convolution and normalization are learned. A modulation transfer strategy is also proposed to accelerate the convergence of modulation parameters for a new class.Experimental results show that FILIT effectively learns new classes without forgetting previously learned classes. Ablation studies demonstrate the efficacy of the semantically-adaptive design and transfer strategy, with a low number of extra parameters required. Further experiments show that a trained FILIT model can even learn semantic classes from datasets in other domains. The proposed method allows users to incrementally add new classes with a few labeled images, reducing the data annotation burden on users. The contributions of this research include the introduction of a few-shot incremental learning method for label-to-image translation, the adoption of semantically-adaptive filters and normalization in the model, and the proposal of a modulation transfer strategy to accelerate incremental learning.