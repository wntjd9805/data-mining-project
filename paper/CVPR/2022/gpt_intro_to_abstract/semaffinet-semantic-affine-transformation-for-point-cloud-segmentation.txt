Point cloud semantic segmentation plays a crucial role in computer vision for tasks such as structural representation learning and stereoscopic scene understanding. It involves partitioning the scene space into regions with semantic meaning based on the conformation and geometry knowledge inherent in point cloud layouts. The successful applications of point cloud semantic segmentation in autonomous driving, robotic manipulation, and virtual reality have motivated researchers to develop more accurate and fine-grained solutions. In this paper, we propose a novel approach called Semantic-Affine Transformation to address the local confusion problem in point cloud segmentation. We introduce class-specific affine parameters that explicitly pull features from the same category closer while pushing features from different categories apart, enhancing the representation ability of mid-level features. We design a semantic-aware network named SemAffiNet, which combines the Semantic-Affine Transformation with Transformer modules to manage semantic information both implicitly and explicitly. Extensive experiments on the ScanNetV2 and NYUv2 datasets demonstrate the superiority and generalization ability of our approach compared to previous state-of-the-art methods. The proposed Semantic-Affine Transformation is evaluated on both 3D point cloud and 2D image segmentation baselines, confirming its effectiveness and generalizability. Overall, our contributions include the introduction of the Semantic-Affine Transformation, the development of the SemAffiNet model, and the evaluation of our method on various datasets and settings.