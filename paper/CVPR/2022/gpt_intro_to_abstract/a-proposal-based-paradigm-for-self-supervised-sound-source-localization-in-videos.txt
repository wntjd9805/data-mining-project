In this paper, we introduce a novel proposal-based paradigm for sound source localization in visual scenes. We highlight the importance of this ability in various practical applications, such as robotic rescue and photography. Existing methods for sound source localization have limitations, including the reliance on image-level annotations and the tendency to highlight discriminative regions rather than the entire sound source.To address these limitations, we propose a self-supervised learning approach using unlabeled videos. We generate confidence score maps that not only explain the network's decision but also allow for sound source localization. However, map-based methods still suffer from issues such as partial region coverage and pathological biases. We argue for the need for semantic object-level localization of the sound source, rather than coarse-grained localization.We propose a paradigm shift towards proposal-based localization, inspired by weakly-supervised object detection. We treat sound source detectors as hidden nodes in a Multiple Instance Learning framework and incorporate audio information as weak annotations. We also introduce a Global Response Map as an unsupervised spatial constraint to weigh instances based on their coverage of the estimated map.Our proposed solution improves training efficiency and achieves state-of-the-art performance in sound source localization. We demonstrate the effectiveness of our approach through experimental results. Our contributions include the proposal-based solution, the introduction of the unsupervised spatial constraint, and the improvement over map-based methods.Overall, our work addresses the challenge of class-agnostic sound source localization in visual scenes, providing a new perspective and methodology for this important task.