Visual grounding (VG) is an important task in computer vision and natural language processing, which aims to align phrase-region pairs from images and sentences. This task has applications in various domains, such as translation, cross-modal retrieval, image captioning, and visual query answering. However, existing state-of-the-art VG methods primarily follow a one-step evaluate-and-rank matching architecture, which limits their performance.In this paper, we propose a search-based visual grounding mechanism to overcome the limitations of the one-step matching architecture. We remodel VG as a progressively optimized visual semantic alignment process, where simple initialization regions are continuously optimized to approach the target regions. To achieve this, we use graph theory to model the highly structured information in the spatial regions and multi-modal input.Our framework, called multi-modal dynamic graph transformer (M-DGT), treats regions as nodes and semantic relations as edges in a graph. M-DGT constructs a graph from a few simple initialization regions and then transforms the nodes and edges based on multi-modal information and graph features. This process enables M-DGT to continuously correct any deviations and improve robustness.The main contributions of this paper are threefold. Firstly, we redefine VG as a progressively optimized visual semantic alignment process, making it solvable in sub-problems that can be solved progressively. Secondly, we propose M-DGT, a novel framework that transforms the region-text matching problem into a graph structure transformation. Finally, M-DGT is fast, accurate, and has high generality, as it can work on any dataset. We validate the effectiveness of M-DGT on phrase localization and referring expression comprehension tasks, achieving state-of-the-art accuracies and high Intersect over Union (IOU) scores.In conclusion, our approach breaks the limitations of existing VG methods by introducing a search-based mechanism and a graph structure transformation. The results demonstrate the effectiveness and generality of our proposed framework in solving the visual grounding problem.