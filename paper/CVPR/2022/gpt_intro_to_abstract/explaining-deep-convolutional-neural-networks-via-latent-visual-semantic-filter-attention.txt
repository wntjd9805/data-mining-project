Convolutional neural networks (CNNs) have achieved impressive results in visual representation learning, but the lack of explainability in these models raises concerns about potential biases and negative effects on users and society. Previous studies have reported biases in computer vision models for face attribute classification, recognition, and image captioning. Many methods have been proposed to interpret the learned representations in CNNs, but they vary in their approach and limitations. This paper aims to overcome these limitations by introducing the Latent Visual Semantic Ex-plainer (LaViSE), a framework that generates textual interpretations of any existing black box model. Unlike existing approaches, LaViSE generates semantic and objective explanations without relying on visualization-based methods or requiring model training with ground-truth explanation annotations. Instead, LaViSE constructs a mapping between visual and semantic spaces using generic image datasets and transfers this mapping to the target domain without semantic labels. The proposed method not only explains individual filters but also utilizes a novel filter attention method to aggregate responses. Experimental results demonstrate that LaViSE can generate novel descriptions for learned filters beyond the training dataset categories and provide more accurate explanations compared to existing methods. Additionally, the framework can be applied to practical applications such as comparative analysis of multiple models or sets of images. The effectiveness of LaViSE is demonstrated through comparisons between a finetuned model and a model trained from scratch, as well as an analysis of gender disparities in datasets, including an examination of social media photographs posted by U.S. politicians. Overall, this paper presents a novel method for interpreting CNNs and showcases its utility in solving real-world problems.