Federated Learning (FL) is a distributed machine learning framework that allows multiple clients to collaboratively train models with decentralized data while ensuring basic privacy. Traditional FL methods assume that participating client models have the same neural architecture, leading to challenges in real-world scenarios where clients have personalized requirements and design their own models independently. To address this heterogeneous federated learning problem, various methods have been proposed, but most rely on a global consensus or shared models, limiting the clients' ability to adjust their learning direction individually. Additionally, existing FL methods struggle to handle label noise, which can significantly impact performance. This paper introduces Robust Heterogeneous Federated Learning (RHFL) as a solution to the robust federated learning problem with noisy and heterogeneous clients. RHFL includes approaches such as aligning logits output distributions, local noise learning with a noise-tolerant loss function, and client confidence re-weighting for external noise. The proposed method computes the optimal weighted combination of participating clients, reducing the contribution of noisy clients and increasing the contribution of clean clients. Experimental results demonstrate that RHFL achieves strong robustness compared to competing methods in various settings.