Shapes obtained from cameras that sample surfaces often result in incomplete data, with varying sampling densities and missing parts. Surface completion, which involves inferring missing parts using non-local hints, has been extensively studied. Deep implicit function (DIF) has emerged as an effective representation for shape completion, with earlier DIFs encoding shapes using a single global latent vector. To preserve geometric details and handle ambiguous partial input, a combination of global and region-specific local codes is proposed. However, local DIF struggles to produce meaningful completions for unseen regions. To address this, generative models are combined, but current local methods allocate excessive model capacity to irrelevant details. In this paper, ShapeFormer, a transformer-based autoregressive model, is introduced to learn a distribution over possible shape completions. Local codes are used to form a sequence of discrete, vector quantized features, reducing representation size while maintaining structure. Transformers are shown to be effective for sequence-based tasks in the image domain, but directly applying them to 3D feature grids is computationally expensive. To mitigate this, Vector Quantized Deep Implicit Functions (VQDIF) is introduced, a compact and structured 3D representation that encodes shapes as sequences of discrete 2-tuples. ShapeFormer completes shapes by generating complete sequences conditioned on partial observations, using an auto-regressive objective. The model is evaluated on ambiguous partial observations of various shape types and incomplete sources, demonstrating high-quality and diverse completions. The contributions of this paper include a novel DIF representation, a transformer-based autoregressive model, and state-of-the-art results for multi-modal shape completion.