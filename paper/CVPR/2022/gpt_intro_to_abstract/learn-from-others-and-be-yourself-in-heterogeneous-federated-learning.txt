Deep learning algorithms have made significant progress in recent years, mainly due to the availability of large-scale data. However, in real-world scenarios, data are often dispersed among different participants, such as mobile devices or organizations. Privacy concerns and data protection regulations prevent participants from integrating their data to train a model. To address this issue, federated learning has emerged as a privacy-preserving paradigm, allowing participants to collaboratively learn a model without sharing their private data. Federated learning has shown promising results in real-world settings, but it also faces several challenges. One significant challenge is the heterogeneity problem, where distributed data may have different distributions and characteristics. Existing methods mainly focus on handling label distribution skew but neglect the domain shift, where data of the same label may have different features. Another challenge is catastrophic forgetting, which refers to the tendency of the model to forget previous knowledge while learning new information. In this paper, we propose a novel federated learning method called FCCL (Federated Cross-Correlation and Continual Learning) to tackle these challenges. FCCL leverages unlabeled public data and self-supervised learning to address the heterogeneity problem. To handle catastrophic forgetting, FCCL incorporates knowledge distillation techniques in both inter and intra domain learning. Extensive experiments on image classification tasks demonstrate that FCCL outperforms existing methods in terms of inter and intra domain performance.