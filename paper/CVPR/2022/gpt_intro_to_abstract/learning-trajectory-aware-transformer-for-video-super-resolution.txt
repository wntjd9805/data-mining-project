Video super-resolution (VSR) is a fundamental task in computer vision that aims to enhance visual quality by recovering a high-resolution (HR) video from a low-resolution (LR) counterpart. It has significant value in practical applications such as video surveillance, high-definition television, and satellite imagery. Existing VSR approaches can be categorized into two paradigms: those that utilize adjacent frames as inputs and those that investigate temporal utilization through recurrent mechanisms. However, these approaches face challenges in capturing distant frames or achieving long-term modeling capabilities. Inspired by the progress of Transformers in natural language processing, this paper proposes a Trajectory-aware Transformer for effective video representation learning in VSR. The proposed method formulates video frames into pre-aligned trajectories of visual tokens and calculates attention within each trajectory. It effectively links relevant visual tokens along temporal dimensions to capture object motions. The location map and cross-scale feature tokenization module further enhance the method's performance in handling scale-changing problems. Experimental results demonstrate that the proposed Trajectory-aware Transformer outperforms state-of-the-art methods in four widely-used VSR benchmarks, particularly achieving significant improvements in challenging datasets.