This paper introduces the challenge of understanding the 3D world from natural images, which has diverse applications in computer vision and graphics. The traditional approach involves collecting 2D and 3D data and using direct or photometric-driven supervision to learn a 3D predictor. However, this data collection is often difficult or impractical. To address this issue, researchers have explored learning from single images, utilizing object-specific shape models or auxiliary information. Another approach is fully unsupervised learning, which learns 3D representations from single images without additional supervision. Previous unsupervised approaches focus on using viewpoint cues, but they struggle with limited or unavailable viewpoint cues. To complement viewpoint cues, an aperture rendering GAN (AR-GAN) was proposed to exploit defocus cues. However, AR-GAN does not jointly utilize both cues effectively. To address this, this paper proposes aperture rendering NeRF (AR-NeRF), a unified model that leverages both defocus and viewpoint cues. AR-NeRF represents both factors through a common ray-tracing framework, enabling the representation of defocus effects and viewpoint changes. Aperture randomized training is introduced to disentangle defocus-aware and defocus-independent representations in an unsupervised manner. AR-NeRF is evaluated on various natural image datasets and demonstrated superior or comparable performance compared to baseline models. This paper contributes a unified approach for unsupervised learning of depth and defocus effects, incorporating both viewpoint and defocus cues. Empirical results highlight the utility of AR-NeRF in various datasets.