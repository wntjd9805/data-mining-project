Abstract:The ability to generate photorealistic views of a scene from limited observations has numerous applications in fields such as virtual/augmented reality, cinematography, and gaming. However, existing methods for novel view synthesis (NVS) struggle with real-world scenarios involving complex geometry, lighting variations, and other challenges. Despite advancements in Deep Learning approaches, there is still a gap between the state of the art and an ideal model for NVS. In this paper, we propose a new system that enables real-time rendering and quick adaptation to new scenes. Our method leverages point clouds to model scene geometry, which can be easily obtained using RGBD cameras or classic Structure-from-Motion pipelines. We address the limitations of point-cloud-based models by using neural rendering to handle small noise and low point density. Our approach predicts point features from source images, achieving fast scene representations that can be rendered in real-time. Additionally, we introduce view-dependency to neural descriptors and implement alignment techniques to boost the quality of our system. Our contributions include a fast NVS system, an online aggregation method for incorporating features from multiple views, and alignment techniques for in-plane rotations. Experimental results show the effectiveness and speed of our system, paving the way for high-quality and real-time rendering in NVS applications.