This paper introduces a video representation architecture for predicting occluded objects in video. The field of computer vision has explored deep learning methods for operating on point clouds, which are sparse and scalable to large scenes. However, existing approaches lack the ability to generate new points based on previous observations. To address this, the proposed architecture learns to predict 4D point clouds from RGB-D video by using a continuous neural field representation and an attention mechanism. Experimental results demonstrate the effectiveness of the video representation in various occlusion reasoning tasks such as reconstruction, geometry estimation, tracking, and segmentation. The approach remains robust in cluttered scenes and objects of different sizes. In addition, the paper introduces the tasks of dynamic scene completion and object permanence in cluttered situations, and provides new benchmarks for evaluation. Overall, the proposed architecture enables representation learning from large-scale point cloud data. The paper concludes by inviting the community to use the benchmarks to test their own video understanding models.