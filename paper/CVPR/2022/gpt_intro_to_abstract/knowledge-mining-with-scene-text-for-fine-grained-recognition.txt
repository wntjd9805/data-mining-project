This paper explores the extraction of background knowledge and contextual information of scene text to enhance the fine-grained image classification task. Unlike document text, natural scene text is often sparse and consists of keywords rather than complete sentences, making it challenging for classification models to understand the precise meaning conveyed by the text. The paper proposes a method that retrieves relevant knowledge from databases such as WordNet and Wikipedia to complement the literal meaning of the scene text. A deep-learning-based architecture is designed to combine visual contents, scene text, and knowledge for fine-grained image recognition. The method achieves significant improvements in performance and can be applied to other tasks such as visual grounding and text-visual question answering. Additionally, a new dataset containing multiple scene text instances is proposed, which facilitates the study of multi-modal crowd activity analysis.