Image inpainting is a challenging task in computer science that involves filling in missing areas of pictures. It has various real-world applications such as object removal, photo restoration, and image editing. Traditional algorithms search for similar patches to reconstruct the missing areas, but they struggle to preserve both coherent textures and reasonable structures in large images. Deep learning methods utilizing Convolutional Neural Networks (CNNs) and Generative Adversarial Networks (GANs) have shown promise in efficiently conducting image inpainting tasks. However, these methods still face dilemmas such as limited receptive fields, missing holistic structures, heavy computations, and the lack of positional information in masked regions. Some existing approaches have partially addressed these issues by leveraging attention mechanisms, Fast Fourier Convolution (FFC), and auxiliary information like edges and segmentation. However, these approaches have limitations and are often costly to train. Furthermore, the importance of positional information in image inpainting has not been explicitly discussed or utilized in previous works. In this paper, we propose a novel approach called ZeroRA-based Incremental Transformer Structure (ZITS) inpainting framework enhanced with Masking Positional Encoding (MPE). We leverage a transformer-based model to tackle holistic structures and use a simple CNN for efficient upsampling. We also introduce a Zero-initialized Residual Addition (ZeroRA) incremental training strategy to incorporate structural information into a pretrained inpainting model. Additionally, we utilize positional encoding for the mask region to improve the performance of image restoration. Our experiments on various datasets demonstrate that our proposed model outperforms other state-of-the-art competitors in image inpainting tasks.