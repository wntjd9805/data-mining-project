The CLIP family of neural networks has shown impressive performance on visual recognition tasks, including zero-shot performance on ImageNet that matches a fully supervised ResNet-50 model. These visual representations have also been beneficial for other computer vision tasks such as object detection, image captioning, and visual question answering. In this paper, we investigate the effectiveness of CLIP's visual representations in the domain of Embodied AI tasks, which involve agents navigating and interacting with their environments. We demonstrate that CLIP features can improve instruction following in navigation graphs and explore CLIP-powered models for tasks that require low-level instructions to interact with objects. We build simple baselines using CLIP ResNet-50 as a visual encoder and achieve impressive results on Object Goal Navigation and Room Rearrangement tasks. Our findings suggest that CLIP's visual representations encode useful primitives for downstream Embodied AI tasks. We compare CLIP with ImageNet pretraining and find that CLIP outperforms ImageNet in probing studies for object presence, localization, reachability, and free space estimation. We also analyze the correlation between ImageNet accuracy and success rate in Object Goal Navigation and discover that ImageNet accuracy is not a strong indicator of an encoder's suitability for Embodied AI tasks. Furthermore, we use CLIP's visual and textual encoders in a simple architecture to train an agent for zero-shot Object Goal Navigation, achieving promising results in generalizing to new objects. Overall, our study highlights the power of CLIP's visual representations and their potential impact on Embodied AI models.