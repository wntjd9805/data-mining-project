Knowledge Distillation (KD) is a technique that has been widely used in various learning tasks to improve generalization ability. While offline KD involves transferring knowledge from a pre-trained model to a student model, online KD enables students to learn from each other. However, existing KD approaches suffer from low knowledge transferring efficiency and high computational costs. In this paper, we propose a simple and efficient self distillation approach called Self-Distillation from Last Mini-Batch (DLB). DLB saves run-in memory by only storing soft targets from the last mini-batch and provides instant distillation for each training sample. We evaluate DLB on benchmark datasets and demonstrate consistent improvement in generalization ability. This method requires no network architecture modifications and is task-agnostic and model-agnostic. We also analyze the impact of DLB on training dynamics and observe that it promotes training consistency and exhibits robustness to label noise. This study provides insights into the effectiveness of knowledge distillation.