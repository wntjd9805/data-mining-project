In recent years, the fields of computer vision and natural language processing have seen significant developments in convolutional neural networks (CNNs) and self-attention mechanisms. CNNs have been widely used in various image recognition tasks, while self-attention has shown great potential in image generation and super-resolution. With the advent of vision transformers, attention-based modules have even outperformed CNNs in some vision tasks.However, convolution and self-attention modules have traditionally followed different design paradigms. Convolution relies on localized receptive fields and shared filter weights, providing important biases for image processing. On the other hand, self-attention applies a dynamic weighted average operation based on input feature context, allowing for adaptive focus and capturing more informative features.Considering the differing and complementary properties of convolution and self-attention, there is potential to benefit from their integration. Previous research has explored combining these modules, either as augmentations or by substituting convolutions with self-attention. However, existing approaches still consider these modules as distinct parts, without fully exploiting their underlying relations.In this paper, we aim to uncover a closer relationship between self-attention and convolution. We demonstrate that both modules heavily rely on 1×1 convolutions and propose an integrated model named ACmix. ACmix leverages 1×1 convolutions to project input feature maps, generating a rich set of intermediate features that are then aggregated using self-attention and convolution paradigms. Our approach avoids redundant projection operations, achieving an elegant integration with minimal computational overhead.Our contributions are two-fold. Firstly, we reveal a strong underlying relation between self-attention and convolution, providing new insights into their connections and inspiring new learning paradigms. Secondly, we present ACmix, an integrated model that combines the benefits of self-attention and convolution, consistently outperforming pure convolution or self-attention counterparts in empirical evaluations.