Large-scale learning methods based on deep neural networks have made significant advances in 3D vision, playing a critical role in visual perception for intelligent platforms such as robots, drones, and self-driving cars. These platforms rely on real-time depth sensors, like LiDAR, to capture accurate geometric information represented by 3D point clouds. However, the scalability of deep neural networks is limited by the need for massive amounts of labeled point clouds, hindering their applicability to real-world scenarios. To address this issue, unsupervised point cloud domain adaptation is gaining attention as a means of transferring knowledge from a labeled source domain to an unlabeled target domain, both sharing the same feature space. Nevertheless, differences in point scales, object sizes, densities, styles, and sensor perspectives cause deviations between point cloud representations in the target and source domains. In this paper, we propose a self-supervised global-local structure modeling approach for point cloud domain adaptation. The global structure is modeled by scaling up/down the point cloud and predicting the scale, while the local structure is captured by squeezing a random 3D local area onto a 2D plane and reconstructing it using the network. Additionally, we introduce a voting method to assign reliable pseudo labels to target samples, facilitating iterative selection of more target data during training and enhancing learning. To evaluate our approach, we conduct experiments on PointDA, a widely-used 3D domain adaptation benchmark, as well as a Sim-to-Real dataset. Results demonstrate the efficacy of our method in improving the accuracy of unsupervised domain adaptation in point cloud applications. Overall, this paper makes contributions in the areas of structure modeling, knowledge transfer, and unsupervised domain adaptation on point clouds.