In recent years, scene graph generation, which abstracts visual relationships as a graph structure, has gained significant attention in computer vision tasks such as image retrieval, image captioning, and visual question answering. Existing methods for scene graph generation can be categorized into static and dynamic approaches. While static scene graph generation from a single image has been extensively studied, dynamic scene graph generation from videos, which involves capturing temporal relationships, remains less explored. Existing dynamic scene graph generation methods focus on modeling temporal structure information at the feature level but fail to explicitly capture the temporal correlations of visual relationships. This contrasts with humans' ability to infer subsequent relationships based on past relationships. To address this, we propose an anticipatory pre-training paradigm for predicting dynamic scene graphs in videos. This paradigm involves using previous frames to predict relationships in the current frame. By pre-training models to predict visual relationships in unseen frames, our approach explicitly extracts temporal correlations. Additionally, we leverage a large amount of unlabeled data for pre-training, which alleviates the problem of insufficient annotations. We instantiate the anticipatory pre-training paradigm using a Transformer architecture that incorporates a spatial encoder to extract spatial information and a progressive temporal encoder to capture temporal correlations based on visual and semantic features. We further design an efficient attention mechanism for capturing long-term visual context without introducing excessive parameters. Experimental results on the Action Genome dataset demonstrate that our proposed model achieves state-of-the-art performance in dynamic scene graph generation.