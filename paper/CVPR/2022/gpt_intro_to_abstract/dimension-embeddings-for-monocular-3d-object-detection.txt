The introduction of this computer science paper focuses on the importance of 3D object detection in sensing the 3D environment for applications like autonomous driving and robot navigation. Existing methods for 3D object detection rely on point clouds from LiDAR devices or binocular images from stereo cameras, but these can be expensive or suffer from inaccurate calibrations. Monocular methods, which only use one color camera and reason about appearance information, have emerged as a promising alternative. However, monocular 3D object detection is an ill-posed problem where the 3D dimension and distance of objects are entangled. Existing methods mainly focus on estimating object distance, while the estimation of 3D dimensions is often overlooked. To improve dimension estimation in monocular 3D object detection, this paper proposes leveraging dimension similarities among objects and the mapping relationship between visual appearances and object dimensions. The authors propose a dimension-aware embedding space and learnable shape templates to incorporate these clues into a neural network. The experimental results on the KITTI 3D object detection benchmark demonstrate the effectiveness and generalizability of the proposed dimension embedding approach, achieving new state-of-the-art performance with real-time latency. The contributions of the paper include utilizing dimension similarities for dimension learning, incorporating subcategory information through shape templates, and improving existing keypoints-based methods for monocular 3D object detection.