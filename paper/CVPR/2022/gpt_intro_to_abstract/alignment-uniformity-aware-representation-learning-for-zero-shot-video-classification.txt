Zero-shot video classification (ZSVC) is an approach that aims to mimic human ability to recognize unseen objects by training models only on videos of seen classes and making predictions on unobserved ones. Existing ZSVC models map visual and semantic features into a unified representation in the hope that the association can be generalized to unseen classes. However, these methods face two critical problems: semantic gap (inconsistency between visual and semantic features) and domain shift (biases in representations when applied to different datasets with disjoint classes). Most existing methods focus on addressing the semantic gap by learning alignment-aware representations, optimizing the similarity between visual and semantic features using loss functions like MSE loss, ranking loss, or center loss. Architectural designs such as projecting visual features to local object attributes or using attention modules for direct alignment have also been explored. However, these approaches often rely on pre-trained models for extracting visual features and fail to provide a true end-to-end framework for visual-semantic feature learning. To tackle both the semantic gap and domain shift problems, we propose an end-to-end framework that simultaneously preserves alignment and uniformity properties in representations of both seen and unseen classes. Alignment ensures closeness between visual and semantic features within classes, while uniformity encourages the features to distribute uniformly to improve generalizability and mitigate the domain shift implicitly. We introduce a supervised contrastive loss that combines two terms: one for alignment regularization and the other for guiding uniformity between semantic clusters. To explicitly address the domain shift, we generate synthetic features for unseen classes using our class generator. Additionally, we introduce closeness and dispersion scores to quantify the properties and measure model generalizability.Experiments conducted on UCF101 and HMDB51 datasets demonstrate that our method outperforms the state-of-the-art by significant improvements of 28.1% and 27.0% respectively. Our method achieves better closeness within classes and preserves more dispersion between semantic clusters compared to the alternative approach. By providing a compact model that maximizes the preservation of semantic information for existing classes while synthesizing features for unseen classes, our approach offers a promising solution to the challenges posed by zero-shot video classification.