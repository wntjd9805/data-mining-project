3D vision is a crucial aspect for applications such as autonomous driving, mixed reality, and robotics. Many research efforts have been focused on addressing 3D vision problems, such as object classification, detection, and segmentation. Point clouds have become a popular method for representing 3D data in these research endeavors. However, deep learning algorithms heavily rely on large-scale annotated datasets, and manually annotating such datasets for 3D point clouds is laborious. Self-supervised learning has proven to be effective in addressing this issue in the 2D domain. Several works have explored self-supervised representation learning on point clouds, including generative models and reconstruction-based approaches. Additionally, recent works have investigated self-supervised contrastive learning for point cloud understanding. However, existing contrastive learning methods for point clouds only impose invariance to augmentations of 3D point clouds. Inspired by the success of multimodal learning, this paper introduces CrossPoint, a cross-modal contrastive learning approach for 3D point cloud understanding. The goal of CrossPoint is to capture the correspondence between 3D objects and 2D images to learn transferable point cloud representations. The proposed approach embeds augmented versions of point clouds and their corresponding rendered 2D images close to each other in the feature space. The joint intra-modal and cross-modal learning objective aims to relate compositional patterns occurring in both point cloud and image modalities, acquire knowledge on spatial and semantic properties of point clouds, and promote 3D-2D correspondence. CrossPoint does not require a memory bank for negative sampling and achieves notable results in shape classification and other downstream tasks. The contributions of this work include demonstrating the effectiveness of using 3D-2D correspondence for 3D point cloud understanding, proposing a novel self-supervised learning objective incorporating intra-modal and cross-modal loss functions, and achieving superior performance compared to previous unsupervised learning methods in object classification, few-shot learning, and part segmentation tasks. Furthermore, fine-tuning the pretrained image backbone from CrossPoint improves performance in few-shot image classification.