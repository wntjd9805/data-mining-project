Semantic segmentation is a fundamental task in computer vision that aims to assign each pixel a semantic label in an image. Previous methods have achieved good performance by leveraging large amounts of fully annotated labels. However, manual annotation of such labels is time-consuming and labor-intensive. To reduce annotation costs and preserve segmentation performance, recent works have explored sparse annotations such as point-wise and scribble-wise annotations. Existing approaches for semantic segmentation with sparse annotations have limitations, including predictive errors from auxiliary tasks, time-consuming proposal generation, and issues with regularized losses and consistency learning.In this paper, we propose a novel solution to address these shortcomings in semantic segmentation with sparse annotations (SASS). We divide each image into labeled and unlabeled regions and focus on learning from the unlabeled regions. We leverage the similarity between labeled and unlabeled pixels in terms of low-level color and high-level responses. Inspired by the tree filter, we use minimum spanning trees (MSTs) to model the pairwise similarity between adjacent pixels. By building MSTs based on color and semantic features, we generate soft pseudo labels for the unlabeled regions. We introduce a tree energy loss (TEL) that combines the MSTs with network predictions, enabling dynamic online self-training.To evaluate the effectiveness of TEL, we introduce a block-wise annotation setting that falls between point-wise and scribble-wise annotations. Experimental results demonstrate that TEL significantly improves segmentation performance without introducing additional computational costs during inference. Our method achieves state-of-the-art performance under different annotation settings. The main contributions of this work are the proposal of TEL for SASS, which leverages minimum spanning trees to model structural relations among pixels, and the introduction of a block-annotated setting for comprehensive validation. Our approach outperforms existing methods under various annotation settings.