Neural architecture search (NAS) has drawn significant interest in the field of deep learning as it enables the discovery of optimal architectures from a large search space. However, practical deployment of NAS algorithms is hindered by the computational overhead involved. To address this issue, several efficient search strategies have been proposed, including one-shot NAS, network transformation, and architecture optimization. Among these approaches, one-shot NAS stands out for its ability to preserve optimal sub-networks through weight sharing, thereby avoiding exhaustive training for model evaluation.However, existing methods like DARTS fail to accurately reflect the importance of operations for derived architectures. The magnitude of architecture parameters in DARTS does not necessarily reflect the actual operation importance for achieving high validation accuracy. To address this limitation, this paper presents a novel method called Shapley-NAS, which evaluates the operation contribution using the Shapley value of supernet components. Instead of relying solely on architecture parameter magnitudes, Shapley-NAS considers the practical influences of these parameters on task performance and directly evaluates their contributions to validation accuracy.Furthermore, the paper recognizes the complex relationships between operations in the supernet, as combinations of operations can have different joint influences compared to their individual effects. To handle these complex relationships, the Shapley value, a solution from cooperative game theory, is leveraged. The Shapley value quantifies the average marginal contribution of operations by considering all possible combinations, providing an effective measure of operation importance correlated with task performance. Since computing the exact Shapley value is computationally expensive, the paper employs Monte-Carlo sampling with early truncation for efficient approximation.The proposed Shapley-NAS method optimizes the supernet weights and iteratively updates the architecture parameters using a momentum update mechanism to alleviate fluctuation caused by the sampling process. Experimental results demonstrate that the obtained Shapley value has a higher correlation with task performance compared to DARTS. Extensive experiments conducted on different datasets and search spaces show that Shapley-NAS outperforms state-of-the-art architecture search methods. Notably, Shapley-NAS achieves an error rate of 2.43% on CIFAR-10 and a top-1 accuracy of 23.9% on ImageNet under the mobile setting. Moreover, Shapley-NAS achieves optimal architectures on CIFAR-10 and CIFAR-100 and near-optimal solutions on ImageNet-16-120 of the NAS-Bench-201 benchmark.