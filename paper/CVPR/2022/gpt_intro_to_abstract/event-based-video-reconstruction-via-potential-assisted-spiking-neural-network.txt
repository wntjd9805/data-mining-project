Event cameras are a type of vision sensor that offer advantages such as high temporal resolution, high dynamic range, and low power consumption compared to standard cameras. However, the event data generated by these cameras is not easily interpretable by human vision or traditional computer vision systems. Image reconstruction is a technique used to bridge this gap by providing a visual representation of the rich information encoded by events. Early approaches to image reconstruction relied on hand-crafted priors, while more recent methods have leveraged deep neural networks to achieve impressive performance. However, these deep neural networks can be computationally intensive and consume significant power, which is problematic for the low-latency requirements of event cameras. One alternative is to combine the sparse event data with low-power spiking neural networks (SNN), which are more biologically realistic and communicate via discrete spikes. While SNNs have been successfully applied to other visual tasks, there has been little exploration of their use for image reconstruction. In this paper, the authors propose a novel framework called EVSNN for event-based video reconstruction using a fully spiking neural network architecture. They also introduce a hybrid potential-assisted SNN (PA-EVSNN) that improves the temporal receptive field of the EVSNN. Experimental results show that the proposed models achieve comparable performance to existing deep neural network models while being significantly more computationally efficient. The EVSNN and PA-EVSNN architectures are 19.36x and 7.75x more computationally efficient, respectively, than their deep neural network counterparts. Additionally, the proposed models demonstrate improvements of 24.15x and 8.76x in computational efficiency compared to the E2VID model. Overall, the paper presents a promising approach to image reconstruction using SNNs, with potential implications for energy-efficient event-based vision systems.