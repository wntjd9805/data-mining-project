Highly dynamic scenes, such as fast-moving targets or non-linear motions, present challenges for generating high-quality videos due to blurring and missing target information between consecutive frames. Existing frame-based methods have addressed these issues through motion deblurring, frame interpolation, and blurry video enhancement techniques. However, frame-based deblurring methods struggle to predict sharp latent frames from severely blurred videos due to motion ambiguities and erasure of intensity textures. Additionally, frame-based interpolation approaches often assume linear motion between neighboring frames, leading to incorrect predictions in real-world scenarios with non-linear motions.Recent research has shown the advantages of event cameras in motion deblurring and frame interpolation. Event cameras inherently capture precise motions and sharp edges, effective in alleviating motion blur. They can also record almost continuous brightness changes to compensate for missing information between frames, facilitating the recovery of accurate intermediate frames under non-linear motions. However, existing works treat motion deblurring and frame interpolation as separate tasks, while these problems co-occur in real scenes and need to be addressed simultaneously.This paper proposes a unified framework called Event-based Video Deblurring and Interpolation (EVDI) for enhancing blurry videos. The framework consists of a learnable double integral (LDI) network and a fusion network. The LDI network predicts the mapping relation between blurry frames and sharp latent images from corresponding events, allowing flexibility in choosing the timestamp of the latent image within the exposure time of blurry frames or between consecutive blurry frames. The fusion network utilizes information from consecutive blurry frames and event streams to generate a fine result based on the coarse reconstruction of latent images. The proposed method leverages the mutual constraints among blurry frames, sharp latent images, and event streams, and employs a fully self-supervised learning framework for training without explicit ground truth images.The contributions of this paper are three-fold: 1. A unified framework of event-based video deblurring and interpolation that generates high frame-rate sharp videos from blurry inputs.2. A fully self-supervised framework for network training in real-world scenarios without labeled data.3. State-of-the-art results achieved on both synthetic and real-world datasets, while maintaining an efficient network design.