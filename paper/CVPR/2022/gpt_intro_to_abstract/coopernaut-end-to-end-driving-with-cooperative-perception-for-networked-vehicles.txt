The widespread deployment of autonomous driving and advanced driver assistance systems is faced with safety challenges. While deep learning has improved autonomy stacks, learning-based driving policies are still lacking robustness, especially in extreme situations and corner cases. Additionally, the limited sensing capabilities of optical sensors on individual vehicles further exacerbate the problem. This paper introduces COOPERNAUT, an end-to-end cooperative driving model that leverages cooperative perception through vehicle-to-vehicle (V2V) communications. COOPERNAUT learns to fuse encoded LiDAR information shared by nearby vehicles, taking into account network bandwidth limitations. An architecture based on the Point Transformer is designed to preprocess the raw point cloud into compact and spatially aware neural representations, which can be efficiently transmitted over wireless channels. COOPERNAUT is evaluated using a CARLA-based simulation framework, demonstrating improved driving performance and communication efficiency compared to baselines. The contributions of this paper include the introduction of COOPERNAUT, the development of the AUTOCASTSIM framework, and the validation of COOPERNAUT's effectiveness in reducing safety hazards and improving driving performance.