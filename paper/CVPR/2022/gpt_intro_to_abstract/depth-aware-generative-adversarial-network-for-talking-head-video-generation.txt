This paper aims to generate realistic talking head videos using a source image of a person and a driving video, with applications in role-playing video games and virtual anchors. Previous works have focused on decoupling identity and pose information using generative adversarial networks (GANs). However, these methods have not explored the importance of 3D dense geometry, such as pixel-level depth, in face video generation. In this paper, we propose a self-supervised approach to learn pixel-wise depth maps from training face videos without requiring expensive 3D geometry annotations. We then introduce two mechanisms - depth-guided facial keypoint detection and cross-modal attention - to effectively leverage the learned depth information for better talking-head video generation. We evaluate our approach on two datasets and show that it produces accurate depth maps and generates higher-quality face images compared to state-of-the-art methods. Our contributions include the self-supervised depth learning method, the depth-aware generative adversarial network architecture, and the superior performance achieved in generating high-fidelity face videos.