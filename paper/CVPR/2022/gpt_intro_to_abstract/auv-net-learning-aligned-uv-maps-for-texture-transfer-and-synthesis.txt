The field of 3D shape reconstruction and synthesis has seen advancements through deep learning techniques, enabling the reconstruction of 3D shapes from various inputs such as voxels, point clouds, and images. While 3D generative models have been proposed for generating new shapes, texture transfer and synthesis for 3D shapes have received less attention. Existing methods for texture generation often rely on warping a spherical mesh template, limiting the topology and introducing distortions. Other approaches use implicit texture fields but suffer from overly smoothed textures. Traditional UV mapping in computer graphics offers a more flexible and detailed representation, but inconsistencies across different shapes make it challenging for texture synthesis and transfer without dense shape correspondences. To address these limitations, this paper proposes a neural network that predicts the UV mapping and texture image jointly. The network learns to embed 3D coordinates onto a 2D aligned UV space, enabling mapping of corresponding parts of different shapes to the same locations in the texture image. A texture alignment module inspired by linear subspace methods ensures effective alignment, and a masking network helps unwrap 3D shapes of complex structure or topology. The proposed method achieves aligned textures across shapes, allowing for easy texture swapping and facilitating the training of generative models. Extensive experiments on various categories demonstrate the effectiveness of the approach.