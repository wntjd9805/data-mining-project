This paper presents a novel framework called Contrastive Action Representation Learning (CARL) for learning frame-wise action representations with spatio-temporal context information in a self-supervised manner. Unlike previous methods that rely on supervised learning with manually labeled data or video pairs of the same action, CARL leverages contrastive representation learning and a sequence contrastive loss (SCL) to optimize the embedding space for long videos with hundreds of frames. CARL uses a video encoder consisting of a 2D network for spatial encoding and a Transformer encoder for temporal interactions. The proposed framework achieves state-of-the-art performance on multiple tasks across different datasets, outperforming existing methods by a large margin. For example, on the FineGym dataset, CARL achieves 41.75% accuracy, which is +13.94% higher than the best existing method. On the Penn-Action dataset, CARL achieves superior results in fine-grained classification, Kendall's Tau, and fine-grained frame retrieval. Overall, CARL demonstrates the effectiveness of self-supervised learning for modeling long videos and extracting frame-wise representations.