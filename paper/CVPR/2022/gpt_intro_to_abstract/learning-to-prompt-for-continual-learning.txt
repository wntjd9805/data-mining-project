This paper introduces a novel method called Learning to Prompt for Continual Learning (L2P) to tackle the problem of training a single model on non-stationary data distributions in continual learning. Unlike traditional methods that rely on buffering past data or assuming known task identity at test time, L2P leverages the concept of prompting from natural language processing (NLP) to design prompts that "instruct" the model conditionally. The prompts are stored in a prompt pool, and a query mechanism is used to dynamically select task-relevant prompts based on input features. L2P decouples shared and task-specific knowledge, minimizing catastrophic forgetting without the need for a rehearsal buffer. Experimental results show that L2P outperforms previous state-of-the-art methods on multiple continual learning benchmarks, including class- and domain-incremental settings, and even performs competitively without using a rehearsal buffer. This work introduces the idea of prompting in continual learning and provides a new perspective for addressing challenges in the field.