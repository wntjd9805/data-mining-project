The efficient modeling of digital garments is a research area with numerous applications such as fashion design, e-commerce, virtual try-on, and video games. Traditional physics-based simulation methods have been used for garment modeling, but their high computational cost limits their practical use. Recently, learning-based methods have shown promise in approximating the accuracy of physics-based solutions. These methods use supervised learning to find a function that deforms garments based on input body descriptors. However, the need for large datasets and ground-truth meshes hinders the scalability of these methods. In this paper, we propose a self-supervised method for learning dynamic deformations of 3D garments worn by parametric human bodies. We show that the equations of motion used in physics-based methods can be formulated as an optimization problem, allowing us to learn time-dependent and pose-dependent deformations without the need for ground-truth data. The advantages of self-supervision include mitigating smoothing artifacts, generalizing to test sequences outside the training set, and easily incorporating different material models. Our approach outperforms existing supervised methods in terms of data requirements, training time, and inference time.