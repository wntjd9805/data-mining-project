Convolutional neural networks (CNNs) have traditionally been the preferred choice for visual encoders in computer vision systems. However, they have recently been challenged by Vision Transformers (ViTs), which have shown superior performance on various visual tasks. The key reason behind the success of ViTs has been attributed to the multi-head self-attention (MHSA) mechanism, which allows the model to capture long-range dependencies and model large receptive fields. In this work, we focus on exploring the large kernel design of CNNs as an alternative approach to building large receptive fields. We investigate the efficacy of using a few large kernels instead of many small ones in CNNs and propose a new architecture called RepLKNet. RepLKNet employs re-parameterized large convolutions to build up large receptive fields and achieves competitive results on downstream tasks such as object detection and semantic segmentation. Our findings suggest that large kernel designs in CNNs can leverage shape information and lead to improved performance. We hope that our work contributes to a deeper understanding of the mechanisms behind both CNNs and ViTs.