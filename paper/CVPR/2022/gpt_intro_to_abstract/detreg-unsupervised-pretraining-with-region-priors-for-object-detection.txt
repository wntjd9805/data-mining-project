Object detection is a crucial task in computer vision, but it often requires large amounts of human-annotated training data. This can be expensive and impractical for privacy-sensitive applications. However, recent advancements in self-supervised representation learning have shown promising results in reducing the reliance on labeled data, including for object detection.Despite these advancements, current approaches have limitations in learning good representations for object detection, specifically in pretraining the localization and region embedding components of the detection network. Most existing methods only pretrain the backbone of the detector, while others pretrain the entire network but do not specialize in localizing objects effectively.In this paper, we propose a model called DETReg (Detection with Transformers using Region priors) that addresses these limitations. Unlike existing methods, DETReg pretrains the entire detection network using an object-centric approach. It learns to both localize and encode objects during the pretraining stage, which leads to improved performance compared to existing methods.DETReg consists of two object-centric and category-agnostic pretraining tasks: an Object Localization Task and an Object Embedding Task. The localization task utilizes simple region proposal methods for bounding-box supervision, while the embedding task aims to predict the embeddings of a self-supervised image encoder on object regions. These tasks enable the detector to learn representations that are robust to transformations and improve object detection performance.We evaluate DETReg on standard object detection benchmarks such as MS COCO, PASCAL VOC, and Airbus Ship Detection. The results show that DETReg outperforms baseline methods, especially when limited annotated data is available. It achieves improvements of 4 AP points on PASCAL VOC, 1.6 AP points on MS COCO, and 1.2 AP points on Airbus Ship Detection compared to backbone-only image-classification pretraining. DETReg also performs well in semi-supervised learning scenarios and with limited data.In conclusion, our DETReg model demonstrates the effectiveness of pretraining the entire detection network, including the localization and embedding components, using object-centric self-supervised learning. It achieves state-of-the-art performance on various object detection benchmarks and shows promising results for privacy-sensitive applications and limited data scenarios.