Inverse rendering is a challenging problem in computer vision that aims to decompose a scene into its intrinsic factors of shape, lighting, and material. This decomposition enables various downstream tasks, such as virtual object insertion, material editing, and relighting. While there have been significant advancements in inverse rendering for outdoor scenes, indoor scenes pose additional challenges due to complex interactions among factors like shadows, specularities, and interreflections. Previous works have focused on specific aspects of scene decomposition or joint estimation. However, the ill-posed nature of the problem, along with the complexity of indoor scenes, makes it difficult to find a unique solution. Classical methods rely on strong heuristic priors, which may not hold for realistic indoor scenes with complex geometry or lighting conditions. Convolutional Neural Networks (CNNs) have shown great progress in inverse rendering but have limitations in capturing long-range interactions across the image space. This limitation becomes evident in scenes with strong shadows or highlights. Vision transformers (ViTs) have recently emerged as a powerful approach for computer vision tasks, leveraging the global reasoning ability provided by spatial attention mechanisms. Dense vision transformers, in particular, are well-suited for dense prediction tasks like inverse rendering. In this paper, we propose a transformer-based approach called IRISformer (Transformer for Inverse Rendering in Indoor Scenes) to address the challenges of complex light transport in inverse rendering. We compare our approach to CNN-based methods and demonstrate improvements in material consistency, geometry, and global ambient lighting. The heatmaps generated by our approach show how the transformer attends to large global regions and uses long-range interactions to reason about geometry, material, and lighting, leading to more accurate estimations. Our proposed framework achieves state-of-the-art results on various tasks related to inverse rendering, including object insertion and material editing, on real-world datasets. Our contributions include the first dense vision transformer-based framework for inverse rendering in a multi-task setting, improved handling of global interactions between scene components, and high-quality applications in augmented reality.