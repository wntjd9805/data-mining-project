Transformer models, particularly Vision Transformers (ViTs), have greatly impacted computer vision applications such as image classification, object detection, and image generation. Unlike traditional convolution-based approaches, ViTs leverage self-attention mechanisms to aggregate information from image patches, allowing them to model global-range relationships. However, ViT training suffers from instability, particularly when going deeper, due to the over-smoothening phenomenon, which results in highly similar representations and degraded discrimination ability. This paper aims to comprehensively study and address the issue of redundancy in ViTs at three levels: patch embedding, attention map, and weight space. The authors propose diversity regularizers that encourage representation diversity and coverage at each level, unleashing the true discriminative power and flexibility of ViTs. The regularizers demonstrate effective redundancy elimination, diversity promotion, and improved generalization. Extensive experiments on ImageNet datasets validate the proposed approach, showing consistent and significant performance improvements for different ViT models, including DeiT and Swin transformers. The proposed regularizers enhance accuracy by up to 1.76% for DeiT and 0.32% for Swin models. This research provides a comprehensive investigation of redundancy in ViTs and offers effective solutions to enhance their performance.