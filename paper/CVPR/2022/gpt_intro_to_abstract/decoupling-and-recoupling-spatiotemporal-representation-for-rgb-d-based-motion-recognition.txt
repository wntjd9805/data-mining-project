RGB-D-based motion recognition has gained significant attention in computer vision due to its wide range of applications in video surveillance and human-object interaction. Previous methods using CNN and RNN-based models have shown improved performance in gesture and action recognition by leveraging color and depth cues. Additionally, transformer-based methods have achieved surprising results by incorporating cross-attention modules for multi-modality fusion. However, these methods still face challenges related to limited data optimization, handling redundant information, and insufficient interaction between multi-modal spatiotemporal information. To address these issues, this paper proposes a new approach for multi-modal spatiotemporal representation learning in RGB-D-based motion recognition. The approach consists of a decoupled spatial representation learning network (DSN), a decoupled temporal representation learning network (DTN), and a cross-modal adaptive posterior fusion module (CAPF). The DSN and DTN employ decoupling and recoupling strategies to capture spatial and temporal features independently and then reintegrate them, respectively. The CAPF enables interactive learning between the two modalities, resulting in high-quality multi-modal spatiotemporal features. Experimental results on public RGB+D gesture/action datasets demonstrate that the proposed method achieves state-of-the-art performance.