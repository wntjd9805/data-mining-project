Meta-learning, also known as learning-to-learn (LTL), has gained significant attention in various domains such as few-shot image classification, meta reinforcement learning, natural language processing, and computational biology. One popular meta-learning approach is model-agnostic meta-learning (MAML), which formulates a bi-level optimization problem and is often applied with deep neural networks (DNNs). Despite its empirical success, there is limited theoretical understanding of MAML with DNNs, particularly regarding the global convergence of the optimization process. In this paper, we analyze the optimization properties of MAML with DNNs and provide a positive answer to the question of whether MAML with DNNs can converge to global minima. Our theoretical analysis shows that for over-parameterized DNNs, the training loss of MAML converges to zero at a linear rate. Additionally, we introduce Meta Neural Tangent Kernels (MetaNTK), a novel class of kernels that can describe the DNN trained by MAML. We demonstrate the practical implications of our theory by applying MetaNTK to accelerate neural architecture search (NAS) for few-shot learning. Our proposed method, MetaNTK-NAS, achieves comparable or better performance than state-of-the-art NAS methods while significantly reducing the search cost. Our contributions include providing a rigorous theoretical analysis of the global convergence of MAML with DNNs and proposing a theory-inspired efficient NAS method for few-shot learning.