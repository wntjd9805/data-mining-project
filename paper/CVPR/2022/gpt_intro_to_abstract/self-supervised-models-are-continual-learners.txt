Self-supervised learning (SSL) has gained popularity as a paradigm for unsupervised visual representation learning. While SSL methods can achieve representations of similar quality to supervised learning without annotations, this assumes offline training with large amounts of data and resources. However, in real-world scenarios where new unlabeled data is progressively made available, this assumption does not hold. Integrating new knowledge into the model requires repeating training on the entire dataset, which is impractical and computationally expensive. This paper introduces Continual Self-Supervised Learning (CSSL), a framework for addressing the forgetting phenomenon in SSL. The authors propose CaSSLe, which converts SSL losses into distillation losses and trains the model to predict past representations to encourage memory retention. CaSSLe can be seamlessly integrated with state-of-the-art SSL methods, is easy to implement, and does not require additional hyperparameter tuning.Experimental results demonstrate that SSL methods trained with CaSSLe outperform related methods and supervised continual learning baselines. The authors conduct a comprehensive analysis of six SSL methods in various continual learning settings, revealing interesting properties of SSL methods. In the class-incremental setting, SSL methods approach or outperform supervised learning, while this advantage is not generally observed in data-incremental and domain-incremental settings.