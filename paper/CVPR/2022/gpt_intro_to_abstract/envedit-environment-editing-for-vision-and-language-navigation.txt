The Vision-and-Language Navigation (VLN) task involves guiding an agent through an environment using natural language instructions. However, the limited availability and diversity of existing VLN datasets pose challenges for generalization to unseen environments. Previous methods have attempted to address this issue by augmenting the training environments, but they lack interpretability and fail to introduce new modifications. In this paper, we propose ENVEDIT, an approach that creates new environments with different styles, appearances, and objects through style transfer and image synthesis techniques. These synthetic environments serve as environment-level data augmentation during training, enabling the agent to generalize to unseen environments. We conduct experiments on the Room-to-Room (R2R) and Room-Across-Room (RxR) datasets, demonstrating that ENVEDIT outperforms other non-pre-training methods and improves the performance of state-of-the-art pre-trained agents. Furthermore, we show that ensembling agents augmented on different edited environments leads to complementary improvements. Our approach provides a novel and effective solution to overcome environment bias in VLN tasks.