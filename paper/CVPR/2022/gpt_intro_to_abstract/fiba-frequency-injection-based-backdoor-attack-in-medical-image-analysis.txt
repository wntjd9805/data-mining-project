Deep neural networks (DNNs) are widely used in computer-aided diagnosis (CAD) systems for medical image analysis. However, recent research has shown that DNNs are susceptible to attacks during both training and inference stages. While attacks in the inference stage involve adversarial samples to fool the model, backdoor attacks aim to maliciously alter the model in the training phase. Despite the increasing focus on adversarial samples, backdoor attacks in the context of medical image analysis have received less attention.Backdoor attacks involve embedding a hidden trigger into DNNs, allowing the model to perform well on benign samples until the trigger is activated by the attacker, resulting in a change in the predicted label. Existing backdoor attacks can be categorized into visible attacks, where the trigger is visible, and invisible attacks, where the trigger is stealthy. However, both types of attacks rely on spatial triggers, which can corrupt the semantics of poisoned pixels and fail on dense prediction tasks.To address this limitation, we propose a novel invisible frequency-injection backdoor attack (FIBA) paradigm that injects the trigger in the frequency domain. This approach leverages the insights from visual psychophysics, which demonstrate that models of the visual cortex decompose images into amplitude and phase spectra. We blend the spectral amplitudes of a trigger image and a benign image while keeping the phase spectrum unchanged to preserve the semantics of poisoned pixels. The poisoned image is obtained by applying the inverse fast Fourier transform (iFFT) to the synthetic spectrum and original phase spectrum. This frequency-injection backdoor attack is capable of attacking both classification and dense prediction models.Our main contributions include a unified backdoor attack method in the medical image analysis domain, targeting different imaging modalities and tasks. We also propose a frequency-injection based backdoor attack method that preserves the semantics of poisoned pixels and can attack both classification and dense prediction tasks. Extensive experiments on three benchmarks demonstrate the effectiveness of our proposed method in attacking and bypassing backdoor defense mechanisms.