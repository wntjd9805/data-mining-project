Recent progress in AutoML for deep learning has focused on automating the design of various components such as architectures and learning strategies. However, one crucial component that has been under-investigated is the automatic design of loss functions for generic tasks. Default loss functions, such as cross-entropy and L1/L2 losses, often do not align well with the final evaluation metrics, leading to sub-optimal solutions. Handcrafted loss functions have been proposed for specific evaluation metrics, but they require expertise and limit extendibility. This paper introduces AutoLoss-Zero, a general framework for automatically searching loss functions for different tasks and evaluation metrics. The framework builds a search space consisting of primitive mathematical operators and employs an evolutionary algorithm to discover high-quality loss functions from scratch. To improve search efficiency, a loss-rejection protocol and a gradient-equivalence-check strategy are introduced. The framework is validated on various computer vision tasks, demonstrating that the searched loss functions perform on par with or outperform existing handcrafted and specifically searched loss functions. The contributions of this paper include the AutoLoss-Zero framework, the loss-rejection protocol, the gradient-equivalence-check strategy, and the transferability of the searched loss functions across different models and datasets.