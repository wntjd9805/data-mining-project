Modeling real scenes from image data and rendering photo-realistic novel views is a challenging problem in computer vision and graphics. Current methods, such as NeRF and its extensions, reconstruct radiance fields using global MLPs for the entire space through ray marching. However, this approach leads to slow reconstruction times and unnecessary sampling of empty space.To address these issues, we propose Point-NeRF, a novel point-based radiance field representation that leverages classical point clouds to model a continuous volumetric radiance field. Unlike NeRF, Point-NeRF can be effectively initialized via a pre-trained deep neural network, making it more efficient. By using point clouds that approximate the scene geometry, Point-NeRF avoids ray sampling in empty space, resulting in more accurate rendering.Our Point-NeRF representation consists of a point cloud with per-point neural features, encoding the local scene geometry and appearance. We treat these neural points as local neural basis functions in 3D, allowing us to model a continuous volumetric radiance field. We propose using an MLP network to aggregate neural points in a 3D location's neighborhood to regress the volume density and view-dependent radiance, enabling high-quality rendering through differentiable ray marching.We introduce a learning-based framework to efficiently initialize and optimize the point-based radiance fields. We leverage deep multi-view stereo techniques for initial field generation and utilize a deep CNN to extract per-point features from input images. The neural points from multiple views are combined to form a point-based radiance field, which is optimized using point-based volume rendering networks. This approach leads to a generalizable model that can predict a point-based radiance field at inference time.Additionally, our approach can generate a radiance field based on point clouds from other reconstruction techniques. We address the issue of holes and outliers in the reconstructed point clouds by implementing point growing and pruning during the optimization process. This improves the final reconstruction and rendering quality.We evaluate our model on various datasets and demonstrate that our approach achieves state-of-the-art novel view synthesis, outperforming prior arts including point-based methods, NeRF, NSVF, and other generalizable neural methods. Our results show that Point-NeRF provides more efficient and accurate reconstruction and rendering of scenes.