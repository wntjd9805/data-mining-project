Quantization is a popular technique used in lossy compression to map continuous signals to discrete values. It has been extended to vector feature spaces with methods like Vector Quantization (VQ) and Product Quantization (PQ). Quantization is commonly applied in conjunction with Deep Neural Networks (DNNs) for various tasks such as classification, data retrieval, and compression.This paper focuses on the use of quantization for likelihood estimation in the visual domain. Likelihood estimation models aim to minimize the discrepancy between the data distribution and a prior model. While Auto-Regressive (AR) models are effective in likelihood estimation, they become computationally inefficient for domains with long sequences, like images. Variational Auto-encoders (VAEs) provide a compressed feature representation that can serve as a training objective but suffer from the collapse of the continuous prior.To address these issues, the paper introduces Depthwise Quantization (DQ), a method that quantizes each decomposed feature sub-tensor using a different quantizer. Rate-distortion theory is employed to interpret quantization as an encoding function with limited capacity. The paper provides a theoretical upper bound on the capacity when applying DQ to a decoupled feature tensor. Experimental evaluations on ImageNet demonstrate the superior performance of DQ in likelihood estimation tasks and as a bottleneck in a hierarchical Auto-Encoder model.Key contributions of this work include:1. The proposal of Depthwise Quantization (DQ) for decomposing feature tensors along the axis of weak statistical dependence.2. Theoretical analysis supporting the improved quantization performance achieved by DQ, along with experimental validation.3. The introduction of the Depth-Quantized Auto-Encoder, an enhanced hierarchical Auto-Encoder model utilizing DQ at different hierarchies.4. The extension of parametric Mutual Information (MI) quantization estimators for DNNs with learned priors, demonstrating the implicit decoupling of the prior. The paper showcases that DQ surpasses existing explicit models in likelihood estimation tasks, particularly when there is a strong assumption on cross-correlation. Moreover, when trained end-to-end, DQ effectively reduces cross-correlation among decomposed feature tensors and improves reconstruction loss and likelihood estimation. The provided code is publicly available for further exploration.