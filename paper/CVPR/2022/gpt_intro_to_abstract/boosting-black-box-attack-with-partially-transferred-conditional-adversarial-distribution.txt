Adversarial examples pose a serious threat to deep neural networks (DNNs). Existing adversarial attacks can be categorized as either white-box attacks, where the attacker has access to the DNN model's parameters, or black-box attacks, where only query feedback is available. Black-box attacks are more practical in real-world scenarios where accessing model parameters is difficult. However, achieving a high attack success rate with limited query budgets is challenging. One approach to improve attack performance is adversarial transferability, which leverages the transfer of adversarial examples between white-box surrogate models and the target model. However, differences between surrogate and target models, such as architectural and training dataset biases, can significantly influence the effectiveness of transferability. To address this issue, we focus on the conditional adversarial distribution (CAD) conditioned on benign examples. Transferring CAD accurately can aid in searching for successful adversarial perturbations. We propose a novel transfer mechanism that transfers only partial parameters of CAD while learning the remaining parameters based on query feedback from the target model. This allows for flexible adjustment of the CAD to mitigate the negative effects of surrogate biases. We utilize the conditional generative flow model, c-Glow, to model the CAD accurately. We also develop an efficient training algorithm using randomly sampled perturbations to approximate the CAD of surrogate models. Extensive experiments demonstrate the effectiveness of our method in both closed-set and open-set black-box attack scenarios on benchmark datasets and real-world APIs. Our contributions include an effective and efficient black-box attack method with a robust transfer mechanism, the use of c-Glow to approximate the CAD, and improved attack success rate and query efficiency compared to state-of-the-art methods.