Visual grounding is a technique that aims to link natural language descriptions with target objects. While significant progress has been made in 2D visual grounding, the emergence of 3D sensors and 3D vision technology has led to increased interest in 3D visual grounding. This paper focuses on the challenges and unique properties of 3D data, which include more complex input data and variant spatial relationships. Previous approaches to 3D visual grounding have mainly followed a two-stage scheme, generating candidate objects in the 3D scene and then selecting the best match. However, these approaches do not consider the intrinsic properties of 3D data, such as dynamic view changes and the effect on position encoding. This paper proposes a Multi-View Transformer (MVT) for 3D visual grounding, which learns a view-robust representation by projecting the 3D scene into a multi-view space and aggregating position information across different views. Additionally, a language-guided object classification task is introduced to enhance the object encoder. Experimental results on benchmark datasets demonstrate that the MVT outperforms existing methods, achieving significant improvements in accuracy.