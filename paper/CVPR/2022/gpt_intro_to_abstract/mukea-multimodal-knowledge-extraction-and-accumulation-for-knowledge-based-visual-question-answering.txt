In this paper, the authors propose a novel Multimodal Knowledge Extraction and Accumulation framework (MuKEA) for Visual Question Answering based on external Knowledge Bases (KB-VQA). They highlight how KB-VQA tasks are more challenging than traditional VQA tasks, as they require models to have human-like cross-modal scene understanding using external knowledge. Previous works have focused on capturing knowledge from structured knowledge graphs or unstructured/semi-structured sources like Wikipedia. However, these sources are limited in representing high-order predicate and multimodal knowledge needed for complex problems. The authors argue that representing and accumulating complex multimodal knowledge while maintaining the advantages of traditional knowledge graphs is an essential but understudied problem.The authors discuss existing progress in multimodal knowledge graphs, which aim to correlate visual content with textual facts to form augmented knowledge graphs. However, these graphs still fail to model high-order complex relationships. To address these challenges, the authors propose MuKEA, which aims to accumulate multimodal knowledge with complex relationships from VQA samples and perform explainable reasoning. They introduce a novel schema to represent multimodal knowledge units using explicit triplets, where visual objects, fact answers, and implicit relations are embedded. They propose three objective loss functions to learn the representations of these triplets. They also present a pre-training and fine-tuning strategy to accumulate knowledge from both out-domain and in-domain VQA samples.The main contributions of this work are: (1) the proposal of an end-to-end multimodal knowledge representation learning framework that models inexpressible multimodal facts and provides complementary knowledge with existing knowledge graphs; (2) the exploration of a pre-training and fine-tuning strategy to accumulate knowledge and form a neural multimodal knowledge base, enabling automatic knowledge association and answer prediction; (3) the demonstration of the model's strong generalization ability, outperforming existing state-of-the-art models on challenging KB-VQA datasets.Overall, this paper introduces a novel framework for KB-VQA that addresses the limitations of existing knowledge bases and knowledge graphs, allowing for more comprehensive and accurate cross-modal scene understanding.