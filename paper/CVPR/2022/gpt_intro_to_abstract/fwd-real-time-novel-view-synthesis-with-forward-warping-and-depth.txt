Novel View Synthesis (NVS) is a challenging task in computer graphics that aims to generate realistic images from unseen viewpoints. Early approaches focused on image-based rendering (IBR), which involved constructing light fields or proxy geometry from posed images and synthesizing target views through re-sampling or blending. However, these methods were limited by the quality of 3D reconstruction and performed poorly with sparse input images.Neural Radiance Fields (NeRF) have emerged as leading methods for NVS, representing scene radiance fields through MLPs. By querying the MLPs for color and density information, these methods achieve impressive results without requiring explicit geometry. However, they are not readily generalizable and suffer from slow speeds due to the large number of MLP queries.Recent approaches, such as PixelNeRF, IBR-Net, and MVSNeRF, have made progress in achieving generalizability without per-scene optimization. Nevertheless, they still suffer from slow speeds due to the need for numerous MLP queries. Fast and generalizable NeRF variants remain an under-explored area.In this paper, we propose a generalizable NVS method termed FWD, which aims to achieve real-time speed and high-quality synthesis without the need for dense view collections. Our approach utilizes forward warping of features based on depths and incorporates a novel fusion transformer for end-to-end training. By optimizing regressed depths and features for synthesis, we are able to achieve the desired target of fast and generalizable NVS.