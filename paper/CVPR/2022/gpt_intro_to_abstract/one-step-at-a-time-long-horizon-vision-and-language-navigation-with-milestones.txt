As autonomous agents become more integrated into our daily lives, the development of agents that can understand natural language commands and perform corresponding tasks is crucial. Various benchmarks and algorithms have been proposed in the realm of robot instruction following, such as vision-and-language navigation (VLN). However, one of the critical challenges in VLN arises when tasks become substantially longer and more complex, consisting of multiple sequential subtasks. Current agents struggle with the lack of awareness of their progress within a subtask sequence. To address this issue, we propose equipping VLN agents with an explicit task tracker, called M-TRACK, which keeps track of the agent's progress within a subtask and guides its actions. M-TRACK consists of two components: milestone builder and milestone checker. The milestone builder extracts the necessary completion conditions for each subtask from the language instruction, while the milestone checker ground the extracted target objects in the perceived environment and checks if the agent is ready to move on to the next subtask. M-TRACK can interact with the agent by selectively feeding it relevant instructions for the current subtask and proactively rejecting actions that could lead to subtask failures. We validate M-TRACK on the ALFRED dataset, a large-scale VLN dataset for household tasks, and integrate it into two baseline VLN models. Experimental results demonstrate notable and consistent performance gains, showing that M-TRACK helps agents better follow the sequence of subtasks and stay on the right track.