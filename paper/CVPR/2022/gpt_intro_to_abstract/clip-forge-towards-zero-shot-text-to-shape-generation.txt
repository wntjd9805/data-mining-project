This paper introduces CLIP-Forge, a model that tackles the problem of generating 3D shapes from text input. While text-to-image generation has seen significant progress, generating 3D shapes has remained a more challenging task due to the scarcity of shape-text pair data. To overcome this limitation, CLIP-Forge leverages pre-trained image-text joint embedding models and exploits the ability to render 3D shapes into images using standard graphics pipelines. This approach allows for training the model using existing text-image paired datasets. The success of CLIP-Forge demonstrates the potential for bridging the gap between natural language and geometric shape in the field of artificial intelligence and cognitive science research. The model has applications in creative design, manufacturing, animation, and games.