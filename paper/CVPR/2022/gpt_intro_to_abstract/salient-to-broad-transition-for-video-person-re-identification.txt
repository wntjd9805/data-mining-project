Video person re-identification (re-id) has made significant progress in recent years with the help of convolutional neural networks (CNNs). However, effectively utilizing the rich temporal information among video frames remains a challenge. Some recent approaches attempt to exploit temporal relations for mutual enhancement between frames using self-attention mechanism or Graph Convolution Networks (GCNs). Although these methods have shown promising results, they suffer from drawbacks such as limited attention regions and mixing of frame embeddings due to mutual enhancement. To address these issues, this paper proposes a Salient-to-Broad Module (SBM) that gradually enlarges the attention regions of consecutive frames, allowing the representations to be more informative and powerful. Additionally, an Integration-and-Distribution Module (IDM) is introduced to integrate and distribute informative global features, enabling message passing across all frames. Experimental results on benchmark datasets demonstrate the effectiveness of the proposed method, achieving state-of-the-art performance in video re-identification tasks.