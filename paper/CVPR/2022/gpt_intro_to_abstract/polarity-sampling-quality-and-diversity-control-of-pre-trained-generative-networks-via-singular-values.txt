Deep Generative Networks (DGNs) are widely used to generate high-dimensional datasets, such as natural images. Different frameworks, including Generative Adversarial Networks (GANs), Variational AutoEncoders (VAEs), and flow-based models, can be used to approximate the data distribution. The evaluation metric commonly used to assess the quality of generation is the Fréchet Inception Distance (FID). FID combines measures of quality and diversity of the generated samples. State-of-the-art DGNs have achieved low FID scores, but the optimal trade-off between precision and recall of generated samples varies depending on the application. Some DGNs incorporate controllable parameters, like truncated latent space sampling, to adjust the trade-off between precision and recall. However, these methods have limitations and lack a theoretical understanding. In this paper, we propose a principled solution called Polarity Sampling to control the precision and recall of DGN samples. Polarity Sampling adapts the latent space distribution by introducing a polarity parameter, ρ, which forces the distribution to concentrate on modes (ρ < 0) or anti-modes (ρ > 0) of the DGN distribution. The process only depends on the top singular values of the DGN's output Jacobian matrices and can be implemented online. Polarity Sampling is derived theoretically from the analytical DGN data distribution, and we provide pseudocode and approximation schemes to control its computational complexity. Our experiments demonstrate that Polarity Sampling efficiently controls the precision and recall trade-off and improves FID scores for various DGNs and datasets. We also show that negative Polarity Sampling enables exploration of the modes of trained GANs and VAEs, visualizing the modes and assessing perceptual smoothness around them.