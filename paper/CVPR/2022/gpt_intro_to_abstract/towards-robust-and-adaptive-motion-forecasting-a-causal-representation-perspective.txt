Motion forecasting is a crucial task for autonomous systems operating in dynamic environments. However, it is challenging due to strong spatial-temporal interactions influenced by physical laws and social norms. Traditional models based on domain knowledge often lack social awareness in complex scenes. As an alternative, learning motion representations from observational data has become a popular approach. Despite progress, the current learning paradigm for motion forecasting has two major shortcomings: struggles to discover physical laws from data and inefficiency in knowledge transfer. These issues persist even with larger models and are rooted in statistical learning principles that only seek correlations for the prediction task at hand, without considering robustness and reusability under distribution shifts. This paper aims to address these challenges by incorporating causal representation into motion forecasting. Causal relations in statistical modeling offer a mathematical language to articulate distribution changes and provide insights into representation learning. Cognitive science studies have also highlighted the importance of causal knowledge in the motion context, including infants' ability to reason about physical and social causalities. To tackle these challenges, a new formalism of motion forecasting is introduced, describing human motion behaviors as a process with three groups of latent variables: domain-invariant causal variables, domain-specific confounders, and non-causal spurious features. The proposed method consists of three components. First, causal invariance of motion representations is promoted by seeking commonalities across multiple domains. Motion forecasting models are trained with a penalty on the variation of empirical risks across environments to suppress spurious features and exploit causally invariant ones for robust generalization. Second, a modular architecture is designed to factorize the representations of invariant mechanisms and style confounders. This modular design allows the model to precisely localize and adapt a small subset of parameters to account for underlying style shifts. Third, a style contrastive loss is introduced to strengthen the modular structure of motion styles. This loss encourages the style encoder to produce an embedding space capturing style relations between different scenes, facilitating incremental knowledge transfer to new motion styles.The proposed method is evaluated on synthetic simulation datasets and controlled real-world experiments. Results show superior out-of-distribution generalization ability in the presence of spurious correlations and improved transferability in the low-shot setting under variations of motion styles. This work paves the way for the integration of causal modeling and representation learning in the motion context, offering a promising direction for reliable and adaptive autonomy.