Research on neural networks for scene representations and image synthesis has made significant progress in recent years. While existing methods focus on reproducing color images, they often struggle with reconstructing metric and clean meshes. In this paper, we propose a method that takes advantage of additional range measurements from consumer-level depth cameras to overcome these limitations. We show that utilizing both color and depth information can lead to improved geometry reconstruction. Specifically, we adapt the neural radiance field formulation to learn a truncated signed distance field (TSDF), enabling us to reconstruct geometry in regions where only color information is available. To address initial noisy camera poses, we jointly optimize our scene representation network with the camera poses. Our method utilizes an implicit function to predict signed distance values, which are then used for mesh extraction using Marching Cubes. Previous work that incorporates depth measurements in neural radiance fields has primarily focused on novel view synthesis, while our method aims at high-quality 3D reconstruction of room-scale scenes. Comparisons with state-of-the-art scene reconstruction methods demonstrate that our approach improves both the qualitative and quantitative quality of geometry reconstructions. Overall, our RGB-D based scene reconstruction method leverages dense color and depth observations, leading to improved geometry reconstruction and compensating for misalignments in the input data. We showcase the effectiveness of our method on both synthetic and real data from ScanNet.