Video-and-language pre-training is a technique that aims to learn effective multimodal transfer for downstream tasks such as text-video retrieval and video question-answering. However, videos present challenges in terms of capacity and computation efficiency due to their redundancy in consecutive frames. Prior approaches have used offline-extracted video features to overcome these challenges, but they are suboptimal when transferring to different target domains. Recent approaches have adopted sparse sampling of frames to enable end-to-end pre-training and fine-tuning of video backbones. However, current video-text pre-training models have limitations in modeling cross-modal interaction and fine-grained regional visual information. In this paper, we propose ALPRO, a new video-and-language pre-training framework that addresses these limitations. ALPRO utilizes a transformer-based video encoder, a text encoder, and a multimodal encoder to capture cross-modal interaction. It also introduces a visually-grounded pre-training task called prompting entity modeling, which improves region-entity alignment. ALPRO achieves state-of-the-art performance on video-text retrieval and videoQA tasks, demonstrating its effectiveness in learning cross-modal representations from sparse video frames and texts.