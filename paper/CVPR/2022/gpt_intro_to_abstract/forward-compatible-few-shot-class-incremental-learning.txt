Recent breakthroughs in deep neural networks have revolutionized many vision tasks. However, in real-world applications, data often come in a stream format with emerging new classes. This necessitates the development of models that can incorporate new class knowledge incrementally, a concept known as Class-Incremental Learning (CIL). The main challenge in CIL is catastrophic forgetting, where the discriminability of old classes drastically declines when updating the model with new classes. Several techniques have been proposed to address catastrophic forgetting, but they assume the availability of sufficient instances for new classes.In this paper, we focus on the scenario of Few-Shot Class-Incremental Learning (FSCIL), where only a limited number of instances are available for new classes. We propose the ForwArd Compatible Training (FACT) method to overcome the forgetting problem and overfitting in FSCIL. FACT incorporates the concept of forward compatibility, envisioning future updates and reserving embedding space for possible future classes. To achieve growability, we pre-assign multiple virtual prototypes in the embedding space and optimize them to create reserved space for incoming new classes. To ensure providence, we generate virtual instances through instance mixture, allowing explicit supervision in reserving embedding space.Experimental evaluations on benchmark datasets demonstrate the effectiveness of FACT in FSCIL. By leveraging forward compatibility, the reserved embedding space allows for seamless incorporation of new classes without compromising the discriminability of old classes. These reserved virtual prototypes serve as informative basis vectors during inference, resulting in a strong incremental classification model. The proposed FACT method contributes to advancing the field of incremental learning in computer vision.