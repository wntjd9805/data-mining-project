Unsupervised domain adaptation (UDA) models have been extensively explored in computer vision tasks, such as image recognition, semantic segmentation, and object detection, due to their potential in reducing the necessity of large-scale labeling. However, applying image-based domain adaptation methods directly to the domain adaptive action recognition task faces challenges posed by the complexity of video data, which includes not only appearance differences but also motion variances. To address this complexity, recent works have incorporated additional modalities, such as optical flow and audio, to enhance the transferability of features across domains. However, aligning modalities and domains simultaneously can distract the learning target. In this paper, we propose a Mutual Complementarity (MC) module that allows each modality to refine its feature by absorbing transferable knowledge from other modalities, enhancing the transferability of all modalities. We also introduce a Cross-Modal Spatial Consensus (SC) module to identify and emphasize transferable regions shared among different modalities. Experimental results on the UCF-HMDB dataset and EPIC-Kitchens-55 dataset show that our proposed method outperforms state-of-the-art methods significantly. Furthermore, our method achieves remarkable enhancement on the EPIC-Kitchens-100 dataset, which contains challenging fine-grained actions. Our contributions include the novel approach of cross-modal interaction for increasing feature transferability, the use of correlation-based operation for evaluating the transferability of spatial locations, and state-of-the-art performance on multiple datasets.