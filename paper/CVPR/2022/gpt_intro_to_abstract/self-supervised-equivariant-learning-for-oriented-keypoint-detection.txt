Detecting robust keypoints is crucial for various computer vision tasks, including image matching, visual localization, SLAM, and 3D reconstruction. These keypoints should be invariant to changes in viewpoint, illumination, and geometric variations induced by rotation. Early methods used hand-crafted filters on shallow gradient-based feature maps to detect keypoints. However, these techniques cannot be applied to deep feature maps from standard networks that exhibit unpredictable variations. Recent approaches rely on learning from data, but often fail to detect reliable keypoints due to geometric variations. In this paper, we propose a self-supervised equivariant learning method for oriented keypoint detection. We utilize group-equivariant CNNs and introduce an orientation alignment loss to estimate characteristic orientations. We also employ a window-based loss for geometric consistency and generate synthetic image pairs for training. We evaluate our method against existing models on synthetic rotations and demonstrate its effectiveness in image matching and 6 DoF pose estimation tasks. Our contributions include a self-supervised framework for rotation-invariant keypoints, a dense orientation alignment loss, and extensive evaluations on standard benchmarks.