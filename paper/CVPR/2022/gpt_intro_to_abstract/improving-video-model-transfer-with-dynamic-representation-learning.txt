Deep learning has achieved great success in image recognition, and convolutional neural networks (CNNs) have gained popularity for video classification, specifically for action recognition. However, current performance in action recognition falls short compared to image recognition, due in part to the difficulty of learning video representations that can generalize well. This difficulty arises from biases present in current action datasets collected from the web, such as YouTube, which exhibit various types of biases that hinder the generalization of trained video models to unseen domains. One prevalent bias is the spatial bias, which is the spurious correlation between action labels and the spatial appearance of video frames. This bias allows classifiers to infer action labels without modeling the temporal video component, leading to overoptimistic performance on popular action recognition benchmarks. Dataset bias tends to favor certain representations over others, leading to the dominance of CNN architectures that favor local rather than long-range dependencies. The authors hypothesize that this bias towards localized spatio-temporal representations hampers generalization to unseen domains. They propose a new approach called dynamic representation learning (DRL) to address this problem. DRL involves an adversarial optimization between the video network and a spatial student to measure and minimize the spatial bias of the video representation. The authors conduct experimental evaluations to demonstrate the effectiveness of DRL in improving robustness and transferability of learned video representations. The contributions of this work include the introduction of the dynamic score as a quantitative measure of temporal modeling capacity, the proposal of DRL as a pre-training strategy to optimize dynamic scores, and a comprehensive set of experiments to validate the importance of dynamic modeling and the advantage of DRL over baseline methods in video transfer learning.