Deep Neural Networks (DNNs) have shown remarkable success in various real-world applications. However, their large model size and high computational cost hinder their deployment on resource-constrained devices. To address this issue, quantization-based methods have been proposed to compress and accelerate DNNs. Although these methods have advantages, such as reducing model size and enabling efficient bitwise operations, they still have a performance gap compared to full-precision counterparts, especially with low-bit quantization. Nonuniform quantization has been proposed to mitigate quantization errors, but it often sacrifices hardware implementation efficiency. This paper proposes a new quantization method called Nonuniform-to-Uniform Quantizer (N2UQ) that maintains hardware simplicity while offering the flexibility of nonuniform quantization. The N2UQ design enforces equidistant output quantization levels while learning thresholds on input values to fit underlying distributions. To optimize the N2UQ, a novel backward approximation method called Generalized Straight-Through Estimator (G-STE) is derived. G-STE incorporates gradient learning to adapt input thresholds and provides a finer-grained approximation to the quantization function. Additionally, weight regularization is proposed to reduce information loss from weight quantization. The proposed N2UQ, combined with G-STE and weight regularization, consistently improves accuracy compared to state-of-the-art quantization methods. Experimental results demonstrate the effectiveness of the proposed approach, with the 2-bit ResNet-50 model achieving 76.4% top-1 accuracy on ImageNet, reducing the gap to its real-valued counterpart to only 0.6%.