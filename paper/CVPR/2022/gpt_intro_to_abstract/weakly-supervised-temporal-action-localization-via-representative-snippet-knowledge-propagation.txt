Temporal action localization in videos is an important task with numerous applications. While most existing methods for this task rely on fully supervised training with video-level labels and frame-wise annotations, weakly-supervised methods attempt to localize action instances using only video-level supervision. However, due to the absence of fine-grained annotations, these methods often suffer from a discrepancy between classification and localization. To address this issue, pseudo label-based methods have been proposed, but they only leverage limited information within each snippet, leading to inaccurate pseudo labels. In this paper, we propose a novel framework for weakly supervised temporal action localization that incorporates contextual information for pseudo label generation. We introduce representative snippets and use an intra- and inter-video knowledge propagation approach to improve the quality of pseudo labels. We employ an expectation-maximization attention mechanism to mine representative snippets, and a bipartite random walk module to propagate their knowledge. Experimental results demonstrate that our framework consistently improves the localization performance of existing methods and achieves state-of-the-art results on benchmark datasets.