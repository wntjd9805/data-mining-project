This paper addresses the challenge of performing dense computer vision tasks, such as image segmentation and object localization, without the need for labeled data. While existing methods utilize weak annotations, fully unsupervised dense image understanding remains under-explored. In this study, we propose a novel approach that combines deep learning with traditional graph-theoretic methods for unsupervised image segmentation. Our method involves extracting dense features from image patches using a self-supervised network. We then construct a weighted graph based on the semantic affinity of patch pairs and perform an eigendecomposition of the graph's Laplacian matrix. Remarkably, the eigenvectors of the Laplacian directly correspond to semantically meaningful image regions, with the eigenvector associated with the smallest nonzero eigenvalue often representing the most prominent object. By extracting bounding boxes or masks from this eigenvector, our method surpasses the state of the art in unsupervised object localization and segmentation.Furthermore, we propose a pipeline for semantic segmentation, which converts the eigensegments into discrete image regions and associates each region with a semantic feature vector. By jointly clustering these regions, we obtain consistent semantic labels across a dataset. Self-training is then performed using these labels to refine the results, achieving impressive performance on complex images without the need for fine-tuning. Importantly, our method can segment multiple semantic regions, outperforming prior methods on benchmark datasets.Additionally, we demonstrate that a variant of our method is well-suited for soft image decomposition, allowing for the breakdown of an RGB image into multiple RGB-A layers with soft boundaries. This decomposition facilitates real-world image editing tasks such as background replacement.Overall, our approach showcases the benefits of combining deep learning with traditional graph-theoretic methods for unsupervised dense computer vision tasks. The simplicity and superior performance of our method make it a strong baseline for future work in the field of computer vision.