The perception of lane structure is crucial for autonomous driving systems, as it plays a key role in various applications such as lane keeping and trajectory planning. However, existing 3D lane detection datasets are either unpublished or synthesized in simulated environments, which limits the model's generalization ability to real-world scenarios. Most image-based lane detection methods assume a flat ground, but in reality, roads are not always flat, and camera extrinsics can be affected by vehicle motion, leading to incorrect perception of the 3D road structure. To address these issues, this work presents ONCE-3DLanes, a real-world 3D lane detection dataset consisting of 211K images with labeled 3D lane points. It is the largest and most diverse dataset published to date, covering various road scenarios, weather conditions, and geographical locations. A spatial-aware lane detection method called SALAD is also introduced, which directly predicts 2D lane segmentation results and spatial contextual information to reconstruct 3D lanes without relying on explicit or implicit inverse perspective mapping projection. The contributions of this work include the ONCE-3DLanes dataset and the SALAD method, which demonstrate the feasibility and effectiveness of 3D lane detection in real-world scenarios without the need for expensive sensors or HD maps.