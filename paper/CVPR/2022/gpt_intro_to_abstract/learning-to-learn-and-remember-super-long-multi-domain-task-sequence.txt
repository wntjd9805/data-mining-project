Catastrophic forgetting (CF) is a common issue in machine learning when there is a shift in data distribution. However, this issue is often overlooked in the more challenging problem of meta-learning on a sequence of domains, where domain shift occurs sequentially. This problem has significant implications for real-world applications such as robot learning and personalized dialogue/recommendation systems. Existing methods for continual learning can only mitigate forgetting on a short sequence of tasks, making them infeasible for meta-learning with a large number of tasks. To address this, we propose a meta optimizer that adjusts the learning rates based on the gradient of the meta loss for memory tasks. We apply this optimizer to both domain-aware and domain-agnostic meta-learning settings, incorporating the fact of heterogeneous domain nature. We also construct a large-scale dataset consisting of a sequence of 10 domains to evaluate our methods. Experimental results show that our approach outperforms related strong baselines significantly, demonstrating its effectiveness in mitigating catastrophic forgetting in sequential domain meta-learning.