This paper introduces a novel method for visual question answering (VQA) that addresses the challenges of knowledge acquisition and reasoning. Previous methods rely on statistical correlations between inputs and answers, while this method aims to incorporate both visual and external knowledge into the reasoning process. The proposed method utilizes neural module networks to parse the input question into a functional program and dynamically assembles a network of explainable neural modules to execute the program. Additionally, it augments queries with visual and external knowledge to improve relevance and specificity, and uses a memory-augmented attention method to integrate intermediate results from both knowledge sources throughout the reasoning process. Experimental results demonstrate the effectiveness, generalizability, and explainability of the proposed method. Overall, this work contributes to the understanding and development of intelligent systems capable of reasoning about knowledge in the context of visual question answering.