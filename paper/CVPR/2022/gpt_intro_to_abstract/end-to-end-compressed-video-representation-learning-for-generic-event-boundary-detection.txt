Video traffic is expected to dominate internet traffic in the coming years, making the understanding of video content using AI technology a crucial area of research. However, it remains a challenging task due to the complex temporal evolution in video data. This paper focuses on the generic event boundary detection task, which aims to segment longer videos into shorter temporal segments based on humans' perception of event boundaries. Previous methods have formulated this task as a classification problem but neglected the temporal relations between consecutive frames, leading to inefficient feature extraction. Inspired by methods that operate in the compressed domain, the authors propose an end-to-end trained network that exploits discriminative features for event boundary detection in MPEG-4 compressed videos. By leveraging motion vectors and residuals, which are generated during the video encoding process, the features of reference frames are refined, and a temporal contrastive module is used to capture contextual information in the temporal domain for event boundary prediction. The proposed method achieves comparable results to state-of-the-art methods with significantly faster running speeds. The contributions of this paper include the development of an end-to-end compressed video representation learning method, a spatial-channel compressed encoder to compute features with low cost, and a temporal contrastive module for event boundary determination. Experimental results on the Kinetics-GEBD dataset demonstrate the effectiveness of the proposed method.