Active learning is a valuable technique in machine learning systems where labeled data is scarce or expensive to obtain. The main idea behind active learning is that a model can achieve good predictive performance with fewer labeled samples if it knows which samples should be labeled. Traditional active learning algorithms iteratively select samples to label based on their prediction results. However, these methods require accurate labeling, which can be challenging when dealing with large numbers of categories.A recent approach called one-bit supervision has proposed labeling samples with a simple yes-or-no answer to a specific class question. This method allows for more efficient query strategies compared to traditional labeling procedures. With one-bit supervision, we can query more samples to gather more information, using only one bit of information per query instead of log2C bits, where C is the number of classes.In this paper, we propose a novel active learning approach that combines one-bit supervision with contrastive learning. Contrastive learning is a strategy that aims to learn an encoder that maps positive pairs to similar representations and pushes away negative pairs. We argue that contrastive loss is a natural fit for the one-bit query and treat yes queries as positive pairs and no queries as negative pairs.Our proposed approach includes training the model with supervised cross-entropy loss and contrastive loss. We also develop an uncertainty metric based on the model's prediction variance to determine which samples to query. For correct predictions with yes queries, we extend the contrastive loss to incorporate multiple positive samples, thereby encouraging similar representations for images with the same label. For incorrect predictions with no queries, we integrate the negative label information into contrastive learning to push those samples away from the queried class. We also introduce a negative loss to penalize negative samples away from the incorrect prediction class, optimizing for hard negatives in that category.By integrating one-bit supervision into contrastive learning, our approach outperforms previous methods on well-known image classification benchmarks. Particularly on ImageNet, with only 10% of the labels in terms of bit information, our approach exceeds the state-of-the-art that utilizes 30% labels. Our contributions include the introduction of a novel active learning framework that combines contrastive learning with one-bit supervision, demonstrating its superiority through experimental results. We believe that this framework can inspire future research and contribute to the active learning community.