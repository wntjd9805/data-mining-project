This paper introduces a novel self-supervised approach called PoseTriplet for video-based 3D human pose estimation. The goal is to infer 3D pose sequences from videos, which is important for applications like action recognition and mixed reality. Existing methods mainly rely on fully-supervised paradigms with ground truth 3D data, but this is time-consuming and costly to capture. To address the lack of 3D data, two categories of methods have been explored: semi-supervised settings with limited 3D annotations and weakly-supervised settings with only 2D poses. However, these methods are limited in handling challenging scenarios and unseen poses. The proposed PoseTriplet approach aims to generate physically and semantically plausible 2D-3D pose pairs through a reinforcement-learning-based imitator and a pose hallucinator, alongside a pose estimator. These three components co-evolve in a dual-loop strategy during training, progressively generating, refining, and hallucinating 3D data. Once trained, each component can serve as an off-the-shelf tool for pose estimation or imitation. Experimental results show that PoseTriplet outperforms state-of-the-art self-supervised methods and even approaches fully-supervised results in terms of pose estimation accuracy. The contribution of this work is a novel scheme for self-supervised 3D pose estimation, achieved through the co-evolution of a pose estimator, imitator, and hallucinator, leading to realistic 3D pose sequences and improved performance.