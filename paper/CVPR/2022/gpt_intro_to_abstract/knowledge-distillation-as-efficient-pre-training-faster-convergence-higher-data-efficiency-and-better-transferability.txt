Recent advancements in computer vision tasks have greatly benefited from pre-training on large-scale datasets. It has become common practice to pre-train models on datasets like ImageNet and then fine-tune them on downstream tasks, especially when the data for these tasks is limited. However, the increasing scale of pre-training data and the limited accessibility of private data have made pre-training on large datasets inefficient or even impossible for all architectures.This paper introduces Knowledge Distillation as Efficient Pre-training (KDEP) as a solution to address this challenge. KDEP aims to transfer the feature extraction capability of a well-trained teacher model, obtained from large-scale data, to a student model for future downstream tasks. Unlike traditional Knowledge Distillation (KD) methods that focus on distilling the knowledge of a specific task, KDEP leverages the condensed data knowledge encoded in a pre-trained model to efficiently pre-train new architectures with a relatively small set of pre-training data.Empirical studies on existing KD methods for KDEP reveal that methods such as logits KD and feature-level KD lead to inferior performance, indicating the inability of existing KD methods to fully leverage the knowledge condensed in the teacher model when pre-training new models with limited data and computation budget. Further investigation reveals potential issues related to magnitude differences among channels, which hinder network optimization. To address this, the paper proposes a Power Temperature Scaling (PTS) function that reduces variance differences while preserving the original relative magnitude, enhancing feature learning.The results of KDEP demonstrate several key findings: faster convergence compared to supervised pre-training (SP), higher data-efficiency with only 10% or 20% of ImageNet-1K data, and better transferability with the same computation budget and data amount as SP. The proposed KDEP method offers the potential to pre-train a teacher model once and distill it to all other student models, making pre-training more efficient and applicable to various architectures.