This paper introduces a new paradigm for multi-object tracking (MOT) in videos using Transformer attention. MOT involves tracking the trajectories of objects while maintaining their identities. Most existing approaches follow a two-step tracking-by-detection paradigm, where objects are detected in individual frames and then associated between frames. However, recent variations, such as tracking-by-regression, rely on additional optimization or models. In this work, the authors propose tracking-by-attention, which performs tracking and detection jointly using attention. They present TrackFormer, an end-to-end trainable Transformer architecture that encodes frame-level features and decodes queries into bounding boxes with identities. The data association is achieved through track queries, which follow objects in space and time. TrackFormer achieves state-of-the-art performance on benchmark datasets and extends to include mask prediction. The contributions include the tracking-by-attention paradigm, autoregressive track queries, and the TrackFormer model's performance on MOT and segmentation benchmarks. The authors hope this work inspires further exploration of the tracking-by-attention paradigm.