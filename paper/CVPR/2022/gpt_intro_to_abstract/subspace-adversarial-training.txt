Adversarial training (AT) is a widely-used approach in deep neural networks to improve robustness by minimizing risks under worst-case perturbations. However, existing methods, such as single-step AT, suffer from catastrophic overfitting, which leads to a sudden drop in robust accuracy. Previous works have attempted to address this issue by using learning rate schedules and regularizations or generating more precise adversarial examples. However, these methods have limitations and still experience overfitting. In this paper, we propose a new method called Subspace Adversarial Training (Sub-AT) to resolve the overfitting problem in AT. Inspired by the connection between large gradients and overfitting, we control the magnitude of the gradient by restricting the gradient descent in a subspace instead of the entire parameter space. We demonstrate that Sub-AT effectively controls the average gradient norm, addressing catastrophic overfitting and significantly improving robust accuracy. Moreover, Sub-AT can also mitigate robust overfitting in multi-step AT.By training in a subspace, we can allow larger steps and radius without the need for delicate regularizations. As a result, our Sub-AT achieves competitive robustness with standard multi-step AT, answering the question of whether single-step AT can achieve comparable robustness against iterative attacks. Our method uncovers the potential of single-step AT and can inspire more efficient and powerful AT algorithms.The contributions of this paper include: 1. We reveal the close link between fast-growing gradients and overfitting in single-step AT, providing an explanation for robust overfitting in multi-step AT.2. We propose Sub-AT, an efficient AT method that constrains AT in a carefully extracted subspace, effectively controlling gradient growth, and resolving both catastrophic and robust overfitting.3. Sub-AT achieves state-of-the-art adversarial robustness in single-step AT and can be trained with larger steps and radius, bringing further improvements. Our pure single-step AT achieves over 51% robust accuracy against PGD-50 attack on CIFAR-10, comparable to multi-step AT with significant time benefits.