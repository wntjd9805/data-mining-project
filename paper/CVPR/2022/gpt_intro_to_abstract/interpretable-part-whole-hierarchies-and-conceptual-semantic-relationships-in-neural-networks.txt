The widespread use of neural networks and other learning models has raised concerns about our ability to explain their behavior. In fields like autonomous driving, healthcare, and finance, interpretability is greatly desired for neural networks due to the high stakes involved. While these networks have achieved remarkable performances, their complexity makes it difficult to understand how they work. They are often treated as "black boxes" with numerous parameters that are tuned based on experience rather than a clear understanding of how they affect the output. Interpreting the decision-making process becomes crucial when considering the trustworthiness of a model that achieves high accuracy without providing explanations. Classifying performance based on single metrics, such as accuracy, only provides an incomplete view of the real-world task. Humans, on the other hand, can infer relationships between different visual elements and explain their decisions based on past experiences and beliefs. The goal is to have neural networks exhibit similar behavior, allowing for the identification of hierarchical relationships between samples and how the model has learned to describe each one. Interpretability in deep learning can be defined as the extraction of relevant knowledge from the model regarding relationships contained in the data or learned by the model. Various techniques have been developed to improve interpretability in image classification, but they often focus on specific types of data relationships or model-learned relationships. The goal of our architecture, called Agglomerator, is to achieve part-whole agreement and hierarchical organization of the feature space, mimicking the human ability to parse visual scenes. This paper introduces the Agglomerator model, explains its interpretability regarding part-whole relationships, hierarchical organization of the feature space, and presents results that outperform or match current methods on common datasets while using fewer parameters.