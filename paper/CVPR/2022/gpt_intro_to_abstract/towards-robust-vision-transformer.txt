The popularity of transformers in Natural Language Processing (NLP) applications has led to an interest in exploring their use as a primary backbone for computer vision applications. The Vision Transformer (ViT) has successfully applied a pure transformer for classification, achieving impressive speed-accuracy trade-offs through self-attention. However, existing ViT models primarily focus on standard accuracy and computation cost, ignoring the influence on model robustness and generalization. In this paper, we analyze various ViT models and discover that certain modifications, while improving standard accuracy, can severely impact robustness. For example, the PVT model achieves high standard accuracy but suffers from a significant drop in robust accuracy. To understand the trade-offs between accuracy and robustness, we investigate the impact of patch embedding, position embedding, transformer blocks, and classification head on model robustness.Based on our findings, we propose a Robust Vision Transformer (RVT) that significantly improves both robustness and accuracy compared to other transformers. Additionally, we introduce two plug-and-play techniques to further enhance the RVT: Position-Aware Attention Scaling (PAAS) and patch-wise augmentation. PAAS improves the self-attention mechanism by filtering out redundant and noisy position correlations, leading to improved model robustness. The patch-wise augmentation method adds affinity and diversity to training data, reducing the risk of overfitting and improving model generalization.Experimental results on ImageNet and six robustness benchmarks demonstrate that RVT achieves the best trade-offs between standard accuracy and robustness compared to previous ViTs and convolutional neural networks (CNNs). The RVT-S* variant achieves the top rank on ImageNet-C, ImageNet-Sketch, and ImageNet-R benchmarks. This work provides a systematic analysis of ViTs, presents the RVT model with improved robustness, and introduces new techniques for further enhancing both robustness and accuracy.