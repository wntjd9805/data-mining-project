This paper introduces the concept of treating the internet as a multimodal trove of information for question answering systems. It discusses the limitations of current QA systems that only extract information from text, ignoring the knowledge present in images. The paper argues that progress in reasoning over linguistic and visually grounded meanings requires a unified system that treats both text and images as knowledge carriers. To facilitate research in this area, the authors propose a novel benchmark called WEBQA, which focuses on multi-hop, multimodal, open-domain question answering. The benchmark requires systems to incorporate both text and images, retrieve relevant knowledge, aggregate information from multiple sources, and generate answers in natural language. The paper also provides a comparison of existing multimodal knowledge-seeking benchmarks. Lastly, the authors experiment with state-of-the-art multimodal reasoning and text generation models to identify future directions for improvement in this field.