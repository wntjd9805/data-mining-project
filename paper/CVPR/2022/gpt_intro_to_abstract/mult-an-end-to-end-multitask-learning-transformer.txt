This paper introduces MulT, an end-to-end multitask transformer architecture for handling multiple high-level vision tasks in a single model. Previous works have shown the success of transformers in language tasks and image classification, but few have explored their use in multitask learning. MulT aims to connect vision tasks in 2D, 3D, and semantic domains, explicitly modeling task dependencies. The architecture consists of a transformer-based encoder to transform input images into a shared latent representation, and task-specific transformer decoders with a shared attention mechanism. This shared attention improves the performance of each vision task and enables the encoding of task inter-dependencies. The paper also shows that the MulT model generalizes and adapts well to new domains, outperforming existing multitask CNN-based models on various benchmarks. The contributions of this work include proposing the multitask transformer architecture, introducing shared attention between decoders, and demonstrating better performance and generalization compared to single-task architectures and existing CNN-based models.