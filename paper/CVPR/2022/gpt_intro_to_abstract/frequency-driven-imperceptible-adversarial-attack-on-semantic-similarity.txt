The introduction presents the problem of deep neural networks (DNNs) being vulnerable to image corruptions and adversarial examples, which limits their applications in security-critical domains. The current approaches for attacking DNNs suffer from inherent limitations in cross-dataset generalization and poor imperceptibility to the human visual system (HVS). To address these issues, the authors propose a new adversarial attack called the semantic similarity attack (SSA), which operates in the feature space and leverages the similarity of feature representations. They also introduce a perturbation constraint, the low-frequency constraint, to limit perturbations within imperceptible high-frequency components. The authors conduct extensive experiments on CIFAR-10, CIFAR-100, and ImageNet-1K datasets, demonstrating that their proposed attack produces significantly imperceptible perturbations and outperforms state-of-the-art methods. They also show that the adversarial perturbations generated by their method are more transferable across different architectures and datasets. Overall, this work contributes to the understanding and improvement of the robustness of DNNs in the face of adversarial attacks.