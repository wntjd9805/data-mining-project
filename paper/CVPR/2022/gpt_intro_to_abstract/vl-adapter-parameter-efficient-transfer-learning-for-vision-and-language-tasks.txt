Large-scale pre-training of vision-and-language (V&L) models has become a standard framework for tackling V&L tasks. However, the size of these models presents a challenge in terms of memory and storage cost. To address this problem, parameter-efficient training methods have been proposed, including the use of adapters, which allow for high performance with only a small set of parameter updates. While adapters have been successful in text classification and image-text alignment, their application to more challenging V&L problems, such as visual question answering and image/video captioning, has not been explored. In this paper, we aim to efficiently tune language models on diverse downstream V&L tasks while achieving performance comparable to full fine-tuning. We analyze different adapter-based and prompt-based methods in a unified multi-task learning setup, using encoder-decoder language models and CLIP as our visual encoder. We experiment with diverse image-text and video-text tasks and demonstrate that adapter training with weight-sharing can achieve the same performance as full fine-tuning while updating only a small percentage of parameters. We also analyze the impact of freezing CLIP, different architectural components, weight-sharing techniques, task-specific prompts, and vision-language pretraining. Overall, our contributions include benchmarking parameter-efficient training techniques, empirical demonstration of the effectiveness of adapters, and comprehensive analysis of various design aspects in V&L models.