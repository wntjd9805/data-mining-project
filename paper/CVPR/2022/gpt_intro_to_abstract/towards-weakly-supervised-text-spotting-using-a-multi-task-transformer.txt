Text spotting, the ability to detect and read text in images, is crucial for various applications in computer vision. While early systems used separate architectures for text detection and recognition, recent approaches aim to develop a unified end-to-end architecture. However, these architectures still have limitations in optimizing the recognition head for the predictions of the detection head. Moreover, current methods heavily rely on expensive spatial annotations, hindering their practical use. In this paper, we propose TextTranSpotter (TTS), a new text spotting approach that only requires transcript annotations for real data. TTS can be trained in both fully-supervised and weakly-supervised manner, providing a trade-off between model performance and annotation cost. We introduce a novel architecture and loss function that better integrate text detection and recognition tasks. Our approach leverages transformers to create a multitask network, learning a single object query embedding for both tasks. The recognition head is a Recurrent Neural Network (RNN), taking advantage of the transformer output to learn the relevant areas of interest. We employ a weakly-supervised training scheme using a text-based Hungarian matching loss, which optimizes both detection and recognition tasks simultaneously. Our experiments demonstrate that our weakly-supervised model achieves competitive results with existing fully-supervised methods. Our contributions include the development of a weakly-supervised training scheme, a multi-task transformer-based approach, and a framework that offers both fully-supervised and weakly-supervised training options.