Food plays a significant role in our lives, and the ability to understand and generate cooking recipes has become an exciting area of research in computer science. Traditionally, the computer vision community has focused on image-level food classification, but recent attention has turned to the challenge of mapping recipes to images using multi-modal representations. However, existing retrieval systems are limited in their ability to handle queries outside of a preexisting database, and generating a full recipe from an image remains a difficult task. In this paper, we propose representing cooking recipes as step-by-step instructional programs, capturing the sequential dependencies and relationships within the cooking process. These programs consist of actions represented as functions and operate on specific ingredients under certain conditions. Additionally, programs can be represented as graphs, with actions and ingredients as nodes and connections representing the relationships between them.Our goal is to generate cooking programs based on food images or cooking recipes. To achieve this, we develop a model that combines a vision and text encoder to create joint embeddings of visual and textual data. These embeddings are then used in a program decoder to generate cooking programs. The model is trained end-to-end, optimizing a ranking loss between the visual and text embeddings and two losses on the program sequence predictions. Importantly, we account for the fact that certain actions can be permuted without affecting the input-output connections between functions, and design a loss function that operates on the set of all valid program sequences for each recipe.To validate our approach, we crowdsource cooking programs for recipes from the Recipe1M dataset, using tasks specifically designed for naive annotators. Experimental results demonstrate that our model improves cross-modal retrieval compared to existing methods when trained to generate programs. Furthermore, generating programs leads to better food recognition results compared to predicting raw cooking instructions. Additionally, we explore the generation of food images by manipulating cooking programs using a GAN.Overall, our paper presents a novel approach to understanding and generating cooking recipes, leveraging the natural pairing of food images and recipes. Our model demonstrates promising results in cross-modal retrieval and food recognition tasks, highlighting the potential of program-based representations in the field of computer vision and AI in cooking.