Transformers have greatly improved performance in computer vision tasks due to their large model capacity and access to large amounts of data. The ViT model, in particular, has achieved state-of-the-art performance in vision tasks but comes with high computation cost and carbon emissions. In this paper, we propose a progressive learning scheme for ViTs to make their training more efficient and sustainable. We introduce the Growing Ticket Hypothesis, which suggests that training a sub-network of a large ViT model first, and then growing it to the full network, can achieve comparable performance with the same number of training iterations. We generalize and automate progressive learning on ViTs by developing the growth operator and growth schedule. We also propose the momentum growth (MoGrow) operator and an automated progressive learning algorithm (AutoProg) for lossless training acceleration. Our experiments show that AutoProg consistently speeds up ViTs training by more than 40% on different variants, including DeiT and VOLO, with competitive performance on larger input sizes and other datasets. Overall, our contributions include a manual baseline for progressive learning, the AutoProg training scheme, and remarkable training acceleration for ViTs on ImageNet.