Self-supervision is an active area of research in vision and language representation learning. Existing methods have shown impressive performance on challenging tasks by pre-training models on unlabeled data in a self-supervised manner. However, when it comes to joint vision-language tasks, such as visual question answering and image-text retrieval, the fusion of vision and language features becomes challenging due to differences in their embedding spaces. To address this, recent state-of-the-art approaches have introduced a two-stage process of cross-modal alignment and fusion encoding. However, the limitation of this approach is that it does not fully guarantee the expressiveness of the learned features. Additionally, existing methods that use global information fail to consider localized and structural information within the input. To overcome these limitations, this paper proposes a novel VLP framework called triple contrastive learning (TCL) that leverages both cross-modal and intra-modal self-supervision. TCL introduces three contrastive modules: cross-modal alignment, intra-modal contrastive, and local MI maximization, all of which aim to maximize mutual information (MI) between different views of the input. By combining these modules, TCL learns semantically meaningful representations for both cross-modal and intra-modal inputs, capturing both global and local information. Experimental results show that TCL outperforms existing methods on various vision+language benchmarks, such as image-text retrieval and VQA, achieving a new state-of-the-art performance. Ablation studies are conducted to analyze the effectiveness of each component in TCL.