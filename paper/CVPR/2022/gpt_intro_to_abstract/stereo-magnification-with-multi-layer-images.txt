Recent advancements in image-based rendering and novel view synthesis have been driven by the emergence of neural rendering approaches. Among these approaches, semi-transparent multi-layer representations have gained attention due to their advantages in terms of fast rendering time, compatibility with traditional graphics engines, and high quality re-rendering near input frames. However, existing methods that build multi-layer representations over regularly spaced surfaces face limitations in accurately approximating the true geometry of the scene, thereby limiting generalization to novel views and introducing artifacts. Some recent works attempt to address this issue by using an excessive number of spheres and merging the resulting geometry, but this post-processing step degrades the quality of novel view synthesis. In contrast, more traditional image-based rendering methods estimate scene geometry in a non-discretized form, resulting in finer approximations but requiring a slow neural rendering step to compensate for geometry estimation errors. To address these challenges, we propose a new approach called StereoLayers that combines scene geometry adaptation with multi-layer representation. Our method focuses on the stereo magnification problem, reconstructing a scene from only two input images. It employs a two-stage process: firstly, a geometric proxy is constructed using a small number of mesh layers with continuous depth coordinates, tailored to the specific scene. Secondly, transparency and color textures are estimated for each layer, resulting in the final scene representation. Both stages utilize deep neural networks trained on similar scenes, and the networks are trained jointly in an end-to-end fashion using a differentiable rendering framework.In our evaluation, we compare our approach to existing methods on popular datasets, RealEstate10k and LLFF, and introduce a new dataset for benchmarking novel view synthesis. Our findings demonstrate that scene-adaptive geometry in StereoLayers leads to superior quality in novel view synthesis compared to non-adaptive geometry approaches. Additionally, our approach outperforms the IBRNet system in terms of rendering time. Our method produces compact scene representations suitable for real-time rendering, even on low-end devices.To summarize, our contributions include a novel method for geometric reconstruction of scenes from stereo pairs, which leverages scene-adapted multi-layer geometry. We utilize two jointly trained deep neural networks to estimate layer geometry, transparency, and color textures. Furthermore, we evaluate our approach on existing datasets and introduce a new challenging dataset for training and evaluating novel view synthesis methods.