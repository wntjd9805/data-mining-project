The use of 6D pose estimation is vital in various emerging applications such as autonomous driving, intelligent robotic grasping, and augmented reality. RGB-D sensors, which capture both color and depth information, have become more affordable and are increasingly used as a data source for 6D pose estimation. Current state-of-the-art methods typically employ separate backbone networks to extract features from RGB and depth images, followed by a fusion process. However, the inability to extract features with a single backbone stems from the "projection breakdown" problem caused by spatial transformations in the traditional convolutional neural network (CNN) pipeline. To address this problem, we propose a simple yet effective method that explicitly feeds additional UV data along with depth information to a 2D CNN. This combination acts as 3D data, preserving the self-contained information (depth, u, v) for each pixel and decoupling the depth value from its built-in coordinates. By ensuring that the projection equation holds after spatial transformations, we can extract features using the traditional CNN pipeline, including data augmentation methods. Experimental results demonstrate that while data augmentation harms accuracy without UV data, it improves accuracy when UV data is utilized.Furthermore, our proposed method, Uni6D, leverages a unified backbone based on Mask R-CNN to extract features from RGB-D images. Unlike existing methods that employ post-processing operations, Uni6D utilizes an extra RT head and abc head to directly predict the rotation matrix and translation vector, eliminating the need for iterative voting and regression mechanisms. Uni6D achieves state-of-the-art performance in terms of accuracy and real-time inference speed.In summary, our contributions include: (1) identifying and addressing the "projection breakdown" problem in CNN-based depth image processing by introducing extra UV data, enabling feature extraction using a single backbone; (2) proposing the Uni6D method that optimizes the abc and RT heads in a multitask manner for efficient and accurate pose estimation; and (3) providing extensive experimental and ablation studies that highlight the advantages of our method, demonstrating superior time efficiency and performance compared to existing approaches.