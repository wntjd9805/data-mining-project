The task of Video Scene Segmentation is challenging due to the need for quickly switching between stories and scenes in order to make the movie plot tighter and more intriguing. This paper focuses on the category-agnostic task of Video Scene Segmentation, where only the scene boundary label is available. Previous studies have mainly dealt with short video clips and have required manual segmentation, making the task labor-intensive. This paper proposes a self-supervised learning scheme to learn better representations for Video Scene Segmentation. A representation learning scheme based on Scene Consistency is introduced to obtain better shot representations in unlabeled long-term videos. Additionally, a simple and unbiased temporal model is proposed to assess the quality of shot representations. The paper also introduces a fair and reasonable benchmark for pretraining and evaluation and demonstrates that the proposed method outperforms state-of-the-art methods and improves the performance of existing supervised methods without additional complexity.