Abstract:In conventional supervised learning, the use of large-scale labeled datasets leads to high accuracy but is labor-intensive and time-consuming. Self-supervised learning (SSL) has emerged as an attractive alternative, utilizing artificial labels for training. State-of-the-art SSL frameworks like SimCLR and MoCo employ contrastive learning (CL) with wide and deep models to achieve performance comparable to supervised training. However, training from scratch using self-supervised learning requires wider models than supervised learning, leading to increased computational costs. In the context of supervised learning, network sparsification techniques have been explored, but sparsifying SSL models trained from scratch remains unexplored. In this paper, we focus on efficient dynamic sparse feature learning by training models from scratch in a self-supervised manner. We investigate how sparsity techniques used in dynamic computation reduction can be applied to SSL models. By reducing computational requirements, our research aims to enhance the practicality and efficiency of self-supervised learning.