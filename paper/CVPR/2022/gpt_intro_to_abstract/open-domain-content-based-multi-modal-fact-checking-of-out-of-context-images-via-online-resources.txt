The spread of 'fake news' and the use of generative AI technologies to create deepfakes have raised concerns about the harmful consequences of misinformation. One common method of creating misinformation is through image-repurposing, where real images are misrepresented and used to support false narratives. This practice is accessible to even those without technical expertise, making it a significant risk. Previous work has attempted to construct synthetic out-of-context datasets to study image re-purposing, but gathering large-scale labeled datasets remains challenging. Additionally, automated fact-checking methods have focused primarily on textual claims, leaving the fact-checking of multi-modal claims underexplored. In this paper, we propose an automated fact-checking framework that aggregates evidence from images, articles, and various sources to judge the veracity of a multi-modal claim. Our framework retrieves evidence in a fully automated and open-domain manner, without pre-identifying or curating 'golden evidence' for the model. We introduce the Consistency-Checking Network (CCN), which utilizes memory networks and a CLIP component to evaluate the consistency of a claim against the evidence and the image-caption pair itself. We conduct evaluations and user studies to assess the effectiveness of our framework, demonstrating its significant improvement in detection compared to baseline methods.Our contributions include formalizing the task of multi-modal fact-checking, introducing the 'multi-modal cycle-consistency check' to gather evidence, proposing the CCN framework for inspectable fact-checking, and demonstrating the efficacy of our evidence-augmented method through evaluations and user studies.