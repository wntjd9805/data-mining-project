This paper introduces the task of lip reading, which involves recognizing speech from silent video. Lip reading has various practical applications, such as improving speech recognition in noisy environments and aiding speech-impaired individuals. While both lip reading and audio-based automatic speech recognition (ASR) aim to transcribe speech, they differ in terms of input complexity, with lip reading requiring the processing of high-dimensional video data. This poses challenges for training large end-to-end models due to computational constraints and inherent ambiguities in lip movements. This paper proposes a design approach that takes into account the peculiarities of the visual domain, including visual encoding and text tokenization. The authors present a novel visual backbone for lip reading that incorporates attention-based pooling to track and aggregate lip movement representations. They also advocate for the use of sub-word tokens instead of character-level tokens, as they provide semantic meaning and reduce runtime and memory requirements. In addition, the paper addresses the issue of visual speech detection in real-world silent videos, proposing a method called Visual Speech Detection (VSD) to identify speech regions using visual information alone. The authors demonstrate the benefits of their design choices through experiments and show improvements in lip reading performance compared to prior works. They also highlight the potential impact of improved lip movement representations on related downstream tasks. The paper concludes with a discussion of ethical concerns and limitations. Further details, including video examples, code, and pre-trained models, are available on the project page.