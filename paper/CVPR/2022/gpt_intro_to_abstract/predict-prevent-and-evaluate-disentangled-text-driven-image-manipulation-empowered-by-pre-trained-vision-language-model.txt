Disentangled image manipulation is a challenging task in computer science, as it involves changing specific attributes of an image while keeping others unchanged. Previous works have used encoder-decoder architectures and manual annotations to manipulate attributes, but this process is time-consuming. Recent advancements in large-scale pre-trained models, such as GANs and CLIP, have provided new opportunities for image manipulation. However, these methods still require human annotations and have limitations in manipulating attributes.In this paper, we propose a novel framework called Predict, Prevent, and Evaluate (PPE) to achieve disentangled image manipulation with minimal human labor. Firstly, we leverage the CLIP model to predict possibly entangled attributes based on the distributions of attributes in the real world. Secondly, we introduce an entanglement loss to prevent entanglements during training by penalizing changes in the possibly entangled attributes. Lastly, we introduce a new evaluation metric that measures both the manipulation effect and the entanglement condition based on the CLIP distance between attribute texts and images.To evaluate our method, we implement it based on the latent mapper from StyleCLIP and conduct experiments on the CelebA-HQ dataset for face editing tasks. Our qualitative and quantitative results demonstrate superior disentangled performance compared to the StyleCLIP baseline and show better linear consistency. The main contributions of this paper include the prediction of entangled attributes, the introduction of an entanglement loss, the development of a new evaluation metric, and the achievement of disentangled image manipulation with minimal manual labor. Extensive experiments validate the effectiveness of our approach, indicating impressive results.