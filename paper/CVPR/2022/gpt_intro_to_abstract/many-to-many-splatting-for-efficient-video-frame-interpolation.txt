Video frame interpolation (VFI) is a classic problem in video processing that aims to increase frame rates by synthesizing intermediate frames. It has applications in slow-motion animation, video editing, and video compression. Although many techniques have been proposed, VFI remains challenging due to factors such as occlusions, large motion, and lighting changes. Existing approaches can be categorized as motion-free or motion-based, depending on whether they incorporate optical flow cues. Motion-free models rely on kernel prediction or spatio-temporal decoding but are limited to fixed time steps and have increasing runtime with more output frames. Motion-based approaches establish dense correspondences between frames and apply warping, but the estimation of bilateral motion can be difficult and incorrect flows can degrade interpolation quality. These methods often require refinement networks and suffer from inefficiency when applied to multi-frame interpolation tasks. To address these challenges and improve efficiency, this paper presents a Many-to-Many (M2M) splatting framework. The proposed M2M splatting technique estimates multiple bidirectional flow fields and efficiently forward warps input images to the desired time step, considering overlapping pixels. The resolution and quality of the optical flow play a critical role in this process, so an off-the-shelf optical flow estimator is used to derive low-resolution motion estimates. These estimates are then used to train a Motion Refinement Network (MRN) that predicts multiple flow vectors for each pixel at full resolution, enabling image synthesis through many-splatting. Conventional motion-based methods only estimate one inter-frame motion vector per pixel, leading to many-to-one splatting and creating holes in the warped result. In contrast, the proposed many-to-many splatting allows for complex interactions among pixels, with each source pixel rendering multiple target pixels and each target pixel synthesized with a larger area of visual context. A learning-based fusion strategy is introduced to merge overlapping pixels that map to the same location. The optical flow estimation step only needs to be performed once for a given input frame pair, making subsequent in-between frame generation faster than previous approaches that rely on refinement networks. Experimental results demonstrate the effectiveness and efficiency of the M2M framework, with fast interpolation speeds achieved for 2K videos using a Titan X GPU. The paper concludes by highlighting the contributions of a Motion-Refinement Network, a learning-based pixel fusion strategy, and the Many-to-Many (M2M) splatting synthesis model in enabling efficient and effective frame interpolation.