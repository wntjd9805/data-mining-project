Taking pictures of models wearing clothing is crucial for online apparel retailers to promote sales. However, hiring models and professional studios for each clothing item is expensive. To address this, image virtual try-on techniques have gained significant attention. Existing methods for image virtual try-on are costly to train as they rely on paired image data, which is time-consuming and infeasible to collect at scale. Additionally, the use of proprietary model images for testing further increases expenses. To overcome these challenges, we propose a weakly-supervised method called Deep Generative Projection (DGP) that generates realistic try-on results using cheap unpaired data. DGP is based on the idea that people can imagine how they will look in different clothes based on their experience and do not necessarily rely on paired annotations. The DGP method utilizes a perspective transformation and a pretrained StyleGAN, trained on unsupervised fashion images, to align the clothing to the model's body and generate realistic clothing model images. This approach eliminates the need for paired data or proprietary model images during training, making it more practical for industrial applications. In conclusion, our method makes significant contributions by proposing a framework for generating clothing model images, consuming only unpaired data, and outperforming existing methods in both numerical and visual quality, while demonstrating robustness against preprocessing mistakes.