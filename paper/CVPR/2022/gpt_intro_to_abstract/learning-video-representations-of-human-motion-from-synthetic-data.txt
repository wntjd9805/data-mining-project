Spatiotemporal semantic features play a crucial role in video understanding. However, existing methods for extracting these features have limitations. This paper introduces a novel approach for learning strong motion representations from a data perspective. To support this approach, a large-scale motion-oriented human action dataset called GATA is collected. GATA overcomes the bias of scene context found in existing datasets and provides a diverse and comprehensive representation of human actions. Using the GATA dataset, a contrastive learning framework is designed to learn general and robust human motion representations. The framework leverages a ready-made action recognition model and encodes the knowledge into the representations, which can be easily transferred to downstream action understanding tasks. Experiments conducted on GATA demonstrate significant improvements in performance on downstream tasks and highlight the enhanced motion modeling achieved with the proposed dataset.Additionally, the paper explores the complementarity between the synthetic GATA dataset and web-crawled videos. Joint training with the Kinetics dataset and a domain adaptation method are proposed to bridge the domain gap between synthetic and realistic data. The effectiveness of these approaches is demonstrated through detailed experiments.In summary, this paper makes three contributions: 1. The introduction of the GATA dataset, synthesized using a high-performance data collection pipeline, for comprehensive human action modeling.2. The formalization of the GATA training process using a contrastive learning framework, which enables the learning of strong motion representations.3. Extensive experiments and analyses that validate the effectiveness of the proposed GATA dataset and demonstrate performance improvements in downstream tasks and motion modeling.