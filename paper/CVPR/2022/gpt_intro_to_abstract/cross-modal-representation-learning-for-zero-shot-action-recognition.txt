Zero-shot action recognition (ZSAR) is a challenging task in computer vision, as it requires models to infer meaningful information about unseen concepts based only on data from seen training concepts. While supervised action recognition methods have seen great success, ZSAR approaches have lagged behind due to the inherent challenges of distribution shift and semantic gap. Recent ZSAR methods have focused on exploiting visual features from pretrained action recognition models and studying visual-to-semantic mapping. However, these approaches have limitations as the visual features may not contain enough information and the pretrained models may be trained on classes that should not be seen during ZSAR. Additionally, the manual labeling of action classes with limited descriptions can result in imprecise or incomplete semantic information. In this paper, we propose a cross-modal framework called ResT (ResNet-Transformer) for ZSAR. ResT associates both visual and semantic spaces while preserving descriptive and discriminative information in the visual embedding space. Unlike previous approaches, ResT does not use pretrained feature extractors and integrates the learning of visual representations and visual-semantic associations into a unified architecture. We also develop a semantic transfer scheme to composite visual prototypes for unseen classes, based on the observation that actions are implicitly compositional. This transfer scheme alleviates the hubness problem and preserves visual distinction. We evaluate our approach on multiple benchmark datasets and achieve state-of-the-art results in ZSAR. Our approach is end-to-end trainable, offers a good accuracy-complexity trade-off, and can be used with different feature encoder backbones and pretrained models.