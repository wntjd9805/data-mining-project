Human-object interaction (HOI) detection plays a crucial role in complex visual reasoning tasks such as scene understanding and action recognition. The goal of HOI detection is to detect all HOI triplets, consisting of a human, an object, and an action, in each image. Existing literature on HOI detection can be divided into two-stage and one-stage approaches. Two-stage approaches first detect people and objects independently using off-the-shelf detectors, and then predict interaction classes for each pair of person and object bounding box. However, this approach has drawbacks of agnostic object detectors and the need to enumerate over all pairs. On the other hand, one-stage approaches directly detect all components of an HOI triplet in an end-to-end manner, but they can be time-consuming and expensive. This paper focuses on one-stage approaches based on Transformer architectures. These approaches employ a CNN backbone to extract image features, followed by an encoder-decoder architecture. Some methods use two decoders to detect instances and interactions in parallel, while others use a single decoder. However, there are limitations to these designs, such as predicting irrelevant object-action pairs and the challenge of decoding simple queries for complex relational structures. In this paper, we propose a Semantic and Spatial Refined Transformer (SSRT) that addresses these limitations. SSRT employs a Support Feature Generator (SFG) and a Query Refiner (QR) to generate semantic and spatial features from pre-selected object-action pairs and integrate them for decoding. Experimental results on V-COCO and HICO-DET datasets demonstrate that SSRT achieves state-of-the-art performance in HOI detection. An extensive ablation study is also conducted to evaluate the model design and parameter choices.