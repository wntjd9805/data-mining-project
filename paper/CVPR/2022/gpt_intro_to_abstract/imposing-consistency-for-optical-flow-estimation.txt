This paper introduces two consistency strategies for optical flow estimation to address the challenges associated with occlusions and the lack of precise annotations. The first strategy, called occlusion consistency, generates random occlusion masks to create additional image pairs and trains the network to predict the masks and zero-forced flow fields in a self-supervised manner. Unlike previous methods, this approach does not require forward-backward iterations and allows the network to distinguish between occlusion patterns and actual motion indicators. The second strategy involves transformation-based consistency regularization, which imposes equivariance through whole-image geometric transformations. This regularization helps improve the model's accuracy by enforcing consistency between transformed image pairs. These proposed self- and semi-supervised consistency learning strategies not only complement existing approaches but also achieve significant improvements in accuracy. The experiments demonstrate that the proposed method outperforms the state-of-the-art baseline model on the KITTI-2015 scene flow non-stereo monocular dataset. Overall, this paper contributes a novel occlusion consistency strategy, incorporates transformation consistency equivalence, and demonstrates the effectiveness of these two consistency strategies in improving optical flow estimation accuracy.