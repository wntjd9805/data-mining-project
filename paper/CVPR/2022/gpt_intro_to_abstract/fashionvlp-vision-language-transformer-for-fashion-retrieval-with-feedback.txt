Feedback-based fashion image retrieval involves fetching clothing images that match a customer's needs and preferences. This paper proposes a VLP transformer-based model called FashionVLP for fashion image retrieval with textual feedback. The model leverages prior knowledge from large image-text corpora and utilizes image features from multiple fashion-related context levels. The model consists of two parallel blocks for processing the reference image and feedback, and for processing target images. The model is trained using cosine similarity and a batch-based classification loss. Evaluation on three fashion image retrieval datasets shows that FashionVLP outperforms previous works, with a significant relative gain of 23% on the challenging FashionIQ dataset. The model also surpasses the state-of-the-art on Shoes and Fashion200K datasets. Overall, this work contributes a new transformer-based model for fashion image retrieval, effective incorporation of fashion-related visual context, and improved performance on benchmark datasets.