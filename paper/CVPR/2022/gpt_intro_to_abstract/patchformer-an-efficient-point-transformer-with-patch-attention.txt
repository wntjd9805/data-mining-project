Transformer has gained attention in various domains due to its ability to capture long-range dependencies. In the context of point cloud learning, self-attention (SA) has been used in Transformer-based architectures to model token relationships. However, the computational complexity and memory usage associated with generating and using the attention maps limit the efficiency of existing point Transformers. To address this issue, we propose a lightweight attention mechanism called PAT, which calculates the attention map using low-rank approximation. We observe that a 3D shape is composed of local parts with similar semantics, and exploit this by clustering local points into patches and estimating a base for each patch. We then approximate the global attention map using a product of self queries and self bases, which is low-rank and discards noisy information.Additionally, we introduce a Multi-Scale aTtention (MST) block to capture multi-scale features and improve efficiency. This block transforms point clouds into voxel grids and utilizes depth-width convolution for efficient computation. We also incorporate 3D relative position bias and build attentions to non-overlapping local 3D windows.Using these proposed blocks, we construct our neural architecture, called PatchFormer, and evaluate its performance on the ModelNet40, ShapeNet, and S3DIS datasets. Our experiments demonstrate that PatchFormer achieves strong performance with a 9.2Ã— speed-up compared to previous point Transformers.The main contributions of our work are:1. Introducing PatchFormer, a efficient point cloud learning network with improved performance compared to previous point Transformers.2. Proposing PAT, the first linear attention mechanism in the point cloud analysis paradigm.3. Presenting the lightweight voxel-based MST block to enable multi-scale relationship building in point cloud learning tasks.