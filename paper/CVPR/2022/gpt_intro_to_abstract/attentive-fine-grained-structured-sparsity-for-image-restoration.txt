Advances in deep learning have led to successful image restoration tasks, such as super-resolution and de-blurring. However, these methods often require heavy computational resources, making it challenging to apply them in practical applications. Network pruning has been widely used to reduce the computational burden by eliminating less critical weights in neural networks. It has shown significant effectiveness in achieving a better trade-off between accuracy and efficiency in image classification and segmentation tasks.There are two main types of network pruning: unstructured pruning and structured pruning. Unstructured pruning aims to remove individual weights that have less impact on model accuracy, but it is difficult to accelerate resulting models due to irregular sparsity patterns. On the other hand, structured pruning removes predetermined structures to enable acceleration on GPUs, but it often leads to performance degradation in image restoration models.Recently, N:M fine-grained structured sparsity has emerged as a better alternative, combining the strengths of both pruning methods. N:M structured sparsity enforces a certain number of weights to have non-zero values within a group of consecutive weights. This sparsity pattern has the potential for hardware acceleration. However, training a network with N:M sparsity has proven to be challenging, limiting its direct application.In this paper, we propose a layer-wise N:M sparsity search framework for efficient image restoration networks, called Searching for Layer-wise N:M structured Sparsity (SLS). We define the pruning unit as a sparse tensor determined by the magnitude of weights. To determine the number of units to preserve, we introduce a trainable score for each pruning unit based on the magnitude-based importance.Furthermore, we present an adaptive inference method that uses multiple models trained by SLS with different efficiency levels. This adaptive inference technique selects the most appropriate pruned model at inference time based on the difficulty of the restoration task, improving the efficiency-accuracy trade-off and allowing flexible adoption of computational budgets.Our contributions include the proposal of SLS for determining layer-wise N:M sparsity levels, the introduction of an adaptive inference method for better trade-off control at inference time, and empirical validation of our pruned models achieving state-of-the-art performance in super-resolution and de-blurring tasks.