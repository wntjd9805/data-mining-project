Multi-object tracking is a crucial task in computer vision, particularly in applications such as mobile robotics. Tracking-by-detection, which involves detecting objects in each frame and associating them over time, has become the dominant paradigm. Local trackers handle pairwise associations based on location or identity features but lack a holistic understanding of trajectories. On the other hand, global trackers use graph-based optimization but are detached from the detector and can be slow. In this paper, we propose a novel approach that represents global tracking using a deep neural network. Our framework, called Global TRacking (GTR), integrates a transformer-based model with object detectors to perform joint detection and tracking. By encoding detections from multiple frames and using trajectory queries, GTR groups detections into trajectories. During training, the model is supervised using ground-truth trajectories, and during inference, trajectory queries adapt with the image content. Our framework is end-to-end trainable and can be easily integrated with state-of-the-art object detectors. Experimental results on the TAO and MOT17 datasets demonstrate the effectiveness of our approach in achieving high tracking performance, outperforming existing methods in terms of tracking mAP, MOTA, and HOTA. The use of transformer models in computer vision, particularly in object detection, has motivated our work, as the cross-attention structure between queries and encoder features aligns well with the association objective in multi-object tracking. By operating on detected objects rather than raw pixels, our framework leverages the advances in object detection and achieves superior tracking results in challenging scenarios.