Abstract:This paper addresses the need for photorealistic indoor scene reconstruction in 3D content creation. The goal is to create "digital twins" that accurately recreate appearances, allowing for applications such as augmented reality and realistic simulations. While significant progress has been made in reconstructing 3D geometry from photographs, capturing material and lighting information is essential for photorealistic applications. This paper presents a method that goes beyond geometry acquisition and captures spatially-varying material and lighting interactions. The approach combines expressive material priors and physically-based rendering to solve the challenge of ascribing materials to an indoor scene under arbitrary illumination. Procedural node graphs are used as compact yet expressive priors for scene material properties, allowing for the estimation of procedural materials from observed parts of the scene. The proposed method utilizes a fully differentiable pipeline to optimize material parameters and estimate scene illumination. The results show that this approach can infer spatially-varying materials and lighting, even from a single image. The transferred materials can be applied to the input geometry, creating a fully relightable 3D scene that can be rendered under novel viewpoints and lighting conditions. This method enables the creation of high-quality, photorealistic replicas of complex indoor scenes, surpassing the capabilities of current scene-level inverse rendering methods.