This article presents a method for simultaneously estimating the 3D pose and shape of multiple people in an RGB image, along with their relative depth. While previous methods have made progress in regressing 3D pose and shape for individual people, none have explicitly reasoned about depth in multi-person interactions. The authors propose a unified approach that addresses this issue and jointly regresses multiple people and their relative depth relations in one shot. They highlight the challenges faced by previous methods, such as occlusion, body size and appearance variation, and the ambiguity of monocular depth. To overcome these challenges, the authors introduce a new network architecture that reasons in 3D and a new dataset and loss functions to train without ground-truth depth. Additionally, they observe that crowded scenes contain rich information about the relative relationships between people and utilize this information for depth reasoning. The authors develop a novel method called BEV (Bird's Eye View) that combines a front-view representation with an imaginary bird's eye view to explicitly reason about depth. The BEV method employs a localization pipeline and heatmap-based detections to estimate the 3D translation of people in the scene. They also propose a weakly supervised training scheme and utilize a dataset with weak annotations of depth layers and human ages. The BEV method is evaluated on multiple datasets and outperforms previous methods in terms of relative depth reasoning, pose estimation, and mesh reconstruction. The main contributions of this work are the construction of a 3D representation to alleviate monocular depth ambiguity, the collection of a dataset with weak annotations for training and evaluation, and the development of depth reasoning in multi-person scenes through a weakly supervised training scheme.