Deep neural networks (DNNs) have been extensively applied to visual recognition problems using large-scale and annotated datasets. However, these datasets often exhibit a long-tailed distribution, where the majority of samples belong to a few dominant classes while the remaining classes have limited samples. Training on such imbalanced data leads to biased classifiers and distorted embedding spaces. Existing approaches have primarily focused on addressing the biased classifier issue, while neglecting the distortion in the embedding space. In this paper, we propose a Gaussian clouded logit (GCL) adjustment loss function to address both problems. By disturbing the logit of different classes with varying amplitudes, we balance the valid samples of different classes and calibrate the embedding space distortion. Additionally, we introduce a class-based effective number (CBEN) sampling strategy to rebalance the classifier and further enhance the performance of GCL. Experimental results on long-tailed benchmark datasets demonstrate the superiority of our proposed method over state-of-the-art techniques. Our contributions include the GCL adjustment loss function, the CBEN sampling strategy, and the improved performance achieved by our method.