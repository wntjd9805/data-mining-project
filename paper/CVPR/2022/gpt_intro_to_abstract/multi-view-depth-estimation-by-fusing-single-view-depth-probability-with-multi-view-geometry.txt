MaGNet (Monocular and Geometric Network) is a framework that aims to address the limitations of both single-view and multi-view depth estimation methods in 3D scene reconstruction and understanding. Single-view methods use monocular cues to estimate depth but are limited by the inherent ambiguity of the problem. On the other hand, multi-view methods use geometric cues but have limitations in terms of depth candidate evaluation, occlusion and object motion, and unreliable matching for texture-less or reflective surfaces.To overcome these limitations, MaGNet proposes a fusion of single-view depth probability with multi-view geometry. The framework takes a sequence of monocular images with known intrinsics and camera poses as input. The forward pass of MaGNet involves several steps: estimating the single-view depth probability distribution for each image using a deep feature extractor, sampling a small number of depth candidates for each pixel in the reference image from the estimated depth probability distribution, projecting the sampled candidates to neighboring views and measuring matching scores based on feature vector dot products, multiplying the matching scores by a binary depth consistency weight inferred from the single-view depth probability to improve accuracy and robustness, and using the resulting thin cost-volume to obtain a more accurate multi-view depth probability distribution. These steps can be repeated for iterative refinement.The contributions of MaGNet include probabilistic depth sampling, where per-pixel candidates are sampled from the single-view depth probability distribution, resulting in higher accuracy and lower computational cost compared to uniform sampling. Depth consistency weighting is also introduced, using the single-view depth probability to encode the depth consistency of candidates and improve robustness and accuracy. Furthermore, iterative refinement is implemented to handle failure cases where the initial single-view depth probability distribution is inaccurate.Experimental results demonstrate that MaGNet achieves state-of-the-art performance on ScanNet, 7-Scenes, and KITTI datasets. The framework is also shown to be more robust against challenging artifacts such as reflective and texture-less surfaces. Overall, MaGNet offers a novel approach for combining monocular and geometric cues in depth estimation for improved accuracy and robustness in 3D scene reconstruction and understanding.