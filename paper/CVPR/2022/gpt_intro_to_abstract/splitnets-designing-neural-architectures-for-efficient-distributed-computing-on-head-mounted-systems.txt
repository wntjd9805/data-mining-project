Virtual Reality (VR) and Augmented Reality (AR) are emerging as dominant computing platforms. These platforms utilize head mounted devices (HMD) with multiple cameras for computer vision (CV) and machine learning (ML)-powered human-computer interaction functions. Recent advancements in camera technology have led to the development of tiny multi-layer stacked cameras with AI computing capabilities. These intelligent cameras have limited resources compared to traditional AI processors but can perform preprocessing tasks directly after image acquisition, reducing data movement. In a typical HMD system, these intelligent image sensors and a central processor called the aggregator form the backbone for implementing complex CV/ML functions. In such systems, it is natural to distribute the machine learning workload between the sensors and the aggregator, improving overall system latency and reducing sensor-aggregator communication.Distributed computing for Deep Neural Networks (DNNs) between sensors and aggregators relies heavily on network architecture to meet application and hardware constraints such as memory, communication bandwidth, latency, and more. Previous work has focused on manually searching for network partitions or injecting bottleneck modules into existing models. However, these methods often lead to suboptimal results and performance degradation.This paper proposes SplitNets, a split-aware Neural Architecture Search (NAS) framework, for automatically searching for optimal network architectures and network splitting solutions in distributed computing systems on AR/VR glasses. SplitNets optimizes task performance and system efficiency while considering resource constraints of the mobile AR/VR system featuring smart sensors. The framework answers two key questions: 1) Can optimal network architectures and their splitting solutions be jointly searched while satisfying resource constraints? 2) Can an optimal network architecture be learned to compress and fuse features from multiple sensors in order to achieve state-of-the-art performance and efficiency compared to centralized models?SplitNets introduces techniques for module initialization and sampling to stabilize training and mitigate accuracy degradation. It also extends the framework to support searching of splitting modules with view fusion for multi-view tasks. The paper presents a series of experiments on single-view classification and multi-view 3D classification tasks to validate the importance of joint model and splitting position search for improving task and system performance.The results demonstrate that optimized network architectures and model partitions identified by SplitNets outperform existing solutions and effectively leverage the distributed computing system on AR/VR glasses.