Graph classification has become a prominent topic in various fields, such as biochemistry, social network analysis, and computer vision. Recently, there has been growing interest in the reverse problem of graph classification, which involves recognizing a compressed subgraph that is most predictive of the graph label. This subgraph not only improves predictive performance by eliminating noisy and redundant information, but also serves as an intrinsic explanation for the graph model's predictions. Therefore, subgraph recognition is a fundamental problem with numerous applications, such as identifying the substructure of molecules or generating explanatory subgraphs for Graph Neural Networks (GNNs). Previously, the subgraph recognition problem was studied under the Graph Information Bottleneck (GIB) framework. While GIB provides theoretical analysis, its optimization process is inefficient and unstable due to the estimation of mutual information. To address these issues, we propose the Variational Graph Information Bottleneck (VGIB) framework. VGIB formulates subgraph recognition as two steps: graph perturbation and subgraph selection. In the graph perturbation step, VGIB selectively injects noise into the input graph to obtain a perturbed graph, which balances the compression condition by modulating the information flow. VGIB employs the Gumbel-Softmax reparametrization and a customized Gaussian prior to handle the discrete action space and preserve semantic information. In the subgraph selection step, the injected noise is dropped to obtain the informative subgraph. To evaluate VGIB, we conduct experiments on various tasks, including GNN explainability, graph interpretation, and graph classification. The results show that VGIB not only improves optimization efficiency but also outperforms baseline methods in terms of subgraph quality.