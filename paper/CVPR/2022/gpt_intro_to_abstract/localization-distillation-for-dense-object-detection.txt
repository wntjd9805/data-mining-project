Localization is a critical aspect in object detection, with bounding box regression being the prevailing method for localization. However, there is often ambiguity in localizing objects where edges are not sufficient for confident detection. Lightweight detectors are particularly susceptible to this issue. To address this problem, knowledge distillation (KD) has been used to boost the performance of smaller networks by transferring knowledge from larger teacher networks. While previous KD methods in object detection have focused on transferring semantic knowledge, the importance of localization knowledge distillation has been overlooked. Existing techniques enforce consistency in deep features between teacher-student pairs and utilize various imitation regions for distillation. However, the mixed nature of semantic and localization knowledge on feature maps makes it difficult to determine the benefit and suitable regions for transfer. To address these challenges, we propose a divide-and-conquer distillation strategy that separates the transfer of semantic and localization knowledge. Our approach aims to improve object detection performance by selectively transferring each type of knowledge.