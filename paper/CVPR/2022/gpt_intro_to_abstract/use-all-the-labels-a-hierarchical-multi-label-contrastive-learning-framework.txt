Hierarchical multi-labels are common in various real-world scenarios, such as biological classification and product categorization. However, traditional supervised approaches often overlook the hierarchical relationship between classes, which can limit the performance and generalization of models. In representation learning frameworks, it is crucial to utilize all available supervisory signals, including the hierarchical categorization, in order to effectively represent the data. Unfortunately, representation learning approaches that exploit this hierarchical relationship have received little attention.This paper addresses this gap by proposing a novel representation learning framework for hierarchical multi-labels. The framework leverages the available labels to learn an embedding function that preserves the label hierarchy in the embedding space. The authors introduce two new losses, the Hierarchical Multi-label Contrastive Loss (HiMulCon) and the Hierarchical Constraint Enforcing Loss (HiConE), which exploit the relationship between hierarchical multi-labels and ensure that the learned representations retain the label relationship.The HiMulCon loss enforces a penalty based on the proximity between the anchor image and the matching image in the label space, defined as the overlap in ancestry in the tree structure. On the other hand, the HiConE loss prevents hierarchy violations by ensuring that the loss from pairs farther apart in the label space is never smaller than the loss from closer pairs.The proposed framework is not limited to hierarchical multi-label scenarios and can be applied to other situations. It reduces to the supervised contrastive approach when only single-level labels are available and to the SimCLR approach when no labels are provided. The authors demonstrate the effectiveness of their framework through experiments comparing it to existing approaches on various downstream tasks, such as category prediction, sub-category retrieval, and clustering. The results show that their approach successfully preserves the hierarchical relationship between labels in the representation space and generalizes well to unseen data.