This paper introduces a hair editing framework that aims to provide a more intuitive and convenient interaction method using text descriptions or reference images. The framework builds upon the success of StyleCLIP, which leverages the image text representation capabilities of CLIP for image manipulation. However, StyleCLIP has limitations such as the need to train separate mappers for each hair editing description, poor disentanglement of hairstyle and hair color attributes, and the lack of support for reference image-based editing. To overcome these limitations, the proposed framework supports different texts or reference images as hairstyle/color conditions within one model. The framework utilizes a pre-trained StyleGAN generator and focuses on learning a mapper network to map input conditions to corresponding latent code changes. The authors explore the potential of CLIP beyond measuring image text similarity by introducing new designs, including shared condition embedding, disentangled information injection, and a modulation module for direct control of input conditions on latent codes. To ensure hair editing based on text or reference image conditions while preserving irrelevant attributes, the paper introduces three types of losses: text manipulation loss, image manipulation loss, and attribute preservation loss. Quantitative and qualitative comparisons, as well as user studies, demonstrate the superiority of the proposed method in terms of manipulation accuracy, fidelity, and attribute preservation. The paper includes extensive ablation analysis to justify the network structure and loss function designs. In summary, the contributions of this research are the unification of text and reference image conditions within one framework for interactive hair editing, network structure designs and loss functions tailored for disentangled hairstyle and hair color manipulation, and extensive experimentation and analysis to showcase the effectiveness of the proposed method.