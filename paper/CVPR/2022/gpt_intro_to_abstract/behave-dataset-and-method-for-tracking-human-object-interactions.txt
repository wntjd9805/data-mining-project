The last decade has witnessed significant advancements in the modeling of human appearance, including body pose, shape, faces, and clothing. These models have found practical applications in virtual try-on, personalized avatar creation, augmented and mixed reality, and human-robot collaboration. However, while there has been progress in capturing and synthesizing human interactions, existing methods often rely on expensive motion capture systems or wearable sensors, limiting the diversity and accuracy of the captured data. To address this limitation, we propose BEHAVE, a method for capturing diverse 3D human interactions in natural environments using portable and affordable RGBD cameras. Tracking human interactions from consumer-grade cameras is challenging due to noise, incompleteness, and occlusion in depth data. Capturing accurate human-object contacts is also difficult, as contacts represent small regions close to the observable limit. To overcome these challenges, we propose a neural model that completes human and object shape while predicting correspondence and orientation fields. To evaluate BEHAVE, we have captured the largest dataset of human-object interactions in natural environments, including 20 objects, 8 subjects, and 5 different locations. We provide ground truth data for research in this direction and plan to release our code and dataset to the community. Our contributions include proposing an accurate approach for 3D tracking of humans, objects, and contacts in natural environments, collecting a large dataset for studying human-object interactions, and providing code and data for further research in this area.