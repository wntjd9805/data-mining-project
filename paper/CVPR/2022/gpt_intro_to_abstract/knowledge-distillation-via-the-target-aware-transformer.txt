Knowledge distillation is a technique used to enhance the performance of machine learning algorithms. It involves transferring knowledge from a larger teacher neural network to a smaller student network. This is achieved by formulating an external loss function that guides the student feature map to mimic the teacher's. Previous works have focused on distilling knowledge from the final layer of neural networks. However, it has been discovered that distilling intermediate feature maps is a more effective approach. The current one-to-one matching distillation approach assumes that the spatial information of each pixel is the same, which is often not valid in practice. To address this, we propose a novel one-to-all spatial matching knowledge distillation approach. Our method distills the teacher's features at each spatial location into all components of the student features through a target-aware transformer. This allows for a weighted summation of all student components, addressing the limitations of one-to-one matching distillation. Our method computes the correlation between feature spatial locations, but this can become intractable for large feature maps. To overcome this, we extend our pipeline in a two-step hierarchical fashion. We evaluate the effectiveness of our approach on image classification and semantic segmentation tasks, achieving state-of-the-art performance compared to related alternatives. Our contributions include the introduction of the target-aware transformer and the hierarchical distillation technique, which improve knowledge distillation performance and make it applicable to tasks with large feature maps.