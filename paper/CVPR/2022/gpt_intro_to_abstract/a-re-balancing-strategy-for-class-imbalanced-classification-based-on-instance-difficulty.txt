The performance of classification models on synthetic datasets has improved significantly, but real-world datasets often exhibit imbalanced data distributions. Imbalance is reflected by the sizes of different classes, with some having a large number of instances (majority classes) and others having very few (minority classes). This poses challenges for neural models, as they tend to be biased towards the majority classes and perform poorly on the minority classes. Existing methods for addressing class imbalance focus on adjusting the observed class distribution at a coarse level, either through re-sampling or re-weighting. However, these strategies do not account for the differences in difficulty within classes. We propose an instance-level re-balancing strategy that considers the difficulty of individual instances based on their learning speed. We record instance predictions during training and measure difficulty based on prediction variations. Our method assigns higher weights to difficult instances during re-sampling. We conduct empirical experiments to demonstrate the effectiveness of our approach, achieving state-of-the-art results on long-tailed image classification datasets. Our contributions include highlighting the limitations of class-level methods, proposing a new difficulty definition for instances, and presenting an instance-level re-balancing strategy with theoretical proof.