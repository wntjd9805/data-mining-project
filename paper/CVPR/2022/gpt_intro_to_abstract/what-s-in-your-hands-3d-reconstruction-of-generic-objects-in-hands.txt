Humans interact with their surrounding world using their hands, perceiving the 3D shape of objects. This paper aims to develop a recognition system for hand-object interactions (HOI) that can perceive and reason about the geometric information of these interactions. Previous work has made progress in inferring the 3D shape of hands and objects in isolation, but understanding HOI for manipulable objects has been limited. Current approaches focus on reconstructing known objects based on 3D templates, which hinders the reconstruction of unknown objects and struggles with various object shapes. In contrast, this paper focuses on reconstructing HOI for novel objects without object templates. The key observation is that hand articulation provides strong cues for the object in interaction, and the hand pose can be used to predict the object shape. The proposed approach estimates hand pose first and then reconstructs the object in a normalized hand-centric coordinate frame. The method is evaluated on synthetic and real-world datasets, outperforming prior works in terms of object shape prediction. The benefits of articulation-aware coordinates are also analyzed, and improvements are made by encouraging interaction between the predicted hand pose and the object.