Automatic layout design using creative AI has gained significant attention, offering a valuable technique to assist designers in efficiently creating media content. Existing approaches based on deep generative models, such as GAN and VAE, have been proposed to synthesize graphic layouts by learning from human-designed data. However, text logo design, a challenging task involving various factors like fonts, layouts, and textures, has not been adequately addressed. Current approaches mainly focus on visual, category, and topic information, neglecting the importance of text semantics in determining layout design. To address this gap, we propose a GAN-based model that treats layout synthesis as a sequence generation problem and employs a dual-modality fusion scheme to encode both visual and linguistic cues. To capture fine-grained details such as glyph collisions and character placing trajectories, a dual-discriminator module is introduced. We also propose a differentiable composition method to bridge the gap between synthesized layouts and rendered text logo images. We construct a large-scale dataset called TextLogo3K for training and evaluation, enabling extensive experiments that demonstrate the effectiveness and advantages of our approach. Moreover, we show how our model can be integrated with font generation and texture transfer models to automatically synthesize visually-pleasing text logos. This work is the first to incorporate both linguistic and visual information in text logo synthesis, and TextLogo3K is the first dataset specifically designed for this task.