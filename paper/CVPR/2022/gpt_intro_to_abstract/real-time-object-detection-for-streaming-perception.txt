This paper addresses the challenge of online perception for autonomous driving, specifically the issue of inconsistency between perceptive results and the changed state of the environment. To overcome this issue, the paper proposes a streaming accuracy metric that evaluates the output of the perception stack in real time, forcing the perception to forecast the state when the model finishes processing. The paper also introduces a meta-detector named Streamer that incorporates decision-theoretic scheduling, asynchronous tracking, and future forecasting to mitigate the performance drop in streaming perception. Additionally, the paper highlights the effectiveness of "fast enough" real-time object detectors, which eliminate the need for an accuracy and latency trade-off in streaming perception. Building on this, the paper presents a streaming perception model that predicts the results of the next frame at the current state. The model is trained using triplets of the last, current, and next frame, and incorporates a Dual-Flow Perception module and a Trend Aware Loss to improve training efficiency. Experimental results on the Argoverse-HD dataset demonstrate significant improvements in streaming perception performance compared to the baseline real-time detector. Overall, this work contributes a simplified and effective approach to online perception for autonomous driving, achieving robust forecasting under different moving speeds of the vehicle.