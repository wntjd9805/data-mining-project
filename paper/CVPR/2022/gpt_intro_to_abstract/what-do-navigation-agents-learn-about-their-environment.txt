Embodying AI in agents to perceive, communicate, reason, and act in their environment has garnered significant attention from the computer vision, natural language processing, and robotics communities. Notably, the development of powerful simulators like AI2-THOR, Habitat, and iGibson has enabled agents to navigate, reason, collaborate, manipulate, and follow instructions. While progress has been made in various tasks and benchmarks using black box neural networks, these solutions lack interpretability regarding the concepts and skills learned or the actions taken. Interpretable systems are crucial for embodied AI as they will eventually be deployed on physical robots interacting with humans. In the field of image classification, interpretability methods have been developed, but they do not fully leverage the rich metadata available in synthetic environments. This paper proposes a framework for interpreting hidden representations of embodied agents trained in simulated worlds. The framework is applied to Object Navigation and Point Goal Navigation tasks, probing the agents' hidden representations to evaluate if they encode task-related aspects. The SHAP model interpretation method is then employed to determine the most relevant hidden units for predicting these concepts. The framework unveils interesting insights about the encoded information and relevant units, such as sparse target representation, learning of concepts like reachable locations and visit history, and the encoding of progress and reduced reliance on visual information. Additionally, ablation experiments demonstrate no impact on model performance after removing 10% of the units, suggesting redundancy in the representation. Overall, this work introduces a specialized interpretability framework, provides novel insights about navigational agents, and explores the redundancy within the represented information.