Depth sensors play a crucial role in various robotic applications such as mapping, navigation, and object manipulation. Among these sensors, active stereovision depth sensors, like the Intel RealSenseâ„¢D series, are widely used due to their high spatial resolution, accuracy, and affordability. However, these sensors suffer from common stereo matching issues, such as over smoothing, edge fattening, and holes for specular and transparent objects, making them less suitable for applications requiring high precision and completeness. Learning-based methods have been proposed to address these issues by generating more accurate and complete depth maps through prior samples. However, collecting large-scale stereo datasets with ground truth depth in the real world is time-consuming and expensive. Self-supervised stereo methods use reprojection or related losses as supervision, but these losses fluctuate, hindering meaningful optimization. Another approach is to use simulation data, but there is a domain gap between the simulation and real world, affecting the reliability of network transferability. This paper introduces a mixed domain learning method that combines self-supervised learning in the real domain and supervised learning in the simulation domain. By training the network on shape primitives with ground truth depth in the simulation domain and unrelated scenes with reprojection as self-supervision in the real domain, the proposed method achieves comparable performance on out-of-distribution objects in the real domain. The paper also proposes the use of temporal infrared (IR) imagery by adjusting the brightness of the emitted IR pattern and extracting the binary pattern from temporal image sequences, eliminating the influence of scene texture and illumination strength decay. Experimental results demonstrate that the proposed method outperforms state-of-the-art learning-based stereo methods and commercial depth sensors, with ablation studies further confirming the effectiveness of each module.