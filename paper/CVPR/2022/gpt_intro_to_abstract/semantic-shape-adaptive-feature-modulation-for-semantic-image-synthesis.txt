Semantic image synthesis refers to the generation of semantically aligned and photo-realistic images based on given semantic maps. This task allows for flexible control over image content by drawing or editing the input semantic maps, making it useful in content creation and image editing scenarios. Generative Adversarial Networks (GANs) have been widely adopted to tackle this problem, with many approaches focusing on modeling the relationship between semantic classes and visual appearances. Previous methods, such as SPADE and CC-FPSE, utilize semantic layouts to modulate activations in the generator. However, these semantic maps only provide object-level layouts, lacking the fine-grained structure of object instances. In this paper, we propose a Shape-aware Position Descriptor (SPD) to encode object shape and part-level layout information into each pixel's positional feature. Additionally, we introduce the Semantic-shape Adaptive Feature Modulation (SAFM) block to combine semantic maps and SPD features, allowing for adaptive modulation of input feature maps. Experimental results on Cityscapes, COCO-stuff, and ADE20K datasets demonstrate that our method outperforms state-of-the-art approaches, generating more photo-realistic results with rich details.