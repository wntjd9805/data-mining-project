In this paper, we focus on the joint estimation of optical flow and scene flow in monocular camera frames with sparse depth measurements from LiDAR. Previous methods have employed modular networks or end-to-end architectures to estimate flow, but these approaches fail to fully utilize the complementarity between submodules or modalities, leading to limitations in overall performance. To address this, we propose a multi-stage and bidirectional fusion pipeline that achieves better performance with fewer parameters. Within each stage, modalities are learned separately using modality-specific architectures, and a learnable bidirectional bridge is used to pass complementary information between the branches. Additionally, we incorporate a point-based branch to process point clouds and extract fine 3D geometric information. We introduce a learnable fusion operator to handle the lack of one-to-one correspondence between image features and point clouds, and a transformation operator to balance the density distribution of LiDAR point clouds. Experimental results demonstrate that our approach outperforms existing methods on benchmark datasets with significantly fewer parameters.