In the field of autonomous driving, vehicles are equipped with multiple sensors such as camera, Lidar, and Radar to improve object detection performance. Existing works have focused on fusing Lidar and camera, but visual sensors like camera are sensitive to adverse weather conditions. Radar, on the other hand, is more robust in such conditions and has been widely adopted in autonomous vehicles. However, existing fusion models assume access to two reliable sensor streams, and performance may suffer if one sensor is unavailable or corrupted. To address this issue, we propose a framework called Self-Training Multi-modal Vehicle Detection Network (ST-MVDNet) that leverages the Mean Teacher framework to optimize the fusion model with both clear and missing sensor streams. ST-MVDNet enhances feature extraction and improves robustness to missing sensors. Experimental results demonstrate the limitations of a multi-modal detection network with missing sensors and show that our proposed framework outperforms existing state-of-the-art methods by a large margin on the Oxford Radar Robotcar dataset.