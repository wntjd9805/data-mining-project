Neural Radiance Fields (NeRF) have shown promise in scene encoding by using multi-layer perceptrons (MLPs) as weights. One important application is Novel View Synthesis, where the goal is to generate new views of a scene from unseen camera poses. While NeRF can produce high-fidelity novel views, it is often impractical due to overfitting and the need for multiple scene views for training. Several follow-up works have attempted to generalize NeRF to new scenes. However, existing methods mainly focus on multi-view images during training and inference. Single-shot novel view synthesis is challenging due to incomplete content information within a single image. To address this limitation, this paper explores a human brain-inspired approach of learning a prior implicit model and mapping observations to the learned model. Generative Adversarial Networks (GANs) have shown success in image synthesis and transformation, including 3D content synthesis. This suggests the possibility of using GAN inversion for 3D reconstruction without multi-view images. Existing 3D-aware generative models have achieved high 3D consistency, making them suitable for few-shot 3D reconstruction using adversarial training and radiance fields. In this paper, the authors propose Pix2NeRF, an end-to-end pipeline that translates input images to NeRF for novel view synthesis without pre-training or fine-tuning. The method utilizes an encoder to map images to a latent space and is trained through a combination of Ï€-GAN inversion and adaptation of the encoder and generator. Experimental results show that Pix2NeRF achieves comparable performance to state-of-the-art generative NeRF models in terms of synthesizing high-fidelity novel views. The contributions of this paper include the development of Pix2NeRF, the first unsupervised single-shot NeRF model, and the exploration of conditional GAN-based NeRF and NeRF-based GAN inversion. Extensive ablation studies are conducted to justify the design choices of the proposed method.