This paper introduces the concept of expected population statistics in batch normalization (BN) and investigates the effects of estimation shift on a batch normalized network. The authors observe that the estimation shift of BN can accumulate in a network, leading to degraded performance under small-batch-size training and the need for population statistics adaptation in the presence of distribution shift during testing. They propose a solution called XBNBlock which replaces one BN with batch-free normalization (BFN) in the bottleneck of residual-style networks. Experimental results on ImageNet and COCO benchmarks demonstrate that XBNBlock consistently improves the performance of ResNet and ResNeXt architectures, with absolute gains in accuracy and bounding box AP. Additionally, XBNBlock shows increased robustness to distribution shift.