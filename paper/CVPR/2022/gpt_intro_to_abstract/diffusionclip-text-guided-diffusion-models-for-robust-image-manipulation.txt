GAN inversion methods combined with Contrastive Language-Image Pretraining (CLIP) have gained popularity for their ability to manipulate images guided by text prompts. However, their real-world application on diverse types of images is still limited due to the inability to reconstruct images with novel poses, views, and details. This issue is particularly challenging when dealing with images from datasets with high variance. Diffusion models, such as denoising diffusion probabilistic models (DDPM) and score-based generative models, have achieved significant successes in image generation tasks, surpassing other generative models. Inspired by this, we propose DiffusionCLIP, a CLIP-guided robust image manipulation method using diffusion models. This approach converts an input image to latent noises through forward diffusion and then inverts the noises back to the original image through reverse diffusion. The key idea of DiffusionCLIP is to fine-tune the score function in the reverse diffusion process using a CLIP loss to control the attributes of the generated image based on text prompts. DiffusionCLIP can successfully perform image manipulation in both trained and unseen domains, and even translate images from one unseen domain to another or generate images in an unseen domain from strokes. It also enables changing multiple attributes simultaneously through one sampling process. DiffusionCLIP takes a step towards general application by manipulating images from the widely varying ImageNet dataset, which has been rarely explored with GAN inversion. We also propose a systematic approach to find optimal sampling conditions for high-quality and speedy image manipulation. Our method outperforms state-of-the-art baselines in terms of robustness and accuracy, as demonstrated through qualitative comparison and human evaluation.