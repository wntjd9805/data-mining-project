Semantic segmentation is a fundamental task in computer vision that involves partitioning an image into different semantic regions. Traditional segmentation methods have primarily focused on regular resolution images and struggle to handle ultra-high resolution images effectively due to memory and computational limitations. In this paper, we propose ISDNet, a novel framework for efficient ultra-high resolution image segmentation. Inspired by the bilateral architecture, our framework integrates shallow and deep networks to capture spatial and contextual information effectively. Unlike conventional bilateral models, we input different scale inputs for the shallow and deep branches. We also observe that incorporating heterogeneous information and auxiliary tasks, such as super-resolution, can further improve the performance of our method. We introduce a Relation-Aware feature Fusion (RAF) module to fuse features from the shallow and deep branches based on their relationship. Additionally, we employ auxiliary losses to enhance the features learned from the deep branch. Experimental results on Deepglobe, Inria Aerial, and Cityscapes datasets demonstrate the effectiveness of our method in achieving high accuracy, fast speed, and low memory consumption during inference. Our framework can be generalized to combine other semantic segmentation networks, including transformer-based methods like SegFormer.