Video is a dynamic medium that requires modeling of temporal dynamics for a comprehensive understanding. Recent advancements in video analysis have focused on integrating motion through temporal convolutions and self-attention mechanisms. However, these methods assume that the same positions across frames correspond to the same objects, which may not always be true in practice due to motion. To address this issue, we propose an inter-frame attention approach that considers changes in video content caused by motion to enhance feature alignment across frames. Our method utilizes local neighboring regions and takes into account inter-frame correlations to achieve better temporal feature aggregation. We also incorporate deformable design and motion cues to support regions with large deformation. We introduce the Stand-alone Inter-Frame Attention (SIFA) block, which can be seamlessly integrated with existing video models. Through extensive experiments, we demonstrate that our SIFA-Net and SIFA-Transformer backbones outperform state-of-the-art methods in action recognition tasks.