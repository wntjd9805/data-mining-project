Sign languages are essential for communication among deaf communities. These visually complex languages utilize manual and non-manual markers to convey information. While recent advances in automatic speech recognition (ASR) have enabled automatic captioning and text-based searchability for spoken language videos, sign language content lacks these capabilities. Existing sign language translation systems and sign spotting approaches are limited in their ability to accurately index and search sign language videos. In this paper, we propose a novel approach to sign language video retrieval with free-form textual queries by learning a joint embedding space between text and video. We leverage the How2Sign American Sign Language (ASL) dataset, which is the largest public source of sign language videos with aligned captions. To address the challenges of translation complexity, limited paired and annotated data, and annotation scarcity, we construct cross-modal embeddings and introduce the SPOT-ALIGN framework for automatic annotation. Our contributions include the introduction of the sign language video retrieval task with free-form textual queries, baseline models for evaluation, the SPOT-ALIGN framework, and a manually annotated test set for the How2Sign benchmark. Through experiments, we demonstrate the efficacy of our approach and the potential for scalable and efficient sign language video retrieval.