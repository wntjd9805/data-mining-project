Parsing the environment from multiple viewing angles is crucial for autonomous agents and assistive robots. However, existing semantic segmentation networks trained on forward views often suffer a drop in performance when tested with viewpoint shifts. To address this issue, we propose a method for reducing the performance drop by transporting semantic information from views with rich annotations to views with no available annotations using multi-camera systems. Most existing methods focus on domain alignment, assuming invariance exists for the task. However, the domain discrepancy we consider is mainly caused by content shift due to viewpoint change. We introduce a view transformation network that hallucinates target semantic images using source counterparts, with an attention mechanism injecting the desired inductive bias. We also propose a functional label hallucination strategy to improve label accuracy and adaptability. To evaluate our method, we introduce a new dataset with varying viewpoints. We conduct a comprehensive study of state-of-the-art unsupervised domain adaptation methods and demonstrate the effectiveness of our approach in achieving the best adaptation gains across different target domains. Our contributions include benchmarking UDA methods for viewpoint shifts, a novel architecture for semantic information hallucination, and a state-of-the-art method for addressing performance drops in semantic segmentation caused by viewpoint shifts in multi-camera systems.