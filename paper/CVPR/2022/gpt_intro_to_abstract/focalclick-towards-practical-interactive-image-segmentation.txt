Interactive segmentation has gained significant attention in recent years, as it allows users to conveniently annotate masks using simple interactions such as clicks or scribbles. This paper specifically focuses on the click-based method, where users place positive/negative clicks to define foreground and background, and the model updates predictions after each click. Previous works have used Gaussian maps or disks to represent the clicks and concatenated them with the image for input into a segmentation model. However, these methods have certain limitations when applied to practical scenarios. Firstly, they are not efficient on low-power devices and struggle to maintain accuracy when reducing the input size for faster processing. Secondly, they are not compatible with preexisting masks, often provided by offline models or other pre-processing methods. This paper introduces FocalClick, which addresses these issues by making the model focus on noteworthy patches and enabling localized corrections on preexisting masks. The proposed pipeline includes a Target Crop for coarse segmentation and a Refiner for localized corrections, resulting in faster processing and improved efficiency. Additionally, the paper introduces a benchmark and a solution for Interactive Mask Correction, allowing evaluation and modification of preexisting masks. The contributions of this work include significantly smaller FLOPs, the first benchmark for mask correction, and a better alignment of interactive segmentation with practical needs in both industry and academia.