Current deep learning methods in computer vision have shown significant progress in various tasks. However, these methods often require high computational resources and abundant data, which can be costly to obtain in real-world settings. To address this issue, few-shot learning has been proposed as a way to develop efficient learning algorithms that can perform well with limited data. Furthermore, there is a potential security threat to deep neural networks from adversarial examples, which are tailored examples that disrupt the inference of neural networks while appearing similar to natural examples. In the few-shot setting, the scarcity of data makes deep learning methods even more vulnerable to attacks from adversarial examples. The main goal of adversarially robust Few-Shot Image Classification (FSIC) is to build models that perform well in standard few-shot classification tasks and are also robust against adversarial examples. Existing research in this area is sparse and primarily based on meta-learning approaches. However, there is still a considerable gap between the label spaces used in meta-training and meta-testing, leading to potential performance issues when dealing with adversarial examples from different label spaces. In this paper, we propose a novel adversarially robust FSIC framework that learns a robust embedding model and generalizes it to unforeseen adversarial few-shot classification tasks. We introduce an auxiliary adversarial-aware module to differentiate between legitimate examples and their corresponding adversarial examples. To address the imbalance among adversarial examples, we devise an adversarial-reweighted method based on loss variation. Additionally, we include a postprocessing module to enhance the quality of adversarial features. We evaluate our method on two standard few-shot classification benchmarks and demonstrate its effectiveness in terms of accuracy on legitimate and adversarial examples, as well as its robustness against different attack strengths and generalizability in cross-domain scenarios. Our proposed method achieves state-of-the-art performance in adversarially robust FSIC and provides multiple benefits simultaneously.