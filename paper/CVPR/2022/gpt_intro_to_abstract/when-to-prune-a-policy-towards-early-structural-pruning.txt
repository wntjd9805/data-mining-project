The success of convolutional neural networks (CNNs) in computer vision tasks has led to a rise in network complexity, which poses challenges for deployment on resource-constrained devices. In response, compression techniques such as pruning have been used to reduce the memory footprint and computing power consumption of CNNs. Pruning aims to remove insignificant network parameters without impacting accuracy, and structural pruning, in particular, removes entire filters or neurons to achieve structural sparsity. Network pruning typically involves three steps: original training, pruning, and fine-tuning. However, the original training of an over-parameterized network remains time-consuming and resource-intensive. This paper proposes pruning-aware training (PaT), which aims to prune the network early during training to immediately benefit training and save time on additional fine-tuning. The key challenge is determining the optimal point to start pruning during training. The paper introduces the Early Pruning Indicator (EPI), a novel metric that estimates the similarity in network structure between consecutive epochs. When the pruning structure stabilizes, it is safe to prune without accuracy loss. Experimental results show that PaT with EPI outperforms other pruning-at-initialization methods, reduces training resources, and is agnostic to the pruning method used. This work contributes a practical and efficient approach to pruning during training, significantly improving training speed and resource utilization.