This paper focuses on the problem of scene graph generation (SGG), which aims to capture interactions between objects in images. SGG is important for various high-level visual tasks such as object detection, image captioning, and visual question answering. Recent advancements in SGG have utilized visual features extracted from neural networks or graph neural networks combined with language embeddings or statistical priors for predicting relations between objects. However, existing SGG benchmarks, such as Visual Genome, suffer from biases and inaccuracies due to factors like long-tail data distribution and subjective annotation of relations. While previous methods have addressed biases arising from frequently occurring labels, this paper explores another bias related to label informativeness. The authors hypothesize that certain relation labels, called implicit labels, are more informative than others. To test this hypothesis, experiments were conducted training a SGG model on either explicit or implicit relations. Surprisingly, training on implicit relations resulted in good performance not only on implicit relations but also on unseen explicit relations. Motivated by this finding, the authors propose a novel model-agnostic training procedure that mines missing implicit labels from partially labeled data, leading to performance improvements in SGG tasks. The proposed method involves a two-stage training pipeline and incorporates label refinement and regularization strategies. Experimental results demonstrate significant performance gains in standard and zero-shot SGG settings on the Visual Genome dataset when applying the proposed method to existing scene graph generation models. In summary, this paper addresses the issue of missing informative labels in SGG benchmarks, and presents a training procedure that improves the performance of SGG models.