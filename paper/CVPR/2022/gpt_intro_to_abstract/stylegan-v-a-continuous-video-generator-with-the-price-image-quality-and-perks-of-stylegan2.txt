Recent advances in deep learning have led to significant improvements in image generation, resulting in unprecedented photo-realistic quality. However, video generation has not achieved the same level of success and struggles to fit complex real-world datasets. The challenges in video generation are attributed to the more complex nature of the underlying data distribution and the computational intensity required for representing long high-resolution videos. This paper proposes treating videos as continuous signals rather than discrete sequences of images, allowing for more efficient representation and reducing the need for expensive architectures. The proposed model, StyleGAN-V, builds upon the image-based StyleGAN2 and can generate arbitrarily long videos at high frame rates in a non-autoregressive manner. The model demonstrates capabilities for semantic manipulation and achieves comparable training efficiency and image quality as the classical image-based StyleGAN2. Evaluation on multiple benchmarks shows that the proposed method outperforms other approaches in terms of video synthesis metrics.