Generative adversarial networks (GANs) have achieved remarkable success in generating high-quality images and videos. Image-to-image translation tasks, such as style transfer and super-resolution, have greatly benefited from the performance of GANs. However, image-to-image generation is more complex compared to other tasks, as it involves a larger output space. Existing GANs have high computational demands and a large number of parameters, making them inefficient and limiting their usage on resource-constrained platforms.Knowledge distillation (KD) has been effective in improving the performance of small models by imitating the predictions and intermediate features of a larger teacher model. Previous research has explored the application of KD to GANs but has not achieved significant improvements. This paper investigates the reason behind the limited success of KD on GANs from a frequency perspective.The authors conduct an experiment using discrete wavelet transformation (DWT) to decompose generated and ground truth images into different frequency bands. They observe that while GANs perform well on low frequency bands, they struggle to generate high frequency details in images. Additionally, smaller GANs perform worse on high frequency bands compared to larger GANs. These observations emphasize the importance of high frequency information in GAN compression.Traditional knowledge distillation in GANs minimizes the difference between images generated by students and teachers without prioritizing high frequency. Motivated by the observations, the authors propose wavelet knowledge distillation, which focuses on students learning high frequency information. They apply DWT to decompose images generated by teachers and students into different frequency bands and minimize the L1 loss only on the high frequency bands.The effectiveness of the proposed method is demonstrated through experiments on both paired and unpaired image-to-image translation tasks. Results show significant compression and acceleration while maintaining performance on various datasets. The authors also study the effectiveness of different frequency bands and the impact of knowledge distillation schemes. Furthermore, they explore the relationship between discriminators and generators during model compression and show that compressing discriminators improves the performance of compressed generators.In summary, this paper analyzes the performance of GANs from a frequency perspective, proposes wavelet knowledge distillation to address the lack of high frequency information generation in smaller GANs, and demonstrates the effectiveness of the proposed method through experiments. The relationship between discriminators and generators during model compression is also explored, highlighting the importance of compressing discriminators for improved generator performance.