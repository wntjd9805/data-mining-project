Matching partial point clouds from range sensors is crucial for various applications in 3D computer vision, such as SLAM and dynamic tracking and reconstruction. This work aims to develop a robust method for matching point clouds in both rigid and deformable scenes. Recent advancements in representation learning for 3D data have made significant progress in point cloud feature extraction and nearest neighbor search. However, these methods may face challenges when scenes contain repetitive geometry patterns, leading to ambiguity. To address this issue, we propose Lepard, a novel partial point cloud matching method that incorporates 3D positional knowledge. Lepard utilizes a fully convolutional feature extractor, a concept of Transformer, and differentiable matching. Additionally, we introduce techniques such as disentangling point cloud representations, explicit position encoding, and a repositioning module to enhance the incorporation of 3D position information. Ablation studies demonstrate the effectiveness of these techniques. Furthermore, we introduce a benchmark dataset called 4DMatch, which contains non-rigidly deforming point clouds across the time axis, posing additional challenges for matching and registration. In experiments, Lepard combined with RANSAC and ICP achieves state-of-the-art registration recall on existing benchmarks, and on the newly proposed 4DMatch benchmarks, it outperforms previous methods in non-rigid matching recall.