Multi-view stereo (MVS) is a critical field that focuses on extracting geometry from photographs. Traditional methods have made significant progress in reconstructing dense 3D representations by utilizing stereo correspondence from multiple images. However, the effectiveness of MVS can be further improved by leveraging learning-based approaches, which have more powerful representation capabilities in challenging scenarios such as low-texture regions and reflections. Learning-based MVS methods can be categorized into regression and classification.Regression methods aim to regress the depth from the 3D cost volume using soft-argmin, which weights each depth hypothesis based on their cost. While this approach can achieve sub-pixel estimation of depth, it requires the model to learn a complex combination of weights under indirect constraints, leading to overfitting. On the other hand, classification methods, such as R-MVSNet, predict the probability of each depth hypothesis and select the one with the maximum probability as the final estimation. Although classification methods cannot directly infer the exact depth, they provide robustness through the use of cross-entropy loss and can reflect confidence in the estimated probability distribution.In this paper, we propose a unified representation for depth, called UniMVSNet, that combines the advantages of both regression and classification. We observe that estimating weights for all depth hypotheses is redundant, and the model only needs to perform regression on the optimal depth hypothesis, which contains the ground truth depth. To achieve this, our approach introduces a unified representation called Uniﬁcation, which applies loss directly on the regularized probability volume. We estimate the Unity, which represents the proximity of the optimal depth hypothesis to the ground truth, using at most one non-zero continuous target. This approach is more efficient than using offset alone.Additionally, we address the sample imbalance challenge in both category and hardness by proposing the Uniﬁed Focal Loss (UFL), which extends existing focal loss methods. Our proposed framework, UniMVSNet, demonstrates superior performance compared to previous MVS methods on DTU and Tanks and Temples benchmarks. The experimental results confirm that our approach achieves state-of-the-art performance in reconstructing dense 3D representations.