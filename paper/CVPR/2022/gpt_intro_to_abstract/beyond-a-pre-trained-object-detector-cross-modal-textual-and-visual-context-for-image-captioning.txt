This paper introduces the challenges and limitations of using a frozen pre-trained object detector to encode an input image for vision-and-language (VL) tasks, such as image captioning. While the object detector provides object-centric information, it may fail to encode other crucial information necessary for the target VL tasks, such as object predicates and image/scene level information. To address these issues, the paper proposes two modules: a cross-modal retrieval module that leverages the CLIP model to retrieve contextual text descriptions, and an image conditioning module that optimizes the conditional relationship between the detected objects and the input image. The proposed method improves the state-of-the-art image captioning model by a significant margin and provides comprehensive quantitative and qualitative analyses. The contributions of this paper include identifying the issues with using frozen pre-trained object detectors, proposing the cross-modal retrieval and image conditioning modules, and improving the performance of image captioning models.