This paper introduces the concept of natural language grounding in the context of finding specific moments in videos. The motivation for this task arises from various applications such as video search, editing, and aiding patients with memory dysfunction. Despite recent advancements in deep learning architectures, hidden biases have been identified in existing video-language grounding datasets. This paper addresses these limitations by introducing a novel large-scale dataset called MAD (Movie Audio Descriptions) that focuses on authentic, long-form videos with highly descriptive annotations. The data collection approach involves transcribing audio descriptions and removing actor speech, resulting in an "untrimmed video" setup. The MAD dataset contains over 384K natural language sentences grounded on more than 1.2K hours of video, offering new challenges for video-language grounding. The paper also presents a comprehensive empirical study that demonstrates the benefits of the MAD dataset as a benchmark for video-language grounding, highlighting the difficulties faced by current baselines in long-form videos. The contributions of this paper include the MAD dataset, a scalable data collection pipeline, and insights into the limitations of current video-language grounding approaches.