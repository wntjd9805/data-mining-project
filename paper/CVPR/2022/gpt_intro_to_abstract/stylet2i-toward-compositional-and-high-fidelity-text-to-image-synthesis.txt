Text-to-image synthesis is a task that involves generating an image based on a given input text. This has various applications in fields such as art creation, computer-aided design, and data augmentation. However, many previous methods overlook the aspect of compositionality in this task. In particular, when the input text contains underrepresented compositions of attributes, previous methods tend to generate images with poor quality. This is especially evident when the attributes are not commonly found in the training dataset. This issue leads to problems of bias and stereotyping, as the models inherit these biases from the dataset. To address this problem, we propose the StyleT2I framework, which aims to improve the compositionality of text-to-image synthesis. We introduce a CLIP-guided Contrastive Loss, which helps the network distinguish different compositions among different sentences. This loss is guided by CLIP, a pre-trained model on large-scale image-text pairs. Furthermore, our framework focuses on disentangled representations in the latent space of a generative model. Each disentangled representation corresponds exclusively to one attribute in the dataset, allowing for better compositionality. We also introduce a Semantic Matching Loss and a Spatial Constraint to identify attributes' latent directions that enable intended spatial region manipulations. During the inference stage, we employ Compositional Attribute Adjustment to correct any incorrect attribute synthesis by adjusting the latent code based on the identified attribute directions. To strike a balance between image-text alignment and image fidelity, we use a norm penalty. To evaluate the compositionality of our framework, we devise a new test split for the CelebA-HQ dataset that contains unseen compositions of attributes. Additionally, we design a new evaluation metric for the CUB dataset to ensure correct bird species synthesis. Our extensive quantitative results, qualitative results, and user studies demonstrate the advantages of our method in terms of both image-text alignment and fidelity for compositional text-to-image synthesis. In summary, our contributions include the development of StyleT2I, a framework that focuses on improving the compositionality of text-to-image synthesis. We propose novel loss functions and adjustment techniques to achieve better disentanglement of attributes and image synthesis. We also introduce new evaluation metrics to better assess the compositionality of synthesized images.