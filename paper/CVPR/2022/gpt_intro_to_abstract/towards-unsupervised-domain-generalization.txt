Deep neural network based approaches have achieved impressive results in tasks with similar training and test data distributions. However, they often struggle when faced with significant distribution shifts. In response to this challenge, domain generalization (DG) algorithms have been proposed that leverage labeled data from multiple domains during training to generalize well to unseen domains.While labeled data is crucial for current DG methods to learn domain invariant features, acquiring sufficient labeled data from multiple domains can be costly or unavailable. To address this issue, we introduce unsupervised domain generalization (UDG), a novel generalization problem that aims to learn discriminative representations that generalize well across domains without relying on labeled data. In UDG, models are trained with unlabeled heterogeneous data and then fine-tuned and evaluated on labeled data.Contrastive learning (CL) has shown promise in unsupervised learning by learning discriminative representations for downstream tasks. However, current CL methods are limited in their ability to handle severe distribution shifts across domains. Additionally, in UDG, the strong heterogeneity of the training data poses challenges to existing CL methods.To overcome these limitations, we propose Domain-Aware Representation LearnING (DARLING), a novel contrastive learning algorithm for UDG that combines the objectives of DG and contrastive learning. DARLING selects valid sources of negative samples based on the similarity between different domains, effectively forcing the model to ignore domain-related features and learn invariant representations across domains.In our experiments, we demonstrate that the proposed unsupervised pre-training protocol significantly improves generalization even with standard fine-tuning methods. We also show that DARLING outperforms state-of-the-art methods in both quantitative and qualitative experiments. Furthermore, we argue that unsupervised pretraining on heterogeneous unlabeled data is a more rational alternative to initialization with weights pretrained on ImageNet for DG, as it better captures the significant distribution shifts across domains encountered in most DG datasets.