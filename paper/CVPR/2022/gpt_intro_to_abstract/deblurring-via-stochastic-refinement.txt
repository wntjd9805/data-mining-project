Image deblurring is a well-known problem in the field of computer vision, with blurring artifacts caused by various factors such as camera shakes, moving objects, or an out-of-focus lens. Traditional methods for single image deblurring formulate the problem as a variational optimization task and seek a solution that satisfies certain image and blur kernel priors. However, with the emergence of deep learning, convolutional neural networks (CNNs) have become the standard approach for deblurring.Existing CNN-based deblurring models are typically trained using supervised learning with simulated sharp-blurry image pairs, minimizing pixel loss such as L1 or L2 to optimize for metrics like peak signal-to-noise ratio (PSNR). However, these metrics only partially correspond to human perception, and optimizing solely for them can result in reconstructions with visibly lower quality. To address this, recent works have introduced additional loss terms that aim to improve the perceived quality of generated images.Another approach to inverse problems like deblurring involves using deep generative models, and Generative Adversarial Networks (GANs) have been successfully applied to deblurring tasks, achieving competitive performance by training the network with adversarial losses to improve perceptual plausibility. However, existing GAN-based methods for deblurring are deterministic and often introduce artifacts not present in the original clean image, leading to high distortion.In this paper, the authors propose a different perspective on deblurring, treating it as a conditional generative modeling task. They introduce a "predict-and-refine" conditional diffusion model, which combines a deterministic data-adaptive predictor with a stochastic sampler to generate diverse samples from the posterior distribution. This approach allows for more efficient sampling compared to the standard diffusion model and produces realistic images without sacrificing pixel-level distortion. The authors claim that this is the first blind deblurring technique leveraging a deep generative model that is capable of producing diverse samples.The proposed method generates a variety of plausible and photo-realistic results, achieving state-of-the-art performance in terms of both distortion and perceptual quality on multiple standard datasets. Additionally, by aggregating different numbers of generated deblurred samples, the framework allows for convenient traversing of the Perception-Distortion curve without the need for retraining or fine-tuning. These results demonstrate the benefits of stochastic diffusion-based methods for deblurring and challenge the dominant strategy of producing deterministic reconstructions.