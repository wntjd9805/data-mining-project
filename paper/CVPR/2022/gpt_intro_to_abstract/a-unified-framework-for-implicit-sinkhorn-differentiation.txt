Computing matchings and permutations is a fundamental problem in computer vision and machine learning, with applications in pose estimation, 3D reconstruction, ranking, and sorting. The Sinkhorn operator, derived from entropy regularized optimal transport, is a popular tool for addressing this problem. It can be efficiently computed using iterative matrix scaling and is differentiable, making it suitable for integration into deep learning frameworks.The main challenge lies in computing the first-order derivative of a Sinkhorn layer. The standard approach of automatic differentiation has a high computational cost and memory demand, limiting its practicality for GPU processing. Recent works have explored implicit gradients as an alternative, which are computationally inexpensive but more complex to derive and implement. However, these approaches often lack generality and theoretical analysis.In this paper, we present a unified framework for implicit differentiation techniques for Sinkhorn layers. We provide a simple module that can be easily applied to the most general formulation. Our contributions include: 1) an efficient algorithm for computing gradients of a generic Sinkhorn layer, derived from first principles; 2) theoretical guarantees for the accuracy of the resulting gradients based on the approximation error in the forward pass; and 3) a PyTorch module that can be seamlessly integrated into existing approaches based on automatic differentiation, yielding improved results with reduced GPU memory usage.Our framework encompasses existing methods as special cases and provides a practical solution for gradient computation in Sinkhorn layers. Theoretical analysis supports the use of implicit gradients, enhancing the understanding and applicability of this approach.