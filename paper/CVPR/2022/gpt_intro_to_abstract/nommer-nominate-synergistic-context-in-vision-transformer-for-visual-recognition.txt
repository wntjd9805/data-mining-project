Abstract:Convolutional Neural Networks (CNNs) have long been the preferred method for computer vision tasks. However, recent advancements in the Vision Transformer (ViT) and its variants have challenged this status quo. ViTs have shown superior performance due to their ability to communicate global information and their content-dependent learning nature. However, the global dependency reasoning of ViTs is computationally expensive. To address this, recent works have introduced CNN-like inductive bias, building token relations within local windows to improve data efficiency. However, these local self-attention-based ViTs neglect the global contextual relations, particularly in the early stages. To overcome this limitation, a new approach called "global-local ViTs" has emerged, incorporating both local and global visual dependencies. However, existing global-local ViTs focus only on fusing global-local context without proper modulation, which can negatively impact recognition performance. Inspired by the human visual system's ability to simultaneously process peripheral and foveal vision, we propose a novel ViT framework called NomMer. In NomMer, locally aggregated context represents foveal vision, while globally aggregated context represents peripheral-like information. We introduce a context leverage strategy that dynamically nominates useful dependency information from local and global context. To address the challenges of achieving harmonious synergy between nominated context and preserving information without increasing computational costs, we propose the Synergistic Context Nominator (SCN) and the Compressed Global Context Aggregator (CGCA). Experimental results demonstrate the effectiveness of NomMer in image classification, object detection, and semantic segmentation tasks, achieving promising performance while reducing parameters. Our contributions include the novel SCN module, the CGCA module, and the NomMer framework that enables nominated global-local context for various visual data cases and tasks.