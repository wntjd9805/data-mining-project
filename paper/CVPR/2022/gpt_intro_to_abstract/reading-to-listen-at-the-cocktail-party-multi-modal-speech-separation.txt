This paper explores the concept of the "cocktail party effect," which is the ability of humans to selectively listen to a specific speaker in a noisy environment. The paper highlights how this ability is enhanced when information from multiple modalities, such as visual cues and natural language context, is available. The authors propose a framework called VoiceFormer for multi-modal speech separation and enhancement, which can isolate speech based on the text content of the speaker's utterance, their lip movements, or both. The framework can handle cues from multiple modalities, even if they are not temporally synchronized or have a common temporal rate. The authors discuss the advantages of this framework, such as robustness to temporal misalignments. Previous approaches have focused on either synchronous cues, static cues, or naive fusion of multiple cues. However, no unified framework exists for conditioning on asynchronous information or seamlessly combining multiple sources of information. The authors' contributions include enabling conditioning on asynchronous visual streams and enhancing speech based on textual input. These contributions are facilitated by a Transformer-based network, which operates directly on the waveform level. The network uses a U-Net architecture with the Transformer as the bottleneck, allowing for the modeling of a longer temporal context. The authors present the VoiceFormer architecture as a modern multi-modal speech enhancement solution that surpasses other baselines in speaker separation and speech enhancement tasks.