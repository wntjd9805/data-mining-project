Deep generative models, such as generative adversarial nets (GANs) and variational autoencoders (VAEs), have demonstrated their ability to model complex real-world datasets in various applications. While GANs transform a noise distribution into a desired space to approximate the real data distribution, VAEs optimize a lower bound on the data's log-likelihood. However, neither of these models provides an exact data likelihood. Autoregressive models and flow-based models optimize the exact log-likelihood of real data, with flow-based models offering the advantages of tractable log-likelihood, exact latent-variable inference, and parallelizability. Nonetheless, most flow-based models require multiple flow layers to capture non-linear long-range data dependencies. To address this limitation, attention mechanisms have emerged as a promising way to model these dependencies in deep neural networks. By selectively focusing on relevant information, attention mechanisms can efficiently model data dependencies. However, existing attention mechanisms in flow-based generative models only model dependency within a short range of the coupling layer. This paper introduces invertible attentions for flow-based models (AttnFlow), which can be introduced at any position in the flow feature maps. The key idea is to exploit a masked attention learning scheme to maintain invertibility and tractable Jacobian determinants in the flows. Two types of invertible attention mechanisms are proposed: invertible map-based (iMap) attention and invertible transformer-based (iTrans) attention, which encode different types of correlations on the flow feature maps. The introduced attention models are evaluated in the context of unconditional and conditional normalizing flow-based generative models for multiple image synthesis tasks.