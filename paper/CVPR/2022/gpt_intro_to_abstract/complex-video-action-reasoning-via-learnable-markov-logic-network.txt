Action recognition in video understanding has received significant attention in recent years, with the emergence of deep learning and the use of 3D convolutional networks (3D CNNs). These networks have revolutionized the field by capturing complex long-range semantic dependencies across video frames. However, deep neural networks still suffer from deficiencies in their interpretability and vulnerability to adversarial attacks. To address these issues, we propose an action reasoning framework that combines accurate performance with convincing interpretability. Inspired by cognitive science and neuroscience findings, we decompose complex actions into spatio-temporal scene graphs, which depict how a person interacts with surrounding objects over time. We address the challenges of automatically learning evolving patterns from data and conducting high-confidence inference under noisy information using a novel explainable action reasoning framework. We encode semantic-level state changes using first-order logic and generate logical rules using a recurrent policy network. To handle uncertainty, we adopt a Markov Logic Network (MLN) that assigns weights to logical rules. The framework is trained in two stages: rule exploration and weight learning. Through comprehensive experiments on challenging video benchmarks, our framework achieves excellent performance in action recognition and can be integrated with deep models, even with limited training data. Our contributions include improved interpretability, automatic rule generation, and superior performance.