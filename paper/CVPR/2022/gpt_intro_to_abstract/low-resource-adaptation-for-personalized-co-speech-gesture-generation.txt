The use of virtual reality technology to enhance human communication has become increasingly popular. In particular, there is interest in creating more realistic online communication platforms through the use of embodied virtual agents and remote avatars. These advancements align with the progression of speech-based technologies, such as intelligent personal assistants, which may also benefit from incorporating nonverbal communication. A key technical challenge in enabling immersive verbal and nonverbal communication through avatars is generating visual gestures based on speech and language input. Additionally, generating personalized visual gestures that reflect the unique behaviors of an individual presents an even greater challenge. The main objective of this paper is to develop a personalized gesture generation model, specifically for avatars. The authors propose an approach called DiffGAN, which efficiently personalizes co-speech gesture generation models from a high-resource source speaker to a low-resource target speaker. Unlike previous approaches that require access to source training data, DiffGAN identifies distribution shifts in crossmodal grounding relationships and updates parameters in a single layer of the source model. This allows for efficient adaptation with limited resources. The effectiveness of DiffGAN is evaluated using a diverse publicly available dataset. The results show consistent improvements in human judgment preference scores compared to strong baselines, as well as other quantitative improvements. Furthermore, DiffGAN is able to extrapolate to gestures in the target distribution without prior exposure to them in the source distribution.