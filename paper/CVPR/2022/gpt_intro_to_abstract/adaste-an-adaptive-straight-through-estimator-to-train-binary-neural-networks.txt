Deep neural networks (DNNs) are increasingly being deployed on computing hardware with limited resources, such as mobile and IoT devices. This has led to the emergence of research on training methods specifically designed for quantized DNNs. In this paper, we focus on DNNs with binary weights limited to {+1, -1}, as these networks mainly perform additions and subtractions for inference. Learning in such binary weight neural networks (BiNNs) can be formulated as an optimization program with binary constraints on the network parameters. However, addressing these binary constraints is challenging due to the combinatorial and non-differentiable nature of the optimization problem. Gradient-based methods are preferred for addressing these constraints, but various techniques have been proposed to convert the problem into a differentiable surrogate. One successful algorithm called BinaryConnect is based on straight-through estimators (STE) and ignores the sign mapping during gradient formation. In this work, we propose a new framework for training binary neural networks. We formulate the training problem as a bilevel optimization task and then relax it using an optimal value reformulation. Additionally, we introduce a novel scheme to calculate meaningful gradient surrogates for updating the network parameters. Our method resembles BinaryConnect but leverages an adaptive variant of the straight-through gradient estimator. This adaptive estimation is based on a data-dependent mapping and uses finite-difference surrogates to handle vanishing gradients. Our proposed method can be viewed as a mirror descent method with a data-dependent and varying distance-like mapping.