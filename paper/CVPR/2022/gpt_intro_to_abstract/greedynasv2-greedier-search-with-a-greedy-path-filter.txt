Neural architecture search (NAS) is a popular approach in deep learning that seeks to find the optimal architecture within a given space to improve performance. One-shot NAS is a particularly efficient method that uses a supernet to represent all candidate architectures and searches for the best one in a single trial. However, the large search space in NAS can pose challenges, as uniformly sampling paths from the supernet may lead to inappropriate training. Various strategies, such as fair sampling and Monte-Carlo tree search, have been proposed to address this issue. This paper focuses on the multi-path sampling strategy with rejection by GreedyNAS, which identifies good paths and updates them while discarding weak paths. GreedyNAS, however, faces limitations when dealing with larger search spaces and maintaining a candidate pool. To overcome these challenges, this paper introduces GreedyNASv2, which incorporates explicit search space shrinkage. By identifying weak paths with confidence, the search space can be greedily shrunk, allowing the supernet to focus on evaluating potentially good paths. The key is to learn a path filter that distinguishes weak paths from the entire search space. This is formulated as a Positive-Unlabeled (PU) learning problem, where weak paths serve as positive examples to be discarded and the remaining paths as unlabeled examples. The path filter is used to efficiently predict whether a new path is weak or not. Additionally, a path embedding is learned to represent paths, allowing similar operations to be merged. Experimental results on the ImageNet dataset demonstrate the effectiveness of GreedyNASv2, which outperforms baseline methods in terms of performance and search cost. The superior performance is further validated on a larger search space and compared to state-of-the-art NAS models. Comparison with uniform sampling and the multi-path sampler in GreedyNAS on the NAS-Bench-Macro benchmark further supports the effectiveness of GreedyNASv2.