Efficiently solving integer linear programs (ILPs) on parallel computation devices is a challenge in computer science. Current state-of-the-art ILP solvers do not benefit much from parallelism, making them non-amenable for execution on GPUs. This limitation hinders the application of ILPs in large structured prediction problems. In this paper, we propose a novel approach that utilizes operations with sufficient parallelism for implementation on GPUs. Our approach shows a balance between general applicability and efficiency for structured prediction problems. We achieve execution speeds comparable to hand-crafted specialized CPU solvers, but we are outperformed by specialized GPU solvers. Developing fast specialized solvers is time-consuming and needs to be repeated for each new problem class. Our work builds upon previous research on Lagrange decomposition and proposes massively parallelizable GPU-amenable routines for dual optimization and primal rounding, resulting in significant runtime improvements compared to existing approaches.