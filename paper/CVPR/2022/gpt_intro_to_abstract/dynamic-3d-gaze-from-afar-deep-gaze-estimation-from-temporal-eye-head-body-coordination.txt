This paper introduces a novel method for estimating the gaze direction of people in videos captured from a distance. The method leverages the temporal coordination of gaze, head, and body orientations in a Bayesian framework. Unlike other appearance-based methods, this method does not rely on the appearance of the eyes and can estimate gaze direction even when the person is facing away from the camera. A new dataset for gaze estimation in the wild with ground truth annotation is also introduced. The method is targeted towards surveillance and monitoring views, where the person may be at a distance of a few meters to 10 meters. Previous methods that estimate gaze from surveillance images approximate the gaze with head or body orientation, which is not accurate enough for most downstream tasks. The proposed method leverages the temporal coordination of gaze, head, and body orientations to estimate the person's dynamically changing 3D gaze direction. Gaze estimation is formulated as Bayesian prediction based on a learned angular-temporal prior of gaze direction conditioned on the head and body orientations. A cascade of two learned deep networks is used to model the complex gaze-head-body coordination. A new dataset of annotated surveillance videos of freely moving people taken from a distance is introduced, showcasing the accuracy and generalization capabilities of the method. The data and code for the method are available for download on the project web page.