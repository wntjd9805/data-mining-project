Humans have the ability to integrate multi-sensory data to understand the physical world around them. For example, when we hear the sound of an object falling, we can often determine its approximate location, size, and material based on the characteristics of the sound. This integration of auditory and visual information is known as asynchronous audio-visual integration. In this paper, we propose a new embodied AI challenge for multi-modal physical scene understanding, in which an agent must navigate within a room to locate and identify a physical object that has fallen to the ground. To support this task, we use the ThreeDWorld (TDW) simulation platform, which provides real-time synthesis of impact sounds, photo-realistic rendering, and physical simulation. We address the limitations of previous work by incorporating physical simulation of objects' interaction behavior and impact sounds, increasing the diversity of object locations and introducing distractor objects. We evaluate several agents on this benchmark and discuss the challenges they face in completing the task successfully. Our contributions include the introduction of the embodied physical sound source localization task, the augmentation of the TDW platform, the creation of the Fallen Objects dataset, and the development of baseline agents. We believe that models performing well on this challenge will contribute to the development of more intelligent robots that can infer physical information from multi-sensory data.