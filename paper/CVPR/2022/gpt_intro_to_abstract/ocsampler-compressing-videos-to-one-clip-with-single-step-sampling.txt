This paper addresses the challenge of effectively and efficiently recognizing actions or events in the context of the increasing volume of video data on social media platforms and online video content. Previous approaches have focused on using complicated temporal modules or 2D/3D-CNNs, but these methods are computationally expensive and not suitable for resource-constrained real-world scenarios. To mitigate this issue, researchers have designed lightweight modules, but these models treat all videos equally and lack adaptive frame selection mechanisms. This paper proposes a novel OCSampler framework that compresses videos into a single clip by evaluating clip-based rewards on a per-video basis in one pass. The framework utilizes a light-weight CNN to obtain coarse global information and a policy network trained with reinforcement learning to select the most valuable combination of clips. The proposed method achieves significant savings in computational cost and inference time while maintaining accuracy. Our approach also adaptively allocates computation based on the contributions of different temporal locations, further improving efficiency. Experimental results on benchmark datasets demonstrate that OCSampler outperforms state-of-the-art methods in terms of both accuracy and efficiency. The proposed framework can be extended to incorporate an adaptive frame budget to reduce computation for "easy" videos. The frames selected by our method can also be generalized to enhance the efficacy and efficiency of any classifier.