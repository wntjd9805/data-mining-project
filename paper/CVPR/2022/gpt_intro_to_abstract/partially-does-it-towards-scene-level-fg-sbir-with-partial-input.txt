The advent of touch-screen devices has fueled research on sketch analysis, particularly focusing on object-level sketches and their abstract-ness, creativeness, and applications in image retrieval and 3D synthesis/editing. More recently, there has been a shift towards scene-level analysis, which encompasses abstraction not only on individual objects but also on global scene configurations. However, it has been observed that a significant portion of scene sketches are "partial", meaning that they do not include all objects in the corresponding photos and contain significant empty regions due to object-level abstraction. This partialness poses a challenge for scene-level fine-grained sketch-based image retrieval (FG-SBIR). Existing models collapse as scene sketches become more partial, highlighting the need for a solution that can handle partial input and achieve state-of-the-art performance.Global Average Pooling, commonly used by existing FG-SBIR methods, is inadequate for preserving spatial scene configuration information. Alternative approaches such as computing distances between pairs of local features or using soft attentions independently in each domain do not effectively address the cross-modal gap between sketch and photo. Graph-based matching approaches also have limitations, including the requirement for bounding-box annotations and complexity in graph construction.To address the "partial" problem, this paper proposes a solution based on cross-modal region associativity. Classic transportation theory, specifically optimal transport (OT), is used to model the region associativity in a partially-aware fashion. Additionally, intra-modal scene configurations are captured using region adjacency matrices. During cross-modal comparison, the differences between these matrices are computed, and scalar values are obtained for each corresponding region to use with OT. This approach effectively accounts for differences in scene configurations and the local partialness of sketches.The contributions of this paper are as follows: first, the significant prevalence of partial scene-level sketches is demonstrated, along with the collapse of existing FG-SBIR models as scene sketches become more partial. Second, a solution is proposed that models partially-aware cross-modal region associativity using OT. Third, the OT approach is enhanced by capturing intra-modal spatial relationships for partial scene sketches. Finally, the proposed method is robust to partial input and achieves state-of-the-art performance on existing scene sketch datasets.