Neural Architecture Search (NAS) has gained considerable attention for its potential in automating the process of designing architectures. However, existing methods based on reinforcement learning and evolutionary algorithms suffer from high computational overheads. To address this issue, various approaches have been proposed, including performance estimation, network morphisms, and one-shot architecture search methods. One-shot methods, in particular, utilize weight sharing techniques and train a supernet covering all candidate sub-networks, improving search efficiency.Although differentiable architecture search methods, such as DARTS, offer simplicity and computational efficiency, they still face challenges in terms of robustness and architecture generalization. DARTS often suffers from performance collapse, where the searched architecture accumulates parameter-free operations, leading to degradation in performance. Several methods have been proposed to address this issue, but explicit regularization of architecture parameters optimization remains understudied. Additionally, the optimal architecture obtained on a specific dataset may not generalize well to other datasets.To tackle these challenges, we propose a Beta-Decay regularization approach for DARTS-based methods. Unlike previous methods that regularize learnable architecture parameters, we apply regularization on activated architecture parameters. This regularization mitigates unfair competition among operations, solves the domination problem of parameter-free operations, and ensures the generalization ability of the searched architecture. We provide theoretical analysis supporting the effectiveness of the proposed approach.Extensive experiments on different search spaces and datasets validate the efficacy of our method. Our search scheme demonstrates properties such as continuously rising performance during the search process and the ability for the searched architecture to perform well on various datasets. Moreover, our approach only requires a single search on a proxy dataset, but yields promising performance on multiple datasets.