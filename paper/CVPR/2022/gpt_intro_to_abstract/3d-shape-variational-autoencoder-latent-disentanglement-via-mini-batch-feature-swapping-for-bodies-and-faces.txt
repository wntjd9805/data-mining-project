This paper addresses the task of generating 3D human faces and bodies, which is currently performed manually by skilled artists or with semi-automated avatar design tools. Existing generative models have limitations in flexibility and the creation of local features. The focus of this study is on achieving disentanglement among sets of generative coefficients controlling the identity of a character. A disentangled latent representation is defined as one where changes in one latent unit affect only one factor of variation while being invariant to changes in other factors. The authors propose using variational autoencoders (VAEs) as deep-learning-based generative models due to their superior representation capabilities and stable training procedures. Previous work has attempted to address the latent disentanglement problem for 3D shapes, but there is still a need for proper disentanglement of identity features. The authors propose a method that leverages feature swapping in the input data to encourage a more disentangled, interpretable, and structured representation. They also define a VAE architecture for generating 3D meshes and introduce a novel loss function. The proposed method is applicable to different network configurations and can be used with voxel- or point-cloud-based generative models. The key contributions of this approach are the new mini-batching procedure, the novel loss function, and the creation of a 3D-VAE capable of generating 3D meshes from a more interpretable and structured latent representation.