Deep Neural Networks (DNNs) have greatly advanced computer vision applications in recent years, such as image recognition, semantic segmentation, object detection, and cross-modal retrieval. However, collecting high-quality annotations for large-scale datasets is expensive and time-consuming in real-world scenarios. To address this challenge, researchers have proposed various methods, such as using search engines or crowdsourcing, to obtain training datasets with low-quality labels. However, DNNs often overfit these noisy labels, resulting in poor performance. Therefore, it is crucial to develop methods to improve the robustness of DNNs against noisy labels.Early robust learning methods attempted to model noisy labels using noise transition matrices and refine losses based on them. However, estimating the noise transition matrix accurately is challenging, as it relies on either prior knowledge or a subset of high-quality labeled data. Recent methods have focused on selecting high-confidence samples as clean data and filtering out others based on human-defined rules. While these methods filter out noisy samples, they fail to fully utilize the hidden information in these samples, leading to lower robustness in DNNs. Alternatively, a series of methods, known as relabeling-based methods, have emerged, which relabel noisy samples using the model's predictions. However, this approach increases computation cost and may introduce confirmation bias, where prediction errors accumulate and harm performance.In this paper, we propose a method called LaCoL (Latent Contrastive Learning) to improve the robustness and generalization of DNNs by leveraging the implicit negative correlations in noisy data. LaCoL randomly selects negative samples that do not belong to the same category as the anchor image and constructs negative correlations between them. We then employ a weakly-supervised contrastive learning method in the latent metric space to capture these negative correlations. We also incorporate both weak and strong augmentations during training to enhance model generalization. Additionally, we introduce a cross-space similarity consistency regularization to align the label space and metric space, making the learned negative correlations in metric space more effective in improving classification performance.Experimental results demonstrate that LaCoL outperforms state-of-the-art methods on both synthetic and real-world noisy datasets. Our method effectively improves the robustness and generalization of traditional DNNs by exploiting the hidden negative correlations in noisy data.