Sign languages are rich visual languages with intricate co-articulated movements of manual and non-manual features. Sign Language Production (SLP) aims to automatically translate spoken language sentences into sign language sequences. Previous deep-learning approaches to SLP have either produced isolated sequences that ignore co-articulation or continuous sequences that lack articulation. These methods also struggle to generalize beyond limited domains. This paper proposes an SLP method that translates spoken language to gloss sequences and models the temporal co-articulation between signs. A Frame Selection Network is introduced to learn the optimal subset of frames representing a continuous signing sequence. A video-to-video synthesis model called SIGNGAN is then used to generate photo-realistic sign language videos. To improve hand image synthesis, a keypoint-based loss is proposed. Controllable video generation is achieved by modeling a multi-modal distribution of sign language videos in different styles. Extensive evaluations are conducted, showing the effectiveness of the proposed approach in natural signing motion, back translation performance, synthesis quality, and understandability. The contributions of this paper include the first SLP model for large-scale sign language sequences, a innovative Frame Selection Network, SIGNGAN for photo-realistic video generation, and extensive user evaluation demonstrating the superiority of the proposed method.