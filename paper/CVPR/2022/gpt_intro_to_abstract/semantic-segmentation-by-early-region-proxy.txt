Semantic segmentation is a critical task in computer vision that has been primarily performed using Convolutional Neural Networks (CNNs) in the deep learning era. However, CNNs have certain limitations for semantic segmentation, including limited context modeling and coarse predictions. Many previous works have focused on addressing these issues, improving context modeling and fine-grained feature prediction. Recently, the Transformer architecture, originally from natural language processing, has gained attention in the computer vision community. Vision Transformers (ViTs) have been utilized as backbones for semantic segmentation tasks, leveraging their attention mechanism for better context learning. However, in these models, the ViT mainly serves as a feature extractor and does not fully exploit its sequence-to-sequence encoder capabilities.In this paper, we propose a novel approach to semantic segmentation that aims to be closer to the essence of the task. Instead of operating on regular grids of features, we interpret the image as a set of interconnected regions, where each region represents a group of adjacent pixels with similar semantics. We introduce a RegProxy approach, which learns regions at an early stage, models inter-region relations using Transformer, and encodes regions in a sequence-to-sequence fashion. We also develop a mechanism to describe region geometrics and ensure the tessellation of the entire region set, enabling per-region prediction for semantic segmentation. The entire process is end-to-end trainable and differentiable.Our model is built on ViTs for image classification by adding minimal overhead. Through extensive experiments, we demonstrate the competitive performance and efficiency trade-off of RegProxy across various model capacities and multiple datasets. Our approach achieves state-of-the-art results while providing a more accurate and fine-grained segmentation compared to traditional per-pixel prediction methods.