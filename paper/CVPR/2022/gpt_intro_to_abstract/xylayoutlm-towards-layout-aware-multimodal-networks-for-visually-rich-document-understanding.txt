Significant progress has been made in natural language processing and visual understanding, but less attention has been given to the multimodal document understanding domain. The Visually-Rich Document Understanding (VRDU) task combines image, text, and layout information from scanned/digital-born documents. This technology has applications in report/receipt understanding, automatic form filling, and document relation extraction. Researchers have developed pipelines for tackling this task, including textual-based, convolution-based, and GCN-based methods. However, existing methods still face limitations in terms of reading orders and handling sequences of variable lengths. In this paper, we propose the Augmented XY Cut method to sort input tokens and generate proper reading orders in VRDU tasks. We also introduce the Dilated Conditional Position Encoding as a position embedding generator to adaptively process textual and visual tokens. Our XYLayoutLM model achieves competitive performance on semantic entity recognition and relation extraction tasks in VRDU.