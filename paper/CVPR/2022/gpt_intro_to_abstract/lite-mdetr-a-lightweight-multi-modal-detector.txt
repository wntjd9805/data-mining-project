In recent years, multi-modal models combining vision and language data have shown promising results in various tasks. However, these models often have large model complexity and require computationally costly pre-training on large-scale datasets. This hinders their deployment on resource-limited platforms such as mobile devices. In this paper, we address the need for efficient multi-modal models by proposing a lightweight approach called Lite-MDETR. We identify that linear transformer layers contribute significantly to the model size, and we introduce Lightweight Dictionary Lookup Transform (DLT) layers to replace these costly layers. Our experiments demonstrate that Lite-MDETR using DLT achieves comparable accuracy to the original model while reducing the model size by a factor of 2.03×. Additionally, the incorporation of orthogonal quantization further reduces the model size by 1.94×. Notably, these results are achieved without the need for pre-training on large-scale datasets. This work opens up possibilities for the deployment of multi-modal models on resource-limited platforms, such as mobile devices.