Visual tracking is a fundamental task in computer vision, and with the increased use of unmanned aerial vehicles (UAVs), tracking-based applications are rapidly developing in the aerial domain. However, aerial tracking faces challenges such as motion blur, camera motion, and occlusion, and the limited computational resources of aerial platforms hinder the deployment of time-consuming tracking methods. To address these challenges, an ideal tracker for aerial tracking needs to be robust and efficient. This paper introduces a comprehensive framework called TCTrack for exploiting temporal contexts in Siamese-based networks for aerial tracking. The framework incorporates temporal information at two levels: features and similarity maps. At the feature level, an online temporally adaptive convolution (TAdaConv) is proposed, which dynamically adjusts the convolution weights based on previous frames to extract features. This results in a temporally adaptive convolutional neural network (TAdaCNN) that improves tracking performance without significantly affecting the frame rate. At the similarity map level, an adaptive temporal transformer (AT-Trans) is introduced to refine the similarity map using encoder-decoder architecture. The encoder produces temporal prior knowledge by integrating previous priors with the current similarity map, and the decoder refines the similarity map based on the produced temporal prior knowledge in an adaptive way. The effectiveness and efficiency of the TCTrack framework are evaluated on standard aerial tracking benchmarks and compared with state-of-the-art trackers. The results demonstrate competitive accuracy and precision, with a high frame rate on PC and impressive stability and robustness on NVIDIA Jetson AGX Xavier for real-world deployment. The proposed framework effectively models temporal contexts in Siamese-based aerial tracking, improving tracking performance in complex aerial conditions.