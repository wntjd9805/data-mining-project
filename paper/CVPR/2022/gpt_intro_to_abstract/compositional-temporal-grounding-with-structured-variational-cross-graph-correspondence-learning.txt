Understanding rich and diverse activities in videos is a primary objective of video understanding. Existing works in activity recognition and localization have limitations in scaling to various complex activities due to their reliance on pre-defined classes. To address this, the use of systematic compositionality in human language has been proposed as a solution, allowing for the formation of novel compositions to describe unseen activities. Temporal grounding in videos, the task of identifying specific moments in a video that correspond to a given query sentence, has gained attention as a means of achieving compositional generalization. However, current temporal grounding datasets do not specifically test for this ability. To address this gap, we introduce a new task called Compositional Temporal Grounding and construct two datasets to measure the compositional generalizability of existing methods. Our evaluation of modern state-of-the-art temporal grounding models reveals that they fail to achieve compositional generalization, and are insensitive to word order. These observations suggest that current models are driven by superficial correlations, motivating the proposal of a novel VarIational croSs-graph reAsoning (VISA) framework for compositional temporal grounding. This framework explicitly models the semantic structures of video and language, and infers fine-grained correspondence between them. Results demonstrate the superiority of the VISA approach in achieving compositional generalizability.