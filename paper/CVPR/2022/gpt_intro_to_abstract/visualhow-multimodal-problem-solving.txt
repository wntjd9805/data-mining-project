The field of vision-language studies has made significant progress in developing systems that can understand and generate natural language information based on visual input. However, current models primarily focus on specific vision-language tasks and perform well only on standardized benchmarks, limiting their applicability to real-life problem-solving scenarios. In this paper, we introduce a novel research problem called VisualHow, which aims to generate step-by-step vision-language descriptions of how to solve a problem. We propose a large-scale dataset and systematic evaluation of different modeling approaches for VisualHow. Our goal is to enable the development of intelligent systems that can help humans solve real-life problems by providing both textual descriptions and visual illustrations. The VisualHow dataset contains diverse categories of problems, multimodal solutions described in multiple steps, and fine-grained annotations such as solution graphs and multimodal attention. This dataset enables several new vision-language tasks and our experiments provide insights and suggestions for improving model performance. The contributions of this work include the VisualHow study, a new dataset, and experiments on various aspects of the VisualHow problem. This research lays the foundation for developing novel vision-language methods and advancing multimodal understanding of real-life problems and solutions.