Recent advances in self-supervised visual representation learning using deep neural networks and contrastive learning have shown significant progress. However, these methods have not yet been successful in learning temporal correspondences from video, which is a fundamental problem in computer vision. Learning visual correspondence across space and time is crucial for various vision tasks such as video object tracking, video object segmentation, and optical flow estimation. Collecting a large-scale visual correspondence dataset is labor-intensive and relies on human annotators, making it challenging to obtain high-quality labels. To address this problem, self-supervised visual correspondence learning has emerged as a solution, leveraging raw videos for training. Most existing methods for self-supervised visual correspondence learning rely on noisy labels or ambiguous supervisions, leading to local optima and overfitting. In this paper, we propose a new self-cycle regularization method that incorporates a cycle-consistency constraint for each edge in the graph, preventing early-stage overfitting. We also introduce a Bayesian framework that learns visual representations for dense correspondences using a complete space-time graph from an input video. Furthermore, we present a domain contrastive loss that discriminates representations of different videos in the embedding space, enabling comprehensive representations for multiple domains. Our method outperforms state-of-the-art algorithms and converges much faster on various video benchmarks. Our contributions include the introduction of self-cycle edges, a mixture of sequential Bayesian filters, batch-wise domain contrastive loss, and improved performance on video tasks.