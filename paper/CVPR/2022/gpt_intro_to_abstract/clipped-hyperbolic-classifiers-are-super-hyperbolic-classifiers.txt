Many datasets contain hierarchical data structures, such as WordNet and social networks. Representing these hierarchies in Euclidean space does not capture their semantic or functional resemblance. Hyperbolic space, which has constant negative curvature, has been utilized to embed hierarchical data with low distortion due to its exponential volume growth. Hyperbolic space has been successfully applied in various domains, including analyzing hierarchical structure in single-cell data and embedding complex networks. Recent algorithms operate directly in hyperbolic space to leverage its representational power. Hyperbolic Neural Networks (HNNs) are an alternative to Euclidean neural networks (ENNs) that adopt a hybrid architecture, combining Euclidean feature extraction with hyperbolic classification. While HNNs outperform ENNs on datasets with explicit hierarchies, they underperform on standard classification benchmarks with flat or non-hierarchical structures. Existing improvements on HNNs mainly focus on reducing parameters or adding different types of neural network layers, without understanding why HNNs perform worse on standard benchmarks. The main issue is that HNNs' hybrid architecture leads to vanishing gradients during training, as the hyperbolic embeddings are pushed towards the boundary of the hyperbolic space. To address this problem, the paper proposes a simple but effective solution: clipping the Euclidean feature magnitude during training, preventing embeddings from approaching the boundary. Experimental results demonstrate that these clipped HNNs outperform standard HNNs and achieve performance on par with ENNs on standard benchmarks, while also exhibiting better adversarial robustness and out-of-distribution detection. The paper contributes by providing a detailed analysis of the vanishing gradient issue in HNNs, proposing a feature clipping solution, and demonstrating the improved performance of clipped HNNs on various benchmarks.