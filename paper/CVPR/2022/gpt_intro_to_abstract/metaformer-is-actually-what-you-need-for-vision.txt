Transformers have become widely popular in computer vision, particularly for image classification tasks. The transformer encoder, consisting of an attention module and other components, has been a key factor in their success. While attention-based token mixers have traditionally been used, recent studies have explored alternative token mixers, such as spatial MLPs and Fourier Transform. These studies have achieved competitive performance, challenging the dominance of attention as the token mixer. In this paper, we introduce a general architecture called MetaFormer, which abstracts the transformer into a framework where the token mixer is not specified. We hypothesize that MetaFormer is more essential for achieving competitive performance than specific token mixers. To verify this, we propose PoolFormer, which uses a simple non-parametric operator (pooling) as the token mixer in the MetaFormer architecture. Surprisingly, PoolFormer achieves highly competitive performance, even outperforming well-tuned vision transformer and MLP-like models. We argue that MetaFormer is crucial for vision models to achieve competitive performance and suggest future research focus on improving MetaFormer rather than specific token mixers. Additionally, we evaluate PoolFormer on multiple vision tasks and find it performs competitively compared to state-of-the-art models using sophisticated token mixers. PoolFormer can serve as a strong baseline for future MetaFormer architecture design.