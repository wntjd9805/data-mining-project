Videos present a unique opportunity for understanding beyond what can be gleaned from a single image. This paper delves into the question of what aspects of video tasks require video analysis rather than image-level understanding. Previous research has explored this question in the context of action classification in videos using convolutional models for image classification. The study found that temporal understanding was not necessary to perform well on certain video datasets. The recent development of self-supervised image-language models provides an opportunity to reevaluate this question in the context of video-language tasks. The authors propose the atemporal probe (ATP) model as a stronger baseline for video-language tasks, leveraging frozen self-supervised image-language models to capture image-level understanding. Through applying ATP to various video-language datasets, the authors discover that many benchmarks can be well-addressed with single-frame image understanding, even outperforming state-of-the-art video-language models. The ATP model also aids in identifying questions that require temporal and causal understanding. Furthermore, insights from ATP can contribute to improving dataset designs and video-level temporal modeling. The paper concludes by highlighting key avenues for future development of video-language datasets and models based on the ATP technique.