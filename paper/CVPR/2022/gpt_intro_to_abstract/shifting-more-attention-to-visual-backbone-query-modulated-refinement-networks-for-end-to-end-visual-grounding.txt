This paper introduces the concept of visual grounding, which involves localizing objects in images based on natural language queries. Visual grounding has gained attention in the field of computer vision and machine learning due to its potential applications in various tasks such as visual question answering, language navigation, and image captioning. Early methods of visual grounding focused on extending object detection architectures, both one-stage and two-stage methods. However, the complexity of fusion modules in two-stage methods limits their ability in multimodal reasoning. More recently, Transformer models have been applied to conduct multimodal reasoning more succinctly. However, existing methods do not pay enough attention to the visual backbone, which plays a crucial role in effective multimodal reasoning. The visual backbone determines whether all integral visual content in the image is successfully extracted for matching the query text. The difference between the visual grounding task and the pre-training task of the backbones may lead to an inconsistency in visual features. To address this issue, the authors propose a query-modulated refinement network (QRNet) that adjusts the visual feature maps with the guidance of query text. QRNet is designed based on Swin-Transformer and a novel Query-aware Dynamic Attention (QD-ATT) mechanism, which computes query-dependent spatial and channel attentions for refining visual features. The proposed QRNet significantly outperforms existing methods on several datasets. The main contributions of this paper are the proposal of QRNet to address the inconsistency issue, the introduction of the QD-ATT mechanism for refining visual features, and the development of a flexible visual grounding framework based on the query-modulated refinement network.