Deep neural networks (DNNs) are vulnerable to imperceptible adversarial examples, which has led to extensive study on model robustness. One interesting property of these adversarial examples is their transferability between different models. It is commonly believed that the white-box strength of an attack is at odds with its transferability. Previous works have suggested that longer iterations in the iterative fast gradient sign method (I-FGSM) lead to overfitting and lower transfer rates. However, we show that the lower transfer rates of I-FGSM can be partially attributed to the lower perturbation magnitude and increasing the number of iterations actually improves transferability, eventually outperforming the fast gradient sign method (FGSM). We also introduce a new metric called interest class rank (ICR) to evaluate the top-k attack strength, which provides a unified perspective on attack strength. We propose a new loss function, relative cross-entropy (RCE) loss, that prioritizes maximizing the distance from the interest class, leading to stronger top-k attacks in both white-box and transferable black-box settings. Our work challenges the conventional beliefs on attack strength and transferability, and provides a comprehensive evaluation of adversarial examples' top-k strength and transferability using ICR.