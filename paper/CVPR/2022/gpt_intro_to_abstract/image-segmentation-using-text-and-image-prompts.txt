This paper introduces the CLIPSeg model, a flexible zero/one-shot segmentation system that can address multiple tasks simultaneously. The ability to generalize to unseen data is important for applications in artificial intelligence, such as household robots understanding prompts involving unseen objects or uncommon expressions. However, computer vision systems struggle with this form of inference. Image segmentation requires predicting the class and location of each pixel, and classical semantic segmentation models are limited to segmenting the categories they have been trained on. This paper explores different approaches that extend the capabilities of segmentation models, including generalized zero-shot segmentation, one-shot segmentation, and referring expression segmentation. The CLIPSeg model is capable of segmenting based on arbitrary text queries or example images, and it utilizes the pre-trained CLIP model as a backbone. A thin conditional segmentation layer is trained on top of CLIP to relate activations inside CLIP with output segmentation. The paper employs a generic binary prediction setting, distinguishing between foreground matching the prompt and background. The CLIPSeg model achieves competitive performance in low-shot segmentation tasks and can generalize to unseen classes and expressions. The main technical contribution of this paper is the CLIPSeg model, which extends the CLIP transformer for zero-shot and one-shot segmentation tasks with a lightweight transformer-based decoder. The segmentation target can be specified through text or image, allowing for a unified model for multiple benchmarks. The model is able to generalize to new queries involving unseen words and explores various forms of visual prompt engineering. The paper also evaluates the generalization of the model to novel forms of prompts involving affordances.