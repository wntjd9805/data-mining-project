The estimation of 3D room layout from indoor RGB images plays a crucial role in understanding 3D scenes. Panoramic images with a wider field of view provide contextual information about the entire room. Recent advancements in deep neural networks and the use of panoramic cameras have led to significant achievements in estimating room layouts from single panoramas. However, previous approaches often neglect the planar attributes of walls and use post-processing techniques that oversimplify boundaries. This paper proposes an efficient network called LGT-Net for panoramic room layout estimation. It utilizes a feature extractor and a specially designed Transformer architecture called SWG-Transformer for sequence processing. The proposed network directly predicts room height and floor boundaries using two output branches. Loss functions are introduced to incorporate omnidirectional and planar geometry awareness. Additionally, a relative position embedding enhances the spatial identification ability for panoramas. Extensive experiments on benchmark datasets demonstrate the effectiveness of the proposed approach, outperforming state-of-the-art techniques. The contributions of this work include the representation of room layout using horizon-depth and room height, the use of Transformer as a sequence processor, and the design of a relative position embedding for enhanced spatial identification.