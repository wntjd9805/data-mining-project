Deep learning has become a popular approach for various real-world applications. However, trained models are prone to theft and adversaries can use query data from publicly available APIs to perform model extraction attacks. Existing methods for mitigating such attacks fall into two categories: watermarking techniques and fingerprinting techniques. Watermarking involves introducing a backdoor into the model to detect piracy, but this can result in a drop in utility and potential for attackers to inject their own backdoors. Fingerprinting, on the other hand, relies on capturing the decision boundary of the model, but current approaches based on adversarial examples have limitations. In this paper, we propose a more effective model extraction detection scheme using Universal Adversarial Perturbations (UAPs) that capture the global characteristics of the decision boundary. We address the challenges of obtaining UAPs without white-box access and distinguishing between piracy and homologous models. We propose a fingerprinting function and an encoder to map fingerprints into a joint representation space. Our framework demonstrates high detection rates and the ability to detect post-modified piracy models. We also employ contrastive learning to address similarity gaps between homologous and piracy models. Overall, our framework provides a more accurate and robust protection against model extraction attacks.