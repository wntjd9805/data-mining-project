This paper presents a research work focusing on active speaker localization (ASL) in augmented reality (AR) scenarios. The goal is to detect and track the spatio-temporal location of all active speakers within and beyond the camera's field of view (FOV) from an egocentric perspective. An egocentric AR device with outward-looking cameras and microphones is used to capture audio-visual data from the wearer's point of view. The proposed system employs a novel multi-channel audio-visual deep network that localizes active speakers from any direction on the sphere. The network combines audio features extracted from a device-mounted microphone array with video frames to compute a joint representation and generate an active speaker map. The system accounts for changing speaker orientations, speaker movements in and out of the visual FOV, and turn-taking in conversations. It is designed to be agnostic to the number of microphone channels and suitable for different AR devices. The proposed network is trained using a dataset called EasyCom and achieves improved results compared to previous audio-visual approaches for active speaker detection. Its real-time capabilities make it suitable for applications in the AR domain, enabling the localization and spatialization of audio-visual activity in a world-locked frame of reference.