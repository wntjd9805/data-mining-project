Autonomous driving relies on robust dynamic object perception and accurate vehicle decision-making. Camera and Lidar sensors are commonly used for surrounding object recognition, with cameras providing visual features and Lidar offering high-resolution point clouds.However, radar sensors have unique advantages in automotive applications. Radar operates at a wavelength that can penetrate adverse weather conditions and is resilient to light conditions. Additionally, radar is cost-effective and reliable compared to Lidar. Despite these advantages, radar's angular resolution is still lacking, limiting its object localization and identification capabilities.Previous research has focused on leveraging multimodal sensor fusion, such as combining radar and Lidar or developing deep learning approaches. In this paper, our approach aims to enhance radar perception solely using radar information, reducing resource requirements and simplifying the sensor synchronization process.We propose a method that utilizes ego-centric bird-eye-view radar point clouds presented in a Cartesian frame. By leveraging temporal information, we assume that objects detected within successive frames by radar share consistent attributes, such as existence, length, and orientation. We introduce temporal relational layers that explicitly handle object-level relations across frames, linking similar objects and smoothing features. We then infer object heatmaps and attributes based on the updated feature representation.Our major contributions include enhancing radar perception with temporal information, designing a customized temporal relational layer, and evaluating our method on the Radiate dataset for object detection and multiple object tracking. Through comprehensive comparisons with baseline methods, we demonstrate the consistent improvements achieved by our approach.