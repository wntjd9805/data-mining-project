Most existing zero-shot learning (ZSL) methods in image classification and action recognition only utilize unimodal input, such as images. However, humans rely on multi-modal sensory inputs to gather context and capture complementary information. This paper addresses the challenging task of (generalized) ZSL with multi-modal audio-visual data by leveraging the natural alignment of audio and visual information in videos. Previous studies have explored zero-shot video recognition using multi-modal inputs, but they suffer from the overlap of classes between training and validation sets, hindering generalizability. To overcome this, the paper proposes three benchmark datasets from the VGGSound, UCF101, and ActivityNet datasets, which can serve as a unified and challenging playground for ZSL research in the audio-visual domain. The paper introduces the Audio-Visual Cross-Attention (AVCA) framework, which aligns an audio-visual representation with textual label embeddings using cross-modal attention. AVCA is computationally lightweight and efficient, extracting audio and visual features from pretrained networks instead of raw data. The framework is trained using novel loss functions based on triplet losses and regularisation to preserve salient unimodal information in the learnt multi-modal representations. Experimental results demonstrate that AVCA achieves state-of-the-art performance on the proposed benchmarks, outperforming unimodal and multi-modal ZSL methods. The paper also provides a qualitative analysis of the learnt multi-modal embedding space, showing well-separated clustering for both seen and unseen classes. Overall, the contributions of this work include introducing benchmark datasets, proposing the AVCA framework, and demonstrating its superior performance compared to existing methods.