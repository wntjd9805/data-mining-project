Deep neural networks have shown impressive performance in various tasks, but acquiring sufficient labeled data for each new task is costly and inefficient. Transfer learning, specifically unsupervised domain adaptation, has gained significant attention as a way to reduce the burden of labeling. In this setting, labeled datasets from related but different source domains are used to recognize unlabeled instances in a target domain. Existing unsupervised domain adaptation methods often require access to the raw source data and employ techniques like domain adversarial training or maximum mean discrepancy minimization to align source and target features. However, in scenarios where the raw source data is missing or cannot be shared due to privacy policies, recent studies have attempted to use trained models instead of raw data as supervision, yielding promising adaptation results. However, these methods still rely on elegantly trained source models and raise concerns related to privacy and the suitability of the source network for low-resource target users. This paper focuses on a challenging scenario where the source model is trained without additional techniques and is provided to the target domain as a black-box predictor. To address this problem, a new adaptation framework called DIstill and fine-tuNE (DINE) is proposed. DINE distills knowledge from source model predictions and fine-tunes the distilled model with target data to achieve adaptation. Adaptive label smoothing and structural regularizations are introduced to improve the distillation process. DINE demonstrates superiority over baselines in various benchmarks, including single-source, multi-source, and partial-set unsupervised domain adaptation tasks. It is safe, efficient, and does not rely on adversarial training or data synthesis, making it converge faster than existing methods.