The goal of this paper is to address the limitations of current domain generalization methods in machine learning. While deep learning approaches perform well when training and testing data are from the same distribution, they struggle when the test data comes from a different distribution. Existing methods in domain generalization, which aim to capture invariant properties or mechanisms between domains, have shown promising results in toy instances but fail to outperform basic empirical risk minimization (ERM) approach on realistic instances. To improve domain generalization methods, it is important to understand the failure modes and why they occur. In this paper, the authors characterize the general failure modes of domain generalization methods, including training set underfitting, test set inseparability, training-test misalignment, and classifier non-invariance. They develop tools to measure the contribution of each failure mode to the model's total error. Furthermore, the authors identify two common patterns of generalization failures: achieving domain-invariant representations across training domains but not on unseen domains, and increasing domain invariance across all domains but degrading the representations of unseen domains. Additionally, the authors demonstrate that by fixing the representations, it is possible to isolate the failure of classifier non-invariance and significantly improve generalization even with basic algorithms. These findings suggest that domain-invariant representations are not necessarily required for successful domain generalization. Overall, this paper provides insights into the failure modes of current domain generalization methods and proposes approaches to improve their performance.