Vision-and-Language pre-trained (VLP) models have revolutionized research on vision-and-language tasks, but rely on large-scale aligned image-text corpora which are costly to create. This paper explores the possibility of unsupervised V+L pre-training without parallel text and images (UVLP). The authors compare the performance of a pre-trained model using round-robin input with that of a model using joint image-text input. They also analyze the correlation between alignment of image-text pairs and the performance of downstream tasks. Based on their analysis, the authors propose Unsupervised Vision-and-Language Pre-training via Retrieval-based Multi-Granular Alignment (µ-VLA). This approach constructs a weakly-aligned image-text dataset and gradually learns multi-granular alignment to bridge the gap between modalities. Experimental results demonstrate the effectiveness of µ-VLA, achieving state-of-the-art performance on downstream tasks. The authors also validate the approach under a more realistic setting, showing its robustness. Overall, this paper provides insights into unsupervised V+L pre-training and proposes a novel retrieval-based approach that outperforms existing methods.