Video inpainting is an important task that involves filling in corrupted regions across video frames to maintain visual coherence. It has a wide range of applications, including object removal, watermark removal, and video retargeting. Traditional methods for video inpainting have relied on leveraging complementary content from neighboring frames, but recent advancements in deep learning have led to the development of new video inpainting techniques.These deep learning-based methods can be categorized into two groups: pixel-based methods and flow-based methods. Pixel-based methods directly synthesize pixels in the video frames, while flow-based methods complete the optical flows to guide the warping procedure from valid regions to fill in the corrupted regions. Flow-based methods have the advantage of preserving high-frequency details in the inpainted video frames, resulting in more visually pleasing results.However, existing flow-based methods still suffer from inaccurate flow completion, leading to errors in warping and inpainting. Additionally, there is often a style difference between frames, which can cause spatial incoherence between the valid regions and the warped regions. To address these challenges, this paper introduces the concept of inertia as a prior for flow completion in local temporal sequences. Inertia ensures strong coherence of optical flows within a window, leading to more accurate flow completion. This is achieved by aligning features from consecutive flows under the inertia prior, generating a richer temporal context representation.To further improve spatial coherence, an Adaptive Style Fusion Network (ASFN) is proposed. ASFN utilizes gradient information from valid regions to guide the optimization of gradients in the warped regions, correcting for style variations across video frames. Experimental results show the effectiveness of ASFN in achieving better spatial coherence.To train ASFN, a data simulation pipeline is designed to reduce the cost of data preparation and enable more efficient training. The proposed method also achieves memory-efficient inference and can handle videos up to 4K resolution.Overall, this work makes the following contributions: the introduction of an inertia prior for flow completion, the development of the Inertia-Guided Flow Completion (IGFC) Network, the design of the Adaptive Style Fusion Network (ASFN) for spatial coherence enhancement, and the establishment of a data simulation pipeline for ASFN training.