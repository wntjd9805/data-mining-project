In recent years, the Transformer model has had a significant impact on natural language processing and computer vision tasks. While Transformer-based approaches have been successful in vision tasks such as image classification and object detection, there is still room for improvement in representing features at multiple scales for dense prediction tasks such as object detection and semantic segmentation.Current state-of-the-art Vision Transformers focus on handling self-attention in dense prediction tasks but pay less attention to building effective multi-scale representations. This paper proposes a Multi-Path Vision Transformer (MPViT) that effectively represents multi-scale features for dense prediction tasks. Inspired by CNN models, MPViT utilizes a multi-scale patch embedding scheme that tokenizes visual patches of different sizes simultaneously. These tokens are then independently passed through Transformer encoders in parallel, allowing for global self-attention at different scales. The resulting features are aggregated, enabling both fine and coarse feature representations.To validate the effectiveness of MPViT, experiments were conducted on ImageNet-1K, COCO, and ADE20K datasets for image classification, object detection, and semantic segmentation tasks. The results show that MPViT consistently outperforms recent state-of-the-art Vision Transformers, even with fewer parameters and computational requirements.The main contributions of this work are the proposal of the multi-scale embedding with multi-path structure scheme for dense prediction tasks, the introduction of global-to-local feature interaction to leverage the advantages of both convolutions and self-attention, and the validation of MPViT as a backbone model for various vision tasks, achieving state-of-the-art performance. Ablation studies and qualitative analysis are also provided to analyze the effects of different configurations and demonstrate the efficiency and effectiveness of MPViT.