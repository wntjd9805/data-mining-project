This paper addresses the issue of interpreting the information learned by deep neural networks (DNNs) trained for video understanding tasks. While deep spatiotemporal models have achieved impressive performance in tasks such as action recognition and video object segmentation, the interpretation of these models remains largely unexplored. This lack of explainability is unsatisfying from both scientific and practical perspectives, as it limits our understanding of the decision-making process and can have ethical consequences. In this study, we propose a quantitative paradigm for assessing the bias toward static or dynamic information in spatiotemporal models. We introduce a novel method for quantifying static and dynamic bias and identifying units that encode both factors. Through experiments on action recognition and video object segmentation, we investigate the impact of model architecture and training dataset on static and dynamic biases. Our findings reveal that most networks exhibit heavy bias toward static information, except for two-stream architectures with cross connections that encourage the capture of dynamics. Contrary to previous beliefs, our results show that the Diving48 dataset is not dynamically biased, and the Something-Something-v2 dataset is better suited for evaluating a model's ability to capture dynamics. Overall, our study contributes a general technique for understanding spatiotemporal models and provides insights into the static and dynamic biases present in these networks.