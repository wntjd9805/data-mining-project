This paper introduces the concept of weak supervision in the form of set membership as an alternative to fully supervised or unsupervised learning. Set supervision involves subsets of training data in which some factors of variation have fixed values while others are freely varying. The authors propose a novel approach called approximate bijective correspondence (ABC) to isolate the active factors of variation through the process of finding correspondence between elements of different sets. By leveraging ABC, the learned representations can ignore inactive factors and focus on the active factors common to all sets, allowing for the incorporation of unsupervised or out-of-domain data. The paper demonstrates the effectiveness of ABC through experiments on image datasets, including the challenging task of pose estimation in real images. The results show that ABC can learn meaningful embeddings of unseen objects without pose annotations, successfully isolating active factors of variation from nuisance factors. Overall, ABC offers a practical and efficient solution for learning representations with weak set supervision.