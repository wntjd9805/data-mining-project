This paper introduces the rapid development of online video-sharing platforms and the need for effective video recommendation systems. These systems rely on similarity evaluation algorithms that consider multi-modal information, including visual content and metadata. However, creating a video similarity benchmark dataset poses significant challenges due to the diverse modalities. Existing approaches use semantic tags for supervision, but these tags may not satisfy higher precision requirements. This paper proposes a benchmark dataset called Tencent-MVSE, which includes 135,705 video pairs with finely annotated similarity scores. The dataset also includes Chinese titles, ASR text, and human-annotated categories and tags to support multi-modal evaluation. The paper also presents a baseline model that utilizes the advanced single-stream multi-modal Transformer (MMT) architecture for similarity evaluation. The MMT model is pretrained using masked language modeling, masked frame modeling, and video-text matching tasks to improve performance. The experiment results demonstrate the effectiveness of the proposed pre-training strategies. Overall, this paper contributes the Tencent-MVSE dataset, a benchmark for multi-modal video similarity evaluation, and a baseline model for further research in this field.