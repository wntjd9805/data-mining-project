This paper addresses the challenge of limited GPU memory in training video understanding models. While video models benefit from capturing context features over the entire video, it is often not feasible to train them with a large number of input frames due to memory limitations. The authors propose a memory saving technique called Stochastic Backpropagation (SBP) which randomly drops a proportion of the backward paths during training, while keeping all the forward paths. This random removal of backward paths allows for significant memory savings without compromising the effectiveness of the network. Experimental results on various video tasks demonstrate that training with SBP reduces GPU memory usage and speeds up training without significant loss of accuracy. SBP is a general technique that can be applied to a wide range of video tasks and models, making it a valuable solution for addressing the memory limitations in training video understanding models.