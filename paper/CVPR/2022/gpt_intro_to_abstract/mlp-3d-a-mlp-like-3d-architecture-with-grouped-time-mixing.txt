This paper discusses the advancements in Convolutional Neural Networks (CNNs) and their impact on image and video understanding. While CNNs have achieved top performances in computer vision, there is growing interest in attention models from Natural Language Processing (NLP) and vision transformers as alternative choices for image recognition. However, the application of MLP-like architectures on video data remains challenging due to the complexity of motion and visual details. This paper proposes MLP-3D networks, a novel architecture for spatio-temporal dependency modeling in videos. The network divides input video clips into tubelets and maps them into visual tokens. These tokens are then processed through stacked MLP-3D blocks that capture inter-token and intra-token information. The main contribution of this work is the introduction of a novel family of MLP-style operations called Grouped Time Mixing (GTM) to model temporal dynamics in an efficient and effective manner. Experiments conducted on Something-Something and Kinetics datasets show that MLP-3D networks achieve comparable or superior performances compared to 3D CNNs and computationally expensive video transformers. This work demonstrates the potential of MLP-like architectures for video understanding.