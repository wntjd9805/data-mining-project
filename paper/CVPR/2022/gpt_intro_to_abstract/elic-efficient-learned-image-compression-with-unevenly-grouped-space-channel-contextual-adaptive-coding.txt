Lossy image compression using deep learning has made significant advancements in the past years, surpassing conventional formats like JPEG and BPG in terms of rate-distortion performance. Recent works even outperform the hand-crafted image coding standard VVC. However, to make these approaches practical, the decoding speed of learned image compression needs to be assessed. One important technique is joint backward-and-forward adaptive entropy modeling, which reduces redundancy and bit-rate. However, it slows down decoding due to spatial dimension issues. To address this, researchers have proposed parallel replacements and context models. The paper investigates an uneven grouping scheme to accelerate the channel-conditional method and combines it with a parallel spatial context model. Additionally, the development of more complex transform networks improves coding performance but slows down inference. The authors propose re-balancing the computation between the main transform and entropy estimation to obtain low-latency compression models. The paper introduces the Space-Channel ConTeXt (SCCTX) model, which combines the proposed techniques and achieves state-of-the-art performance. Furthermore, the paper addresses the issue of generating preview images from compressed representations. Overall, the paper contributes to the field of learned image compression by proposing efficient models and addressing critical issues related to efficiency and preview generation.