This paper addresses the problem of accurate 3D geometry estimation in computer vision, specifically in the context of depth and ego motion prediction. While specialized sensors like LiDAR can provide precise depth measurements, they are costly and have reduced output density. Therefore, estimating depth from images captured by a monocular or binocular camera system is a more attractive alternative due to lower cost and simpler setup. However, stereo setups require careful calibration and synchronization.The recent advancements in deep learning have reduced the performance gap in depth estimation, especially in a supervised setting. However, collecting high-quality ground truth data is expensive, leading to the emergence of self-supervised monocular depth estimation methods that leverage large-scale unlabeled datasets. Such approaches learn depth and ego motion together and utilize 3D reprojection models to synthesize consecutive images, minimizing the photometric difference between the target and synthesized image during training.Self-supervised monocular depth estimation relies on assumptions that may not always hold true, such as rigid scenes with camera motion, reconstructing all image regions from neighboring frames, and constant brightness of surfaces. To address these issues, previous works have employed masking techniques, stereo images, semantic segmentation, and optical flow to guide the training process or improve feature representation.However, propagating correct training signals in the self-supervised setting remains challenging, particularly for pixels in occluded areas or moving objects. Additionally, the photometric loss can be inadequate in uniform texture areas or for repetitive structures.To tackle these challenges, this paper presents a novel self-distillation based self-supervised learning framework for monocular depth estimation (SD-SSMDE). The proposed framework consists of a two-stage training strategy with self-supervised learning in the first stage to generate high resolution pseudo labels and supervised learning in the second stage using a similar or more lightweight network. It also introduces a novel architecture for the depth network to achieve more accurate results. The paper addresses the scale ambiguity issue by incorporating scale in pseudo labels, making the depth predictions scale-consistent between frames. Furthermore, a filtering strategy based on 3D consistency between consecutive views is proposed to remove large errors in pseudo labels.Extensive experiments on the KITTI and Cityscapes datasets demonstrate that the proposed network and two-stage training framework achieve state-of-the-art results, surpassing or achieving comparable performance with current approaches.