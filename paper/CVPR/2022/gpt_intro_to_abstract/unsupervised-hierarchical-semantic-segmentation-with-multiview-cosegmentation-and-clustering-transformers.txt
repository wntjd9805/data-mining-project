Semantic segmentation is a challenging task that involves assigning semantic categories to each pixel in an image. Learning a segmenter for semantic segmentation from unlabeled data is even more difficult, as the pixel groupings and semantic categories are unknown. Existing approaches for unsupervised semantic segmentation have limitations in terms of visual similarity, spatial stability, and image-wise feature learning. In this paper, we propose a novel unsupervised semantic segmentation method that embraces the ambiguity of grouping granularity and exploits the inherent grouping hierarchy in visual scenes. Our method formulates unsupervised semantic segmentation as a pixel-wise feature learning problem and utilizes hierarchical groupings and clustering transformers to ensure grouping consistency and predictability across different levels of granularity. We introduce two technical components: multiview cosegmentation for spatial consistency and feature learning, and clustering transformers for semantic consistency. Our approach enables the discovery of semantic parts and wholes in a data-driven manner, regardless of whether the images come from object-centric or scene-centric datasets. Experimental results demonstrate that our method outperforms existing unsupervised semantic segmentation methods by a large margin on both object-centric and scene-centric datasets, showcasing the effectiveness of our approach in discovering semantics based on visual similarity and statistical co-occurrences.