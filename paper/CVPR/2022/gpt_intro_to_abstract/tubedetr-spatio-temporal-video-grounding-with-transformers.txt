This paper focuses on the task of spatio-temporal video grounding, which involves associating natural language with detailed spatio-temporal visual representations in videos. The goal is to localize a spatio-temporal tube for a target object described in the input text. This task is challenging due to the diversity and complexity of videos. The use of attention-based models in natural language processing has inspired the development of transformers for computer vision tasks. In this paper, the authors propose a transformer encoder-decoder model for spatio-temporal video grounding. Unlike existing approaches, their architecture uses time queries to jointly perform temporal localization and visual grounding, enabling the learning of powerful contextualized representations. The architecture includes components for modeling temporal, spatial, and multi-modal interactions. The video-text encoder encodes interactions over sparsely sampled frames, while the space-time decoder models temporal, spatial, and multi-modal interactions. The paper presents comprehensive experiments on two benchmarks, demonstrating the effectiveness of the proposed framework. The proposed approach, called TubeDETR, outperforms state-of-the-art methods by a significant margin. The code and trained models are publicly available.