This work explores neural scene representations for world mapping, specifically focusing on 3D reconstruction and novel view synthesis using data commonly captured by mapping platforms such as Street View. The street-level mapping setting presents challenges due to the large outdoor scenes, natural illumination, and uncontrolled environments. Previous works have mostly focused on synthetic data or small regions of real scenes, making this investigation unique. The scenes in this context contain a wide variety of objects and the camera locations are biased towards walking patterns, resulting in limited coverage of the scene by cameras. Additionally, the presence of the sky introduces a distant element that behaves differently from the solid structures near the cameras. The images captured also exhibit highly varying exposures. Lidar points, which reconstruct a 3D point cloud, have lower resolution in distant parts of the scene and can be absent for certain surfaces. To address these challenges, this paper extends the NeRF model for the Street View setting in several ways. First, it incorporates lidar information alongside RGB signals to compensate for viewpoint sparsity in large-scale scenes. Second, it automatically segments sky pixels and defines a separate structure to provide clear supervision for camera rays pointing at the sky. Lastly, the model automatically adjusts for varying exposure by estimating an affine color transformation for each camera. Experimental results using real-world Street View data demonstrate that these NeRF extensions significantly improve the quality of synthesized views and 3D surface reconstructions compared to state-of-the-art methods. Supplementary material containing additional results and visualizations is provided.