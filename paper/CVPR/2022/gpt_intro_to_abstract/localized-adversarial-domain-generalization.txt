Deep neural networks often struggle in generalizing to unseen domains, leading to poor performance on out-of-distribution (OOD) data due to domain shift. To address this issue, techniques like domain generalization (DG) and domain adaptation (DA) have been developed. DG aims to create a generalized model by exposing the training process to multiple domains without using OOD data from testing domains, while DA requires access to testing domain data during training. This paper focuses on DG methods, specifically adversarial domain generalization (ADG), which have gained popularity recently. ADG methods utilize generative adversarial networks to learn a common feature space for training domains, with the expectation that this feature space will enhance generalization to unseen domains. However, current ADG methods have shown limited performance improvement over baseline methods in practice. This work identifies two limitations of ADG methods: first, the feature representations learned by empirical risk minimization (ERM) are already roughly aligned, challenging the assumption of significant domain shift. Second, the feature space can collapse, leading to overfitting. To overcome these limitations, the paper proposes a localized adversarial domain generalization with space compactness maintenance (LADG) approach. LADG incorporates a localized domain classifier and measures the compactness of the feature space using a differentiable coding ratio. It alleviates the limitations of ADG methods by providing finer-grained domain alignment and addressing feature space collapse. Experimental results validate the effectiveness of LADG in improving DG performance.