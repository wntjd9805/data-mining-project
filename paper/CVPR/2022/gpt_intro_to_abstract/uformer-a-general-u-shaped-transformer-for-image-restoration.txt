In this paper, the authors address the task of image restoration, which involves removing undesired degradation (such as noise, blur, rain) from images. They note that recent methods based on Convolutional Neural Networks (ConvNets) have achieved impressive results but struggle to capture long-range dependencies. To overcome this limitation, the authors propose Uformer, a Transformer-based structure for image restoration. Uformer is built upon the hierarchical encoder-decoder structure of the UNet architecture, with modifications to replace convolution layers with Transformer blocks. The authors introduce two core designs to make Uformer suitable for image restoration tasks. Firstly, they propose the Locally-enhanced Window (LeWin) Transformer block, which performs non-overlapping window-based self-attention to reduce computational complexity while capturing global dependencies. Secondly, they propose a learnable multi-scale restoration modulator that adjusts features in multiple layers of the Uformer decoder. This modulator takes the form of a multi-scale spatial bias, adding a learnable window-based tensor to adapt features for restoring more details. The authors demonstrate that Uformer achieves state-of-the-art performance on multiple image restoration tasks, outperforming previous methods on benchmark datasets. They emphasize that Uformer's simplicity and effectiveness should encourage further research into Transformer-based architectures for image restoration. The contributions of this paper include the introduction of Uformer, an efficient Transformer-based structure, and the use of a learnable multi-scale restoration modulator to enhance restoration quality. Experimental results show that Uformer establishes new state-of-the-art performance on various datasets for image restoration tasks.