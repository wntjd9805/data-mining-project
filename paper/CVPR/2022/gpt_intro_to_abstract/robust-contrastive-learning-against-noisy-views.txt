Contrastive learning has become a popular self-supervised approach in computer vision, text, audio, and video domains. The central idea is to learn representations that capture shared information between different views of the data. However, the choice of contrasting views is crucial, and noisy views can lead to suboptimal representations. Existing approaches have attempted to address this issue, but they are often tied to specific modalities or make assumptions that may not hold in general scenarios. In this work, we propose a principled approach to make contrastive learning robust against noisy views. We draw connections between contrastive learning and classical noisy binary classification, allowing us to leverage the literature on learning with noisy labels. We propose a new contrastive loss function, called Robust InfoNCE (RINCE), that satisfies the symmetric property and provides a means to reweight sample importance without an explicit noise estimator. We provide a theoretical analysis of RINCE and show its connections to dependency measurement. Empirical evaluations demonstrate the robustness of RINCE against noisy views in various scenarios and with different modalities and noise types. Our proposed approach improves over the state-of-the-art in self-supervised learning benchmarks and exhibits scalability and computational efficiency.