This paper introduces the concept of visual context in the field of Visual Question Answering (VQA) and studies the robustness of VQA models in relation to irrelevant objects in images. Previous works have focused on the robustness of VQA models from the perspective of language context, but this paper proposes a different approach by examining the influence of visual context. The authors use a perturbation strategy called SwapMix to swap features of irrelevant objects with features from other similar objects in the dataset. The results show that VQA models heavily rely on visual context and are vulnerable to context perturbations. The authors compare two representative VQA models, MCAN and LXMERT, and find that the latter is more robust to context perturbations, indicating that large-scale pretraining can increase model robustness. The study also reveals that the over-reliance on context depends on the quality of visual representations, with perfect visual encoding reducing the reliance on context. SwapMix is also explored as a data augmentation technique, improving model robustness and effective accuracy in training. The main contributions of this paper are the examination of VQA robustness from the perspective of visual context, the observation of decreased reliance on visual context with perfect visual encodings, and the introduction of context reliance and effective accuracy metrics.