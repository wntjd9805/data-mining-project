Semantic embeddings play a crucial role in various computer vision tasks, such as zero-shot learning (ZSL), fashion trend forecast, and face recognition. Human-annotated attributes are commonly used as semantic embeddings, but acquiring them is a labor-intensive process. Previous methods utilize word embeddings or semantic embeddings from online encyclopedias, but they may not capture visually detectable relations or all discriminative visual cues. To address these limitations, we propose the Visually-Grounded Semantic Embedding (VGSE) Network, which discovers semantic embeddings with minimal human supervision by exploring visual clusters related to different categories. Our model assigns image patches to various clusters based on visual similarity and incorporates class discrimination and semantic relatedness to improve knowledge transfer in ZSL. Experimental results on three ZSL benchmarks demonstrate that our VGSE semantic embeddings consistently improve performance compared to word embeddings and other state-of-the-art methods. Additionally, qualitative evaluation and user study confirm that our embeddings contain rich visual information and convey human-understandable semantics, facilitating knowledge transfer between classes.