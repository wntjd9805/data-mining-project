Self-supervised learning (SSL) has shown remarkable success in training floating point (FP) networks, achieving or surpassing the performance of supervised pretraining in various tasks. However, there has been limited research on SSL for small quantized models. This is crucial in resource-constrained scenarios where binary networks exhibit superior efficiency. To address this gap, we propose a SSL method called Binary Unsupervised RepresentatioN learning (BURN) for binary networks. BURN utilizes a moving FP network as the target, similar to other SSL methods, and optimizes both the FP classifier and the binary network using the KL divergence loss. To overcome challenges of unstable gradients, we enforce feature similarity and dynamically balance the loss function. We conduct extensive evaluations on various downstream tasks and demonstrate that BURN outperforms prior SSL methods by a significant margin. Our contributions include the proposal of BURN, the use of a jointly trained FP classifier, the inclusion of a feature similarity loss, and the analysis of BURN through in-depth investigations.