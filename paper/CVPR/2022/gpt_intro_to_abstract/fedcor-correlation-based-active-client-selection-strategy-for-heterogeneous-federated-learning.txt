Introduction:Federated learning (FL) has gained attention as a distributed learning paradigm that offers data privacy. In FL, training data is distributed across multiple clients, and communication bandwidth and privacy constraints limit the selection of clients for each communication round. FL faces challenges such as client-wise data heterogeneity, which has been addressed by previous studies focusing on amending local model updates or central aggregation. However, these studies neglect the correlations between clients and consider their losses independently. This paper proposes a correlation-based active client selection strategy to alleviate accuracy degradation caused by data heterogeneity and boost the convergence of FL. The proposed strategy takes into account that clients do not contribute equivalently and that their contributions are not independent due to the aggregation of local updates. A toy experiment demonstrates the necessity of considering these correlations for client selection. The paper also defines the global loss function in FL and describes the FL algorithm that involves partial client participation and local model updates. The proposed FL framework, called Fed-Cor, is built on a correlation-based client selection strategy and makes three main contributions: modeling client loss changes with a Gaussian Process, proposing a GP training method to capture client correlations, and demonstrating through experiments that Fed-Cor stabilizes training convergence and significantly improves convergence rates on FMNIST and CIFAR-10 datasets.