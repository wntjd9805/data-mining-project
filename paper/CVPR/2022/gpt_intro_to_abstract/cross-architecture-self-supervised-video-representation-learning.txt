Video representation learning is a crucial task for video understanding, with applications in action recognition, video retrieval, and video temporal detection. Traditional approaches require large-scale video datasets with expensive human annotations, which limits the potential of deep networks in learning video representation. To address this, self-supervised learning has been applied to the video domain, specifically through contrastive learning. However, existing methods primarily focus on capturing global spatio-temporal representation, neglecting important temporal details. In this paper, we propose a new self-supervised video representation method that combines video-level contrastive learning and temporal modeling. We introduce a novel self-supervised temporal learning task that quantitatively measures the temporal difference between a video and its temporal shuffle using edit distance. This allows us to capture meaningful temporal information for discriminating different video instances, such as human actions. Additionally, we develop a cross-architecture contrastive learning framework that utilizes a 3D CNN and a video transformer to generate diverse and meaningful positive pairs, enhancing the video representation. Experimental results on action recognition and video retrieval tasks demonstrate the superiority of our approach compared to state-of-the-art methods.