Semantic segmentation, a crucial task in computer vision, involves labeling each pixel in an image. Deep neural networks have achieved significant success in semantic segmentation; however, they typically require large amounts of pixel-level annotated data, which is labor-intensive and data-hungry. To address this issue, recent methods have explored weakly-supervised semantic segmentation (WSSS) using weak or cheap labels like image-level labels, points, scribbles, or bounding boxes. This paper focuses on WSSS using only image-level labels, which is considered the most challenging scenario. Existing WSSS methods with image-level labels adopt a multi-stage framework that involves training a classification model and revising initial pseudo labels using learned affinity. To improve on these methods, the paper proposes an end-to-end Transformer-based framework for WSSS, which is the first attempt to explore Transformers in this context. The proposed framework includes an Affinity from Attention (AFA) module that learns reliable semantic affinity and propagates pseudo labels based on the learned affinity. Additionally, a Pixel-Adaptive Refinement (PAR) module is introduced to refine the pseudo labels by incorporating RGB and position information of local pixels. Experimental results on benchmark datasets demonstrate the superiority of the proposed method over other state-of-the-art approaches. The contributions of this work include the introduction of the Transformer-based framework for WSSS, the AFA module for learning reliable semantic affinity, and the PAR module for efficient label refinement.