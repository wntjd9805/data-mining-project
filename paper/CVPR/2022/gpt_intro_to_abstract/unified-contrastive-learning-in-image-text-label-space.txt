Learning to recognize visual concepts in images is a longstanding challenge in computer vision research. This task can be approached through supervised learning using annotated image-label pairs or contrastive learning using webly-crawled image-text pairs. While supervised learning yields decent recognition capabilities and powerful transfer learning when provided with clean and large-scale human-annotated data, collecting such data is time-consuming and costly, limiting its scalability. On the other hand, contrastive learning with image-text pairs has shown promise by leveraging large amounts of noisy but diverse data. However, these models often lack discriminative ability for transfer learning. This work aims to bridge the gap and explore a unified approach that combines both discriminative representations and broad concept coverage. A new perspective is introduced, defining an image-text-label space that eliminates the boundary between image-label and image-text data. This perspective allows for the use of a visual encoder and a language encoder to encode images and texts, aligning visual and textual features guided by labels. A unified contrastive learning method called UniCL is proposed, capable of seamlessly accommodating both data types for visual-semantic representation learning. Extensive experiments demonstrate the effectiveness of UniCL, achieving superior performance across various settings including zero-shot, linear probe, fully-finetuning, and transfer learning.