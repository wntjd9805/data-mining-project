Multi-view prediction of scenes is a crucial task in 3D scene understanding with applications in robotics, virtual reality, and augmented reality. Existing research focuses on generating images of scenes from new viewpoints, but these methods often suffer from ghosting and blurry artifacts. In this paper, we propose a novel-view scene layout generation problem, where we aim to generate consistent and sharp scene layouts from different viewpoints. This not only enables novel-view image synthesis, image editing, and amodal object estimation but also provides scene understanding priors. We introduce a learning-based model that consists of an Object Context Transformation (OCT) module, an Object Layout Generation (OLG) module, and an Object Layout Composition (OLC) module. The OCT module extracts contextualized object representations, the OLG module predicts the shape, size, and position for each object, and the OLC module composites all the predicted object layouts to generate the output scene layout. We evaluate our model on indoor and outdoor scenes and demonstrate its effectiveness in generating geometrically and semantically consistent novel-view scene layouts. Our work is the first attempt to address the novel-view scene layout generation problem and provides a novel and useful tool for a wide range of applications.